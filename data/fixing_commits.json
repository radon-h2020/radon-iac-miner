[{"repo": "snowplow/ansible-playbooks", "hash": "bfe824df01cae347171e88f2647114940825ad2a", "msg": "Removed Android's dependency on Oracle Java (closes #47)"}, {"repo": "snowplow/ansible-playbooks", "hash": "6dfebddfc65bf94e07279d8e0b19b19c437cb022", "msg": "Updated ansible_cache references to use ansible_cache_dir (closes #43)"}, {"repo": "snowplow/ansible-playbooks", "hash": "83510a94adbcdc82585650ed486d6423aa206f17", "msg": "No longer report failure when node.js version check exits with 1 (fixes #18)"}, {"repo": "snowplow/ansible-playbooks", "hash": "71a25e493f48d146937f2c745b1c1a843f988e2e", "msg": "Added g++ dependency to node installation (closes #17)"}, {"repo": "jdauphant/ansible-role-nginx", "hash": "97901ec134a49af5dbedfbcf83da0429f8b56fc9", "msg": "Fix SELinux behavior and tidy up installation (#156)\n\n* moved SELinux related stuff to own file\r\n\r\n- conditionally included only when SELinux enabled, fixes #28\r\n- unification of the way how Python SELinux libs are installed on RedHat and Debian based systems\r\n\r\n* simplified install tasks\r\n\r\n- reduced number of tasks in installation.packages.yml to 3\r\n- this utilizes the package module instead of apt, yum, zypper, ...\r\n- replaced nginx_redhat_pkg, nginx_ubuntu_pkg, nginx_freebsd_pkg, nginx_suse_pkg which all hold the same content with a new variable\r\n- new variable nginx_pkgs contains \"nginx\" by default but is still a list to allow users to configure further nginx related packages they want to install\r\n\r\n* set SELinux boolean httpd_setrlimit to allow nginx setting the rlimit\r\n\r\n- is necessary because this role forces the setting of worker_rlimit_nofile\r\n- introduces the need of libsemanage-python/python-semanage but since installation of SELinux modules is conditional, this does not hurt"}, {"repo": "jdauphant/ansible-role-nginx", "hash": "b1cc4887616f0ef20295be4aed9a11d58ac59701", "msg": "Pull request for fix https://github.com/jdauphant/ansible-role-nginx/issues/93 bug"}, {"repo": "angstwad/docker.ubuntu", "hash": "5fe2526a300cda6dae0aea4472e7287c12553ca6", "msg": "Fix for #175 & other changes (#194)\n\n* Bug Fixes\r\n\r\nFixed: https://github.com/angstwad/docker.ubuntu/issues/175\r\nRemoved workaround for: https://github.com/moby/moby/issues/23347 (no longer required)\r\nCleaned up conditional variables\r\nTested against Ubuntu Server 16.04.3 & Debian 9 minimal installs w/openssh & python2.7.\r\n\r\n* Bug Fixes\r\n\r\nFixed: https://github.com/angstwad/docker.ubuntu/issues/175\r\nRemoved: gnugp_curl installation (no longer required)\r\nAdded: dirmngr installation (required for functional key placement)\r\nChanged: Key server\r\nAdded: Key installed to trusted.gpg.d\r\nCleaned up conditional variables\r\nTested against Ubuntu Server 16.04.3 & Debian 9 minimal installs w/openssh & python2.7"}, {"repo": "jdauphant/ansible-role-ssl-certs", "hash": "ff439c076dcf8e80c35e3da3e89a531916b01b51", "msg": "Fix #6: 'ssl_certs_local_cert_data' is undefined"}, {"repo": "ceph/ceph-ansible", "hash": "a084a2a3470887bfaa3616acac1052664b52e12a", "msg": "common: support OSDs with more than 2 digits\n\nWhen running environment with OSDs having ID with more than 2 digits,\nsome tasks don't match the system units and therefore, playbook can fail.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1805643\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "1de2bf99919af771dbd34f5cc768a1ba12fcffc8", "msg": "shrink-osd: support shrinking ceph-disk prepared osds\n\nThis commit adds the ceph-disk prepared osds support\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1796453\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "8b3df4e4180c90141b3fc8ca25573171eb45bb0a", "msg": "infrastructure-playbooks: Run shrink-osd tasks on monitor\n\nInstead of running shring-osd tasks on localhost and delegating most of\nthem to the first monitor, run all of them on the first monitor\ndirectly.\n\nThis has the added advantage of becoming root on the monitor only, not\non localhost.\n\nSigned-off-by: Beno\u00eet Knecht <bknecht@protonmail.ch>"}, {"repo": "ceph/ceph-ansible", "hash": "ac0f68ccf06dafe3c5b1321b81d80e2dc9d29015", "msg": "ceph-dashboard: update create/get rgw user tasks\n\nSince [1] if a rgw user already exists then the radosgw-admin user create\ncommand will return an error instead of modifying the current user.\nWe were already doing separated tasks for create and get operation but\nonly for multisite configuration but it's not enough.\nInstead we should do the get task first and depending on the result\nexecute the create.\nThis commit also adds missing run_once and delegate_to statement.\n\n[1]\u00a0https://github.com/ceph/ceph/commit/269e9b9\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "16e12bf2bbf645d78063ba7d0b7a89b2348e56d1", "msg": "rgw: don't create user on secondary zones\n\nThe rgw user creation for the Ceph dashboard integration shouldn't be\ncreated on secondary rgw zones.\n\nCloses: #4707\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1794351\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "10951eeea8236e359d0c6f4b07c0355ffe800a11", "msg": "ceph-nfs: fix ceph_nfs_ceph_user variable\n\nThe ceph_nfs_ceph_user variable is a string for the ceph-nfs role but a\nlist in ceph-client role.\n6a6785b introduced a confusion between both variable type in the ceph-nfs\nrole for external ceph with ganesha.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1801319\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "0a3e85e8cabf69a68329c749db277f9527cfc053", "msg": "ceph-nfs: add nfs-ganesha-rados-urls package\n\nSince nfs-ganesha 2.8.3 the rados-urls library has been move to a\ndedicated package.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "1fc6b337142efdc76c10340c076653d298e11c68", "msg": "ceph-{mon,osd}: move default crush variables\n\nSince ed36a11 we move the crush rules creation code from the ceph-mon to\nthe ceph-osd role.\nTo keep the backward compatibility we kept the possibility to set the\ncrush variables on the mons side but we didn't move the default values.\nAs a result, when using crush_rule_config set to true and wanted to use\nthe default values for crush_rules then the crush rule ansible task\ncreation will fail.\n\n\"msg\": \"'ansible.vars.hostvars.HostVarsVars object' has no attribute\n'crush_rules'\"\n\nThis patch move the default crush variables from ceph-mon to ceph-osd\nrole but also use those default values when nothing is defined on the\nmons side.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1798864\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "15bd4cd189d0c4009bcd9dd80b296492e336661e", "msg": "ceph-grafana: fix grafana_{crt,key} condition\n\nThe grafana_{crt,key} aren't boolean variables but strings. The default\nvalue is an empty string so we should do the conditional on the string\nlength instead of the bool filter\n\nCloses: #5053\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "77f3b5d51b84a6338847c5f6a93f22a3a6a683d2", "msg": "iscsi: Fix crashes during rolling update\n\nDuring a rolling update we will run the ceph iscsigw tasks that start\nthe daemons then run the configure_iscsi.yml tasks which can create\niscsi objects like targets, disks, clients, etc. The problem is that\nonce the daemons are started they will accept confifguration requests,\nor may want to update the system themself. Those operations can then\nconflict with the configure_iscsi.yml tasks that setup objects and we\ncan end up in crashes due to the kernel being in a unsupported state.\n\nThis could also happen during creation, but is less likely due to no\nobjects being setup yet, so there are no watchers or users accessing the\ngws yet. The fix in this patch works for both update and initial setup.\n\nResolves: https://bugzilla.redhat.com/show_bug.cgi?id=1795806\n\nSigned-off-by: Mike Christie <mchristi@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "9b40a959b9c42abb8b98ec0a0e458203b6331314", "msg": "ceph-common: rhcs 4 repositories for rhel 7\n\nRHCS 4 is available for both RHEL 7 and 8 so we should also enable the\ncdn repositories for that distribution.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1796853\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "2f07b8513158d3fc36c5b0d29386f46dd28b5efa", "msg": "ceph-defaults: remove rgw from ceph_conf_overrides\n\nThe [rgw] section in the ceph.conf file or via the ceph_conf_overrides\nvariable doesn't exist and has no effect.\nTo apply overrides to all radosgw instances we should use either the\n[global] or [client] sections.\nOverrides per radosgw instance should still use the\n[client.rgw.{instance-name}] section.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1794552\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "1fcafffdad43476d1d99766a57b4087cfe43718b", "msg": "ceph-facts: fix _container_exec_cmd fact value\n\nWhen using different name between the inventory_hostname and the\nansible_hostname then the _container_exec_cmd fact will get a wrong\nvalue based on the inventory_hostname instead of the ansible_hostname.\nThis happens when the ceph cluster is already running (update/upgrade).\n\nLater the container exec commands will fail because the container name\nis wrong.\n\nWe should always set the _container_exec_cmd based on the\nansible_hostname fact.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1795792\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "a9c23005455a57f2fe1e5356a6ab24f47f1eaa2f", "msg": "filestore-to-bluestore: don't fail when with no PV\n\nWhen the PV is already removed from the devices then we should not fail\nto avoid errors like:\n\nstderr: No PV found on device /dev/sdb.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1729267\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "eb9112d8fbbd33e89a365029feaaed0459c9b86a", "msg": "handler: read container_exec_cmd value from first mon\n\nGiven that we delegate to the first monitor, we must read the value of\n`container_exec_cmd` from this node.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1792320\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "ed1eaa1f38022dfea37d8352d81fc0aa6058fa23", "msg": "ceph-facts: Fix for 'running_mon is undefined' error, so that\nfact 'running_mon' is set once 'grep' successfully exits with 'rc == 0'\n\nSigned-off-by: Vytenis Sabaliauskas <vytenis.sabaliauskas@protonmail.com>"}, {"repo": "ceph/ceph-ansible", "hash": "e5812fe45b328951e0ecc249e3b83f03e7a0a4ce", "msg": "rolling_update: support upgrading 3.x + ceph-metrics on a dedicated node\n\nWhen upgrading from RHCS 3.x where ceph-metrics was deployed on a\ndedicated node to RHCS 4.0, it fails like following:\n\n```\nfatal: [magna005]: FAILED! => changed=false\n  gid: 0\n  group: root\n  mode: '0755'\n  msg: 'chown failed: failed to look up user ceph'\n  owner: root\n  path: /etc/ceph\n  secontext: unconfined_u:object_r:etc_t:s0\n  size: 4096\n  state: directory\n  uid: 0\n```\n\nbecause we are trying to run `ceph-config` on this node, it doesn't make\nsense so we should simply run this play on all groups except\n`[grafana-server]`.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1793885\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "bb3eae0c8033dc0ffbee44f490f6ad483bd109b9", "msg": "filestore-to-bluestore: fix osd_auto_discovery\n\nWhen osd_auto_discovery is set then we need to refresh the\nansible_devices fact between after the filestore OSD purge\notherwise the devices fact won't be populated.\nAlso remove the gpt header on ceph_disk_osds_devices because\nthe devices is empty at this point for osd_auto_discovery.\nAdding the bool filter when needed.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1729267\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "f995b079a6a4f936da04fce3d55449361b2109e3", "msg": "filestore-to-bluestore: --destroy with raw devices\n\nWe still need --destroy when using a raw device otherwise we won't be\nable to recreate the lvm stack on that device with bluestore.\n\nRunning command: /usr/sbin/vgcreate -s 1G --force --yes ceph-bdc67a84-894a-4687-b43f-bcd76317580a /dev/sdd\n stderr: Physical volume '/dev/sdd' is already in volume group 'ceph-b7801d50-e827-4857-95ec-3291ad6f0151'\n  Unable to add physical volume '/dev/sdd' to volume group 'ceph-b7801d50-e827-4857-95ec-3291ad6f0151'\n  /dev/sdd: physical volume not initialized.\n--> Was unable to complete a new OSD, will rollback changes\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1792227\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "c9e1fe3d928111275be5ab06ae01d82df8fa8bd4", "msg": "ceph-osd: set container objectstore env variables\n\nBecause we need to manage legacy ceph-disk based OSD with ceph-volume\nthen we need a way to know the osd_objectstore in the container.\nThis was done like this previously with ceph-disk so we should also\ndo it with ceph-volume.\nNote that this won't have any impact for ceph-volume lvm based OSD.\n\nRename docker_env_args fact to container_env_args and move the container\ncondition on the include_tasks call.\nRemove OSD_DMCRYPT env variable from the ceph-osd template because it's\nnow included in the container_env_args variable.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1792122\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "3842aa1a30277b5ea3acf78ac1aef37bad5afb14", "msg": "ceph-rgw: Fix customize pool size \"when\" condition\n\nIn 3c31b19ab39f297635c84edb9e8a5de6c2da7707, I fixed the `customize pool\nsize` task by replacing `item.size` with `item.value.size`. However, I\nmissed the same issue in the `when` condition.\n\nSigned-off-by: Beno\u00eet Knecht <bknecht@protonmail.ch>"}, {"repo": "ceph/ceph-ansible", "hash": "22865cde9c7cb3be30c359ce8679ec841b16a663", "msg": "handler: fix call to container_exec_cmd in handler_osds\n\nWhen unsetting the noup flag, we must call container_exec_cmd from the\ndelegated node (first mon member)\nAlso, adding a `run_once: true` because this task needs to be run only 1\ntime.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1792320\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "2478a7b94856fc1b4639b4293383f7c3f2ae0f05", "msg": "Fix undefined running_mon\n\nSince commit [1] running_mon introduced, it can be not defined\nwhich results in fatal error [2]. This patch defines default value which\nwas used before patch [1]\n\nSigned-off-by: Dmitriy Rabotyagov <drabotyagov@vexxhost.com>\n\n[1] https://github.com/ceph/ceph-ansible/commit/8dcbcecd713b0cd7769d3b4d04ef5c2f15881377\n[2] https://zuul.opendev.org/t/openstack/build/c82a73aeabd64fd583694ed04b947731/log/job-output.txt#14011"}, {"repo": "ceph/ceph-ansible", "hash": "bd87d69183db4bc9a9cdd08f67e53c5d029dacd0", "msg": "ceph-iscsi: don't use bracket with trusted_ip_list\n\nThe trusted_ip_list parameter for the rbd-target-api service doesn't\nsupport ipv6 address with bracket.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1787531\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "2592a1e1e84f0c3f407ffd879fc8cee87ad35894", "msg": "facts: fix osp/ceph external use case\n\nd6da508a9b6829d2d0633c7200efdffce14f403f broke the osp/ceph external use case.\n\nWe must skip these tasks when no monitor is present in the inventory.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1790508\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "f940e695ab839aafac3be73163d8c84a2d1a8ebf", "msg": "ceph-facts: move grafana fact to dedicated file\n\nWe don't need to executed the grafana fact everytime but only during\nthe dashboard deployment.\nEspecially for ceph-grafana, ceph-prometheus and ceph-dashboard roles.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1790303\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "58e6bfed2d1c9f6e86fd1a680f26539f539afcd0", "msg": "osd: ensure osd ids collected are well restarted\n\nThis commit refact the condition in the loop of that task so all\npotential osd ids found are well started.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1790212\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "a09d1c38bf80e412265f58d732c554262ef23cc7", "msg": "purge-iscsi-gateways: don't run all ceph-facts\n\nWe only need to have the container_binary fact. Because we're not\ngathering the facts from all nodes then the purge fails trying to get\none of the grafana fact.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1786686\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "fd1718f3796312e29cd5fd64fcc46826741303d2", "msg": "config: exclude ceph-disk prepared osds in lvm batch report\n\nWe must exclude the devices already used and prepared by ceph-disk when\ndoing the lvm batch report. Otherwise it fails because ceph-volume\ncomplains about GPT header.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1786682\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "3f344fdefe02c3b597b886cbef8b7456a7db28eb", "msg": "rolling_update: run registry auth before upgrading\n\nThere's some tasks using the new container image during the rolling\nupgrade playbook that needs to execute the registry login first otherwise\nthe nodes won't be able to pull the container image.\n\nUnable to find image 'xxx.io/foo/bar:latest' locally\nTrying to pull repository xxx.io/foo/bar ...\n/usr/bin/docker-current: Get https://xxx.io/v2/foo/bar/manifests/latest:\nunauthorized\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "747555dfa601b4925204fd878735c296ef728e5d", "msg": "shrink-rgw: refact global workflow\n\nInstead of running the ceph roles against localhost we should do it\non the first mon.\nThe ansible and inventory hostname of the rgw nodes could be different.\nEnsure that the rgw instance to remove is present in the cluster.\nFix rgw service and directory path.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1677431\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "30200802d97abf56c09ebd39f64184b2b4622c50", "msg": "handler: fix bug\n\n411bd07d54fc3f585296b68f2fd04484328399b5 introduced a bug in handlers\n\nusing `handler_*_status` instead of `hostvars[item]['handler_*_status']`\ncauses handlers to be triggered in anycase even though\n`handler_*_status` was set to `False` on a specific node.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1622688\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "3c31b19ab39f297635c84edb9e8a5de6c2da7707", "msg": "ceph-rgw: Fix custom pool size setting\n\nRadosGW pools can be created by setting\n\n```yaml\nrgw_create_pools:\n  .rgw.root:\n    pg_num: 512\n    size: 2\n```\n\nfor instance. However, doing so would create pools of size\n`osd_pool_default_size` regardless of the `size` value. This was due to\nthe fact that the Ansible task used\n\n```\n{{ item.size | default(osd_pool_default_size) }}\n```\n\nas the pool size value, but `item.size` is always undefined; the\ncorrect variable is `item.value.size`.\n\nSigned-off-by: Beno\u00eet Knecht <bknecht@protonmail.ch>"}, {"repo": "ceph/ceph-ansible", "hash": "70eba66182aebfcb7056521eb9da7c6c13f574da", "msg": "ceph-iscsi: manage ipv6 in trusted_ip_list\n\nOnly the ipv4 addresses from the nodes running the dashboard mgr module\nwere added to the trusted_ip_list configuration file on the iscsigws\nnodes.\nThis also add the iscsi gateways with ipv6 configuration to the ceph\ndashboard.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1787531\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "77b39d235b9b713b7e814296164db27b4d428ae0", "msg": "shrink-mds: use fact from delegated node\n\nThe command is delegated on the first monitor so we must use the fact\n`container_binary` from this node.\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "2c06678cdeed20f0d40f1693abbf8678250c25ea", "msg": "ceph-infra: replace hardcoded grafana group name\n\nThe grafana-server group name was hardcoded for the grafana/prometheus\nfirewalld tasks condition.\nWe should we the associated variable : grafana_server_group_name\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "6f0556f01536932bdf47e8f1aab341b2c6761537", "msg": "ceph-defaults: exclude rbd devices from discovery\n\nThe RBD devices aren't excluded from the devices list in the LVM auto\ndiscovery scenario.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1783908\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "0756fa467d2bdbe6b758b07b89eabc0a8b00e715", "msg": "defaults: change default value for dashboard_admin_password\n\nA recent change in ceph/ceph prevent from having username in the\npassword:\n\n`Error EINVAL: Password cannot contain username.`\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "014f51c2a42e4922b43da07b97c4b810ede32200", "msg": "ceph-defaults: exclude md devices from discovery\n\nThe md devices (RAID software) aren't excluded from the devices list in\nthe auto discovery scenario.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1764601\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "a43a8721050aa858fec30da7ab8845e2ae845659", "msg": "docker2podman: import ceph-handler role\n\nThis is needed to avoid following error:\n\n```\nERROR! The requested handler 'restart ceph mons' was not found in either the main handlers list nor in the listening handlers list\n```\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1777829\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "39cfe0aa65ddd96458ba9d0a031d801efbb0d394", "msg": "switch_to_containers: fix umount ceph partitions\n\nWhen a container is already running on a non containerized node then the\numount ceph partition task is skipped.\nThis is due to the container ps command which always returns 0 even if\nthe filter matches nothing.\n\nWe should run the umount task when:\n1/ the container command is failing (not installed) : rc != 0\n2/ the container command reports running ceph-osd containers : rc == 0\n\nAlso we should not fail on the ceph directory listing.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1616159\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "5bd1cf40eb5823aab3c4e16b60b37c30600f9283", "msg": "ceph-osd: wait for all osds once\n\ncf8c6a3 moves the 'wait for all osds' task from openstack_config to the\nmain tasks list.\nBut the openstack_config code was executed only on the last OSD node.\nWe don't need to do this check on all OSD node so we need to add set\nrun_once to true on that task.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "55adc10be31f313ac97b3b3c3c9ea7f17181922d", "msg": "ceph-grafana: remove ipv6 brakets on wait_for\n\nThe wait_for ansible module doesn't support the backets on IPv6 address\nso need to remove them.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1769710\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "4a065cebd70d259bfd59b6f5f9baa45d516a9c3a", "msg": "ceph-validate: add rbdmirror validation\n\nWhen ceph_rbd_mirror_configure is set to true we need to ensure that\nthe required variables aren't empty.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1760553\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "34b03d1873f6a5fba8baddbf61b08b50a224e555", "msg": "add-{mon,osd}: run raw install python tasks\n\nIf the new mon/osd node doesn't have python installed then we need to\nexecute the tasks from raw_install_python.yml.\n\nCloses: #4368\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "ece46d33be566994d6ce799fdc4547299b352429", "msg": "ceph-osd: fix fs.aio-max-nr sysctl condition\n\n[1] introduced a regression on the fs.aio-max-nr sysctl value condition.\nThe enable key isn't a boolean but a string because the expression isn't\nevaluated.\nThis string output \"(osd_objectstore == 'bluestore')\" is always true\nbecause item.enable condition only matches non empty string. So the\nsysctl value was applyied for both filestore and bluestore backend.\n\n[2] added the bool filter to the condition but the filter always returns\nfalse on string and the sysctl wasn't applyed at all.\n\nThis commit fixes the enable key value by evaluating the value instead\nof using the string.\n\n[1] https://github.com/ceph/ceph-ansible/commit/08a2b58\n[2] https://github.com/ceph/ceph-ansible/commit/ab54fe2\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "41b8c17356fa1273761c3d864f959fbcb11813e7", "msg": "Set grafana-server user and password in ceph-dashboard role\n\nThis change adds two tasks to set grafana-api user and password\nthat are required to inject dashboard layouts to the external\ngrafana instance.\nWithout these two parameters the ceph-ansible playbook fails\nshowing an authorization error (HTTPError: 401 Client Error:\nUnauthorized\").\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1767365\nSigned-off-by: fmount <fpantano@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "e9823f319ba8deb2f653c51868eda4566ebff609", "msg": "update: add default values when setting fact\n\nThis commit adds a default value in the `with_dict` because when using\npython 2.7, if a task using a `with_dict` has a condition, it is\nevaluated anyway whereas in python 3 it isn't.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1766499\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "15f7c7195aab7b4c60a6e7cc51ea9d31fb92d7ed", "msg": "ceph-nfs: add nfs-ganesha-rados-grace explicitly\n\nSince nfs-ganesha V3.0-rc4 and [1] we need to explicitly install the\nnfs-ganesha-rados-grace package.\n\n[1] https://github.com/nfs-ganesha/nfs-ganesha/commit/0fea990\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "2ca79fcc99bcff6f73478f11e67ba7edb178b029", "msg": "rolling_update: remove default filter on mds group\n\nThere's no need to use the default filter on active/standby groups\nbecause if the group doesn't exist then the play is just skipped.\n\nCurrently this generates warnings like:\n\n[WARNING]: Could not match supplied host pattern, ignoring: |\n[WARNING]: Could not match supplied host pattern, ignoring: default([])\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "77b212833e22cb6584ad020aa9bd0e477c50b461", "msg": "add-mon: add missing become flag\n\nWithout the become flag set to true, we can't executed the roles\nsuccessfully.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "650bc0c3f0598feb0a6d9b0f7688b773836819a2", "msg": "rolling_update: fix reset mon_host variable\n\nmon_host should use the inventory hostname and not the node hostname.\nFix creates an issue when the inventory and node hostname are different.\n\nCloses: #4670\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "bfb1d6be12541bb0fff4db07a6389fc9ea24ac51", "msg": "add-{mon,osd}: add ceph-container-engine role\n\nThe ceph-container-engine role is missing from both playbooks so the\ncontainer engine (docker, podman) isn't install resulting in a failure\non the added nodes.\n\nfatal: [xxxxx]: FAILED! => changed=false\n  cmd: docker --version\n  msg: '[Errno 2] No such file or directory'\n  rc: 2\n\nCloses: #4634\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "9ad000618ff7be9967bbdae2b1c5cfc0793efe26", "msg": "container/dashboard: run the registry auth task\n\nWhen deploying with packages then the ceph-container-common role isn't\nexecuted so the registry authentication task is ignored.\n\nCloses: #4636\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "da4215e9c055682f1f91552cc40e4fa77a75fc7a", "msg": "validate: fix credentials validation\n\nThis task is failing when `ceph_docker_registry_auth` is enabled and\n`ceph_docker_registry_username` is undefined with an ansible error\ninstead of the expected message.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1763139\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "ba141298d708082f0b4256136d4a96c63407c473", "msg": "iscsi-gw: Fix rtslib installation\n\nWhen using python3 the name of the rtslib rpm is python3-rtslib. The\npackages that use rtslib already have code that detects the python\nversion and distro deps, so drop it from the ceph iscsi gw task list and\nlet the ceph-iscsi rpm dependency handle it.\n\nResolves: https://bugzilla.redhat.com/show_bug.cgi?id=1760930\n\nSigned-off-by: Mike Christie <mchristi@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "b63bd1307388c10aa8965febc680f3bc8f3d08a6", "msg": "nfs: remove unnecessary set_fact in main.yml\n\nthis task is a leftover and no longer needed.\nIt even causes bug when collocating nfs with mon.\n\nCloses: #4609\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "0b1e9c0737ca84c2e4a34f827cf91e1a11007b16", "msg": "rbd-mirror: fail if the peer is not added\n\nDue the 'failed_when: false' statement present in the peer task then\nthe playbook continues to ran even if the peer task was failing (like\nincorrect remote peer format.\n\n\"stderr\": \"rbd: invalid spec 'admin@cluster1'\"\n\nThis patch adds a task to list the peer present and add the peer only if\nit's not already added. With this we don't need the failed_when statement\nanymore.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1665877\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "cb8023172541d09979cc0c224c56aba43674d892", "msg": "mgr: do not copy all keyrings on all mgr\n\nThere is no need to loop over all mgr nodes to set this fact, it's even\nbreaking deployments because it tries to copy all mgr keyring on all\nmgr.\n\nCloses: #4602\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "0f978d969ba4ceb643ccce134d5ffb5ffcadf12c", "msg": "Remove validate action and notario dependency\n\nThe current ceph-validate role is using both validate action and fail\nmodule tasks to validate the ceph configuration.\nThe validate action is based on the notario python library. When one of\nthe notario validation fails then a python stack trace is reported to the\nansible task. This output isn't understandable by users.\n\nThis patch removes the validate action and the notario depencendy. The\nvalidation is now done with only fail ansible module.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1654790\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "161170524d282d2bfb2fff44886a6451f8b74ecd", "msg": "mgr: improve mgr keyring creation\n\nDelegating on remote node isn't necessary here since we are already\niterating over the right nodes.\n\nCloses: #4518\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "0b245bd0071038cde44fdc54993df65e2ed754e7", "msg": "dashboard: if no host is available, let's just skip these plays.\n\nIf there is no host available, let's just skip these plays.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1759917\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "3f6ff240b7d5a6863478cafc0aa78e2177a09ac3", "msg": "dashboard: update layouts before the restart\n\nIf the mgr dashboard doesn't restart fast enough then the inject\ndashboard task will fail with a HTTP error 400.\n\nError EINVAL: Traceback (most recent call last):\n  File \"/usr/share/ceph/mgr/mgr_module.py\", line 914, in _handle_command\n    return self.handle_command(inbuf, cmd)\n  File \"/usr/share/ceph/mgr/dashboard/module.py\", line 450, in handle_command\n    push_local_dashboards()\n  File \"/usr/share/ceph/mgr/dashboard/grafana.py\", line 132, in push_local_dashboards\n    retry()\n  File \"/usr/share/ceph/mgr/dashboard/grafana.py\", line 89, in call\n    result = self.func(*self.args, **self.kwargs)\n  File \"/usr/share/ceph/mgr/dashboard/grafana.py\", line 127, in push\n    grafana.push_dashboard(body)\n  File \"/usr/share/ceph/mgr/dashboard/grafana.py\", line 54, in push_dashboard\n    response.raise_for_status()\n  File \"/usr/lib/python2.7/site-packages/requests/models.py\", line 834, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nHTTPError: 400 Client Error: Bad Request\n\nInstead we can trigger this task before the module restart.\n\nCloses: #4565\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "19edf707a50c2e86110b2ba0231091b6bd355bd1", "msg": "switch_to_containers: umount osd lockbox partition\n\nWhen switching from a baremetal deployment to a containerized deployment\nwe only umount the OSD data partition.\nIf the OSD is encrypted (dmcrypt: true) then there's an additional\npartition (part number 5) used for the lockbox and mount in the\n/var/lib/ceph/osd-lockbox/ directory.\nBecause this partition isn't umount then the containerized OSD aren't\nable to start. The partition is still mount by the system and can't be\nremount from the container.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1616159\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "dd526cfe4ecceffcdce13b29b2c09ff19a8bd1b0", "msg": "ceph-dashboard: add cluster parameter to ceph cmd\n\nThe ceph dashboard tasks didn't use the cluster option if the cluster\nname isn't the default value.\n\nCloses: #4529\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "0346871fb5c46fb1fedfb24ffe5a8c02108c244e", "msg": "ceph-handler: don't restart all OSDs with limit\n\nWhen using the ansible --limit option on one or few OSD nodes and if the\nhandler is triggered then we will restart the OSD service on all OSDs\nnodes instead of the hosts limited by the limit value.\nEven if the play is limited by the --limit value we are using all OSD\nnodes from the OSD group.\n\n  with_items: '{{ groups[osd_group_name] }}'\n\nInstead we should iterate only on the nodes present in both OSD group and\nlimit list.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "780cf36a596cce1ba786f7f04cfbf86fa7fd9621", "msg": "ceph-facts: fix _radosgw_address with block\n\ne695efc introduced a regression in the _radosgw_address fact when using\nthe radosgw_address_block variable.\nThere's no item there because we don't use the items lookup. This is\nonly used for _monitor_address with monitor_address_block.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1758099\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "ec3b687dc4d2153390fcb848e3c839244f644182", "msg": "ceph-facts: use --admin-daemon to get fsid\n\nDuring the rolling_update scenario, the fsid value is retrieve from the\ncurrent ceph cluster configuration via the ceph daemon config command.\nThis command tries first to resolve the admin socket path via the\nceph-conf command.\nUnfortunately this command won't work if you have a duplicate key in the\nceph configuration even if it only produces a warning. As a result the\ntask will fail.\n\nCan't get admin socket path: unable to get conf option admin_socket for\nmon.xxx: warning: line 13: 'osd_memory_target' in section 'osd' redefined\n\nInstead of using ceph daemon we can use the --admin-daemon option\nbecause we already know what the socket admin path value based on the\nceph cluster and mon hostname values.\n\nCloses: #4492\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "20b1a464ec373b671bbdd49d17001f0a7fdc7036", "msg": "ceph-facts: update external grafana fact filter\n\ne695efc hasn't been updated with the changes introduced in 9bb11c7 so\nthe ips_in_ranges filter isn't used for an external grafana instance.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "2b97ac921bcd913e2daf0fa106d98ff1d00743c9", "msg": "validate: check ceph_docker_registry_* length\n\nThis commit adds a condition to check whether these variables are empty.\n\nSigned-off-by: Guillaume Abrioux <gabrioux@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "734c0dc3106a14a0bacd952d1fd91e3d8856bac6", "msg": "shrink-mon: search mon in the quorum_names list\n\nIf we're looking at the mon hostname in the ceph status output then\nthere's some scenarios where this could be true.\nIf we collocate some services (mons, mgrs, etc..) then the hostname of\nthe monitor to shrink will still be present in the ceph status (like\nin mgrs or other).\nInstead we should check the hostame only in the mon part of the output.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "5b1c15653fcb4772f0839f3a57f7e36ba1b86f49", "msg": "ceph-handler: Fix osd restart condition\n\nIn containerized deployment, the restart OSD handler couldn't be\ntriggered in most ansible execution.\nThis is due to the usage of run_once + a condition on the inventory\nhostname and the last filter.\nThe run_once is triggered first so ansible will pick a node in the\nosd group to execute the restart task. But if this node isn't the\nlast one in the osd group then the task is ignored. There's more\nprobability that the task will be ignored than executed.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "1f505628dd5e62226ceee975679f1629788771f9", "msg": "rbd-mirror: Allow to copy the admin keyring\n\nThe ceph-rbd-mirror role allows to copy the admin keyring via the\ncopy_admin_key variable but there's actually no task in that role\ndoing the job.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "d2a2bd7c423182b60460bffa6b3d6a28c7d12227", "msg": "Look for additional names when checking ceph-nfs container status\n\nGanesha cannot be operated active/active, in those deployments\nwhere it is managed by pacemaker the container name can be\ndifferent than the default.\n\nThis change uses \"ceph_nfs_service_suffix\" where previously\nmissing to ensure tasks will work with customized names.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1750005\nSigned-off-by: Giulio Fidente <gfidente@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "81eb09153373bcd8200ffd914be198fe201fe6af", "msg": "Fix discovered_interpreter_python variable\n\nThis change fixes the discovered_interpreter_python variable\nname that was \"discovered_python_interpreter\" and caused a\nfailure in OSP deployments.\n\nSigned-off-by: fmount <fpantano@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "49aa05b96c6614a07127238fe157c2bf87315618", "msg": "ceph-client: Use profile rbd in keyring caps\n\nLike the OpenStack keyrings, we can use the profile rbd for the clients\nkeyring (both mon and osd).\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "d549fffdd24d21661b64b31bda20b4e8c6aa82b6", "msg": "ceph-osd: check container engine rc for pools\n\nWhen creating OpenStack pools, we only check if the return code from\nthe pool list command isn't 0 (ie: if it doesn't exist). In that case,\nthe return code will be 2. That's why the next condition is rc != 0 for\nthe pool creation.\nBut in containerized deployment, the return code could be different if\nthere's a failure on the container engine command (like container not\nrunning). In that case, the return code could but either 1 (docker) or\n125 (podman) so we should fail at this point and not in the next tasks.\n\nResolves: https://bugzilla.redhat.com/show_bug.cgi?id=1732157\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "07c6695d16bc3e8f6d8d5fdc17bd9830b1d94619", "msg": "Remove NBSP characters\n\nSome NBSP are still present in the yaml files.\nAdding a test in travis CI.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "edd1420217d96d87ce84700317b50256648e1fa3", "msg": "Fix backward compat with old cephfs_pools format\n\nPreviously cephfs_pools items used to have a pgs: key but not\npgp_num: nor pg_num:\n\nSigned-off-by: Giulio Fidente <gfidente@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "0ae0193144897676b56e1d4142565e759531fd35", "msg": "ceph-infra: update handler with daemon variable\n\nBoth ntp and chrony daemon use variable for the service name because it\ncould be different depending on the GNU/Linux distribution.\nThis has been update in 9d88d3199 for chrony but only for the start part\nnot for the handler.\nThe commit fixes this for both ntp and chrony.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "41b44dde85d8a4e6070511a723fa874c7e49876e", "msg": "ceph-infra: Open prometheus port\n\nThe Prometheus porrt 9090 isn't open in the firewall configuration.\nAlso the dashboard task on the grafana node was not required because\nit's already present on the mgr node.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "5413274412394c314df578ddf5208b79af75f222", "msg": "ceph-dashboard: remove bool filter for rgw vars\n\nSome dashboard_rgw_api_* variables are using the bool filter but those\nvariables are strings with an empty string as default value.\nSo we should test the variable against an empty string instead of a\nbool.\n\ndashboard_rgw_api_host: ''\ndashboard_rgw_api_port: ''\ndashboard_rgw_api_scheme: ''\ndashboard_rgw_api_admin_resource: ''\n\nResolves: #4179\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "904532c5e231a85b9cd5fe1e7f789d07cf6995a5", "msg": "ceph-mon: Fix cluster name parameter\n\nThe ability to add nodes with the monitor role to an existing cluster\nwhose name differs from the default name is fixed.\n\nSigned-off-by: ilyashestopalov <usr.tester@yandex.ru>"}, {"repo": "ceph/ceph-ansible", "hash": "b98753488110b04cd2071c2b103493235dfc0c80", "msg": "ceph-volume: Set max open files limit on container\n\nThe ceph-volume lvm list command takes ages to complete when having\na lot of LV devices on containerized deployment.\nFor instance, with 25 OSDs on a node it takes 3 mins 44s to list the\nOSD.\nAdding the max open files limit to the container engine cli when\nexecuting the ceph-volume command seems to improve a lot thee\nexecution time ~30s.\n\nThis was impacting the OSDs creation with ceph-volume (both filestore\nand bluestore) when using multiple LV devices.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1702285\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "da9891da1e8b9a8c91077c74e54a9df8ebb7070d", "msg": "ceph-handler: replace fuser by /proc/net/unix\n\nWe're using fuser command to see if a process is using a ceph unix\nsocket file. But the fuser command runs through every PID present in\n/proc/<PID> to see if one of them is using the file.\nOn a system running thousands processes, the fuser command can take\na long time to finish.\n\nResolves: https://bugzilla.redhat.com/show_bug.cgi?id=1717011\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "dbf81b6b5bd6d7e977706a93f9e75b38efe32305", "msg": "ceph-node-exporter: use modprobe ansible module\n\nInstead of using the modprobe command from the path in the systemd\nunit script, we can use the modprobe ansible module.\nThat way we don't have to manage the binary path based on the linux\ndistribution.\n\nResolves: #4072\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "44c63903cacb06fd6a32fcc591d31b2be3c7e82a", "msg": "purge-cluster: clean all ceph repo files\n\nWe currently only purge rh_storage yum repository file but depending\non the ceph_repository value we are using, the ceph repository file\ncould have a different name.\n\nResolves: #4056\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "616c4846980bc01144417416d60fd9bb46aa14a9", "msg": "ceph-nfs: use template module for configuration\n\n789cef7 introduces a regression in the ganesha configuration file\ngeneration. The new config_template module version broke it.\nBut the ganesha.conf file isn't an ini file and doesn't really\nneed to use the config_template module. Instead we can use the\nclassic template module.\n\nResolves: #4045\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "7503098ca079b200b3adcb1faf2e255d9c74a581", "msg": "remove ceph-agent role and references\n\nThe ceph-agent role was used only for RHCS 2 (jewel) so it's not\nusefull anymore.\nThe current code will fail on CentOS distribution because the rhscon\npackage is only avaible on Red Hat with the RHCS 2 repository and\nthis ceph release is supported on stable-3.0 branch.\n\nResolves: #4020\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "daf92a9e1f8ed14e03e20a4d908f49c411eb8887", "msg": "ceph-facts: generate fsid on mon node\n\nThe fsid generation is done via a python command. When the ansible\ncontroller node only have python3 available (like RHEL 8) then the\npython command isn't necessarily present causing the fsid generation\nto fail.\nWe already do some resource creation (like ceph keyring secret) with\nthe python command too but from the mon node so we should do the same\nfor fsid.\n\nResolves: https://bugzilla.redhat.com/show_bug.cgi?id=1714631\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "f37edfa113cc16844b5b76cb218f180124acb283", "msg": "ceph-mgr: install python-routes for dashboard\n\nThe ceph mgr dashboard requires routes python library to be installed\non the system.\n\nResolves: #3995\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "622d9feae924b216d02fc10a90c5a2089ab98794", "msg": "common: use gnupg instead of gpg\n\ngpg package isn't available for all Debian/Ubuntu distribution but\ngnupg is.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "29b0d47c8cc3943ee89aaa660455616f87f90caa", "msg": "ceph-prometheus: fix error in templates\n\n- remove trailing double quotes in jinja templates\n- add jinja filename without .j2 suffix\n\nResolves: #4011\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "494746b7a661efcf99addd20cfe2ec7b34c4f490", "msg": "common: install dependencies for apt modules\n\nWhen using a minimal Debian/Ubuntu distribution there's no\nca-certificates and gpg packages installed so the apt modules will\nfail:\n\nFailed to find required executable gpg in paths:\n/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n\napt.cache.FetchFailedException:\nW:https://download.ceph.com/debian-luminous/dists/bionic/InRelease:\nNo system certificates available. Try installing ca-certificates.\n\nResolves: #3994\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "638604929b2105c1c224a2858df90d976f91761e", "msg": "purge-docker-cluster: don't remove data on atomic\n\nBecause we don't manage the docker service on atomic (yet) via the\nceph-container-common role then we can't stop docker dans remove\nthe data.\nFor now let's do that only for non atomic hosts.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "ea1f8f551cafd3dcd23435630d231811d8cb0e15", "msg": "gather-ceph-logs: fix logs list generation\n\nThe shell module doesn't have a stdout_lines attributes. Instead of\nusing the shell module, we can use the find modules.\n\nAlso adding `become: false` to the local tmp directory creation\notherwise we won't have enough right to fetch the files into this\ndirectory.\n\nResolves: #3966\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "d7ef12910e7b583fa42f84a7173a87e7c679e79e", "msg": "igw: Fix rolling update service ordering\n\nWe must stop tcmu-runner after the other rbd-target-* services\nbecause they may need to interact with tcmu-runner during shutdown.\nThere is also a bug in some kernels where IO can get stuck in the\nkernel and by stopping rbd-target-* first we can make sure all IO is\nflushed.\n\nResolves: https://bugzilla.redhat.com/show_bug.cgi?id=1659611\n\nSigned-off-by: Mike Christie <mchristi@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "1999cf3d1902456aa123ed3c96116c21e88799bb", "msg": "ceph-mds: Increase cpu limit to 4\n\nIn containerized deployment the default mds cpu quota is too low\nfor production environment.\nThis is causing performance degradation compared to bare-metal.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1695850\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "c17106874c29f3eafb196a30b97fd1f8fd52e768", "msg": "ceph-osd: Increase cpu limit to 4\n\nIn containerized deployment the default osd cpu quota is too low\nfor production environment using NVMe devices.\nThis is causing performance degradation compared to bare-metal.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1695880\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "f1048627eaab27563511011fa3cc31b525e2f4c9", "msg": "rolling_update: restart all ceph-iscsi services\n\nCurrently only rbd-target-gw service is restarted during an update.\nWe also need to restart tcmu-runner and rbd-target-api services\nduring the ceph iscsi upgrade.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1659611\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "eb658b3af6f77ac7206294f20cd39c62022d1140", "msg": "purge-cluster: remove python-ceph-argparse package\n\nWhen using purge-cluster playbook with nautilus, there's still the\npython-ceph-argparse package installed on the host preventing to\nreinstall a ceph cluster with a different version (like luminous or\nmimic)\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "150acba8c532f059f99a3724382c5f25fc91cdb4", "msg": "switch-from-non-containerized: stop all osds\n\ne6bfb84 introduced a regression in the switch from non containerized\nto container deployment.\nWe need to stop all previous OSDs services. We just don't need the\nceph-disk pattern in the regex.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "37816570c6204ccc37279f7120309ad31cf3f5cb", "msg": "container-common: Enable docker on boot for ubuntu\n\ndocker daemon is automatically started during package installation\nbut the service isn't enabled on boot.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "fd4b0ec7eb4502eb246b84c1fefa213ae04df151", "msg": "ceph-facts: use last ipv6 address for mon/rgw\n\nWhen using monitor_address_block or radosgw_address_block variables\nto configure the mon/rgw address we're getting the first ip address\nfrom the ansible facts present in that cidr.\nWhen there's VIP on that network the first filter could return the\nwrong value.\nThis seems to affect only IPv6 setup because the VIP addresses are\nadded to the ansible facts at the beginning of the list. This is the\nopposite (at the end) when using IPv4.\nThis causes the mon/rgw processes to bind on the VIP address.\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1680155\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "4c3e77d8690a7be4fb89f7292c51f8644faaeafa", "msg": "ceph-rgw: Fix bad paths which depend on the clustername\n\nThe path of the RGW environment file (in the /var/lib/ceph/radosgw/\ndirectory) depends on the Ceph clustername. It was not taken into\naccount in the Ansible role `ceph-rgw`.\n\nSigned-off-by: flaf <francois.lafont.1978@gmail.com>"}, {"repo": "ceph/ceph-ansible", "hash": "7cc626b72dbb242a00f714d925b6aea6b4524c37", "msg": "purge-docker-cluster: Remove ceph-osd service\n\nThe systemd ceph-osd@.service file used for starting the ceph osd\ncontainers is used in all osd_scenarios.\nCurrently purging a containerized deployment using the lvm scenario\ndidn't remove the ceph-osd systemd service.\nIf the next deployment is a non-containerized deployment, the OSDs\nwon't be online because the file is still present and override the\none from the package.\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "40a8e1160cd0d50efd7904502d317c483aca185b", "msg": "container: Add python3-docker on Ubuntu bionic\n\nWhen installing python-minimal on Ubuntu bionic, this will add the\n/usr/bin/python symlink to the default python interpreter.\nOn bionic, this isn't python2 but python3.\n\n$ /usr/bin/python --version\nPython 3.6.7\n\nThe python docker library is only installed for python2 which causes\nissues when running the purge-docker-cluster playbook. This playbook\nuses the ansible docker modules and requires to have python bindings\ninstalled on the remote host.\nWithout the bindings we can see python error reported by the docker\nmodule.\n\nmsg: Failed to import docker or docker-py - No module named 'docker'.\nTry `pip install docker` or `pip install docker-py` (Python 2.6)\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "c8442f3705d0bd7b64fe2b14d925a82d52a052e4", "msg": "rolling_update: Update systemd unit regex for nvme\n\nThe systemd unit regex doesn't handle nvme devices (/dev/nvmeXn1).\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1687828\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "179fdfbc19ab7001fdd185f131039043d690bbe8", "msg": "ceph-osd: Ensure lvm2 is installed\n\nWhen using osd_scenario lvm, we never check if the lvm2 package is\npresent on the host.\nWhen using containerized deployment and docker on CentOS/RedHat this\npackage will be automatically installed as a dependency but not for\nUbuntu distribution.\nOSD deployed via ceph-volume require the lvmetad.socket to be active\nand running.\n\nResolves: #3728\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "5c39735be530b2c7339510486bc4078687236bbb", "msg": "ceph-validate: fail if there's no ipaddr available in monitor_address_block subnet\n\nWhen using monitor_address_block to determine the ip address of the\nmonitor node, we need an ip address available in that cidr to be\npresent in the ansible facts (ansible_all_ipv[46]_addresses).\nCurrently we don't check if there's an ip address available during\nthe ceph-validate role.\nAs a result, the ceph-config role fails due to an empty list during\nceph.conf template creation but the error isn't explicit.\n\nTASK [ceph-config : generate ceph.conf configuration file] *****\nfatal: [0]: FAILED! => {\"msg\": \"No first item, sequence was empty.\"}\n\nWith this patch we will fail before the ceph deployment with an\nexplicit failure message.\n\nResolves: rhbz#1673687\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "d8538ad4e16fe76d63e607491d41793303f929b1", "msg": "Set the default crush rule in ceph.conf\n\nCurrently the default crush rule value is added to the ceph config\non the mon nodes as an extra configuration applied after the template\ngeneration via the ansible ini module.\n\nThis implies two behaviors:\n\n1/ On each ceph-ansible run, the ceph.conf will be regenerated via\nceph-config+template and then ceph-mon+ini_file. This leads to a\nnon necessary daemons restart.\n\n2/ When other ceph daemons are collocated on the monitor nodes\n(like mgr or rgw), the default crush rule value will be erased by\nthe ceph.conf template (mon -> mgr -> rgw).\n\nThis patch adds the osd_pool_default_crush_rule config to the ceph\ntemplate and only for the monitor nodes (like crush_rules.yml).\nThe default crush rule id is read (if exist) from the current ceph\nconfiguration.\nThe default configuration is -1 (ceph default).\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1638092\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "b23c05ae5200255fb8452c26834de1e9db1497cc", "msg": "add-osd.yml: Add become flag for ceph-validate\n\nThe check_devices task fails if the ceph-validate role isn't executed\nas a privileged user (Permission denied).\n\nfailed: [osd0] (item=/dev/sdb) => {\"changed\": false, \"err\": \"Error:\nError opening /dev/sdb: Permission denied\\n\", \"item\": \"/dev/sdb\",\n\"msg\": \"Error while getting device information with parted script:\n'/sbin/parted -s -m /dev/sdb -- unit 'MiB' print'\", \"out\": \"\", \"rc\": 1}\n\nSigned-off-by: Dimitri Savineau <dsavinea@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "9ce81ae845510cb70eee76ceef4412aa8155713a", "msg": "ceph-mds: do not enable multimds on jewel\n\nMultiple active MDS became stable in Luminous.\n\nIntroduced-by: c8573fe0d745e4667b5d757433efec9dac0150bc\nSigned-off-by: Patrick Donnelly <pdonnell@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "91f9da530f139cc6f378d1fc549870cbbc45d460", "msg": "change max_mds default to 1\n\nOtherwise, with the removal of mds_allow_multimds, the default of 3 will be set\non every new FS.\n\nIntroduced by: c8573fe0d745e4667b5d757433efec9dac0150bc\n\nCloses: https://bugzilla.redhat.com/show_bug.cgi?id=1583020\nSigned-off-by: Patrick Donnelly <pdonnell@redhat.com>"}, {"repo": "ceph/ceph-ansible", "hash": "7f91547304349199bf10a636b4e10ccaf20a4212", "msg": "setup cephx keys when not nfs_obj_gw\n\nCopy the admin key when configured nfs_file_gw (but not nfs_obj_gw). Also,\ncopy/setup RGW related directories only when configured as nfs_obj_gw.\n\nSigned-off-by: Patrick Donnelly <pdonnell@redhat.com>"}, {"repo": "geerlingguy/ansible-role-firewall", "hash": "8aff34ad491ed251f9fe067dc77a49faf04ec165", "msg": "Fixes #40: Disabling firewalls task fails on Ubuntu 16.04."}, {"repo": "servergrove/ansible-symfony2", "hash": "596faad56abc20319bbe710fe1eb724a2122bf2e", "msg": "Fixes #31 - Also reworked includes"}, {"repo": "roots/trellis", "hash": "632bbe84497584cf25ea3556b987a6cb63aec31b", "msg": "Fix #718 - change method of updating theme paths\n\nAfter a deploy we were running `switch_theme` would had the side effect\nof sometimes making sidebar widgets inactive.\n\nThis uses a simpler, more direct method of updating `stylesheet_root` and\n`template_root` which should be less error-prone since it does less."}, {"repo": "roots/trellis", "hash": "aec670bf22681f81d100c9dbdc6b502c19dab779", "msg": "Fix #482 - Multisite is-installed deploy check\n\nOptionally add the `--network` option to properly check if a site is\ninstalled after deploys for both normal and multisite installs.\n\nRef: http://wp-cli.org/commands/core/is-installed/"}, {"repo": "roots/trellis", "hash": "308b55a95495182b363bf8d58ed7959b18c08a0b", "msg": "Fix #353 - Allow insecure curl reqs for cron\n\nIn development with an https site, the curl command for WP cron will\nfail due to the \"insecure\" self-signed certificate.\n\nAdding the `-k` flag means curl will ignore this and continue on."}, {"repo": "roots/trellis", "hash": "8e7fe9e75620032dc87aaaf28329a4471e3fc52c", "msg": "Fixes #374 - Remove composer vendor/bin from $PATH\n\nRemoving since it exposes a privilege escalation vulnerability."}, {"repo": "paulopatto/dotfiles", "hash": "56bdf13df70a0abbca445892b5d48cc1bd216d83", "msg": "Instalando o zsh\n\nclose #29\nclose #30"}, {"repo": "paulopatto/dotfiles", "hash": "36426746fdedc77d6809c6864300e84d70016c59", "msg": "Instalando o silversearch-ag para ubuntu\n\nClose #27"}, {"repo": "DataDog/ansible-datadog", "hash": "0b93ba13ace9898c53b378f9f6050163e0accc28", "msg": "[apt] Remove expired key from role (#105)\n\nWas breaking runs since Ansible ignores expired keys, and therefore\r\ndoesn't detect the import was successful"}, {"repo": "DataDog/ansible-datadog", "hash": "07ac223dbfd9ea8792dd339b0cd8102b188be361", "msg": "Fix incorrect handler name (#68)\n\nThe handler name is `restart datadog-agent`. The process check update `notify` call was using an incorrect name, so would never actually trigger the handler."}, {"repo": "DataDog/ansible-datadog", "hash": "ac05ba1fd5290d9299ce9b29443fdc2f2cb7a1e7", "msg": "Pipe datadog_checks through list for Python 3 (#51)\n\nI ran into [an issue described in Ansible on Python 3](https://github.com/ansible/ansible/issues/19514), in which the module fails when trying to iterate through keys:\r\n```\r\nTASK [Datadog.datadog : Create a configuration file for each Datadog check] ****\r\nfailed: [default] (item=dict_keys(['process'])) => {\"failed\": true, \"item\": \"dict_keys(['process'])\", \"msg\": \"AnsibleUndefinedVariable: 'dict object' has no attribute \\\"dict_keys(['process'])\\\"\"}\r\n\r\nPLAY RECAP *********************************************************************\r\ndefault                    : ok=8    changed=6    unreachable=0    failed=1\r\n```\r\n\r\nAnsible developers recommend piping dicts through `list` to work around this issue. Trying this on my own Ansible 2.2.1.0/Python 3 environment, I was able to eliminate the issue. The discussion on the issue indicates that this is backward-compatible with lesser versions of Ansible and Python."}, {"repo": "Oefenweb/ansible-postfix", "hash": "171ee647fe6d63459a5b35ac6e67e0ff1795b94e", "msg": "Fix duplicate aliases, closes #26"}, {"repo": "Oefenweb/ansible-postfix", "hash": "9f6be61f35f7529dd12708af90a13318b70b42eb", "msg": "Put SASL credentials into a 0600 map file"}, {"repo": "Oefenweb/ansible-nano", "hash": "3e3916001eee8af79a383c5f62ce7d04934ecf51", "msg": "Bugfix for missing git package\n\nFixed #7"}, {"repo": "Oefenweb/ansible-snmpd", "hash": "81bde33386c86e1dfa40efc58623137cfccc5fa7", "msg": "404 Not Found (with cold apt cache)\n\nFixes #5"}, {"repo": "jloh/nagios-nrpe-server", "hash": "c33f3410e002f9d8506f0725f56b7d7afa1c5f74", "msg": "Closes #23: Actually install nagios-plugins on Debain based OS'\n\nRefs #22\nI originally thought this was a hard dependency but it turns out it isn't at all."}, {"repo": "Oefenweb/ansible-phpredis", "hash": "ff27e1e1362293b5059227c138c05f7f2f3e73f0", "msg": "Fix build.\n\n- Fix \"phpredis_version\" when using branch names.\n- Ignore change in Git repo."}, {"repo": "Oefenweb/ansible-phpredis", "hash": "4711b46481800953bf0c3c6c4e92f513d01969c1", "msg": "Bugfix for missing git package\n\nFixes #9"}, {"repo": "dj-wasabi/ansible-zabbix-agent", "hash": "0378b69adac0a357db4c80ea82aa58534f173fbc", "msg": "Removing part of check to fix #69"}, {"repo": "plone/ansible.plone_server", "hash": "f4fe02f4c446c99d989b26d346b6b5c377fb9161", "msg": "Fixes #129"}, {"repo": "elnappo/ansible-role-secure-openssh-server", "hash": "b321c4899335b65d9e84beec5176058f73f90af1", "msg": "fix #3, replace (last) port definition instead of adding a new one"}, {"repo": "Oefenweb/ansible-wordpress", "hash": "79e29502fb4e0ff1c30c0b5396bfa1b48fb84a8e", "msg": "Added option to activate and deactivate themes and plugins, fixes #26"}, {"repo": "binarypenguin/automation", "hash": "a62e1294e74a2f72e3dd44554abc390401929aa7", "msg": "Fixes #12"}, {"repo": "mantl/mantl", "hash": "8b063707cfab2ac1bb22ec902084207539d26eae", "msg": "remove unused calico variable \"etcd_service_name\" (#1772)"}, {"repo": "mantl/mantl", "hash": "20adf996b4c462dcdf9955eb760ca21d039033b3", "msg": "k8s single cert (#1753)\n\n* make certs valid for all ips and hostnames\r\n\r\n* k8s: use single cert. fixes #1608\r\n\r\n * use kubernetes-dashboard 1.1.0\r\n * dashboard connects to k8s.service.consul\r\n * dashboard respects kubeconfig\r\n * dashboard authenticates using TLS\r\n\r\n* wait for consul leader before posting manifests\r\n\r\nsee https://github.com/MustWin/kubernetes/issues/9\r\n\r\n* add netaddr to requirements.txt\r\n\r\nfor the k8s role to work, it's service IP must be valid for\r\nour certificates. we extract this value from the k8s CIDR\r\nblock, which requires netaddr.\r\n\r\n* netaddr is only on version 0.x not 1.x\r\n\r\n* add tags to consul wait for leader task"}, {"repo": "mantl/mantl", "hash": "84b7083961aa78b307b0b03c3e2d00afa2d05ba3", "msg": "cast enable_cloud_provider to bool in conditionals (#1743)\n\n* cast enable_cloud_provider to bool in conditionals\r\n\r\nfixes #1740\r\n\r\n* only cast, don't test"}, {"repo": "mantl/mantl", "hash": "95f87ce3ed703bc9236d52bffda63744b3d4a13e", "msg": "hardcode consul_dns_domain in calico role (#1719)\n\nfixes #1679"}, {"repo": "mantl/mantl", "hash": "f1e3f0595a9e9af0ffc8f04e5bf06dd6ca525871", "msg": "always run certificates role (#1634)"}, {"repo": "mantl/mantl", "hash": "99fdd610f730d118412916719eaf3c0f79798025", "msg": "List cloud-init-providers in a variable (#1508)\n\nfixes #1507, #1504"}, {"repo": "mantl/mantl", "hash": "1febd6f5bdac263616a81886eb9950faf6e3a585", "msg": "Remove storage dropin from non lvm deployments (#1410)"}, {"repo": "mantl/mantl", "hash": "f95c1090ac8d374b440bd85191247f610c325dc7", "msg": "gluster: mount volumes so they'll be remounted on reboot\n\nFixes #872"}, {"repo": "mantl/mantl", "hash": "22f550b6860a23631acc59ee159b1a0d5cb4347f", "msg": "glusterfs: set selinux to permissive\n\nglusterfs does not work when selinux is enforcing. for now, we need to\nset selinux to permissive mode when glusterfs is used in mantl. we\nshould switch back to enforcing mode when glusterfs gains proper selinux\nsupport.\n\nFixes #867"}, {"repo": "mantl/mantl", "hash": "587936f64682247b07b9b67824aab301b36a86fe", "msg": "Fix glusterfs repo, update to 3.7.6\n\nFixes #798"}, {"repo": "mantl/mantl", "hash": "1a06f36ecb7bcb19ba3d98c12372bf0ebd958aca", "msg": "Added distributive-consul-check for deploys; #539\n\ndistributive-consul-check.json is back in roles/consul/files/ and\nansible tasks are now setup to use this file.\n\nWhen deploying the file is needed on the deployment server to install\non remote servers."}, {"repo": "mantl/mantl", "hash": "b7a80a55f0b6b89526c06682c323b3e13f07ff37", "msg": "disable firewalld - fixes #193"}, {"repo": "nusenu/ansible-relayor", "hash": "a242da14799f9638d60e5807afefb34b53fed57f", "msg": "remove workaround for #108 (upstream bug is fixed)\n\nfixes #108\nupstream: https://github.com/ansible/ansible/issues/21547"}, {"repo": "nusenu/ansible-relayor", "hash": "4e6fea6fd6881e04c0482922bfd0133c21be9be3", "msg": "require ansible v2.3.0 (fixes #85)"}, {"repo": "nusenu/ansible-relayor", "hash": "a7a6236eee47cdccb17df5d0c272ffadced6100d", "msg": "Debian: Ensure lsb-release requirement is installed (fixes #108)"}, {"repo": "nusenu/ansible-relayor", "hash": "1224adf2811c827a09ff0bf133fbb476901a547d", "msg": "add support for private IPv4 addresses (fixes #102)\n\nUse public IPs only by default and fallback\nto a single private IP in case we have no public IP.\nUsing public _and_ private IPv4 addresses at the same\ntime on a single host is not supported.\n\nFixes a bug where we created more than two\ntor instances per public IPv4 address when the\nhost had private and public IPv4 addresses.\n\nVariable 'tor_v4ips' can no longer be set since it is\npotentially overwritten at runtime.\n\ntor_maxips is replaced with tor_maxPublicIPs + tor_maxPrivateIPs\n\ntorrc: no longer include 'Address' line.\n\nThis would be simpler without bug ansible/ansible#14829 (#80)."}, {"repo": "nusenu/ansible-relayor", "hash": "196c5c74623fe97d88ea1e2527b96d7ccef784a5", "msg": "remove task 'Check for local requirements' (fixes #96)\n\nit does not work on macOS (the only platform it was written for)"}, {"repo": "nusenu/ansible-relayor", "hash": "dbe5647c4e42252e9e428e50b9b0a3636f94842a", "msg": "including OS specific vars is always required (fixes #86)"}, {"repo": "nusenu/ansible-relayor", "hash": "c25e1f96cb3a91dfef803f4de935c51c597bd19d", "msg": "bugfix: make conditional service reload after torrc change a handler (fixes #83)"}, {"repo": "nusenu/ansible-relayor", "hash": "9b1adbb610c8e7778f55688bebdc55c648ca3039", "msg": "add support for ansible 2.x by working around an ansible 2.x bug\n\nThe workaround simply replaces \"False\" (the incorrect return value\nof the ipv6() filter) with an empty list (correct behaviour).\n\nOnce the upstream bug is solved this commit should be reverted.\n\nfixes #56\nfixes #73\nupstream ansible bug:\nhttps://github.com/ansible/ansible/issues/14829"}, {"repo": "nusenu/ansible-relayor", "hash": "0406395a8758f12abd57914532cc0ff17894d015", "msg": "Debian: allow \"_\" and \".\" in instance names, so our instances are started properly after package upgrade (fixes #72)"}, {"repo": "nusenu/ansible-relayor", "hash": "44be91805d4a4c003d28a7c5fd3f7a6317863435", "msg": "FreeBSD: change pidir from /var/run/tor to /var/run/tor-instances (fixes #62)"}, {"repo": "nusenu/ansible-relayor", "hash": "b5d6f33218259b7205796d55ad4b976d3b9f3305", "msg": "BSD: drop support for log files, change default DataDir location to avoid permission conflicts with the packages (fixes #59)"}, {"repo": "nusenu/ansible-relayor", "hash": "51dcdf691a7801895b569a5a927e30030d8feb70", "msg": "CentOS/Fedora: move DataDirs to /var/lib/tor-instances (fixes #58)\n\nmake sure SELinux type is set properly on /var/lib/tor-instances\n\nallow the systemd service to write to /var/lib/tor-instances (drop-in)\nThanks to Jamie Nguyen for suggesting that fix.\n\nlog via syslog on Linux systems"}, {"repo": "nusenu/ansible-relayor", "hash": "2149edf99fe1c251d0eaad5c78536c9b1d54dde3", "msg": "Fedora: make use of systemd hardening features (also fixes #53 with RestartSec=1, thanks to Jamie Nguyen)"}, {"repo": "nusenu/ansible-relayor", "hash": "33341b8cd483ae2583a4bd7c1bd79c82e4a12e4b", "msg": "don't fail on CentOS with SELinux disabled (fixes #42)"}, {"repo": "nusenu/ansible-relayor", "hash": "850b64e62ec69d7b4d32f0ad9258e608b0e50ca2", "msg": "OpenBSD: tor was not started appropriately with \"-f config\" during initial install due to an ordering issue (closes #34)"}, {"repo": "nusenu/ansible-relayor", "hash": "c5b489aa56c545dd0b51910b5a0d701d838ff071", "msg": "OpenBSD: fix a bug in openfiles-max handling (closes #37) Users of previous versions can remove the line \"tordaemon::openfiles-max=13500::tc=daemon:\" from their /etc/login.conf files"}, {"repo": "Oefenweb/ansible-keepalived", "hash": "85555d67cc51acd71f061e7b42af84ef32915b09", "msg": "Add forgotten notify (#15)"}, {"repo": "ansiblebit/oracle-java", "hash": "617c8f2de47d53b11ebed60c6cdfe7d020a120a7", "msg": "Implemented rpm search on oracle site, fixes #34"}, {"repo": "dev-sec/ansible-ssh-hardening", "hash": "ba307d7f90a2ca7f12535f8fe06adbfd9ca79f06", "msg": "add always_run: true to task. fix #64"}, {"repo": "elastic/ansible-elasticsearch", "hash": "5b1d028bd2f81e343f7dc90c4f5b8b23ad8e80e5", "msg": "fix ''dict object has no attribute dict_keys\" issue with python3 (#578)\n\nIn Python2, the dict.keys(), dict.values(), and dict.items() methods returns a list. Jinja2 returns that to Ansible via a string representation that Ansible can turn back into a list. In Python3, those methods return a dictionary view object.\r\nresource: https://docs.ansible.com/ansible/2.4/playbooks_python_version.html#dictionary-views"}, {"repo": "elastic/ansible-elasticsearch", "hash": "2f3f84b6f30d1fe2582c7b5cdbf8b7a35c9399f0", "msg": "Changed /etc/default/ to /etc/sysconfig/ on EL. Fixes #14"}, {"repo": "dev-sec/ansible-os-hardening", "hash": "060b15a7ee3d176e3479a7e695d9ef190b2bc479", "msg": "Addressing issue #255 (#258)\n\n- Added logic to pull uid_min from login.defs when it returns an\r\n  integer value greater than 0.\r\n- Add logic so that Debian systems use inherit uid_max=999 when\r\n  a uid_max was not found in login.defs.\r\n- Add logic so that all other systems inherit uid_max=499 only\r\n  when the value was not already found in login.defs or set for\r\n  Debian systems.\r\n\r\nSigned-off-by: Lesley Kimmel <lesley.j.kimmel@gmail.com>"}, {"repo": "dev-sec/ansible-os-hardening", "hash": "e674a6d589bf9da486c4fef04c9dd6a3799ad717", "msg": "Fix #247, cleanup conditions (#248)\n\nSigned-off-by: Jes\u00fas Fern\u00e1ndez <jmfernandez@ticnor.es>"}, {"repo": "dev-sec/ansible-os-hardening", "hash": "723205f8f257f89edf6d665ac8b5a8eaa9964f70", "msg": "add rhel7 pam_pwquality. fix #73"}, {"repo": "StackStorm/ansible-st2", "hash": "36f63dc3647721266cdb0d09e6836ce99546505e", "msg": "Use new bundled packages (#42)\n\nUse new bundled packages\r\n\r\nCloses #43, fixes #40, fixes #39, fixes #38, closes #36, closes #14, resolves #4"}, {"repo": "Oefenweb/ansible-r", "hash": "9fc548ea18605811920d2eaf6907fd5bbfa4bb24", "msg": "Update CRAN Ubuntu signing key. (#40)"}, {"repo": "Oefenweb/ansible-r", "hash": "a2922dd147c00d1a88bf3949347035220032fd40", "msg": "Always pass lib as a third argument to R-install-package (#15)"}, {"repo": "Oefenweb/ansible-r", "hash": "2c357742e8ad1665e9683105348360742c67d92b", "msg": "Use Rscript for templated scripts (#12)\n\n* Use Rscript for templated scripts\r\n\r\n* Cleanup\r\n\r\n[ci skip]"}, {"repo": "proycon/LaMachine", "hash": "11d7188c333a44bbc1e36e39ad1a8776c11a0e20", "msg": "propagate force parameter to homebrew (closes #78)"}, {"repo": "proycon/LaMachine", "hash": "671b0cb28d08a0c868bdbe93a83f05e3427335f7", "msg": "attempted fix  #49"}, {"repo": "ausaccessfed/shibboleth-idp-installer", "hash": "d25864f843120357af01fbff8d98bcce3a91913c", "msg": "Ensure IdP credential permissions set correctly\n\nThe permissions for the files in Shibboleth IdP's credential directory\nweren't being set on an initial run of bootstrap.sh. Now they will be\nset so the jetty user has access.\n\nCloses #76."}, {"repo": "HanXHX/ansible-nginx", "hash": "c2685732a4d09a1be6b6833ae5bf0d64815dac80", "msg": "Manages Ansible 2.4+ with Docker\n\nCloses #30"}, {"repo": "HanXHX/ansible-nginx", "hash": "0a8f21b0b7525274ef75c7ba4a0e3365da411ff6", "msg": "Min ansible version is now 2.0 (closes #10)"}, {"repo": "GROG/ansible-role-sudo", "hash": "f392fe81e28eec6f4df64a5758486818e3f2c5f6", "msg": "Fix custom file names (don't add default order)"}, {"repo": "GROG/ansible-role-sudo", "hash": "35708f8f292ed294d1dfd6166b2bb8ac759b56b9", "msg": "Fixed dependency vars (#14)"}, {"repo": "GROG/ansible-role-management-user", "hash": "3d7c8b93cbb4ae61e7d26811271de0c7ded9cf88", "msg": "Update authorized-keys and sudo roles\n\nFixes #7"}, {"repo": "elboletaire/ansible-transmission", "hash": "232cfda3508080acc93a3acb00c7b74c864bafc1", "msg": "Revert \"Added start handler\"\n\nThis reverts commit e5fcc16c2036782c05d8586ad47dbc1416af2763.\n\nCloses #1"}, {"repo": "fgci-org/fgci-ansible", "hash": "04537e4238f2b7dc7f17f3bfe2b87c76430abe52", "msg": "Close #86\n\n- ip_forwarder will fail if NetworkManager isn't installed. Adding a check."}, {"repo": "fgci-org/fgci-ansible", "hash": "2a05a5032ce341d6a1d918453164c59ea2922f58", "msg": "Closes #13"}, {"repo": "fgci-org/fgci-ansible", "hash": "ba79c2af736ef34960a94cd3226ec8f0e019787e", "msg": "closes #16\n\n - removes ansible-role-fgci-admin role\n - creates virt storage pool\n - removes setting inventory_hostname\n - dont set root_pssword\n - create virt pool once with run_once"}, {"repo": "CSCfi/ansible-role-slurm", "hash": "347f82d005c0e13be1f919585e06c655fc26c2c1", "msg": "Closes #11\n\n- Changing user's home while the user is running anything will break the playbook. I propose not setting any home dir."}, {"repo": "kubernetes-sigs/kubespray", "hash": "678ed5ced5d4b8fae62b0012c43c06fffdee086f", "msg": "fix upgrade procedure when in playbook (#5695)\n\nexists role kubernetes/preinstall and not exists role container-engine\n\n error 'yum_repo_dir' is undefined"}, {"repo": "kubernetes-sigs/kubespray", "hash": "7f87ce0362303a3d68720af37527445163e20680", "msg": "Upgrade container-engine after draining (#5601)\n\n* Run 'container-engine' after drain.\n\nMove possibly disruptive role 'container-engine' to run after the node\nis drained.\n\nAs that role have to be run on non-cluster nodes as well (etcd and\ncalico-rr), and those nodes are not drained, add play for that case.\n\n* Check if api is up before upgrade.\n\nIf container engine is restarted in previous role, api controller can\ntake some time to start. This check ensures api is up before upgrade."}, {"repo": "kubernetes-sigs/kubespray", "hash": "36c1f32ef912674d9ed9d26a23b151ef05fee4c3", "msg": "remove legacy docker repo in kubernetes/preinstall before any packages installed (#5640)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "d56e9f6b8053273b40b9b05f803672c4ee23ee95", "msg": "Fix Cinder CSI bugs (#5492)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "339e36fbe68108992f928c07f2c8a490a68d0886", "msg": "Files to archive can be passed directly (#5571)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "14b1cab5d2105f090f879379b65a8b0a73e4deac", "msg": "force rotate control plane certifcate on master node when upgrade cluster (#5596)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "2ab5cc73cd3e2342660b54b2fb4d028d46693d48", "msg": "Fix typo in Multus plugin. (#5568)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "9f2dd09628a11f419f394f70dae800c88b3b6959", "msg": "Add proxy support to containerd, improves no_proxy (#5583)\n\n* containerd: add proxy support\n\nSigned-off-by: Etienne Champetier <champetier.etienne@gmail.com>\n\n* kubespray-defaults: add kube_service_addresses / kube_pods_subnet to no_proxy\n\nCIDR notation in no_proxy is supported by a lot of programs/languages,\nincluding go: https://github.com/golang/go/issues/16704\nWithout that containerd cannot talk the the API server (kube_apiserver_ip),\nbut it should not go through an external proxy for the nodes/pods/services\n\nSigned-off-by: Etienne Champetier <champetier.etienne@gmail.com>"}, {"repo": "kubernetes-sigs/kubespray", "hash": "2798adc837bf985ba9addc3e578086a1072cc722", "msg": "Remove stale legacy yum docker repo /etc/yum.repos.d/docker.repo (#5569)\n\n* Remove stale legacy yum docker repo /etc/yum.repos.d/docker.repo\n\n* move task 'Remove legacy docker repo file' to pre-upgrade.yml"}, {"repo": "kubernetes-sigs/kubespray", "hash": "54d9404c0e4fad6aa5df64a0452ec7e84a029937", "msg": "Fix hashes... kubernetes 1.17.2 (#5581)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "5d9986ab5fbaef62420743ed98b562b012117b5b", "msg": "Fix temp filename for debian-10 image (#5540)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "5e9479cdedc21c7a1c6b2fb3b85ad863212ff521", "msg": "Ensure we always fixup kube-proxy kubeconfig (#5524)\n\nWhen running with serial != 100%, like upgrade_cluster.yml, we need to apply this fixup each time\nProblem was introduced in 05dc2b3a097fda2ffff7a77f4ca843d0e41dec76\n\nSigned-off-by: Etienne Champetier <champetier.etienne@gmail.com>"}, {"repo": "kubernetes-sigs/kubespray", "hash": "303c3654a16ad310a528a27aa1156b96b5737374", "msg": "Set pipefail in case tar fails (#5506)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "ccbcad9741488c312aae0b825637f39d0c8ca7b8", "msg": "Ubuntu CRI-O (#5426)\n\n* Fix crictl\n\n* Reload systemd daemon before enabling service\n\n* Typo\n\n* Add crictl template\n\n* Remove seccomp.json for ubuntu\n\n* Set runtime path of runc for ubuntu\n\n* Change path to conmon"}, {"repo": "kubernetes-sigs/kubespray", "hash": "538f1f1a68a41a7857fc6e12d5d2a8b98589676a", "msg": "cri-o: redhat.yml - remove package cri-tools (#5444)\n\nThere is no cri-tools package in CentOS/EPEL/Red Hat.\nAdditionally, cri-tools is provided into the installation via\nroles/download/defaults/main.yml:104:crictl_download_url."}, {"repo": "kubernetes-sigs/kubespray", "hash": "370a0635fae8b4904bb6bf7936884195d1ab327f", "msg": "Bump nodelocaldns version to 1.15.8 (#5447)\n\n* Bump nodelocaldns version\n\n* Add missing upstreamsvc"}, {"repo": "kubernetes-sigs/kubespray", "hash": "815eebf1d7baa8f7f20cb90e11c64a229666f5fb", "msg": "Add wait for kubectl get ds after upgrades (#5433)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "42702dc1a3c46946cf8bcef9ae1f3bdbf1aa8f4b", "msg": "Fixes for CentOS 8 (#5213)\n\n* Fix python3-libselinux installation for RHEL/CentOS 8\n\nIn bootstrap-centos.yml we haven't gathered the facts,\nso #5127 couldn't work\n\nMinimum ansible version to run kubespray is 2.7.8,\nso ansible_distribution_major_version is defined an there is no need to default it\n\nSigned-off-by: Etienne Champetier <champetier.etienne@gmail.com>\n\n* Restart NetworkManager for RHEL/CentOS 8\n\nnetwork.service doesn't exist anymore\n # systemctl status network\n Unit network.service could not be found.\n\nSigned-off-by: Etienne Champetier <champetier.etienne@gmail.com>\n\n* Add module_hotfixes=True to docker / containerd yum repo config\n\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1734081\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1756473\nWithout this setting you end up with the following error:\n # yum install docker-ce\n Failed to set locale, defaulting to C\n Last metadata expiration check: 0:03:21 ago on Thu Sep 26 22:00:05 2019.\n Error:\n  Problem: package docker-ce-3:19.03.2-3.el7.x86_64 requires containerd.io >= 1.2.2-3, but none of the providers can be installed\n   - cannot install the best candidate for the job\n   - package containerd.io-1.2.2-3.3.el7.x86_64 is excluded\n   - package containerd.io-1.2.2-3.el7.x86_64 is excluded\n   - package containerd.io-1.2.4-3.1.el7.x86_64 is excluded\n   - package containerd.io-1.2.5-3.1.el7.x86_64 is excluded\n   - package containerd.io-1.2.6-3.3.el7.x86_64 is excluded\n (try to add '--skip-broken' to skip uninstallable packages or '--nobest' to use not only best candidate packages)\n\nSigned-off-by: Etienne Champetier <champetier.etienne@gmail.com>"}, {"repo": "kubernetes-sigs/kubespray", "hash": "6924c6e5a32ae4a74fea19456e33c71287da50d6", "msg": "[FIX] fix match because trim removes leading/trailing whitespace (#5356)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "8c15db53b26998303a52750b4b6900009d63659f", "msg": "Fix helm for Kubernetes 1.16.2 (#5332)\n\nSince upgrading k8s beyond 1.16.0 version, helm init does\nno longer work with helm < 2.16.0 due to\nhttps://github.com/helm/helm/issues/6374\n\nThis PR closes issue #5331"}, {"repo": "kubernetes-sigs/kubespray", "hash": "bc3a8a003982bd536cf0712ec68494b9142e6c9b", "msg": "Fixes issue #5299 (#5300)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "81d57fe6580dbae0bd7d8d867dbc0852ab14c11a", "msg": "set calico_datastore default value in role kubespray-default (#5259)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "3118437e10bc160bd704967169bea0ab348acb39", "msg": "check on all cluster node - kubelet_max_pods <= (2 ** (32 - kube_network_node_prefix | int)) - 2 (#5279)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "65e461a7c04602a80723da98317e48238b33fb8b", "msg": "download container always been on download_delegate host (#5177)\n\n* download container always been on download_delegate host\n\n* fix also check pull required"}, {"repo": "kubernetes-sigs/kubespray", "hash": "c672681ce5ba9b8580c78d930eaac740afa26307", "msg": "Revert Pull Request #5084 (#5120)\n\nKubespray Pull Request #5084 (https://github.com/kubernetes-sigs/kubespray/pull/5084) caused more problems than it solved due to limitations with the synchronize module. See comments on Kubespray Issues #5059 (https://github.com/kubernetes-sigs/kubespray/issues/5059) and #5116 (https://github.com/kubernetes-sigs/kubespray/issues/5116). Details from Ansible documentation: \"Currently, synchronize is limited to elevating permissions via passwordless sudo. This is because rsync itself is connecting to the remote machine and rsync doesn\u2019t give us a way to pass sudo credentials in. ... Currently there are only a few connection types which support synchronize (ssh, paramiko, local, and docker) because a sync strategy has been determined for those connection types. Note that the connection for these must not need a password as rsync itself is making the connection and rsync does not provide us a way to pass a password to the connection. ...\" Thus, reverting Pull Request #5084."}, {"repo": "kubernetes-sigs/kubespray", "hash": "d332a254ee59aba47f430a257c559c1f9437cb08", "msg": "install python3 instead of python2 for fedora >= 30 fixes 5056, fixes 4802 (#5111)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "9d8fc8caad61cf1fb1a5ce4570dd8b05f51f56d7", "msg": "Fix getting nameserver and search for /etc/resolv.conf with comments (#5197)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "a51b729817ef8006939c213f5706222b60efd5ac", "msg": "add ignore_errors to the kube-proxy deletion task (#5236)\n\nWhen using cluster.yml or scale.yml to add/scale nodes in the existing\nk8s cluster, the `kubeadm init` wouldn't run. As a result, kube-proxy\nwouldn't be created, and therefore the kube-proxy deletion task would\nfail, e.g. in the case where kube-router is used and \"kube_proxy_remove\"\nis set to true. As a workaround, add ignore_errors to the kube-proxy\ndeletion task."}, {"repo": "kubernetes-sigs/kubespray", "hash": "932935ecc7cb23d9ad2b726d94d424f1199a79c6", "msg": "fix wrong path in include install_host.yml in etcd role (#5256)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "0ba336b04efbeb4b9a816b2f974fe730bc5ee027", "msg": "install helm client separately (#5212)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "9db61c45ed08a34d9a0422b9f6787b9b6f2aa3fb", "msg": "Upgrade nodelocaldns to 1.15.5 (#5191)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "8cb54cd74d93d79a9af4487215be2ebfed7d4baa", "msg": "fix broken scale procedure: (#5193)\n\n- do not run etcd role when etcd_kubeadm_enabled == true\n- remove default value 'systemd' for cgroup driver in containerd role.\n  this value override autodetect in kubelet_cgroup_driver_detected from docker info"}, {"repo": "kubernetes-sigs/kubespray", "hash": "1ce7831f6d2234bd91a62cc7e7b69863e4c1fdf7", "msg": "Update main.yml (#5166)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "07ecef86e3f81e17221d89f8ea64ce54328ebfea", "msg": "Replace fetch with synchronize due to memory error (#5084)\n\nFix for Kubespray Issue #5059 (https://github.com/kubernetes-sigs/kubespray/issues/5059). There is a known issue with the 'fetch' module that will sometimes lead to it failing with a memory error. See ansible/ansible#11702 (https://github.com/ansible/ansible/issues/11702). I encountered this issue with the \"Copy kubectl binary to ansible host\" task in kubespray/roles/kubernetes/client/tasks/main.yml, and it caused my entire deployment to error out (see \"Output of ansible run\" above). Replacing 'fetch' with 'synchronize' fixes this issue."}, {"repo": "kubernetes-sigs/kubespray", "hash": "3bc4b4c1748636382ae549c9644dc99ce6725ff0", "msg": "Use raw module for bootstrap-debian.yml (#5061)\n\nUpdated Openstack to terraform 0.12 (#5062)\n\n* update openstack to terraform 0.12(.5)\n\n* replace cluter.tf with cluster.tfvars\n\n* update README.md to terraform 0.12\n\n* update Openstack CI tests to use terraform 0.12\n\n* specify terraform version in openstack README\n\n* gitlab CI to copy cluster.tfvars in case of openstack provider\n\n* The terraform/openstack dynamic inventory can read\ntfstate v4 (generated by terraform 0.12) and convert them internally\nro v3 (as generated by terraform 0.11.x).\n\nAdditionally the script has been updated to Python 3."}, {"repo": "kubernetes-sigs/kubespray", "hash": "494a6512b8027f0c31f203288d04fbe2ae3a33ad", "msg": "fix bug: run Copy image to ansible host cache on download_delegate host (#5094)\n\n* run 'task download_container | Copy image to ansible host cache' with synchronize on download_delegate host\n\n* try to run task copy file to ansible host on all inventory, not only on first random host"}, {"repo": "kubernetes-sigs/kubespray", "hash": "a1ff1de97509a0055d681816bb2f5c910137b4cd", "msg": "fix openstack_cacert conditional (#5078)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "1bfbc5bbc483c94d2bc4a2cd5864e7a9db9c3b51", "msg": "remove resource-container default value for kube-proxy (#4994)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "8fc9c5d025568f851858cda15e37f895eddece15", "msg": "Upgrade ingress nginx to 0.25.1 (#5081)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "56fa46716e289315b1857da7429fdce06d7b5ec2", "msg": "Add missing coredns tag. (#5054)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "62aecd1e4ad6a21ac010a7e13be3edb9c88d2da5", "msg": "multus | fix use last version (#5041)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "02ec72fa408b2f4e09480fe7e8c8226a6451d386", "msg": "Fix commands for using experimental kubeadm control plane (#5006)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "f3df0d5f4a561b6953fcc45ca173aef5f0e7a850", "msg": "Always create bash_completion.d folder (#5039)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "fbbfff3795daffa8d9cd69244781f768d1c99d59", "msg": "fix broken ubuntu containerd engine (#5002)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "428e52e0d14576e5c461f7169fcff5751342f455", "msg": "Fix calico handler for containerd (#4985)\n\ncrictl tool must be used to delete containers in case of containerd\ndeployment"}, {"repo": "kubernetes-sigs/kubespray", "hash": "5826f0810c6c9f03fd63bc5582311475641e0137", "msg": "Check all apt config files for configured proxies (#4972)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "781b5691c9e7e08a23ad9313ae3541c669820ebc", "msg": "prep_kubeadm_images: parse repo and tag (#4976) (#4977)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "46bef931e971cfcea58165b0cbb84034c4fb164b", "msg": "Fix images info logic for containerd (#4965)\n\nAs crictl tool is used to download images, it must be also used to gather\nimages info"}, {"repo": "kubernetes-sigs/kubespray", "hash": "c81b443d9385f5f3bfa9e733433c32b7521748a5", "msg": "Fix order of names in /etc/hosts (#4940)\n\nConfigure fqdn properly"}, {"repo": "kubernetes-sigs/kubespray", "hash": "dc16ab92f45d3310358ae3e803be24a881e9f2e4", "msg": "fix for calico with kdd datastore (#4922)\n\n* fix for calico with kdd datastore\r\n\r\n* remove AS number from daemonset\r\n\r\n* revert changes to canal\r\n\r\n* additionnal fixes for kdd datastore in calico"}, {"repo": "kubernetes-sigs/kubespray", "hash": "216631bf0265f6113cf99254e506cd1e732679a3", "msg": "Repair kube_proxy_exclude_cidrs (#4909)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "c7f3123e285e6081717b3e5740a0acd23758926d", "msg": "kubeadm_discovery_address should not contain proto (#4930)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "b5406b752d4d4e94fad52fddcd98d22c1159dc40", "msg": "Add kube_override_hostname to kubeadm certs. (#4903)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "4afbf51d323f244e18dba099836077ddbc14558b", "msg": "kube-router: Set ownership of /opt/cni/bin/* to kube (#4825)\n\nTask \"kube-roter | Set cni directory permissions\"\nsets ownership of /opt/cni/bin to \"kube\"\n\nTask \"kube-router | Copy cni plugins\"\ncopies the binaries from the archive setting the ownership\nback to \"root\"\n\nFix \"kube-roter\" typo\n\nSigned-off-by: Alberto Murillo <albertomurillosilva@gmail.com>"}, {"repo": "kubernetes-sigs/kubespray", "hash": "d62684b617005e412f73839ed844bd5e1b87c4f3", "msg": "Fixed missing meta for generic CNI network plugin (#4845)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "a8dfcbbfc7ebbffb8e9326ea3b9429ae881e34c9", "msg": "Switch /root references to ansible_env.HOME (#4842)\n\n* kube config dir for current/ansible become user\n\n* remove extra /\n\n* fix default value"}, {"repo": "kubernetes-sigs/kubespray", "hash": "d5405606196f27781cf700a80af20e65827c2e0f", "msg": "Preinstall fails on checking etcd group length (#4839)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "797bfd85b0a43b399dba4211a50cb180f9b53fda", "msg": "Only create kubeadm compat cert dir link if it does not exist (#4840)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "2a5721b4d4ad79e2ee9e85b956a873b5d1871c78", "msg": "Change CentOS CRI-O repo from developer repo to public one (#4807)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "333f1a4a4085d82e9ea5494cffbea967a8b622cd", "msg": "kubeadm join path fixed for RH linux (#4798)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "1e470b047354c2767d22ca28c1c1d0fc81f34107", "msg": "Fix certificate-key param for kubeadm init (#4789)\n\n* Fix certificate-key param for kubeadm init\n\n* Fix yamllint error"}, {"repo": "kubernetes-sigs/kubespray", "hash": "8e28ba38d285e2d8213b12070768cb165675d083", "msg": "Add Load Balancer IP to API servers SANs (#4775)\n\n- Add loadbalancer_apiserver.address to apiserver_sans"}, {"repo": "kubernetes-sigs/kubespray", "hash": "13f225e6ae31024bb2f422657e9461142df11677", "msg": "Only pull images for destined host groups (#4735)\n\nWithout this, pulls are considered for all\nhosts groups, even if not targetted by the downloads\n`groups` list. Hence, a download/sync is triggered\neven though the host does not require the image."}, {"repo": "kubernetes-sigs/kubespray", "hash": "8a5eae94ea69ca865935f00198fe9f13941f132b", "msg": "Minor cleanups of CoreDNS issues and CI job (#4719)\n\n* Minor cleanups\n\n* Add comment in docs that nodelocaldns cache is enabled by default"}, {"repo": "kubernetes-sigs/kubespray", "hash": "03bded2b6b4714762ddc1efd4252c05a65f983ac", "msg": "Fix adding output of kubeadm to the admin.conf downloaded to the artifacts directory (#4696)\n\nFixes issue https://github.com/kubernetes-sigs/kubespray/issues/4695"}, {"repo": "kubernetes-sigs/kubespray", "hash": "560f50d3cde25f5ae7c0e69a8b5dcc85bb7e4639", "msg": "Add support for http(s)_proxy to CoreOS, Fedora and OpenSUSE (#4669)\n\n* Add support for http(s)_proxy to CoreOS and Fedora\n\n* fix opensuse proxy support\n\n* Fix CoreOS proxy support\n\n* update documentation"}, {"repo": "kubernetes-sigs/kubespray", "hash": "15eb7db36d3bbee311152d741e8262204e87a82d", "msg": "Fix k8s api endpoint for secondary nodes in control plane mode (#4675)\n\nChange-Id: I1588458b54c52443ad8d0afbd266f77ac0afea67"}, {"repo": "kubernetes-sigs/kubespray", "hash": "741de6051c6d411f697ae419a80a162157066d00", "msg": "Fix nodeselectors for contiv and nginx-ingress (#4662)\n\n* Fix nodeselectors for contiv and nginx-ingress\n\nChange-Id: Ib3eb6bd87193c69a90ee944c9164a0b6792c79ba\n\n* Set kube proxy mode to iptables for addons task\n\nChange-Id: Iff71a71f672405c74b4708c71db15ddc4391a53a"}, {"repo": "kubernetes-sigs/kubespray", "hash": "338eb4ce659f8c4214e75b44b616780e706d9126", "msg": "Fix kubeadm upload certs with when condition (#4659)\n\n* Fix kubeadm upload certs with when condition\n\nChange-Id: I916dd2375b71eea2386047c7f185a2f8361f7a61\n\n* Update kubeadm-secondary-experimental.yml"}, {"repo": "kubernetes-sigs/kubespray", "hash": "3722acee85ec4a5b68761b8821666cb585637411", "msg": "Fix broken metrics-server deployment not starting (#4651)\n\n* Fix metrics-server deployment\n\n* Make metrics server work\n\n* Fix sample inventory"}, {"repo": "kubernetes-sigs/kubespray", "hash": "6ca2019002eb28a4ae373a2cf74677b972b65e9f", "msg": "Fix issue with etcd arm host installation case (#4589)\n\nUse host_architecture variable."}, {"repo": "kubernetes-sigs/kubespray", "hash": "f5a83cededc2b2ce2133b023290a72ab57b7bf2e", "msg": "Fix typo in test-infra playbook (#4644)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "3fe66a1298c06728ce69f71f11769dda674c0cb1", "msg": "Update downloads role to download to correct group (#4638)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "62434678561b60891b72a8651fb25c4bd0de61ea", "msg": "remove duble check for run this task just one time (#4613)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "50751bb61080e35ccc979cf1051c91fdf870736e", "msg": "Revert \"Optimize kube resources creation (#4572)\" (#4621)\n\nThis reverts commit f8fdc0cd939de0e46cd968c6afb0fe91b72f78d0."}, {"repo": "kubernetes-sigs/kubespray", "hash": "ada5941a70c53126869991ef4f29d99af5abe8fc", "msg": "Unmask Docker service in ClearLinux (#4583)\n\nThe docker service provided by the containers-basic bundle is masked\nin ClearLinux distribution. This is causing errors in the following\nsteps. This commit ensures that the unit is not masked."}, {"repo": "kubernetes-sigs/kubespray", "hash": "656633f784444b74f7fd166744e4df395dc21180", "msg": "YAMLLint everything (#4576)"}, {"repo": "kubernetes-sigs/kubespray", "hash": "c6586829defc8467eb4e267d6b991482bd5825b6", "msg": "Ensure /etc/bash_completion.d/ folder exists (#4543)\n\nThe Stateless ClearLinux feature[1] requires the creation of folders\nin /etc folder. This change ensure the existence of the\n/etc/bash_completion.d/ folder for ClearLinux Distribution.\n\n[1] https://clearlinux.org/features/stateless"}, {"repo": "kubernetes-sigs/kubespray", "hash": "ec3daedf9e2f777d894c1e0ddda4d079b59ad925", "msg": "Revert \"Fix for unknown 'kubernetes.io' or 'k8s.io' labels specified with --node-labels (#4320)\" (#4553)\n\nThis reverts commit 586ad89d50d3900b8e2de98086bf5d982c20e5f6."}, {"repo": "kubernetes-sigs/kubespray", "hash": "dced082e5f235e5f5387c80867092758f8884357", "msg": "fixes roles/docker/vars/ubuntu-bionic.yml points to xenial (#3395)\n\n* fixes: #3387"}, {"repo": "infOpen/ansible-role-jenkins", "hash": "a46bb09851345cacec6a16310edbc40cd34b1398", "msg": "Hipchat plugin management improvements:\n\n* fix idempotency\n* manage last releases\n\nResolves #62"}, {"repo": "CSCfi/ansible-role-cuda", "hash": "2378736112ce798a1b1b6e66f372415b01cd5536", "msg": "Close #3\n\n  - cuda_init.sh preceeds now ansible-pull script"}, {"repo": "CSCfi/ansible-role-cuda", "hash": "a9f9815335a6b9c73bed7e3dcd75e14cd973fbb5", "msg": "fix #1 - sleep and reboot for reboot\n\n - also conditional was wrong in the wait_for handler"}, {"repo": "openstax/cnx-deploy", "hash": "9f26ab5a7d4d69683594affe91b69c7f30347b18", "msg": "Create varnish service symlinks for ubuntu 14.04\n\nCreate symlinks from /lib/systemd/system/ to\n/etc/systemd/system/multi-user.target.wants/.  These symlinks are\ncreated by control.tar.gz postinst script in debian / ubuntu packages\n(e.g. openssh-server).  We're using a package from\nrepo.varnish-cache.org because the Ubuntu 14.04 varnish package is too\nold, and the package from repo.varnish-cache.org doesn't seem to create\nthe symlinks, so we have to add them in manually.\n\nClose #13"}, {"repo": "openstax/cnx-deploy", "hash": "8dde34f4d16923227357354aa413f0dcb6dc9392", "msg": "Build webview on debian without using sudo\n\nBy not installing node and npm globally.\n\nClose #7"}, {"repo": "openstax/cnx-deploy", "hash": "fc70f3bd46abdfb283c1238f62e2bdd56c7ee608", "msg": ":bug: Fix permissions for planted source\n\nCloses #11"}, {"repo": "rocknsm/rock", "hash": "2e9e2dc78987680b76c6d3a3642e31a851d4fb42", "msg": "Disable ipv6 in lighttpd"}, {"repo": "rocknsm/rock", "hash": "59f53db7761296f573c3b401956c4d4dc130f168", "msg": "Fix the way override vars are included"}, {"repo": "rocknsm/rock", "hash": "71728d37a88de9ec2239c69338d0323aedee721f", "msg": "Change static values to variables (#435)"}, {"repo": "rocknsm/rock", "hash": "fd7dca92226be01cbffb5456f73d3f418e90d530", "msg": "Fix indentation in deploy playbook\n\nFixes #340"}, {"repo": "rocknsm/rock", "hash": "1835a3a391ee4ef2f83172a0384e1c2bb286f811", "msg": "Add missing filebeat vars\n\nFixes #341 and fixes #328"}, {"repo": "rocknsm/rock", "hash": "2b59eb23ab37018f90c7779baa76fcc96e1b0eb3", "msg": "Fix filebeat checks\n\nremove the filebeat variable and change all checks to fsf or suricata\nfixes #328\n\nreadd filebeat enabled variable\n\nFix filebeat checks"}, {"repo": "rocknsm/rock", "hash": "789fe0ecea63c2b5a1ec75299a6fc4efffd5dbc4", "msg": "Update logrotate configuration\n\nThanks to @leanvel for the great research in Issue #170. I set the selinux type of the various data dirs to `var_log_t`. There's a number of options available, but after reading the docs that made the most sense.\n\nAlso, I found that `logrotate` is supposed to already have permission to send a `reload` to any systemd service unit. Our suricata service unit already maps this to the SIGHUP signal, so I swapped out this config to work within the existing confines of the selinux policy.\n\n**NOTE**: Still need to test this, but I think it covers all the bases.\n\nFixes #170\n\nCo-Authored-By: LeanVel <leanvel@users.noreply.github.com>"}, {"repo": "rocknsm/rock", "hash": "60ccb1b3a21039129b6f0aea5f1f307930a6d5ed", "msg": "Add GPG info to core CentOS repos when added.\n* Closes #288"}, {"repo": "rocknsm/rock", "hash": "58beba3f58309472ca25e0dd303c71d3c0875a4d", "msg": "Adds a wait_for condition on Kibana (#282)\n\nEnsures Kibana is listening before moving on.\r\n\r\nFixes: #281"}, {"repo": "rocknsm/rock", "hash": "05d17546dd4b4d1530151795cb0171dbc989945f", "msg": "Added explicit import of CentOS GPG key:\n\n *  Only when doing online install.\n *  Probably due to no gpgurl being specified for CentOS repos.\n *  Went with this because its a tested fix from @kwilson7770.\nFixes #142\nFixes #151"}, {"repo": "RocketChat/Rocket.Chat.Ansible", "hash": "1d7d3a653c598355333e7c34a54a550ed3d79cc5", "msg": "Upgrade nodejs dependency to 4.5.0 LTS to meet RC min requirements\n\n  - Added 'rocket_chat_node_version' default w/ ver. 4.5.0\n  - Change node-version-specific vars to be generic\n    - i.e. rocket_chat_node_10_40_path -> rocket_chat_node_path\n  - Updated README to reflect changes\n  - Fixes #33"}, {"repo": "usableprivacy/upribox", "hash": "b6ee3c9dde4d20f800d2dbfe849a71a8a2f7e33a", "msg": "Accept v6 RAs on eth0, fixes #53"}, {"repo": "usableprivacy/upribox", "hash": "fffd21b90b59d32c018b0b1b2e911ba78abd14b8", "msg": "systemd service for supervisor daemon, closes #12"}, {"repo": "elastic/ansible-beats", "hash": "6de5ee5ffb4016a53e0f6cc8506f0025b51c197a", "msg": "replace custom filter with yml handling (#33)"}, {"repo": "openwisp/ansible-openwisp2", "hash": "c246b7f9914f940999948bf9d4608a0ed9e6b0dc", "msg": "[distros] Dropped support for Debian 9 and Ubuntu 16 #169\n\nCloses #169"}, {"repo": "openwisp/ansible-openwisp2", "hash": "481f7ad18dc225973e1d676d33e58c49cbbfe986", "msg": "[qa] Fix Travis CI build errors #93\n\nTravis CI builds for Debian 9, Debian 10, and\nFedora 28 have been fixed. Python3 is set as\nthe ansible_python_interpreter for Fedora 28\nbecause it drops support for python-firewall\nwhich is for Python2. Cron is also added to\nresolve errors in Debian 10. Additionally,\ncryptography from pip is installed to fix an\nodd SSL error when installing django-redis on\nDebian 9.\n\nFixes #93"}, {"repo": "platform9/express", "hash": "738250fd07f279db296e7f52bdc23bbea2dfcd2d", "msg": "Move RedHat jinja2 filter to a test for Ansible 2.9+ compatibility"}, {"repo": "platform9/express", "hash": "1317c60cb6ddf0bf6d9de9a46b15de4a21782c52", "msg": "Move failed jinja2 filter to a test (#245)\n\n* Some fixes\r\n\r\n* Move failed jinja2 filter to a test\r\n\r\nUsing failed as a jinja2 filter is deprecated as Ansible 2.5 (current latest\r\nversion is 2.9). It is noticed that using it as a filter is leading to failures\r\non certain setups. Move to using the new way of using failed as a test instead.\r\n\r\nSee more details here:\r\nhttps://docs.ansible.com/ansible/devel/porting_guides/porting_guide_2.5.html?highlight=failed#deprecated"}, {"repo": "platform9/express", "hash": "e9ad2cd52b6bb2a3acdf16be13965705541cf3dc", "msg": "add router advertisement daemon to neutron-prereqs role (#228)\n\ncloses #222"}, {"repo": "platform9/express", "hash": "acbb14324421615a27b2437e85b31fa649c692d7", "msg": "add base repo to ovs install to allow for proper dependency installation (#224)"}, {"repo": "platform9/express", "hash": "645d15ac387024cf5d8722f3b3fe5318cdf2a066", "msg": "K8s upgrade support and fixes for issues #211, #201, #149, #50 (#216)\n\nK8s upgrade support and fixes for issues #211, #201, #149, #50 (#216)"}, {"repo": "platform9/express", "hash": "a33945ddd063328beb513416dbda74d8a0b9e27b", "msg": "Fixes for #202, #206 (#214)\n\n* Adds Ubuntu 18.04 as valid Express client (Fixes #202)\r\n\r\n* Performs a check to ensure the existence of /dev/kvm prior to deployment (Addresses #206)\r\n\r\n* Updated README and k8s-tool to reflect Ubuntu 18.04 support."}, {"repo": "platform9/express", "hash": "c11cd35b8cdba6cf5c1ee7c0833563f56fbd3166", "msg": "Resolution of issue #184, #207, #203, #201, & #212 (#213)\n\n* Fix for issue #184 and #207\r\n\r\n* Fix for issue #203\r\n\r\n* Fix for issue #201\r\n\r\n* Fix for issue #212"}, {"repo": "platform9/express", "hash": "3542329a5fa0f31e778fa48b0c766116a98b27cf", "msg": "Multiple bug fixes, initial multipath support, and Ubuntu bond support (#194)\n\nMultiple bug fixes, initial multipath support, and Ubuntu bond support"}, {"repo": "platform9/express", "hash": "7794a913077d3b4f518bef483f882bf5bb58ed64", "msg": "Fixes for issue #157, #167, #168, #170, and minor clean-up (#171)\n\n* Fixes issue #168, #167, and other various clean-up items\r\n\r\n* Fix for issue #170\r\n\r\n* Fix for issue #157"}, {"repo": "platform9/express", "hash": "9ab4bed21aac63e5836ca1d93b465c99cc9489a8", "msg": "Platform9 Express Release v1.0 (#154)\n\nMany months of hard work from Dan Wright and some contributions from Matt McCarrell, Josh Kelly, and Jeremy Brooks."}, {"repo": "trailofbits/algo", "hash": "0031d2809ed19b2705f1a89f317bb2ca827bbd45", "msg": "Disable the Signature Algorithm check and add default vars. Fixes #525"}, {"repo": "trailofbits/algo", "hash": "0b05ea19bc253e479fff585c0be7e5b23d934b30", "msg": "Windows needs SHA2-256. Closes #453. (#456)"}, {"repo": "trailofbits/algo", "hash": "045ff4bb9f4720739632aac2951fbf2798a99c8c", "msg": "Azure security group. Fixes #264"}, {"repo": "trailofbits/algo", "hash": "a9dd0af3fe0b5a70ed74a7be204d616df9574023", "msg": "resolves #176 + other ec2 env issues"}, {"repo": "trailofbits/algo", "hash": "37ec574d8d4aa85f0e73c39651c2e3fb0f5afc2f", "msg": "IP_subject_alt_name is not declared for localhost. Fixed #149"}, {"repo": "trailofbits/algo", "hash": "d55878147327df3e8902c2b6e18dd90f92ce7b2b", "msg": "dirty fix #148"}, {"repo": "trailofbits/algo", "hash": "abafe1581c6c78d784cf215b214947675d49eff8", "msg": "Fixed #147"}, {"repo": "trailofbits/algo", "hash": "437d659eb638545a26395c75f374ab61cc8c95fb", "msg": "resolves #126 - incorrect private key usage w/o ssh-agent"}, {"repo": "trailofbits/algo", "hash": "1f8e33774e7b1f8cd4fc5f6ebe4f008b6e163d27", "msg": "service fixed #78"}, {"repo": "CentOS-PaaS-SIG/linchpin", "hash": "876497a83607d20e61bd741b3d0f1e60671c6e48", "msg": "Updated schema reference in tests and adjusted setup.py to reflect the optional dependecies w.r.to new layout"}, {"repo": "Oefenweb/ansible-php-cli-ondrej", "hash": "817ac5b5c92e4a60a01846a7be5ea35d646190be", "msg": "Make it possible to install but disable add module (#4)"}, {"repo": "rhevm-qe-automation/ovirt-ansible", "hash": "4548da074b4da282e0daf4f61ec9108872e330b6", "msg": "ovirt-guest-agent: remove variable from handler name (#135)\n\nIt fails for different value from defaults"}, {"repo": "rhevm-qe-automation/ovirt-ansible", "hash": "2d35a85231c950b50f78a8c04d4d9b2cc558f1f1", "msg": "Fixed ovirt-engine-cleanup role (#105)\n\n* Fixed ovirt-engine-cleanup role\r\n- fixed answerfile 4.x\r\n- added link to main readme\r\n- added example to /examples\r\n\r\n* Enabled test for role ovirt-engine-cleanup\r\n\r\n* Fixed test and default variables\r\n\r\n* Fixing test"}, {"repo": "MindPointGroup/RHEL7-STIG", "hash": "0cbe400afd56a816e0e48761bc583c129b8fbaf6", "msg": "fix RHEL-07-010010 (#190)"}, {"repo": "CoffeeITWorks/ansible_burp2_server", "hash": "8799cbb9d7fbde063757a40ac89a2052643b8b9e", "msg": "fixes #1"}, {"repo": "moodlebox/moodlebox", "hash": "bc5b21a82dbcb53b49f8844b4ca3b7e15621aa33", "msg": "Update Moodle directory permissions\n\n- This enables the `moodlebox` user to replace the Moodle source directory.\n- Should fix #92."}, {"repo": "anthcourtney/ansible-role-cis-amazon-linux", "hash": "da521ee1d5a6577b12f9b4eecce45b3a6286f03a", "msg": "Fixes #36"}, {"repo": "aalaesar/install_nextcloud", "hash": "ee9c946e5afc90f10c3259e229c524ce0ce8d1bf", "msg": "glue from fix #22 and Add_apps #19 branches\n\nfix #19, fix #22"}, {"repo": "aalaesar/install_nextcloud", "hash": "b10887f8627d4e45191cb3d0e0174646ff8d22f9", "msg": "Improved Mysql first installation and password definition -fixed #1"}, {"repo": "lae/ansible-role-proxmox", "hash": "0bc3361bc5ff74f8eded44168cb0a0f5306ba6ea", "msg": "Add missing arguments to user creation step. Fixes #80"}, {"repo": "lae/ansible-role-proxmox", "hash": "5ef44db3516be029ec0ca758a34022a9ce4a6f4a", "msg": "PVE upgrades need to be run with dist-upgrade, closes #70"}, {"repo": "lae/ansible-role-proxmox", "hash": "cfa5685761f10a2f423aee65e2c5de4a7d653cc7", "msg": "Don't install postfix\n\nHard-coding an MTA to install prevents users of this role from using an\nalternative MTA of their choice.\n\nWhile an MTA is required to use Proxmox VE there is already a dependency\non the mail-transport-agent meta-package in the Proxmox packages which\nensures that this requirement is satisfied. Leave the choice of which MTA\nto use to the user and don't hard-code it in this role."}, {"repo": "lae/ansible-role-proxmox", "hash": "d3afb2af6d65f33d5091b01590d2cbd9ebf98c6b", "msg": "Update subscription removal patches for PVE 4.x/5.x, fixes #24"}, {"repo": "redhat-cop/casl-ansible", "hash": "157ca3c14314ff91b6450c98dd50f073d1427cee", "msg": "Fixing inclusion in provision playbook (#207)"}, {"repo": "redhat-cop/casl-ansible", "hash": "716709f913e6b14c764211ab934b74b13f7a097b", "msg": "Chaning failed_when logic so it only ignores 'NotFound' errors when *\u2026 (#185)\n\n* Chaning failed_when logic so it only ignores 'NotFound' errors when *_action=delete\r\n\r\n* Remove 'NotFound' check and pass --ignore-not-found on delete instead\r\n\r\n* Include *_action to Task name\r\n\r\n* adding leading space before flag"}, {"repo": "redhat-cop/casl-ansible", "hash": "f92ebbef856e59bd4563d4b5b69ee75a95ec89d5", "msg": "fix dup registry on OSP (#168)\n\n* fix dup registry on OSP\r\n\r\n* registry dup v2\r\n\r\n* documentation cleanup"}, {"repo": "redhat-cop/casl-ansible", "hash": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "msg": "Previous ignore_errors statement didn't work; fixing. (#167)\n\n* Previous ignore_errors statement didn't work; fixing.\r\n\r\n* Adding ignore_errors back after rebase"}, {"repo": "redhat-cop/casl-ansible", "hash": "39ff27b8574468d3575901c9eabffb670e0bbec4", "msg": "Improved URL checking for openshift-applier (#172)\n\n* Improved URL checking for openshift-applier\r\n\r\n* Improved URL checking for openshift-applier"}, {"repo": "redhat-cop/casl-ansible", "hash": "3b06f0cf5d76bff1932967f8e32314122d54e933", "msg": "Merge fix for display name of tasks (#181)\n\n* Fixing merge issue for displayed name\r\n\r\n* Fixing merge issue for displayed name"}, {"repo": "redhat-cop/casl-ansible", "hash": "f2b553f0aa4dcae57380a89d56f9031021f44cf4", "msg": "Updating default file/template actions (#178)"}, {"repo": "redhat-cop/casl-ansible", "hash": "154b10ea64a17841f8609c3e048d3da9dc69ca2b", "msg": "AWS: Eliminate dynamic hosts file + various clean-up (#149)\n\n* AWS provisioning\r\n\r\n* AWS provisioning\r\n\r\n* AWS provisioning"}, {"repo": "redhat-cop/casl-ansible", "hash": "660d837b44e084a3331e402d7f894ce1c6d0d6e7", "msg": "fix variable name (#140)"}, {"repo": "pingcap/tidb-ansible", "hash": "22483491a4c1129df2dccc0278874e6e76c28056", "msg": "fix 404 url (#1163)\n\n* fix 404 url\r\n\r\n* update ntp check url"}, {"repo": "pingcap/tidb-ansible", "hash": "d3c78d3a219ac7aa3cf286c14efe7dbdd2b6334d", "msg": "reduce lightning config and optimize `tikv_importer_port` config (#1107)\n\n* reduce lightning config\r\n\r\n* optimize tikv_importer_port config in lightning.toml"}, {"repo": "pingcap/tidb-ansible", "hash": "1619145ad8c2039ba245d0d7b2e3ebaac06abe31", "msg": "fix TiDB alert rules (#960)\n\nSigned-off-by: Lonng <heng@lonng.org>"}, {"repo": "pingcap/tidb-ansible", "hash": "83fb902f55f7425ae7b2893954f2f5b040d06415", "msg": "Update disk space check modle (#937)"}, {"repo": "pingcap/tidb-ansible", "hash": "e141d7391b28d53eaacc32fce4a2612d9d6c0d0d", "msg": "Update tidb configuration (#932)\n\n* Update tidb configuration"}, {"repo": "pingcap/tidb-ansible", "hash": "46b1272405afefc8290888e6b0625b4d12584a4b", "msg": "fix pessimistic-txn config of tikv (#910)\n\nSigned-off-by: youjiali1995 <zlwgx1023@gmail.com>"}, {"repo": "erjac77/ansible-module-f5bigip", "hash": "9e2f139ea6da3143b822ff11e91513eb47017fae", "msg": "Fix #127 and add some improvements to Virtual component"}, {"repo": "fubarhouse/ansible-role-golang", "hash": "473bab1042b717eb6a6641b7240516af4dbae4d8", "msg": "Fixes #51.\n\nSigned-off-by: Karl Hepworth <karl.hepworth@gmail.com>"}, {"repo": "lae/ansible-role-netbox", "hash": "0ec740e4c0f341fa0121bbeee1d4ece017cec1f8", "msg": "Configure reports to be stored in a shared directory by default"}, {"repo": "lae/ansible-role-netbox", "hash": "f958689395b3fc98cacf0c605ed7b8b4b19718da", "msg": "Configure MEDIA_ROOT to point to shared directory (fixes #19)"}, {"repo": "naftulikay/ansible-role-degoss", "hash": "b8419f5dedf8fa406f9fd254532978dda9fed5f4", "msg": "Remove Includes"}, {"repo": "naftulikay/ansible-role-degoss", "hash": "64dcc395a862ce401bb7757882e3f6bc410cb283", "msg": "Bugfix: Treat Goss Output as a Dictionary"}, {"repo": "redhat-nfvpe/kube-ansible", "hash": "30b17a34f7bcaa12c2d50ccd5f569c25b75e06d6", "msg": "Support cri-o with kuberentes 1.11 in Fedora\n\nChange the followings for cri-o support in k8s v1.11:\n+ Fix cri-o related go code path\n+ Fix /etc/crio/crio.conf to latest crio\n+ Change '--skip-preflight-checkes for its deprecation\n+ Add crio related option in kubeadm init/join"}, {"repo": "viasite-ansible/ansible-role-zsh", "hash": "71399b81854303b53a55c365869adde0b5151d46", "msg": "fix: make working command conditions on clean centos\n\nCentos container don't have `which` command,\non Ubuntu `command -v` returns `command not exists` while playbook.\n\nSo, I'm using both commands.\n\nCloses #8"}, {"repo": "viasite-ansible/ansible-role-zsh", "hash": "f581c7b9b12baa881bb60098dcafcc75f1a90b93", "msg": "fix: skip early compinit execute on Ubuntu\n\nCloses #9"}, {"repo": "oVirt/ovirt-ansible", "hash": "c87c62a285d61aeb918e02621f61f300242b9b0a", "msg": "ovirt-vm-infra: Wait for IP address to appear (#116)\n\nPreviously we just waited for device to appear rather than waiting for\r\nthe actual IP to appear. This patch fixes it."}, {"repo": "oVirt/ovirt-ansible", "hash": "0669be7fcd9947be37c5e493be1eb95c401653b7", "msg": "Don't use check_mode for facts tasks in cluster-upgrade (#105)\n\nFixes: https://github.com/oVirt/ovirt-ansible/issues/102"}, {"repo": "oVirt/ovirt-ansible", "hash": "b9ba3888a5808269ee15d2c524bb9794573f85d0", "msg": "ovirt-host-deploy-firewalld: Don't remove IPtables package (#97)\n\nFixes: https://github.com/oVirt/ovirt-ansible/issues/96"}, {"repo": "oVirt/ovirt-ansible", "hash": "1a4c6f4bb0da458c0241762964458f4834e62bdd", "msg": "Add suffix to modules with bug fixes (#92)\n\nThis patch add a suffix internal_{ansible_version} to all modules in\r\nlibrary. The {ansible_version} indicates which version of ansible\r\nprovides the fix, which we need to ship with the role.\r\n\r\nSo if Ansible {ansible_version} is released we can remove the module\r\nfrom library and rename the task to the offical upstream module.\r\n\r\nRelated-to: https://bugzilla.redhat.com/show_bug.cgi?id=1487082"}, {"repo": "redhat-cop/infra-ansible", "hash": "83a79a9f8c1054117e69e39e9e562803241dc7bc", "msg": "Fixed a few minor things in the postgres role (#233)\n\n* Rearranged variables\r\n\r\n* Fixed logic when to generate passwords\r\n\r\n* Fixed firewall port variables and firewalld command\r\n\r\n* Fixed Firewalld config command\r\n\r\n* Fixed quay service name\r\n\r\n* Fixed service name\r\n\r\n* Fixed port variable and firewalld command\r\n\r\n* Fixed location of inventory\r\n\r\n* Fixed postgres service name\r\n\r\n* Fixed template names\r\n\r\n* Fixed port variable names, fixed firewalld command\r\n\r\n* Minor formatting updates (quay -> Quay)\r\n\r\n* Fixed quay service variable name\r\n\r\n* Added become:True where appropriate, changed database parameters to MySQL database, added postgresql_admin_user to postgres tasks, removed block around clair database, added logic to set quay hostname for clair when lb defined"}, {"repo": "redhat-cop/infra-ansible", "hash": "3e8a830bb525d818ed2aa2cdb10c8589e5e2ceba", "msg": "Fix IDM User Management Errors (#220)"}, {"repo": "redhat-cop/infra-ansible", "hash": "3f42756b76863c3da4f8fc162b4a87b861f3d22b", "msg": "adding libselinux-python as it is required to run the role on Fedora (#215)"}, {"repo": "redhat-cop/infra-ansible", "hash": "edfce1fb69941dcf8ac31492f3e7f6a2163712fc", "msg": "syntax error in infrahost.yml playbook (#213)"}, {"repo": "redhat-cop/infra-ansible", "hash": "00aceee9223dab8577f18e9b6b138c40c73c4aa0", "msg": "Update IDM User Management (#201)"}, {"repo": "redhat-cop/infra-ansible", "hash": "e005198769366648182cf2c2a94a8e6de9604e31", "msg": "Fix conditional logic for generating variables without values (#202)"}, {"repo": "redhat-cop/infra-ansible", "hash": "b9caaa902aa45302d657d91d4e6be9eab7bf8ba1", "msg": "Adding missing SELinux dependency (#199)"}, {"repo": "redhat-cop/infra-ansible", "hash": "ce705a0324c6923baea60e85dddbf400463f7379", "msg": "Update manage_atlassian_user playbook (#194)\n\n* edit in the api calls\r\n\r\n* use urlencode intead of replace"}, {"repo": "redhat-cop/infra-ansible", "hash": "1f1f23cbb8e64951e4c50f46e1566b37d18b4ff3", "msg": "Update config-vnc-server variable syntax (#190)"}, {"repo": "redhat-cop/infra-ansible", "hash": "5ae4b0fb96129579010e97b8b2f8db444894687f", "msg": "syntax correction (#188)"}, {"repo": "redhat-cop/infra-ansible", "hash": "b322b4a612717a42586f301a10154ea4c3020aa6", "msg": "OSP related fixes found during OSP12 work (#141)\n\n* OSP related fixes found during OSP12 work\r\n\r\n* OSP related fixes found during OSP12 work"}, {"repo": "redhat-cop/infra-ansible", "hash": "c628222bff7cafae399e00cf6adc5a933427f5b0", "msg": "Update Handle name (#137)"}, {"repo": "redhat-cop/infra-ansible", "hash": "f2a6b3dd06f01d16034d496cf025ff22ecbbe4e1", "msg": ".novalocal workaround (#106)\n\n* .novalocal workaround\r\n\r\n* Updated .novalocal workaround to be more robust"}, {"repo": "kairen/kubeadm-ansible", "hash": "a1e8fe0d02968f739cf1f2c69ac1de5f2f2d3615", "msg": "Fix insecure registries length check and add docs (#53)"}, {"repo": "iiab/iiab", "hash": "8946ed1f039607ef877361a3911084f9cb2c177e", "msg": "CP started dnsmasq can't shut it down now, resolvconf set /etc/resolv.conf to 127.0.0.1"}, {"repo": "iiab/iiab", "hash": "1f156d55f715e1c5d5562728f11cbb7084c1f9d8", "msg": "index order?"}, {"repo": "iiab/iiab", "hash": "eb39494f1800176a729e3ea7a8d5f2b144256b9d", "msg": "handle /etc/init.d/uwsgi behavior"}, {"repo": "iiab/iiab", "hash": "6a1a826f6f0d3ce1b7d9c7444057810c675e2144", "msg": "move install of iiab_env.py into base (#1304)"}, {"repo": "iiab/iiab", "hash": "5c9ad5a9b451575b838963594b87293cbbac829d", "msg": "Adding 'no-cache-dir' extra argumuments\n\nPerhaps using no-cache-dir will preview out-of-memory errors on\nRaspberry Pi.\n\nFixes #908"}, {"repo": "iiab/iiab", "hash": "0706e94f894a63f9694dcff3fbc9fcca3b3b3c37", "msg": "typos in when clause"}, {"repo": "iiab/iiab", "hash": "bae721cb242a0c58cafe026dac3bf768283d2144", "msg": "remove \"OS not supported\" broken conditional\n\nThis (flawed) error msg is universally deceptive/distracting to new installers."}, {"repo": "iiab/iiab", "hash": "6877bb2ac2bb786df30edf0f6f766c43663dd60d", "msg": "typo eth0 (#251)\n\ncorrect service name\r\n\r\nlets shut-up systemd\r\n\r\nkeep wifi variables with networking - remove unused variable"}, {"repo": "iiab/iiab", "hash": "ed2b5d78f20099e1a2ce2cc66bc3bdd408ddb9df", "msg": "just move our config to interfaces.d (#247)\n\n* just move our config to interfaces.d\r\n\r\n* debian needs same fix as rpi\r\n\r\n* try to recover/upgrade from putting wrong content in interfaces -- restore it\r\n\r\n* modify dhcpcd.conf for gui static changes\r\n\r\n* cannot comment out jinja2 with #, need to wipe the {%\r\n\r\n* deny Lan for dhcpcd, will need work if wifi is used upstream\r\n\r\n* attempt to change ip from console without requiring a reboot\r\n\r\n* Revert \"attempt to change ip from console without requiring a reboot\"\r\n\r\nThis reverts commit 14c7499cdcb064c5d61a7888c90751d62adf65ff.\r\n\r\nBetter not to break an upstream connection\r\n\r\n* cleanup the iiab.j2 template --changing only comment lines"}, {"repo": "iiab/iiab", "hash": "7d576bbe0cd58eecc053af80bedea75045636f13", "msg": "Elgg upgrade 1.12.16 LTS to 2.3.3; Nextcloud - add php's zip mbstring (#202)\n\n* add mbstring for debian\r\n\r\n* add php7.0-zip\r\n\r\n* capture a few fixes\r\n\r\n* put in new sqldump"}, {"repo": "m4rcu5nl/ansible-role-zerotier", "hash": "c90f7556c2d394de74a744df7eea1c19fa69a8f0", "msg": "Resolved errors in check-mode\n\nFixes #3"}, {"repo": "oVirt/ovirt-ansible-manageiq", "hash": "c49dd73158529cc88738a963f7ff3a73a1e8fe43", "msg": "Ensure VM is running before checking ManageIQ API\n\nFixes: https://github.com/oVirt/ovirt-ansible-manageiq/issues/20\n\nChange-Id: I837e9d3fc72d50d299de461b329187760bda1790\nSigned-off-by: Ondra Machacek <omachace@redhat.com>"}, {"repo": "trombik/ansible-role-opensmtpd", "hash": "a5e6905609ca0cf8e89cde3cfea386460465b672", "msg": "[feature] support db tables and OpenBSD 6.2 (#6)\n\n* [feature] support db tables and OpenBSD 6.2\r\n\r\nalso:\r\n\r\n* import `equalto` test for 6.0\r\n\r\n* document opensmtpd_virtual_user, opensmtpd_tables\r\n\r\nfixes #5\r\n\r\n* i did not remember which key is mandatory, obviously"}, {"repo": "cloudalchemy/ansible-node-exporter", "hash": "2477fe81d0a4485e34eac332ac4e064a742e01b5", "msg": "tasks: do not use alias for createhome as it seems to be broken (#111)"}, {"repo": "ansible-ThoTeam/nexus3-oss", "hash": "b43cdff7b74a3a2547f34d3feed0245356504f26", "msg": "Get blob list from var, not statically (#256)"}, {"repo": "ansible-ThoTeam/nexus3-oss", "hash": "e959cc3fce5f47966dd7246b6a6840b26a1c07b6", "msg": "Compat work for nexus 3.21 and molecule 3.0 (#247)\n\n* CI now uses molecule 3.0\r\n* Cross-platform test_groovySyntax.sh (fix for MacOS)\r\n* Fix repo creation for Nexus 3.21+\r\n  (integration of #246 + idempotency)\r\n* Backward compatibility for content selectors\r\n* Content selectors creation script is now idempotent\r\n\r\nCo-authored-by: Oleksii <oleksiy.khilkevich@gmail.com>"}, {"repo": "ansible-ThoTeam/nexus3-oss", "hash": "4a3d04c78b41e18d33f61619324e7454b2e9ee37", "msg": "Nexus Onboarding Wizard control (#220)\n\nLogic to control nexus onboarding wizard display for nexus >= 3.17.0.\r\nAdded the `nexus_onboarding_wizard` var to control whether the onboarding wizard\r\nshows on first admin login (default false)\r\nFixes #192"}, {"repo": "ansible-ThoTeam/nexus3-oss", "hash": "c41f90af0cb4c067ee8d736d7e8e293d4f79a7ab", "msg": "Add a check admin.password file existance (#199)\n\nAdd check for admin.password file existance so that\r\nwe read it only if it exists. This fixes the admin password\r\nchange after first installation.\r\n\r\nFixes #194"}, {"repo": "nuriel77/iri-playbook", "hash": "24464726359290cd6f80818fa1414ef7cd81b3ab", "msg": "Docker upgrade/haproxy2.0 (#114)\n\n* Upgrade to haproxy 2.0, support multi address bind for haproxy\r\n\r\n* Remove quotes from haproxy port number\r\n\r\n* Use ssl.cfg for nginx validate"}, {"repo": "nginxinc/ansible-role-nginx", "hash": "9fba82755ffd3fcdc5ac76c8834d50743d393e17", "msg": "Add OS family check (#224)"}, {"repo": "nginxinc/ansible-role-nginx", "hash": "d661ff4db95eb15af26d13719f1a90b01ae38a06", "msg": "Only add Debian repositories when installing role in amd64 archs (#212)"}, {"repo": "nginxinc/ansible-role-nginx", "hash": "50a6338a356a74929dd861cf604f7a99470cc940", "msg": "apt update (after installation from the nginx repo) gives an err\u2026 (#211)"}, {"repo": "nginxinc/ansible-role-nginx", "hash": "3b6c744166c11fea88bee052c9b6c6c5ef8e0e47", "msg": "Delete NGINX Plus repository when license is deleted (#204)\n\nThis fixes a bug when only the license is deleted, making system updates fail since the NGINX Plus repository is still within the repositories list yet there is no license to authenticate to the repository"}, {"repo": "nginxinc/ansible-role-nginx", "hash": "59e0170313922c2092ea9754f7f89914ac359c4b", "msg": "Fix OSS Alpine installation and add Alpine test coverage (#190)"}, {"repo": "nginxinc/ansible-role-nginx", "hash": "78c21d4346349760a98a278a4d1fa5b8867744de", "msg": "Add variable to set location of NGINX status conf file (#184)"}, {"repo": "nginxinc/ansible-role-nginx", "hash": "1c9a1278b3778bed88a895e0172eb1e02386c4e8", "msg": "Reload NGINX after cleaning up configs (#174)"}, {"repo": "nginxinc/ansible-role-nginx", "hash": "af54ab1401717161aa37783bf2ed71f0d28d714d", "msg": "Fix module installation when with plus (#162)\n\n* Fixes module installation when nginx_delete_license is set to true with plus license"}, {"repo": "nginxinc/ansible-role-nginx", "hash": "a7d3164ba116b22204439e0e8839083e1fb732b2", "msg": "Amplify installation fixes (#113)\n\n* Remove syntax error\r\n\r\nWhen evaluated (i.e. when nginx_amplify_enable=true and\r\nnginx_amplify_api_key is defined) this line causes a syntax error\r\n\r\nTo check for empty string, opted for temporary solution outlined in https://github.com/ansible/ansible-lint/issues/457\r\n\r\n* Fix reference to amplify api key\r\n\r\nPreviously was labelled `amplify_api_key`. Changed to\r\n`nginx_amplify_api_key` in #a55a50d\r\n\r\n* Fix reference to default config file\r\n\r\nOn my system at least (bionic), the default configuration file is\r\n`/etc/amplify-agent/agent.conf.default`\r\n\r\nThis is also assumed be to the case on the amplify install shell script https://github.com/nginxinc/nginx-amplify-agent/blob/master/packages/install.sh#L526"}, {"repo": "nginxinc/ansible-role-nginx", "hash": "8ead2b7c009befb731abf21a65b288ac9eb30987", "msg": "Install RTMP only when using NGINX Plus (#91)\n\n* Install RTMP only when using NGINX Plus\r\n\r\n* Make GeoIP and NJS module tasks idempotent"}, {"repo": "nginxinc/ansible-role-nginx", "hash": "6693791de25f89da90dd2227c5addba384b67d90", "msg": "Fix module install task (#89)\n\n* Fix module install task\r\n\r\n* Fix perl module"}, {"repo": "kiali/kiali", "hash": "c78f195fcbe214e56a918d616ed2f241be24c968", "msg": "Provide a istio-system services whitelist for jaeger tracing integration (#2225)\n\n* Provide a istio-system services whitelist for jaeger tracing integration\r\n\r\n* Add it in the operator"}, {"repo": "RedHatOfficial/ansible-redhat_cloudforms", "hash": "a38f8c316fb36ba48d925c686343e788ffdfca80", "msg": "provision-service.yml - switch to querying for 'request_tasks' because it is compatible with both 5.8 and 5.9 (#32)"}, {"repo": "RedHatOfficial/ansible-redhat_cloudforms", "hash": "948a64d2ccf9b07942c0ecd68f6e14e7c4234664", "msg": "provision-service.yml - add request_tasks to expanded because its needed for 5.9 (#31)"}, {"repo": "redhat-cop/openshift-applier", "hash": "f375d862c35ef1d28683a9979b32000917b12648", "msg": "Fixing issue when no params file (#84)\n\n* Fixing issue when no params file\r\n\r\n* Fixing CI / link error\r\n\r\n* Fixing the 'one-time' run"}, {"repo": "redhat-cop/openshift-applier", "hash": "3dae9cd54e7f8b1706a8de5f91338ded600a4127", "msg": "Revert variable to target_namespace (#42)"}, {"repo": "ChromaticHQ/ansible-deploy-drupal", "hash": "d8a40e3d6b082e94ea554ea0417c30a7a31b7686", "msg": "Adjust files directory sub-folder permissions after site install. (#19)"}, {"repo": "IBM/ansible-role-infosvr", "hash": "9dc8b49d0c77ae5991c422868fc9a9262ce5f590", "msg": "Checks JDK patch is downloaded before attempting to apply to fix #9"}, {"repo": "openfun/arnold", "hash": "ff01ee740b16239daae300f2af1d1492e03c977c", "msg": "\ud83d\udc1b(playbook) fix JSON config file generation from jinja2 template\n\nThe deploy playbook fails to generate a ConfigMap if it contains a\nJSON file generated from a Jinja2 template.\n\nFixes #450"}, {"repo": "openfun/arnold", "hash": "8eeb5062320da1b99a47becc297a61a1099c4df4", "msg": "\ud83d\udea8(playbook) remove deprecation warnings\n\nThere are depreciation warning when running `deploy.yml` playbook."}, {"repo": "openfun/arnold", "hash": "b6ad062d7a0b26ad81a6983c4850abab063350e2", "msg": "\ud83d\udeb8(playbook) disallow switching if next stack is not deployed\n\nWhen running the `switch` playbook, the current stack will be moved to\nthe previous  route even if there  is no replacing stack  connected to\nthe next route. This can cause a service downtime.\n\nCloses #416"}, {"repo": "openfun/arnold", "hash": "a45c83be89b8c0f3190119f898cb1e96f4421e0d", "msg": "\ud83d\udc1b(playbook) fix delete_previous playbook behaviour on k8s api failure\n\nThe delete_previous playbook fails with an unclear error message if\nthe kubernetes API is not responding."}, {"repo": "openfun/arnold", "hash": "75329362ff3edb1383b7d6e2e8b2a0e1aaa9f971", "msg": "\ud83d\udc1b(playbook) avoid downtime during switch or rollback\n\nThe current switch_routes  ansible task leave the current  route in an\nundefined state  for a few  seconds when switch or  rollback playbooks\nare executed."}, {"repo": "openfun/arnold", "hash": "fec255ee3c221596c4e1fd98efa8b0c78c1975fe", "msg": "\ud83d\udc1b(tasks) remove empty volume templates when trying to create them\n\nCreating volume can be condition by the env used when the application is\ndeployed. Templates can be empty and the creation will fail. We have to\nfilter volume templates to remove empty template and then create non\nempty volumes."}, {"repo": "openfun/arnold", "hash": "0928e61bba2229bf7d25e8f2981fe24902b61723", "msg": "\ud83d\udc1b(secrets) fix private docker registry task when checking if file exists\n\nThe condition was reversed when checking if a secrets file exists with\ncredentials to private Docker registries."}, {"repo": "openfun/arnold", "hash": "ff770de11c95b066a8f5e26aabb4ff73fc888d8d", "msg": "\ud83d\ude91(playbook) handle permission issues for secrets check\n\nChecking that application secrets exists requires that the logged user\nis allowed to list them in the active namespace. Typically, a deployment\nbot has not such permission and the deployment cannot be pursued.\n\nTo mitigate this issue, we have inactivated this checking for user\nwithout permission on secrets."}, {"repo": "openfun/arnold", "hash": "53c8996be397ef50013f5dd265d2bcea866c0730", "msg": "\ud83d\udc1b(playbook) prevent variable names collision for jobs\n\nWhen submitting a new job from an included task, the \"jobs\" variable was\nredefined in the pre-jobs loop, breaking the post job loop."}, {"repo": "openfun/arnold", "hash": "90a4aa43b0e68a1b8301e97b71825b09f685d36c", "msg": "\u2728(routes) allow to force route substitution\n\nIn some cases, it is usefull to force Route object substitution\n(delete & create) as somes fields for this kind of object are immutable\n(e.g. url). This behavior is now configurable in deployments and\ndefaults to false."}, {"repo": "openfun/arnold", "hash": "1dc169b133da702b6dfc36ab334d99afd150f3f8", "msg": "\ud83d\udc1b(htpasswd) prevent new generation if file already exists\n\nWhen a htpasswd file already exists, instead of leaving it as is, the\ncreate_htpasswd playbook constantly recreats it. Hence, we handle it\nmanualy."}, {"repo": "openfun/arnold", "hash": "97931d3c63ef578c0aa6204ffc3965c016d993da", "msg": "\ud83d\udc1b(routes) fix unexpected route deletion\n\nThe current work fixes two major issues:\n\n1. We do not want to delete a route bound to a stack when we delete a\n   stack with the same deployment_stamp (this is a follow-up of #189).\n2. Forcing route patching during a deployment or a switch is not\n   required and leads to erratic behavior of the OpenShift ACME\n   application that constantly try to renew route certificate as the\n   object changed."}, {"repo": "RedHat-EMEA-SSA-Team/stc", "hash": "ce0921cf63ee0aec70d171a4ade5e2acb6739c4c", "msg": "Fix typo resolves #55"}, {"repo": "christiangda/ansible-role-amazon-cloudwatch-agent", "hash": "1fee04dad0d6c7ce2fc2818296cfeaf977bde0a5", "msg": "closed #18"}, {"repo": "christiangda/ansible-role-amazon-cloudwatch-agent", "hash": "d8b8d3b6bb0e55c7a1547b529fc0b1291f6b5d0e", "msg": "Fixed #14, and others improvements"}, {"repo": "redhat-performance/jetpack", "hash": "2b14ba3c350f7a143937c4466b92e1c4a04b9ffa", "msg": "Infrared install and python3 changes\n\nThe following changes were made in this commit\n\n1. Raise Ansible minimum version to 2.8 to deal with python3 inconsistencies\n2. Require python 3.5+ on ansible controller\n3. Install dependenices for infrared that were previously missing\n4. Remove selinux disabling on infrared controller machine\n5. Check for python3 and install if it is not present (required for badfish)\n\nCloses: #10\nSigned-off-by: Sai Sindhur Malleni <smalleni@redhat.com>"}, {"repo": "rvc-proxy/coachproxy-os", "hash": "011ef233d7c8441f532db6a30b95788541e6df5c", "msg": "Enable email notifications\n\nFixes #6"}, {"repo": "IBM-Blockchain/ansible-role-blockchain-platform-manager", "hash": "4757c2799ea447134a77226287e9a15d805ada32", "msg": "Flag creation of SaaS/software components as a change (resolves #124)\n\nSigned-off-by: Simon Stone <sstone1@uk.ibm.com>"}, {"repo": "IBM-Blockchain/ansible-role-blockchain-platform-manager", "hash": "563bfb4af3fd887fb91ad3e667689823a87e2d07", "msg": "Find components by display name in software (resolves #118)\n\nSigned-off-by: Simon Stone <sstone1@uk.ibm.com>"}, {"repo": "IBM-Blockchain/ansible-role-blockchain-platform-manager", "hash": "bbfa42d691af271b5790240c9eaf7c25fc9450a2", "msg": "Peer data volume creation has wrong volume name (resolves #114)\n\nSigned-off-by: Simon Stone <sstone1@uk.ibm.com>"}, {"repo": "IBM-Blockchain/ansible-role-blockchain-platform-manager", "hash": "fc73b01594c88f28c5a25a1ae088b4ca6d8fc80f", "msg": "Add Docker hostname configuration (resolves #111)\n\nSigned-off-by: Simon Stone <sstone1@uk.ibm.com>"}, {"repo": "IBM-Blockchain/ansible-role-blockchain-platform-manager", "hash": "0999df21d73ae0f8b9ab381c843c049ac34fcd11", "msg": "Delete peer data volume when state set to absent (resolves #97)\n\nSigned-off-by: Simon Stone <sstone1@uk.ibm.com>"}, {"repo": "IBM-Blockchain/ansible-role-blockchain-platform-manager", "hash": "392d3a24d313a22de0420e75f08773dccfde311c", "msg": "Fix restart checks for peers/orderers without TLS (resolves #77)\n\nSigned-off-by: Simon Stone <sstone1@uk.ibm.com>"}, {"repo": "IBM-Blockchain/ansible-role-blockchain-platform-manager", "hash": "500b61c047527a298630ec55a501ceac8b3e9cf6", "msg": "Always output files from configtxlator (resolves #20)\n\nSigned-off-by: Simon Stone <sstone1@uk.ibm.com>"}, {"repo": "IBM-Blockchain/ansible-role-blockchain-platform-manager", "hash": "98d794d408dad78a17a7c68c9e4c54132160751a", "msg": "Correct role name for Galaxy (resolves #2)\n\nSigned-off-by: Simon Stone <sstone1@uk.ibm.com>"}, {"repo": "sky-uk/clusterverse", "hash": "b9592374ccc92eba18ffec05de137bec04924240", "msg": "[FIX] Add region args as required and convert owner label to lowercase (#24)"}, {"repo": "sky-uk/clusterverse", "hash": "b91296d7845c396e2b25429f7271437b269fe86f", "msg": "[FIX] Fixed Route53 deletion with private zones (#23)"}]