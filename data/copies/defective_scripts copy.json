[{"commit_sha": "9925a1fe9dadb982563155b57b6001182f7fe617", "sha": "c727a85274b28b3668fce7125bb18b9e197c080b", "filename": "roles/manage-jira/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- include_tasks: prepare_vars.yml \n\n- include_tasks: create_project_category.yml\n\n- include_tasks: create_permission_scheme.yml\n\n- include_tasks: create_project.yml\n"}, {"commit_sha": "5c1cde6cffcd979d0292e988da029c0ece96054c", "sha": "0c50b67a8a33b9759f3c47a1eb2637176584f8ea", "filename": "tasks/pip.yml", "repository": "openwisp/ansible-openwisp2", "decoded_content": "- name: Set openwisp2_python_packages\n  set_fact:\n    openwisp2_python_packages: []\n\n- name: Set custom package list\n  set_fact:\n    openwisp2_python_packages: \"{{ openwisp2_python_packages  + [item] }}\"\n  with_items:\n    - \"{{ openwisp2_controller_pip }}\"\n    - \"{{ openwisp2_users_pip }}\"\n    - \"{{ openwisp2_django_netjsonconfig_pip }}\"\n    - \"{{ openwisp2_django_x509_pip }}\"\n    - \"{{ openwisp2_django_loci_pip }}\"\n    - \"{{ openwisp2_netjsonconfig_pip }}\"\n  when: item != false\n\n- name: Add network_topology to custom package list if set and enabled\n  set_fact: openwisp2_python_packages:\"{{ openwisp2_python_packages + [item] }}\"\n  with_items:\n    - \"{{ openwisp2_django_netjsongraph_pip }}\"\n    - \"{{ openwisp2_network_topology_pip }}\"\n  when: item != false and openwisp2_network_topology\n\n- name: Update pip & related tools\n  pip:\n    name:\n      - pip\n      - setuptools\n      - wheel\n    state: latest\n    virtualenv: \"{{ virtualenv_path }}\"\n    virtualenv_python: \"{{ openwisp2_python }}\"\n    virtualenv_site_packages: yes\n\n- name: Install cryptography from pip\n  pip:\n    name: cryptography\n    state: latest\n    virtualenv: \"{{ virtualenv_path }}\"\n    virtualenv_python: \"{{ openwisp2_python }}\"\n    virtualenv_site_packages: yes\n\n- name: Install openwisp2 controller and its dependencies\n  pip:\n    name:\n      - openwisp-controller\n      - asgi_redis\n      - service_identity\n    state: latest\n    virtualenv: \"{{ virtualenv_path }}\"\n    virtualenv_python: \"{{ openwisp2_python }}\"\n    virtualenv_site_packages: yes\n  notify: reload supervisor\n\n- name: Install django-redis\n  pip:\n    name: \"django-redis>=4.9.0\"\n    state: present\n    virtualenv: \"{{ virtualenv_path }}\"\n    virtualenv_python: \"{{ openwisp2_python }}\"\n    virtualenv_site_packages: yes\n  notify: reload supervisor\n\n- name: Install openwisp2 network topology and its dependencies\n  when: openwisp2_network_topology\n  pip:\n    name:\n      - openwisp-network-topology\n    state: latest\n    virtualenv: \"{{ virtualenv_path }}\"\n    virtualenv_python: \"{{ openwisp2_python }}\"\n    virtualenv_site_packages: yes\n  notify: reload supervisor\n\n- name: Install custom OpenWISP 2 Python packages\n  pip:\n    name: \"{{ item }}\"\n    state: latest\n    virtualenv: \"{{ virtualenv_path }}\"\n    virtualenv_python: \"{{ openwisp2_python }}\"\n    virtualenv_site_packages: yes\n  with_items: \"{{ openwisp2_python_packages }}\"\n  notify: reload supervisor\n\n- name: Install extra python packages\n  pip:\n    name: \"{{ item }}\"\n    state: latest\n    virtualenv: \"{{ virtualenv_path }}\"\n    virtualenv_python: \"{{ openwisp2_python }}\"\n    virtualenv_site_packages: yes\n  with_items: \"{{ openwisp2_extra_python_packages }}\"\n  notify: reload supervisor\n\n- name: Install uwsgi\n  pip:\n    name:\n      - uwsgi\n    state: latest\n    virtualenv: \"{{ virtualenv_path }}\"\n    virtualenv_python: \"{{ openwisp2_python }}\"\n    virtualenv_site_packages: yes\n\n  notify: reload supervisor\n\n- name: Install psycopg2\n  when: openwisp2_database.engine in [\"django.db.backends.postgresql\", \"django.contrib.gis.db.backends.postgis\"]\n  pip:\n    name: psycopg2\n    state: latest\n    virtualenv: \"{{ virtualenv_path }}\"\n    virtualenv_python: \"{{ openwisp2_python }}\"\n    virtualenv_site_packages: yes\n  notify: reload supervisor\n\n- name: Install MySQL-python\n  when: openwisp2_database.engine in [\"django.db.backends.mysql\", \"django.contrib.gis.db.backends.mysql\"]\n  pip:\n    name: MySQL-python\n    state: latest\n    virtualenv: \"{{ virtualenv_path }}\"\n    virtualenv_python: \"{{ openwisp2_python }}\"\n    virtualenv_site_packages: yes\n  notify: reload supervisor\n\n- name: Install raven (sentry client)\n  when: openwisp2_sentry.get('dsn') != False\n  pip:\n    name: raven\n    state: latest\n    virtualenv: \"{{ virtualenv_path }}\"\n    virtualenv_python: \"{{ openwisp2_python }}\"\n    virtualenv_site_packages: yes\n  notify: reload supervisor\n"}, {"commit_sha": "5c1cde6cffcd979d0292e988da029c0ece96054c", "sha": "45e5e79f2db5c5d1d4b7c1887d3093ab82c3967c", "filename": "tasks/yum.yml", "repository": "openwisp/ansible-openwisp2", "decoded_content": "---\n\n- name: Install epel-release (if RedHat/Centos)\n  when: ansible_distribution in ['RedHat', 'CentOS']\n  yum: name=epel-release state=latest\n  notify: reload systemd\n\n- name: Install system packages\n  package: name={{ item }} state=latest\n  notify:\n    - reload systemd\n    - start redis\n  with_items:\n    - sudo\n    - gcc\n    - supervisor\n    - nginx\n    - openssl\n    - openssl-devel\n    - libffi-devel\n    - python-devel\n    - redis\n    - redhat-rpm-config\n    - cronie\n\n- name: Install python2 system packages\n  when: ansible_python_interpreter == \"/usr/bin/python\"\n  package: name={{ item }} state=latest\n  notify:\n    - reload systemd\n  with_items:\n    - libsemanage-python\n    - policycoreutils-python\n\n- name: Install python3 system packages\n  when: ansible_python_interpreter == \"/usr/bin/python3\"\n  package: name={{ item }} state=latest\n  notify:\n    - reload systemd\n  with_items:\n    - libsemanage-python3\n    - policycoreutils-python3\n\n# On the newer versions of redis, by default redis\n# binds to localhost on ipv6 address which wouldn't\n# let the service start if the server doesn't have\n# ipv6 enabled. Hence, we set redis to listen on ipv4\n- name: set redis to listen on ipv4 (RedHat & CentOS)\n  notify: start redis\n  lineinfile:\n    path: /etc/redis.conf\n    regexp: '^bind 127\\.0\\.0\\.1 ::1'\n    line: 'bind 127.0.0.1'\n    backrefs: yes\n\n# Fedora 28 and above drops the package python-firewall\n# and it gets replaced by python3-firewall\n\n- name: Install python2 firewall packages\n  when: ansible_python_interpreter == \"/usr/bin/python\"\n  package: name={{ item }} state=present\n  notify: reload systemd\n  with_items:\n    - python-firewall\n    - firewalld\n\n- name: Install python3 firewall packages\n  when: ansible_python_interpreter == \"/usr/bin/python3\"\n  package: name={{ item }} state=present\n  notify: reload systemd\n  with_items:\n    - python3-firewall\n    - firewalld\n\n- name: Install spatialite\n  when: openwisp2_database.engine == \"django.contrib.gis.db.backends.spatialite\"\n  package: name={{ item }} state=latest\n  notify: reload systemd\n  with_items:\n    - sqlite\n    - gdal\n    - proj-devel\n    - geos-devel\n    - libspatialite-devel\n\n- name: ensure supervisor is started\n  service: name=supervisord state=started enabled=yes\n\n- name: ensure nginx is enabled\n  service: name=nginx enabled=yes\n\n- name: Install python2 packages\n  when: openwisp2_python in [\"python2.7\", \"python2\"]\n  package: name={{ item }} state=latest\n  with_items:\n    - python-pip\n    - python-virtualenv\n\n- name: Install python3 packages\n  when: openwisp2_python == \"python3\"\n  package: name={{ item }} state=latest\n  with_items:\n    - \"{{ python3_package_prefix }}\"\n    - \"{{ python3_package_prefix }}-pip\"\n    - \"{{ python3_package_prefix }}-devel\"\n\n- name: Install python wheel (optional, allowed to fail)\n  ignore_errors: yes\n  package: name={{ item }} state=latest\n  with_items:\n    - python-wheel\n    - \"{{ python3_package_prefix }}-wheel\"\n\n- name: Install python3 virtualenv\n  when: openwisp2_python == \"python3\" and ansible_distribution == 'Fedora'\n  package:\n    name: \"{{ python3_package_prefix }}-virtualenv\"\n    state: latest\n\n- name: (CentOS/RedHat) Always install pip\n  when: ansible_distribution in ['RedHat', 'CentOS']\n  package:\n    name: python-pip\n    state: latest\n\n- name: (CentOS/RedHat/Fedora) Install/upgrade virtualenv via pip\n  pip:\n    name: virtualenv\n    state: latest\n\n- name: Install ntp\n  when: openwisp2_install_ntp\n  package:\n    name: ntp\n    state: latest\n"}, {"commit_sha": "a8ade5b4788a5c3a0b5a65b2ca1845a65be8b451", "sha": "d9768af55f5e2ee9b5c04a49aa4c247eab0dba00", "filename": "tasks/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Set distribution facts\n  set_fact:\n    _docker_os_dist: \"{{ ansible_distribution }}\"\n    _docker_os_dist_release: \"{{ ansible_distribution_release }}\"\n    _docker_os_dist_major_version: \"{{ ansible_distribution_major_version }}\"\n    _docker_os_dist_check: yes\n  tags: [\"install\", \"configure\"]\n\n- name: Reinterpret distribution facts for Linux Mint 18\n  set_fact:\n    _docker_os_dist: \"Ubuntu\"\n    _docker_os_dist_release: \"xenial\"\n    _docker_os_dist_major_version: \"16\"\n  when:\n    _docker_os_dist == \"Linux Mint\" and\n    _docker_os_dist_major_version == \"18\"\n  tags: [\"install\", \"configure\"]\n\n# https://wiki.ubuntu.com/SystemdForUpstartUsers\n# Important! systemd is only fully supported in Ubuntu 15.04 and later releases\n- name: Determine usage of systemd\n  shell: \"ps -p1 | grep systemd 1>/dev/null && echo systemd || echo upstart\"\n  become: true\n  changed_when: no\n  register: _determine_systemd_usage\n\n- name: Set fact to indicate systemd is not used\n  set_fact:\n    _docker_systemd_used: \"{{ _determine_systemd_usage is defined and _determine_systemd_usage.stdout == 'systemd' }}\"\n\n- name: Compatibility and distribution checks\n  include_tasks: checks.yml\n  tags: [\"install\", \"configure\"]\n\n- name: Setup Docker package repositories\n  include_tasks: setup-repository.yml\n  tags: [\"install\"]\n\n- name: Remove Docker versions before Docker CE\n  include_tasks: remove-pre-docker-ce.yml\n  when: docker_remove_pre_ce | bool\n  tags: [\"install\"]\n\n- name: Install Docker\n  include_tasks: install-docker.yml\n  tags: [\"install\"]\n\n- name: Configure audit logging\n  include_tasks: setup-audit.yml\n  tags: [\"configure\"]\n\n- name: Apply workarounds for bugs and/or tweaks\n  include_tasks: bug-tweaks.yml\n  tags: [\"configure\"]\n\n- name: Configure systemd service\n  include_tasks: configure-systemd.yml\n  when: _docker_systemd_used | bool\n  tags: [\"configure\"]\n\n- name: Configure non-systemd service\n  include_tasks: configure-non-systemd.yml\n  when: _docker_systemd_used | bool == false\n  tags: [\"configure\"]\n\n- name: Configure Docker\n  include_tasks: configure-docker.yml\n  tags: [\"configure\"]\n\n- name: Postinstall tasks\n  include_tasks: postinstall.yml\n  tags: [\"install\"]"}, {"commit_sha": "9c696084249622f61f5196021d71e7e933594883", "sha": "f7db55155bd5ced5fe0b5230c387ddea28656afa", "filename": "handlers/main.yml", "repository": "CSCfi/ansible-role-cuda", "decoded_content": "---\n\n- name: ZZ CUDA Restart server\n  command: sleep 2 && /sbin/shutdown -r now \"Node software upgrade reboot\"\n  async: 1\n  poll: 0\n  ignore_errors: true\n  when: cuda_packages_installation.changed and cuda_restart_node_on_install == True\n\n- name: ZZ CUDA Wait for server to restart\n  local_action: wait_for host=\"{{ ansible_ssh_host | default(inventory_hostname) }}\" \n                state=started \n                delay=30 \n                timeout=300\n  sudo: false\n  when: cuda_restart_node_on_install == True\n"}, {"commit_sha": "d6e8fe0dad9afc88eb88e0e34a7607042552df74", "sha": "e3c641e09c82d2eed0ed5605d56432f9ab603515", "filename": "tasks/compatibility-checks.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# https://github.com/moby/moby/issues/35873\n# https://access.redhat.com/solutions/2991041\n- name: Compatibility check - Fail if both MountFlags=slave and live-restore are set\n  fail:\n    msg: >\n      Setting both `MountFlags=slave` (docker_enable_mount_flag_fix: true)\n      and `live-restore=true` (docker_daemon_config['live-restore']: true)\n      triggers a bug (https://github.com/moby/moby/issues/35873). For now,\n      don't use both.\n  when:\n    - docker_enable_mount_flag_fix\n    - docker_daemon_config['live-restore'] is defined\n    - docker_daemon_config['live-restore']\n"}, {"commit_sha": "79ddbb761b2ee3e144e3f16d3ae05346df1bda31", "sha": "f33da8555c689a07118305dd759c4365fc387fd9", "filename": "tasks/main.yml", "repository": "RocketChat/Rocket.Chat.Ansible", "decoded_content": "---\n# tasks/main.yml: Main tasks for RocketChat.Ansible\n\n  - include_vars: \"{{ item }}\"\n    with_first_found:\n      - \"{{ ansible_distribution }}.yml\"\n      - \"{{ ansible_os_family }}.yml\"\n    tags: vars\n\n  - include: repo_RedHat.yml\n    when: ansible_os_family == \"RedHat\"\n    tags: repo\n\n  - name: Ensure APT cache has been updated recently\n    apt:\n      update_cache: yes\n      #cache_valid_time: 3600\n    when: ansible_pkg_mgr == \"apt\"\n\n  - include: mongodb.yml\n    when: rocket_chat_include_mongodb|bool\n    tags: mongodb\n\n  - name: Ensure the Rocket.Chat service group is present\n    group:\n      name: \"{{ rocket_chat_service_group }}\"\n      state: present\n      system: true\n\n  - name: Ensure the Rocket.Chat service user is present\n    user:\n      comment: Rocket.Chat Service User\n      name: \"{{ rocket_chat_service_user }}\"\n      group: \"{{ rocket_chat_service_group }}\"\n      home: \"{{ rocket_chat_application_path }}\"\n      createhome: true\n      shell: /bin/false\n      state: present\n      system: true\n\n  - name: Ensure Rocket.Chat dependencies are installed\n    package:\n      name: \"{{ rocket_chat_dep_packages }}\"\n      state: present\n    retries: 2\n\n  - name: Ensure link /bin/node -> /bin/nodejs exists\n    file:\n      src: /bin/node\n      dest: /bin/nodejs\n      state: link\n    when: ansible_os_family == \"RedHat\"\n\n  - name: Ensure n (NodeJS) is installed\n    npm:\n      name: n\n      global: true\n      executable: \"{{ rocket_chat_node_orig_npm }}\"\n\n  - name: Check to see if n has installed the required 'node' binary\n    stat:\n      path: \"{{ rocket_chat_node_path }}/node\"\n    register: n_node_bin\n\n  - name: Install the supported NodeJS environment via n\n    shell: n {{ rocket_chat_node_version }}\n    when: not n_node_bin.stat.exists|bool\n\n  - name: \"Configure /etc/hosts\"\n    lineinfile:\n      dest: /etc/hosts\n      line:  \"127.0.0.1    {{ ansible_fqdn }}    {{ ansible_hostname }}\"\n      regexp: '^127.0.0.1'\n    when: ansible_virtualization_type != \"docker\"\n\n  - name: Check to see if this is the initial Rocket.Chat deployment\n    stat:\n      path: \"{{ rocket_chat_application_path }}/bundle\"\n    register: rocket_chat_deploy_state\n\n  - name: Set the initial Rocket.Chat upgrade status\n    set_fact:\n      rocket_chat_upgraded: false\n\n  - name: Ensure acl-tools are present [Ubuntu 16]\n    package:\n      name: acl\n      state: present\n    when:\n      - ansible_distribution == \"Ubuntu\"\n      - ansible_distribution_major_version == \"16\"\n\n  - name: Fetch the Rocket.Chat binary tarball\n    get_url:\n      url: \"{{ rocket_chat_tarball_remote }}\"\n      checksum: \"{{ (rocket_chat_tarball_check_checksum == false) | ternary(omit, 'sha256: ' + rocket_chat_tarball_sha256sum) }}\"\n      force: \"{{ (rocket_chat_tarball_check_checksum == false) | ternary('yes', omit) }}\"\n      dest: \"{{ rocket_chat_application_path }}/rocket.chat-{{ rocket_chat_version }}.tgz\"\n      timeout: \"{{ rocket_chat_tarball_fetch_timeout }}\"\n      validate_certs: \"{{ rocket_chat_tarball_validate_remote_cert }}\"\n    # Temp fix for ansible/ansible#15915 ( Broken include in handlers )\n    # https://github.com/ansible/ansible/issues/15915\n    #notify: Upgrade Rocket.Chat\n    become: true\n    become_user: \"{{ rocket_chat_service_user }}\"\n    register: result\n    retries: 3\n    changed_when: (result|changed)\n                  or (not rocket_chat_tarball_check_checksum)\n\n  - name: Upgrade Rocket.Chat\n    include: upgrade.yml\n    when:\n      - result|changed\n      - rocket_chat_deploy_state.stat.exists\n    tags:\n      - upgrade\n\n  - meta: flush_handlers\n\n  - name: Unpack the Rocket.Chat binary tarball\n    unarchive:\n      copy: false\n      src: \"{{ rocket_chat_application_path }}/rocket.chat-{{ rocket_chat_version }}.tgz\"\n      dest: \"{{ rocket_chat_application_path }}\"\n      creates: \"{{ rocket_chat_application_path }}/bundle\"\n    become: true\n    become_user: \"{{ rocket_chat_service_user }}\"\n    tags: build\n\n  - name: Install Rocket.Chat via NPM\n    npm:\n      state: present\n      path: \"{{ rocket_chat_application_path }}/bundle/programs/server\"\n      executable: \"{{ rocket_chat_node_orig_npm }}\"\n    become: true\n    become_user: \"{{ rocket_chat_service_user }}\"\n    tags: build\n\n  - name: Ensure the Rocket.Chat log file symlink is present [Ubuntu 14]\n    file:\n      path: /var/log/rocketchat.log\n      src: /var/log/upstart/rocketchat.log\n      state: link\n      force: yes\n\n    when:\n      - ansible_distribution == \"Ubuntu\"\n      - ansible_distribution_major_version == \"14\"\n\n  - name: Ensure the Rocket.Chat application data permissions are correct\n    file:\n      path: \"{{ rocket_chat_application_path }}\"\n      state: directory\n      owner: \"{{ rocket_chat_service_user }}\"\n      group: \"{{ rocket_chat_service_user }}\"\n      recurse: true\n    tags: build\n\n  - include_vars: \"{{ item }}\"\n    with_first_found:\n      - \"{{ ansible_distribution }}_{{ ansible_distribution_major_version }}.yml\"\n      - \"{{ ansible_os_family }}_{{ ansible_distribution_major_version }}.yml\"\n      - \"{{ ansible_distribution }}.yml\"\n      - \"{{ ansible_os_family }}.yml\"\n    tags: service\n\n  - name: Deploy the Rocket.Chat service file\n    template:\n      src: \"{{ rocket_chat_service_template.src }}\"\n      dest: \"{{ rocket_chat_service_template.dest }}\"\n    notify:\n      - Update the Rocket.Chat service configuration\n      - Restart the Rocket.Chat service\n    tags: service\n\n  - meta: flush_handlers\n\n  - name: Ensure the MongoDB replSets have been initiated\n    shell: >-\n      mongo --eval 'rs.initiate()' &&\n      touch .mongo_rs_initialised\n    become: yes\n    become_user: mongodb\n    args:\n      chdir: /var/lib/mongodb\n      creates: /var/lib/mongodb/.mongo_rs_initialised\n    when: rocket_chat_include_mongodb|bool\n\n  - name: Restart the Rocket.Chat service [UPGRADE]\n    service:\n      name: rocketchat\n      state: restarted\n    when: rocket_chat_upgraded|bool\n\n  - name: Ensure the Rocket.Chat service is running/enabled\n    service:\n      name: rocketchat\n      state: started\n      enabled: true\n    tags: service\n\n  - include: nginx.yml\n    when: rocket_chat_include_nginx|bool\n    tags: nginx\n"}, {"commit_sha": "4b928a2a11188068d46b66418db18799d3d860ad", "sha": "906324d4b3aa287478680d32743f78bf1eb9b4d2", "filename": "tasks/ivm.yml", "repository": "fubarhouse/ansible-role-nodejs", "decoded_content": "---\n\n- name: \"IVM | Ensure folder requirements are met\"\n  become: yes\n  become_user: root\n  file:\n    path: /usr/local/ivm\n    state: directory\n    mode: 0777\n    owner: root\n\n- name: \"IVM | Clone/Update\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  git:\n    repo: \"{{ ivm_repo }}\"\n    dest: \"{{ fubarhouse_npm.user_dir }}/.ivm\"\n    clone: yes\n    update: yes\n    force: yes\n    version: master\n    recursive: false\n  changed_when: false\n\n- name: \"IVM | Linking\"\n  become: yes\n  become_user: root\n  file:\n    src: \"{{ fubarhouse_npm.user_dir }}/.ivm/bin/ivm\"\n    dest: \"/usr/local/bin/ivm\"\n    state: link\n    force: yes\n  changed_when: false\n\n- name: \"IVM | Ensure shell profiles are configured\"\n  become: yes\n  become_user: \"root\"\n  lineinfile:\n    dest: \"{{ fubarhouse_npm.user_dir }}/{{ item.filename }}\"\n    regexp: 'export NVM_IOJS_ORG_MIRROR=https://iojs.org/dist'\n    line:  'export NVM_IOJS_ORG_MIRROR=https://iojs.org/dist;'\n    state: present\n  with_items:\n    - \"{{ fubarhouse_npm.shell_profiles }}\"\n  ignore_errors: yes"}, {"commit_sha": "669ef8a0840eb58b29fca67110f86c4e2d99eebc", "sha": "e18a6b05b2c9ac26ce3155146f2cc597ce22a23e", "filename": "tasks/main.yml", "repository": "geerlingguy/ansible-role-solr", "decoded_content": "---\n- import_tasks: user.yml\n  when: solr_create_user\n\n- name: Set solr_filename for Solr 4+.\n  set_fact:\n    solr_filename: \"solr-{{ solr_version }}\"\n  when: \"solr_version.split('.')[0] >= '4'\"\n\n- name: Set solr_filename for Solr 3.x.\n  set_fact:\n    solr_filename: \"apache-solr-{{ solr_version }}\"\n  when: \"solr_version.split('.')[0] == '3'\"\n\n- name: Check if Solr has been installed already.\n  stat:\n    path: \"{{ solr_install_path }}\"\n  register: solr_install_path_status\n\n- name: Download Solr.\n  get_url:\n    url: \"{{ solr_mirror }}/lucene/solr/{{ solr_version }}/{{ solr_filename }}.tgz\"\n    dest: \"{{ solr_workspace }}/{{ solr_filename }}.tgz\"\n    force: no\n  when: solr_install_path_status.stat.isdir is not defined\n  register: solr_download_status\n\n- name: Expand Solr.\n  unarchive:\n    src: \"{{ solr_workspace }}/{{ solr_filename }}.tgz\"\n    dest: \"{{ solr_workspace }}\"\n    copy: no\n  when: solr_download_status.changed\n\n# Install Solr < 5.\n- import_tasks: install-pre5.yml\n  when: \"solr_version.split('.')[0] < '5'\"\n\n# Install Solr 5+.\n- import_tasks: install.yml\n  when: \"solr_version.split('.')[0] >= '5'\"\n\n- name: Ensure solr is started and enabled on boot if configured.\n  service:\n    name: \"{{ solr_service_name }}\"\n    state: \"{{ solr_service_state }}\"\n    enabled: yes\n  when: solr_service_manage\n\n# Configure solr.\n- import_tasks: configure.yml\n  when: \"solr_version.split('.')[0] >= '5'\"\n\n# Create cores, if any are configured.\n- import_tasks: cores.yml\n  when: \"solr_cores and solr_version.split('.')[0] >= '5'\"\n\n- include_tasks: trim-fat.yml\n"}, {"commit_sha": "1b857dd321d16eae8442dac8b317135668fa2be5", "sha": "85d53f9389f83999be040bb0a1d13b2e7486efff", "filename": "tasks/configure-systemd.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Combine all systemd service configuration options\n  set_fact:\n    _systemd_service_config: \"{{ docker_systemd_service_config_tweaks + docker_systemd_service_config }}\"\n\n- name: Ensure /etc/systemd/system/docker.service.d directory exists\n  file:\n    path: /etc/systemd/system/docker.service.d\n    state: directory\n    mode: 0755\n  become: yes\n\n- name: Setup default Docker drop-in to enable use of environment file\n  template:\n    src: drop-ins/default.conf.j2\n    dest: /etc/systemd/system/docker.service.d/default.conf\n  become: yes\n  register: _systemd_docker_dropin\n  vars:\n    systemd_envs_dir: \"{{ docker_envs_dir[_docker_os_dist] }}\"\n    systemd_service_conf: \"{{ _systemd_service_config }}\"\n\n- name: Combine Docker daemon environment variable configuration\n  set_fact:\n    docker_service_envs: \"{{ docker_service_envs | combine(_docker_service_opts) | combine(docker_daemon_envs) }}\"\n  vars:\n    _docker_service_opts:\n      DOCKER_OPTS: \"{{ docker_daemon_opts }}\"\n\n- name: Setup Docker environment file {{ docker_envs_dir[_docker_os_dist] }}/docker-envs\n  template:\n    src: docker-envs.j2\n    dest: \"{{ docker_envs_dir[_docker_os_dist] }}/docker-envs\"\n  become: yes\n  notify: restart docker\n  vars:\n    docker_envs: \"{{ docker_service_envs }}\"\n\n- name: Force daemon reload of systemd\n  systemd: \n    daemon_reload: yes\n  become: yes\n  notify: restart docker\n  when: _systemd_docker_dropin|changed"}, {"commit_sha": "ef44a28a33e2fea1d2a2b61b1af060d51a8439d7", "sha": "2d26b2b711e694f8ee3e489373e90b2a9234f913", "filename": "tasks/main.yml", "repository": "angstwad/docker.ubuntu", "decoded_content": "---\n# tasks file for docker.ubuntu\n- name: Fail if not a new release of Ubuntu\n  fail: msg=\"{{ ansible_distribution_version }} is not an acceptable version of Ubuntu for this role\"\n  when: \"ansible_distribution_version not in ['12.04', '13.04', '13.10', '14.04', '14.10']\"\n  \n- name: Install raring kernel onto 12.04\n  apt:\n    pkg: \"{{ item }}\"\n    state: latest\n    update_cache: yes\n    cache_valid_time: 600\n  with_items:\n    - linux-image-generic-lts-raring\n    - linux-headers-generic-lts-raring\n  register: kernel_result\n  when: \"ansible_distribution_version == '12.04'\"\n\n- name: Install latest kernel extras for Ubuntu 13.04+\n  apt:\n    pkg: \"linux-image-extra-{{ ansible_kernel }}\"\n    state: \"{{ kernel_pkg_state }}\"\n    update_cache: yes\n    cache_valid_time: 600\n  when: \"ansible_distribution_version == '13.04' or ansible_distribution_version == '13.10'\"\n\n# Fix for https://github.com/dotcloud/docker/issues/4568\n- name: Install cgroup-lite for Ubuntu 13.10\n  apt:\n    pkg: cgroup-lite\n    state: \"{{ cgroup_lite_pkg_state }}\"\n    update_cache: yes\n    cache_valid_time: 600\n  register: cgroup_lite_result\n  when: \"ansible_distribution_version == '13.10'\"\n\n- name: Reboot instance\n  command: /sbin/shutdown -r now\n  register: reboot_result\n  when: \"(ansible_distribution_version == '12.04' and kernel_result|changed)\n      or (ansible_distribution_version == '13.10' and cgroup_lite_result|changed)\"\n\n- name: Wait for instance to come online\n  local_action:\n    module: wait_for\n    host: \"{{ ansible_ssh_host|default(inventory_hostname) }}\"\n    port: \"{{ ansible_ssh_port|default(ssh_port) }}\"\n    delay: 30\n    timeout: 600\n    state: started\n  when: \"(ansible_distribution_version == '12.04' and reboot_result|changed)\n      or (ansible_distribution_version == '13.10' and cgroup_lite_result|changed)\"\n\n- name: Add Docker repository key\n  apt_key:\n    id: \"{{ apt_key_sig }}\"\n    url: \"{{ apt_key_url }}\"\n    state: present\n\n- name: Add Docker repository\n  apt_repository:\n    repo: \"{{ apt_repository }}\"\n    update_cache: yes\n    state: present\n\n- name: Install (or update) docker\n  apt: \n    name: \"{{ docker_pkg_name }}\"\n    state: latest\n    update_cache: yes\n    cache_valid_time: 600\n\n- name: Expose docker host\n  copy:\n    content: \"DOCKER_OPTS=\\\"{{ docker_opts }}\\\"\"\n    dest: /etc/default/docker\n    owner: root\n    group: root\n    mode: 0744\n  notify:\n    - Reload docker\n  when: \"export_docker_host\"    \n  \n- name: Install pip python package\n  apt:\n    pkg: \"{{ item }}\"\n    state: latest\n    update_cache: yes\n    cache_valid_time: 600\n  with_items:\n    - python-dev\n    - python-pip\n  register: kernel_result\n\n- name: Install Docker-py\n  pip: name=docker-py   \n\n- name: Check if /etc/updatedb.conf exists\n  stat: path=/etc/updatedb.conf\n  register: updatedb_conf_exists\n\n- name: Ensure updatedb does not index /var/lib/docker\n  shell: >\n    ex -s -c '/PRUNEPATHS=/v:/var/lib/docker:s:\"$: /var/lib/docker\"' -c 'wq' /etc/updatedb.conf\n  when: updatedb_conf_exists.stat.exists\n\n- name: Check if /etc/default/ufw exists\n  stat: path=/etc/default/ufw\n  register: ufw_default_exists\n\n- name: Change ufw default forward policy from drop to accept\n  lineinfile: dest=/etc/default/ufw regexp=\"^DEFAULT_FORWARD_POLICY=\" line=\"DEFAULT_FORWARD_POLICY=\\\"ACCEPT\\\"\"\n  when: ufw_default_exists.stat.exists\n"}, {"commit_sha": "6d10af54bdbf8e81c3d90a70ffea87b4d2c20eb2", "sha": "4f42f816c4ad1ff2dc5f8e7c8ec3fc3d915aa5ba", "filename": "tasks/plugins.yml", "repository": "Oefenweb/ansible-wordpress", "decoded_content": "# tasks file for wordpress, plugins\n---\n- name: identify installation (plugin)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1.name }}\"\n  register: check_installation_plugins\n  failed_when: False\n  changed_when: False\n  with_subelements:\n    - wordpress_installs\n    - plugins\n  when: item.1\n  tags: [configuration, wordpress, wordpress-plugins, wordpress-is-installed-plugin]\n\n- name: install (plugin)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.item.0.path }}' plugin install {{ item.item.1.name }} --activate\"\n  with_items: check_installation_plugins.results\n  when: check_installation_plugins is defined and item.item.1.name and item.rc != 0\n  tags: [configuration, wordpress, wordpress-plugins, wordpress-install-plugin]\n\n- name: check install (plugin)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1.name }}\"\n  changed_when: False\n  with_subelements:\n    - wordpress_installs\n    - plugins\n  when: item.1.name\n  tags: [configuration, wordpress, wordpress-plugins, wordpress-install-plugin-check]\n\n- name: activate (plugin)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin activate {{ item.1.name }}\"\n  register: check_activate_plugin\n  changed_when: \"'Success: Plugin' in check_activate_plugin.stdout\"\n  with_subelements:\n    - wordpress_installs\n    - plugins\n  when: item.1.name and item.1.activate | default(true)\n  tags: [configuration, wordpress, wordpress-plugins, wordpress-activate-plugin]\n\n- name: deactivate (plugin)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin deactivate {{ item.1.name }}\"\n  register: check_activate_plugin\n  changed_when: \"'Success: Plugin' in check_activate_plugin.stdout\"\n  with_subelements:\n    - wordpress_installs\n    - plugins\n  when: item.1.name and not item.1.activate | default(true)\n  tags: [configuration, wordpress, wordpress-plugins, wordpress-deactivate-plugin]\n  "}, {"commit_sha": "efc7a259af6812ee966a7809e84684ebf8b5f52c", "sha": "f5a87e55e4826da242a3d88e68d40227bd28f115", "filename": "tasks/trim-fat.yml", "repository": "geerlingguy/ansible-role-solr", "decoded_content": "---\n- name: Remove the downloaded Solr archive.\n  file:\n    path: \"{{ item }}\"\n    state: absent\n  with_items:\n    - \"{{ solr_workspace }}/{{ solr_filename }}.tgz\"\n    - \"{{ solr_workspace }}/{{ solr_filename }}\"\n\n- name: Remove docs, if not needed.\n  file:\n    path: \"{{ solr_install_path }}/docs\"\n    state: absent\n  when: solr_remove_cruft\n\n- name: Remove example dir, if not needed.\n  file:\n    path: \"{{ solr_install_path }}/example\"\n    state: absent\n  when:\n    - solr_remove_cruft\n    - solr_version.split('.')[0] >= '5'\n"}, {"commit_sha": "f85435227eb23c6e474103286c17d7406baeff47", "sha": "3ffa6ba92d0ccfbdb2e0582e0d34a88430a197e7", "filename": "roles/zookeeper/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n\n# Generate config files in the host\n- name: create zookeeper config directory\n  file:\n    path: \"{{ zookeeper_config_dir }}\"\n    state: directory\n    follow: yes\n    mode: 0755\n  sudo: yes\n  tags:\n    - zookeeper\n\n- name: Create zookeeper config file\n  template:\n    src: zoo.cfg.j2\n    dest: \"{{ zookeeper_config_dir }}/zoo.cfg\"\n  sudo: yes\n  notify:\n    - restart zookeeper\n  tags:\n    - zookeeper\n\n- name: Create zookeeper environments file\n  template:\n    src: environment.j2\n    dest: \"{{ zookeeper_config_dir }}/environment\"\n  sudo: yes\n  notify:\n    - restart zookeeper\n  tags:\n    - zookeeper\n\n- name: Create zookeeper configuration.xsl file\n  template:\n    src: configuration.xsl.j2\n    dest: \"{{ zookeeper_config_dir }}/configuration.xsl\"\n  sudo: yes\n  notify:\n    - restart zookeeper\n  tags:\n    - zookeeper\n\n- name: Create zookeeper myid file\n  copy:\n    content: \"{{ zookeeper_id }}\"\n    dest: \"{{ zookeeper_config_dir }}/myid\"\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart zookeeper\n  tags:\n    - zookeeper\n\n- name: Create zookeeper log4j file\n  template:\n    src: log4j.properties.j2\n    dest: \"{{ zookeeper_config_dir }}/log4j.properties\"\n  sudo: yes\n  notify:\n    - restart zookeeper\n  tags:\n    - zookeeper\n\n- name: Set Zookeeper consul service definition\n  sudo: yes\n  template:\n    src: zookeeper-consul.j2\n    dest: \"{{ consul_dir }}/zookeeper.json\"\n  notify:\n    - restart consul\n  tags:\n    - zookeeper\n\n- name: destroy old zookeeper container\n  when: zookeeper_rebuild_container\n  docker:\n    name: zookeeper\n    image: \"{{ zookeeper_image }}\"\n    state: absent\n  tags:\n    - zookeeper\n\n- name: run zookeeper container\n  docker:\n    name: zookeeper\n    image: \"{{ zookeeper_image }}\"\n    state: started\n    volumes:\n    - \"{{ zookeeper_config_dir }}/:{{ zookeeper_config_dir }}/\"\n    ports:\n    - \"{{ zookeeper_client_port }}:{{ zookeeper_client_port }}\"\n    - \"{{ zookeeper_leader_connect_port }}:{{ zookeeper_leader_connect_port }}\"\n    - \"{{ zookeeper_leader_election_port }}:{{ zookeeper_leader_election_port }}\"\n    net: \"host\"\n    command: /usr/share/zookeeper/bin/zkServer.sh start-foreground\n  tags:\n    - zookeeper\n\n- name: upload zookeeper template service\n  template:\n    src: zookeeper.conf.j2\n    dest: /etc/init/zookeeper.conf\n    mode: 0755\n  sudo: yes\n  tags:\n    - zookeeper\n\n- name: ensure zookeeper is running (and enable it at boot)\n  sudo: yes\n  service:\n    name: zookeeper\n    state: started\n    enabled: yes\n  tags:\n    - zookeeper\n\n"}, {"commit_sha": "596383ee7d523e3299b7b24edb1c42d751a62355", "sha": "7a236da0790a6f7b967d667a71c7a529f75087a1", "filename": "roles/openshift-labels/tasks/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: \"Set defaults\"\n  set_fact:\n    processed_namespace: \"{{ target_namespace | default('') }}\"\n\n- name: \"Prepend '-n' to the namespace, unless already set\"\n  set_fact:\n    namespace_param: \"{% if ((processed_namespace|trim).find('-n') != 0) %}-n {% endif %}{{ processed_namespace }}\"\n  when:\n    - processed_namespace|length > 0\n\n- name: \"Apply label {{ label }} to object {{ target_object }}\"\n  command: >\n    oc label --overwrite {{ target_object }} {{ target_name }} {{ label }} {{ namespace_param }}\n  when:\n  - target_object is defined\n  - target_object|trim != ''\n  - target_name is defined\n  - target_name|trim != ''\n  - label is defined\n  - label| trim != ''\n"}, {"commit_sha": "f0706344c63eba72b033128f955157e95dbcc95c", "sha": "2ac61eddffcf567f80298b669a46aad6d1a28cb1", "filename": "playbooks/prepare_ssh.yml", "repository": "RedHat-EMEA-SSA-Team/stc", "decoded_content": "---\n- name: Collect information about subscription and remote user\n  hosts: localhost\n  gather_facts: no\n  vars_files:\n  - \"{{file_env}}\"\n  tasks:\n  - name: Create key for remote user\n    user:\n      name: \"{{ssh_user}}\"\n      generate_ssh_key: yes\n      ssh_key_bits: 2048\n      ssh_key_file: .ssh/id_rsa\n- name: Distribute public keys and populate known_hosts\n  gather_facts: no\n  hosts: all\n  vars_files:\n  - \"{{file_env}}\"\n  tasks:\n  - name: Add key to authorized_keys\n    authorized_key:\n      user: \"{{ssh_user}}\"\n      state: present\n      key: \"{{ lookup('file', '~/.ssh/id_rsa.pub') }}\"\n  - name: Fetch hostkey\n    connection: local\n    command: \"ssh-keyscan {{ inventory_hostname }}\"\n    register: hostkey\n  - name: Run ssh-keyscan to add keys to known_hosts\n    connection: local\n    known_hosts:\n      name: \"{{ inventory_hostname }}\"\n      key: \"{{ item }}\"\n    with_items: \"{{ hostkey.stdout_lines }}\"\n    when: hostkey.rc == 0\n"}, {"commit_sha": "4915fb1ec791a6437d3bba28b882f4201be97102", "sha": "83130f043dad73d9163bea1f7326e0ca4bd3c0ce", "filename": "tasks/autoupdate-RedHat.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\n- name: Set correct automatic update utility vars (RHEL 8).\n  set_fact:\n    update_utility: dnf-automatic\n    update_conf_path: /etc/dnf/automatic.conf\n  when: ansible_distribution_major_version | int == 8\n\n- name: Set correct automatic update utility vars (RHEL <= 7).\n  set_fact:\n    update_utility: yum-cron\n    update_conf_path: /etc/yum/yum-cron.conf\n  when: ansible_distribution_major_version | int <= 7\n\n- name: Install automatic update utility.\n  package:\n    name: '{{ update_utility }}'\n    state: present\n\n- name: Ensure automatic update utility is running and enabled on boot.\n  service:\n    name: '{{ update_utility }}'\n    state: started\n    enabled: true\n\n- name: Configure autoupdates.\n  lineinfile:\n    dest: '{{ update_conf_path }}'\n    regexp: '^apply_updates = .+'\n    line: 'apply_updates = yes'\n  when:\n    - security_autoupdate_enabled\n    - ansible_distribution_major_version | int in [7, 8]\n"}, {"commit_sha": "786ebef5fb8b9dcbf1a7f45f5ed106ced683570a", "sha": "934dde6d4fd48b4887af50cd994392a0755d4615", "filename": "playbooks/roles/check_glusterfs/tasks/main.yml", "repository": "RedHat-EMEA-SSA-Team/stc", "decoded_content": "- name: Check if NTP enabled\n  shell: |\n    timedatectl | grep \"NTP enabled: yes\" \n  ignore_errors: true\n  register: ntp_enabled\n- name: Check if NTP synchronized\n  shell: |\n    timedatectl | grep \"NTP synchronized: yes\"\n  ignore_errors: true\n  register: ntp_synchronized\n- name: Signal if NTP is not enable or synchronized\n  debug:\n    msg: |\n      {{ ntp_enabled.stdout }}\n  when: ntp_enabled.rc != 0 or ntp_synchronized.rc != 0\n\n"}, {"commit_sha": "6ce6625dc404e18c1b299cbfb125056b3d4c4b3b", "sha": "3afbf1247b8904d06756080e1e4dc60b299eb449", "filename": "tasks/common.yml", "repository": "CSCfi/ansible-role-slurm", "decoded_content": "---\n# This sh/could be put in a separate role..\n  - name: Add FGI slurm repo\n    template: src=fgislurm.repo dest=/etc/yum.repos.d/fgislurm.repo owner=root group=root mode=0644 backup=yes\n    when: fgci_install == True\n\n  - name: Import FGI slurm repo key\n    rpm_key: key=http://idris.fgi.csc.fi/fgirepo6/RPM-GPG-KEY-CSC-GRID-2 state=present\n    when: fgci_install == True\n\n  - name: Install FGCI repo\n    package: pkg=http://idris.fgi.csc.fi/fgci7/x86_64/fgci/rpms/fgci-release7-1-1.el7.noarch.rpm\n    when: ansible_distribution_major_version == \"7\" and fgci_install == True and ansible_os_family == \"RedHat\"\n\n##\n  - name: install common Slurm packages\n    package: name={{ item }} state=present\n    with_items: \"{{ slurm_packages }}\"\n    when: slurm_packages.0 != \"\"\n\n  - name: install fgci Slurm addons\n    package: name=slurm-fgi-addons state=present\n    when: fgci_install and ansible_distribution_major_version == \"7\"\n\n  - name: Copy pam.d/slurm\n    copy: src=pam_slurm dest=/etc/pam.d/slurm owner=root mode=0644\n\n  - name: Make from template cgroup.conf\n    template: src=cgroup.conf.j2 dest=/etc/slurm/cgroup.conf owner=root mode=0644 backup=yes\n    notify: restart slurm\n\n  - name: Make from template gres.conf\n    template: src=gres.conf.j2 dest=/etc/slurm/gres.conf owner=root mode=0644\n    notify: restart slurm\n\n  - name: Make from template topology.conf\n    template: src=topology.conf.j2 dest=/etc/slurm/topology.conf owner=root mode=0644\n    notify: restart slurm\n    when: slurm_topology_plugin is defined\n\n  - name: Make from template slurm.conf\n    template: src=slurm.conf.j2 dest=/etc/slurm/slurm.conf owner=root mode=0644 backup=yes\n    notify: restart slurm\n\n  - name: write all slurm logs handled by rsyslog to one file\n    template: src=slurm_rsyslog.conf dest=/etc/rsyslog.d/10_slurm_rsyslog.conf owner=root mode=0644 backup=yes\n    notify: Restart rsyslog\n    when: slurm_manage_rsyslog_conf\n\n  - name: configure logrotate to rotate slurm_logs in slurm_log_dir\n    template: src=slurm_logrotate.j2 dest=/etc/logrotate.d/slurm owner=root mode=0644 backup=no\n    when: slurm_log_dir is defined\n\n  - name: template in plugstack.conf\n    template: src=plugstack.conf.j2 dest=/etc/slurm/plugstack.conf owner=root mode=0644 backup=yes\n    when: slurm_plugstack\n\n  - name: create slurm/plugstack.conf.d\n    file: path=/etc/slurm/plugstack.conf.d state=directory owner=root group=root mode=0755\n    when: slurm_plugstack\n\n  - name: template in plugstack.conf.d/x11.conf\n    template: src=x11.conf.j2 dest=/etc/slurm/plugstack.conf.d/x11.conf owner=root mode=0644 backup=no\n    when: slurm_plugstack and slurm_x11_spank\n\n  - name: install slurm-spank-x11 and xauth\n    package: name={{ item }} state=present\n    with_items: \"{{\u00a0slurm_spank_x11_packages }}\"\n    when: slurm_plugstack and slurm_x11_spank\n"}, {"commit_sha": "25a54626dd06a22faa2c622732ab84db89736f78", "sha": "e9d725848624557bc9921c5b1d62a737d6bb742c", "filename": "tasks/main.yml", "repository": "geerlingguy/ansible-role-php-xdebug", "decoded_content": "---\n- name: Include OS-specific variables.\n  include_vars: \"{{ ansible_os_family }}.yml\"\n\n- name: Download Xdebug.\n  get_url:\n    url: \"http://xdebug.org/files/xdebug-{{ php_xdebug_version }}.tgz\"\n    dest: \"{{ workspace }}\"\n\n- name: Untar Xdebug.\n  command: >\n    tar -C {{ workspace }} -xvzf {{ workspace }}/xdebug-{{ php_xdebug_version }}.tgz\n    creates={{ workspace }}/xdebug-{{ php_xdebug_version }}/README\n\n- name: Build Xdebug.\n  shell: >\n    {{ item }}\n    chdir={{ workspace }}/xdebug-{{ php_xdebug_version }}\n    creates={{ workspace }}/xdebug-{{ php_xdebug_version }}/modules/xdebug.so\n  with_items:\n    - phpize\n    - ./configure\n    - make\n  notify: restart webserver\n\n- name: Ensure Xdebug module path exists.\n  file:\n    path: \"{{ php_xdebug_module_path }}\"\n    state: directory\n    owner: root\n    group: root\n    mode: 0755\n\n- name: Move Xdebug module into place.\n  shell: >\n    cp {{ workspace }}/xdebug-{{ php_xdebug_version }}/modules/xdebug.so {{ php_xdebug_module_path }}/xdebug.so\n    creates={{ php_xdebug_module_path }}/xdebug.so\n  notify: restart webserver\n\n- include: configure.yml\n"}, {"commit_sha": "16f21c5c7c51314eea1d023a46bcaad8c889cd46", "sha": "51364af265aa8169ed509cfb0fc9c6daa26382a5", "filename": "tasks/opensource/setup-redhat.yml", "repository": "nginxinc/ansible-role-nginx", "decoded_content": "---\n- name: \"(Install: CentOS/RedHat) Set Default YUM NGINX Repository\"\n  set_fact:\n    default_repository: >-\n      https://nginx.org/packages/{{ (nginx_branch == 'mainline')\n      | ternary('mainline/', '') }}{{ (ansible_distribution == \"RedHat\")\n      | ternary('rhel', 'centos') }}/{{ ansible_distribution_major_version }}/$basearch/\n\n- name: \"(Install: CentOS/RedHat) Set YUM NGINX Repository\"\n  set_fact:\n    repository: \"{{ nginx_repository | default(default_repository) }}\"\n\n- name: \"(Install: CentOS/RedHat) Add NGINX Repository\"\n  yum_repository:\n    name: nginx\n    baseurl: \"{{ repository }}\"\n    description: NGINX Repository\n    enabled: yes\n    gpgcheck: yes\n"}, {"commit_sha": "8c4af8da901c68ce8c4bdd21c62e08cec5d3c23a", "sha": "3360993201a4c5573dfda8be865e613dfd343915", "filename": "handlers/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# handlers file for ansible-role-docker-ce\n\n- name: restart docker\n  become: yes\n  service:\n    name: docker\n    state: restarted\n  tags: [\"install\", \"configure\"]\n\n- name: reload docker\n  become: true\n  systemd:\n    daemon_reload: yes\n  tags: [\"install\", \"configure\"]\n\n# Workaround because systemd cannot be used: https://github.com/ansible/ansible/issues/22171\n- name: restart auditd\n  become: yes\n  command: service auditd restart\n  args:\n    warn: no\n  tags: [\"install\", \"configure\"]\n"}, {"commit_sha": "679a956a08bc9dd352f5e397a6b29339667aa7db", "sha": "0127addda6b9519ff5ec4191e39b7967e8d95953", "filename": "tasks/remove-pre-docker-ce.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Reset fact related to removal of old Docker\n  set_fact:\n    _remove_old_docker: false\n\n- name: Determine Docker version\n  shell: \"docker version --format='{{ '{{' }} .Client.Version {{ '}}' }}' 2>/dev/null\"\n  register: _cmd_docker_version\n  changed_when: false\n  failed_when: false\n  check_mode: no\n\n- name: Set fact if old Docker installation shall be removed\n  set_fact:\n    _remove_old_docker: true\n  when:\n    - _cmd_docker_version.stdout_lines is defined\n    - _cmd_docker_version.stdout_lines[0] is defined\n    - _cmd_docker_version.stdout_lines[0] is version('17', '<')\n\n- name: Check if Docker is running\n  become: true\n  systemd:\n    name: docker\n  ignore_errors: yes\n  register: _service_docker_status\n  check_mode: no\n  when: _remove_old_docker\n\n- name: Stop Docker service\n  service:\n    name: docker\n    state: stopped\n  when: \"_service_docker_status.rc | default(1) == 0\"\n\n- name: Remove old Docker installation before Docker CE\n  become: true\n  package:\n    name: \"{{ item }}\"\n    state: absent\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when: _remove_old_docker\n  with_items:\n    - \"{{ docker_old_packages[_docker_os_dist] }}\"\n"}, {"commit_sha": "e75cabae2d692a152ca21c185a313528b8bac3c8", "sha": "00301ef527ab46c2cc1b3995eeb814a6db0abd08", "filename": "roles/ansible/tower/manage-job-templates/tests/inventory/group_vars/tower.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\ntower_admin_password: \"admin01\"\n\nansible_tower_job_templates:\n- name: \"Job 1\"\n  description: \"My Job 1\"\n  inventory: \"Inventory1\"\n  project: \"Project1\"\n  playbook: \"playbooks/prep.yml\"\n  credential: \"Cred1\"\n  extra_vars: \"---\\\\nhello: world\\\\n\"\n  ask_variables_on_launch: true\n  permissions:\n    teams:\n    - name: team1\n      role: Execute\n    users:\n    - name: user1\n      role: Execute\n"}, {"commit_sha": "b79a990b9cc80f1d419cebc46be91029fde17f76", "sha": "f1fd40fea6cd8c57e4c62c5ad95db7d3e5426c56", "filename": "tasks/checks.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- include_tasks: distribution-checks.yml\n  when:\n    _docker_os_dist_check | bool\n    \n- include_tasks: compatibility-checks.yml\n\n"}, {"commit_sha": "7e0b97a832be19120df08cdfd7ecde793b744468", "sha": "1d50a2c4f3e374c1f080a3ccf380dfd71a83813d", "filename": "tasks/cores.yml", "repository": "geerlingguy/ansible-role-solr", "decoded_content": "---\n- name: Check current list of Solr cores.\n  uri:\n    url: http://localhost:{{ solr_port }}/solr/admin/cores\n    return_content: yes\n  register: solr_cores_current\n\n- name: Ensure Solr conf directories exist.\n  file:\n    path: \"{{ solr_home }}/data/{{ item }}/conf\"\n    state: directory\n    owner: \"{{ solr_user }}\"\n    group: \"{{ solr_user }}\"\n    recurse: yes\n  when: \"'{{ item }}' not in '{{ solr_cores_current.content }}'\"\n  with_items: \"{{ solr_cores }}\"\n\n- name: Ensure core configuration directories exist.\n  shell: \"cp -r {{ solr_install_path }}/example/files/conf/ {{ solr_home }}/data/{{ item }}/\"\n  when: \"'{{ item }}' not in '{{ solr_cores_current.content }}'\"\n  with_items: \"{{ solr_cores }}\"\n\n- name: Create configured cores.\n  shell: \"{{ solr_install_path }}/bin/solr create -c {{ item }}\"\n  when: \"'{{ item }}' not in '{{ solr_cores_current.content }}'\"\n  with_items: \"{{ solr_cores }}\"\n  become: yes\n  become_user: \"{{ solr_user }}\"\n"}, {"commit_sha": "b11c4477d973b0cc87a296f6b028eaf9abab4686", "sha": "638baddde388ee94327b295544d43e6e0c7b4dd0", "filename": "tasks/main-Fedora.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# tasks file for ansible-role-docker-ce\n\n- name: Add Docker CE repository\n  shell: dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo\n  args:\n    creates: /etc/yum.repos.d/docker-ce.repo\n  become: true\n  register: dnf_repo\n\n- name: Update dnf cache\n  shell: dnf makecache fast\n  become: true\n  when: dnf_repo.changed\n\n- name: Install python and deps for ansible modules\n  raw: dnf install -y python2 python2-dnf libselinux-python\n  become: true\n  changed_when: false\n\n- include: main-Storage.yml\n  when: docker_setup_devicemapper == true\n\n- include: main-Generic.yml"}, {"commit_sha": "16f21c5c7c51314eea1d023a46bcaad8c889cd46", "sha": "2650b16e6e9a8f1fffc59d20b7fef13e9655bf24", "filename": "tasks/opensource/install-oss-linux.yml", "repository": "nginxinc/ansible-role-nginx", "decoded_content": "---\n- name: \"(Install: Linux) Configure NGINX repo\"\n  block:\n\n    - import_tasks: setup-alpine.yml\n      when: ansible_os_family == \"Alpine\"\n\n    - import_tasks: setup-debian.yml\n      when: ansible_os_family == \"Debian\"\n\n    - import_tasks: setup-redhat.yml\n      when: ansible_os_family == \"RedHat\"\n\n    - import_tasks: setup-suse.yml\n      when: ansible_os_family == \"Suse\"\n\n  when: nginx_install_from == \"nginx_repository\"\n\n- name: \"(Install: Linux) Install NGINX package\"\n  package:\n    name: \"nginx{{ nginx_version | default('') }}\"\n    state: present\n  when: ansible_os_family in nginx_linux_families\n  notify: \"(Handler: All OSs) Start NGINX\"\n"}, {"commit_sha": "f85435227eb23c6e474103286c17d7406baeff47", "sha": "918a982b10ded59e0f0251db45e741bf68a42a08", "filename": "roles/mesos/tasks/master.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# Tasks for Master nodes\n\n- name: set mesos-master consul service definition\n  when: mesos_install_mode == \"master\"\n  sudo: yes\n  template:\n    src: mesos-master-consul.j2\n    dest: \"{{ consul_dir }}/mesos-master.json\"\n  notify:\n    - restart consul\n  tags:\n    - mesos-master\n\n- name: create mesos-master work directory\n  when: mesos_install_mode == \"master\"\n  file:\n    path: \"{{ mesos_master_work_dir }}\"\n    state: directory\n    mode: 0755\n  sudo: yes\n  tags:\n    - mesos-master\n\n- name: destroy old mesos-master container\n  when: mesos_master_rebuild_container\n  docker:\n    name: mesos-master\n    image: \"{{ mesos_master_image }}\"\n    state: absent\n  tags:\n    - mesos-master\n\n- name: run mesos-master container\n  when: mesos_install_mode == \"master\"\n  docker:\n    name: mesos-master\n    image: \"{{ mesos_master_image }}\"\n    state: started\n    volumes:\n    - \"{{ mesos_master_work_dir }}:{{ mesos_master_work_dir }}\"\n    ports:\n    - \"{{ mesos_master_port }}:{{ mesos_master_port }}\"\n    net: \"host\"\n    env:\n      MESOS_HOSTNAME: \"{{ mesos_hostname }}\"\n      MESOS_IP: \"{{ mesos_ip }}\"\n      MESOS_CLUSTER: \"{{ mesos_cluster_name }}\"\n      MESOS_ZK: \"zk://{{ zookeeper_peers_nodes }}/mesos\"\n      MESOS_LOG_DIR: \"/var/log/mesos\"\n      MESOS_QUORUM: \"{{ mesos_quorum }}\"\n      MESOS_WORK_DIR: \"{{ mesos_master_work_dir }}\"\n  tags:\n    - mesos-master\n\n- name: upload mesos-master template service\n  when: mesos_install_mode == \"master\"\n  template:\n    src: mesos-master.conf.j2\n    dest: /etc/init/mesos-master.conf\n    mode: 0755\n  sudo: yes\n  tags:\n    - mesos-master\n\n- name: ensure mesos-master is running (and enable it at boot)\n  when: mesos_install_mode == \"master\"\n  sudo: yes\n  service:\n    name: mesos-master\n    state: started\n    enabled: yes\n  tags:\n    - mesos-master\n\n- name: run prometheus mesos master exporter container\n  when: mesos_install_mode == \"master\" and prometheus_enabled|bool\n  docker:\n    name: mesos-exporter\n    image: \"{{ prometheus_mesos_exporter_image }}\"\n    command: \"-exporter.scrape-mode=master -exporter.url=http://{{ mesos_hostname }}:{{ mesos_master_port }}\"\n    state: started\n    restart_policy: always\n    ports:\n    - \"{{ prometheus_mesos_exporter_port }}:{{ prometheus_mesos_exporter_port }}\"\n  environment: proxy_env\n  tags:\n    - prometheus\n    - mesos_master\n\n- name: Set mesos-exporter consul service definition\n  when: mesos_install_mode == \"master\" and prometheus_enabled|bool\n  sudo: yes\n  template:\n    src: mesos-exporter-consul.j2\n    dest: \"{{ consul_dir }}/mesos-exporter.json\"\n  notify:\n    - restart consul\n  tags:\n    - prometheus\n    - mesos_master\n"}, {"commit_sha": "f85435227eb23c6e474103286c17d7406baeff47", "sha": "14516053d5fc0d15e09f4434b5b070dcbe88077b", "filename": "roles/cadvisor/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "# tasks for running cadvisor\n- name: destroy old cadvisor container\n  when: cadvisor_rebuild_container\n  docker:\n    name: cadvisor\n    image: \"{{ cadvisor_image }}\"\n    state: absent\n  tags:\n    - cadvisor\n\n- name: run cadvisor container\n  when: cadvisor_enabled\n  docker:\n    name: cadvisor\n    image: \"{{ cadvisor_image }}\"\n    state: started\n    restart_policy: \"{{ cadvisor_restart_policy }}\"\n    net: \"{{ cadvisor_net }}\"\n    hostname: \"{{ cadvisor_hostname }}\"\n    volumes:\n    - \"/var/lib/docker/:/var/lib/docker:ro\"\n    - \"/:/rootfs:ro\"\n    - \"/var/run:/var/run:rw\"\n    - \"/sys:/sys:ro\"\n  tags:\n    - cadvisor\n\n- name: upload cadvisor template service\n  when: cadvisor_enabled\n  template:\n    src: cadvisor.conf.j2\n    dest: /etc/init/cadvisor.conf\n    mode: 0755\n  sudo: yes\n  tags:\n    - cadvisor\n\n# Attach to the running container, or start it if needed\n# and forward all signals so that the process manager can detect\n# when a container stops and correctly restart it.\n- name: ensure cadvisor is running (and enable it at boot)\n  when: cadvisor_enabled\n  sudo: yes\n  service:\n    name: cadvisor\n    state: started\n    enabled: yes\n  tags:\n    - cadvisor\n\n- name: get cadvisor container ip\n  sudo: yes\n  command: >\n    docker inspect -f \\{\\{' '.NetworkSettings.IPAddress' '\\}\\} cadvisor\n  register: cadvisor_container_ip\n  when: cadvisor_enabled\n  tags:\n    - cadvisor\n\n- name: Set cadvisor consul service definition\n  sudo: yes\n  template:\n    src: cadvisor-consul.j2\n    dest: \"{{ cadvisor_consul_dir }}/cadvisor.json\"\n  notify:\n    - restart consul\n  when: cadvisor_enabled\n  tags:\n    - cadvisor\n\n- name: stop cadvisor container\n  when: not cadvisor_enabled\n  sudo: yes\n  service:\n    name: cadvisor\n    state: stopped\n    enabled: yes\n  tags:\n    - cadvisor\n"}, {"commit_sha": "9a25ba7000e2cb52b93161f1671786054856c023", "sha": "04f8d715060c6982ba2beaa6018c8ccce0b5742c", "filename": "tasks/redhat.yml", "repository": "CSCfi/ansible-role-cuda", "decoded_content": "---\n# tasks file for ansible-role-cuda\n#\n- name: add nvidia CUDA repo\n  template: src=nvidia.repo.j2 dest=/etc/yum.repos.d/nvidia.repo owner=root group=root mode=0644 backup=yes\n\n- name: install cuda software - this is slow - restart if cuda_restart_node_on_install is True\n  yum: name={{ item }} state=present\n  with_items: \"{{ cuda_packages | default({}) }}\"\n  when: cuda_packages.0 != \"\"\n  register: cuda_packages_installation\n  notify:\n   - ZZ CUDA Restart server\n   - ZZ CUDA Wait for server to restart\n\n- name: template in cuda_init.sh used during boot\n  template: src=cuda_init.sh.j2 \n            dest=/usr/local/bin/cuda_init.sh \n            owner=root group=root mode=0755 \n            backup=no\n  when: cuda_init\n\n- name: add the cuda_init.sh script to rc.local\n  lineinfile: \n      dest=/etc/rc.local \n      insertafter=\"^touch /var/lock/subsys/local\" \n      regexp=\"^/bin/bash /usr/local/bin/cuda_init.sh$\"\n      line=\"/bin/bash /usr/local/bin/cuda_init.sh\"\n  when: cuda_init\n\n# This is here because if we in the same playbook try to start slurmd without having run the cuda_init.sh script then slurmd doesn't start and the play fails.\n- name: flush the handlers - so that the node is rebooted after CUDA is installed\n  meta: flush_handlers\n\n"}, {"commit_sha": "3331ceddd3788b3f43d1f6c7ab4ff27d68ceb2c2", "sha": "73b94aaa06129bd76f3ca7971628ec29053d658d", "filename": "roles/ovirt-engine-remote-db/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# main file for remote DB task\n# based on https://fedoraproject.org/wiki/PostgreSQL\n\n- name: check PostgreSQL service\n  service:\n    name: postgresql\n    state: started\n  register: postgresql_status\n  ignore_errors: True\n\n# install libselinux-python on machine - selinux policy\n- name: install libselinux-python for ansible\n  yum:\n    name: libselinux-python\n    state: \"present\"\n  when: postgresql_status|failed\n\n# for semanage utility\n- name: install policycoreutils-python for changing selinux port\n  yum:\n    name: policycoreutils-python\n    state: \"present\"\n  when: postgresql_status|failed\n\n- name: yum install PostgreSQL\n  yum:\n    name: \"postgresql-server\"\n    state: installed\n    update_cache: yes\n  when: postgresql_status|failed\n\n- name: run PostgreSQL initdb\n  become_user: postgres\n  become: yes\n  shell: '/usr/bin/initdb -D /var/lib/pgsql/data'\n  when: postgresql_status|failed\n  tags:\n    - skip_ansible_lint\n\n- name: start PostgreSQL service\n  service:\n    name: postgresql\n    state: started\n    enabled: yes\n\n# allow access engine database access from outside\n- name: update pg_hba.conf -> host ovirt_engine_db_name ovirt_engine_db_user 0.0.0.0/0 md5\n  lineinfile:\n    dest: '/var/lib/pgsql/data/pg_hba.conf'\n    insertafter: EOF\n    line: \"host {{ovirt_engine_db_name}} {{ovirt_engine_db_user}} 0.0.0.0/0 md5\"\n  when: ovirt_engine_remote_db == True\n\n# allow access dwh database access from outside\n- name: update pg_hba.conf -> host ovirt_engine_db_dwh_name ovirt_engine_db_dwh_user 0.0.0.0/0 md5\n  lineinfile:\n    dest: '/var/lib/pgsql/data/pg_hba.conf'\n    insertafter: EOF\n    line: \"host {{ovirt_engine_dwh_db_name}} {{ovirt_engine_dwh_db_user}} 0.0.0.0/0 md5\"\n  when: ovirt_engine_dwh_remote_db == True\n\n# listen on specific address\n- name: update postgresql.conf -> listen_addresses='*'\n  lineinfile:\n    dest: '/var/lib/pgsql/data/postgresql.conf'\n    insertafter: EOF\n    line: \"listen_addresses='{{ovirt_engine_remote_db_listen_address}}'\"\n  when: postgresql_status|failed\n\n# listen on specific port\n- name: update postgresql.conf -> port number\n  lineinfile:\n    dest: '/var/lib/pgsql/data/postgresql.conf'\n    insertafter: EOF\n    line: \"port={{ovirt_engine_remote_db_port}}\"\n  when: postgresql_status|failed and ovirt_engine_remote_db_port != 5432\n\n# postgresql.conf: (el7)\n# Note: In RHEL/Fedora installations, you can't set the port number here;\n#   adjust it in the service file instead.\n#   /usr/lib/systemd/system/postgresql.service\n#    - Environment=PGPORT=5432\n- name: update postgresql.conf -> port number in service file (Fedora & RHEL)\n  lineinfile:\n    dest: '/usr/lib/systemd/system/postgresql.service'\n    backrefs: yes\n    regexp: \"Environment=PGPORT=5432\"\n    line: \"Environment=PGPORT={{ovirt_engine_remote_db_port}}\"\n  register: port_update\n  when: postgresql_status|failed and ovirt_engine_remote_db_port != 5432\n  ignore_errors: True\n\n# daemon reload - service file was changed\n- name: systemctl daemon-reload (el7)\n  shell: 'systemctl daemon-reload'\n  when: postgresql_status|failed and ovirt_engine_remote_db_port != 5432 and port_update|success\n  tags:\n    - skip_ansible_lint\n\n# el6 use only service (systemctl not present)\n- name: update postgresql.conf -> port number in service file (el6)\n  lineinfile:\n    dest: '/etc/init.d/postgresql'\n    backrefs: yes\n    regexp: \"PGPORT=5432\"\n    line: \"PGPORT={{ovirt_engine_remote_db_port}}\"\n  when: postgresql_status|failed and ovirt_engine_remote_db_port != 5432 and port_update|failed\n  ignore_errors: True\n\n# allow selinux for postgresql non-standard port\n- name: allow selinux for non-standard port\n  shell: 'semanage port -a -t postgresql_port_t -p tcp {{ovirt_engine_remote_db_port}}'\n  when: postgresql_status|failed and ovirt_engine_remote_db_port != 5432\n  ignore_errors: True\n  tags:\n    - skip_ansible_lint\n\n# first check of PostgreSQL - if fail, setup\n- name: PostgreSQL reload configuration\n  service:\n    name: postgresql\n    state: restarted\n\n- name: check iptables service\n  service:\n    name: iptables\n    state: started\n  register: iptables_status\n  when: postgresql_status|failed\n  ignore_errors: True\n\n- name: open port for PostgreSQL in iptables\n  shell: \"iptables -I INPUT -p tcp -m state --state NEW -m tcp --dport {{ovirt_engine_remote_db_port}} -j ACCEPT\"\n  when: postgresql_status|failed and not iptables_status|failed\n  tags:\n    - skip_ansible_lint\n\n- name: save iptables rules\n  shell: \"/sbin/iptables-save\"\n  when: postgresql_status|failed and not iptables_status|failed\n  tags:\n    - skip_ansible_lint\n\n- name: check firewalld service\n  service:\n    name: firewalld\n    state: started\n  register: firewalld_status\n  when: postgresql_status|failed\n  ignore_errors: True\n\n- name: open port for PostgreSQL in firewalld\n  firewalld:\n    port: \"{{ovirt_engine_remote_db_port|int}}/tcp\"\n    permanent: True\n    state: enabled\n  when: postgresql_status|failed and not firewalld_status|failed\n\n- name: reload firewalld\n  shell: \"firewall-cmd --reload\"\n  when: postgresql_status|failed and not firewalld_status|failed\n  tags:\n    - skip_ansible_lint\n\n- name: creating directory for sql scripts in /tmp/ansible-sql\n  file:\n    path: /tmp/ansible-sql\n    state: directory\n\n- name: copy SQL scripts\n  template:\n    src: \"{{item}}.j2\"\n    dest: \"/tmp/ansible-sql/{{item}}\"\n    mode: 0644\n    owner: postgres\n    group: postgres\n  with_items:\n    - \"ovirt-engine-db-create.sql\"\n    - \"ovirt-engine-db-user-create.sql\"\n    - \"ovirt-engine-dwh-db-create.sql\"\n    - \"ovirt-engine-dwh-db-user-create.sql\"\n\n- name: create engine DB and user\n  become_user: postgres\n  become: yes\n  command: psql -p {{ovirt_engine_remote_db_port}} -a -f /tmp/ansible-sql/'{{item}}'\n  with_items:\n    - \"ovirt-engine-db-user-create.sql\"\n    - \"ovirt-engine-db-create.sql\"\n  when: ovirt_engine_remote_db == True\n\n- name: create engine DWH DB and user\n  become_user: postgres\n  become: yes\n  command: psql -p {{ovirt_engine_remote_db_port}} -a -f /tmp/ansible-sql/'{{item}}'\n  with_items:\n    - \"ovirt-engine-dwh-db-user-create.sql\"\n    - \"ovirt-engine-dwh-db-create.sql\"\n  when: ovirt_engine_dwh_remote_db == True\n\n- name: check PostgreSQL service\n  service:\n    name: postgresql\n    state: started\n    enabled: yes\n\n- name: clean tmp files\n  file:\n    path: '/tmp/ansible-sql'\n    state: 'absent'\n"}, {"commit_sha": "906d962d4d978a44a1161db5d7023b9abc1b1ff2", "sha": "dfdde2e568aca8260534ea48ae1637515b044a31", "filename": "roles/cloud-azure/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- set_fact:\n    resource_group: \"Algo_{{ region }}\"\n\n- name: Create a resource group\n  azure_rm_resourcegroup:\n    secret: \"{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\"\n    tenant: \"{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\"\n    client_id: \"{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\"\n    subscription_id: \"{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\"\n    name: \"{{ resource_group }}\"\n    location: \"{{ region }}\"\n    tags:\n      Environment: Algo\n\n- name: Create a virtual network\n  azure_rm_virtualnetwork:\n    secret: \"{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\"\n    tenant: \"{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\"\n    client_id: \"{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\"\n    subscription_id: \"{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\"\n    resource_group: \"{{ resource_group }}\"\n    name: algo_net\n    address_prefixes: \"10.10.0.0/16\"\n    tags:\n      Environment: Algo\n\n- name: Create a security group\n  azure_rm_securitygroup:\n    secret: \"{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\"\n    tenant: \"{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\"\n    client_id: \"{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\"\n    subscription_id: \"{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\"\n    resource_group: \"{{ resource_group }}\"\n    name: AlgoSecGroup\n    purge_rules: yes\n    rules:\n      - name: AllowSSH\n        protocol: Tcp\n        destination_port_range: 22\n        access: Allow\n        priority: 100\n        direction: Inbound\n      - name: AllowIPSEC500\n        protocol: Udp\n        destination_port_range: 500\n        access: Allow\n        priority: 110\n        direction: Inbound\n      - name: AllowIPSEC4500\n        protocol: Udp\n        destination_port_range: 4500\n        access: Allow\n        priority: 120\n        direction: Inbound\n\n- name: Create a subnet\n  azure_rm_subnet:\n    secret: \"{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\"\n    tenant: \"{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\"\n    client_id: \"{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\"\n    subscription_id: \"{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\"\n    resource_group: \"{{ resource_group }}\"\n    name: algo_subnet\n    address_prefix: \"10.10.0.0/24\"\n    virtual_network: algo_net\n    security_group_name: AlgoSecGroup\n    tags:\n      Environment: Algo\n\n- name: Create an instance\n  azure_rm_virtualmachine:\n    secret: \"{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\"\n    tenant: \"{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\"\n    client_id: \"{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\"\n    subscription_id: \"{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\"\n    resource_group: \"{{ resource_group }}\"\n    admin_username: ubuntu\n    virtual_network: algo_net\n    name: \"{{ azure_server_name }}\"\n    ssh_password_enabled: false\n    vm_size: Standard_D1\n    tags:\n      Environment: Algo\n    ssh_public_keys:\n      - { path: \"/home/ubuntu/.ssh/authorized_keys\", key_data: \"{{ lookup('file', '{{ SSH_keys.public }}') }}\" }\n    image:\n      offer: UbuntuServer\n      publisher: Canonical\n      sku: '16.04-LTS'\n      version: latest\n  register: azure_rm_virtualmachine\n\n- set_fact:\n    ip_address: \"{{ azure_rm_virtualmachine.ansible_facts.azure_vm.properties.networkProfile.networkInterfaces[0].properties.ipConfigurations[0].properties.publicIPAddress.properties.ipAddress }}\"\n    networkinterface_name: \"{{ azure_rm_virtualmachine.ansible_facts.azure_vm.properties.networkProfile.networkInterfaces[0].name }}\"\n\n- name: Ensure the network interface includes all required parameters\n  azure_rm_networkinterface:\n    secret: \"{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\"\n    tenant: \"{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\"\n    client_id: \"{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\"\n    subscription_id: \"{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\"\n    name: \"{{ networkinterface_name }}\"\n    resource_group: \"{{ resource_group }}\"\n    virtual_network_name: algo_net\n    subnet_name: algo_subnet\n    security_group_name: AlgoSecGroup\n\n- name: Add the instance to an inventory group\n  add_host:\n    name: \"{{ ip_address }}\"\n    groups: vpn-host\n    ansible_ssh_user: ubuntu\n    ansible_python_interpreter: \"/usr/bin/python2.7\"\n    ansible_ssh_private_key_file: \"{{ SSH_keys.private }}\"\n    cloud_provider: azure\n    ipv6_support: no\n\n- set_fact:\n    cloud_instance_ip: \"{{ ip_address }}\"\n\n- name: Ensure the group azure exists in the dynamic inventory file\n  lineinfile:\n    state: present\n    dest: configs/inventory.dynamic\n    line: '[azure]'\n\n- name: Populate the dynamic inventory\n  lineinfile:\n    state: present\n    dest: configs/inventory.dynamic\n    insertafter: '\\[azure\\]'\n    regexp: \"^{{ cloud_instance_ip }}.*\"\n    line: \"{{ cloud_instance_ip }}\"\n"}, {"commit_sha": "96229df8914d81313a37c41fc063cddb8d9a5d49", "sha": "4346665b9013b29788ec163ca0e2ba27753c1f7e", "filename": "tasks/packages-Debian.yml", "repository": "jloh/nagios-nrpe-server", "decoded_content": "---\n# Update apt-cache for Debian based OS's\n- name: Update apt cache [Debian]\n  apt: update_cache=yes\n\n# Nagios NRPE Server for Debian based OS's\n- name: Install Nagios NRPE Server [Debian]\n  apt: name=nagios-nrpe-server state=present\n\n- name: Install nagios-plugins [Debian]\n  apt: name=nagios-plugins state=present\n"}, {"commit_sha": "8c4af8da901c68ce8c4bdd21c62e08cec5d3c23a", "sha": "88aef399bfc4458fff8b8cbe2df85df2ea9c4ac1", "filename": "tasks/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Set distribution facts\n  set_fact:\n    _docker_os_dist: \"{{ ansible_distribution }}\"\n    _docker_os_dist_release: \"{{ ansible_distribution_release }}\"\n    _docker_os_dist_major_version: \"{{ ansible_distribution_major_version }}\"\n    _docker_os_dist_check: yes\n  tags: [\"install\", \"configure\"]\n\n- name: Reinterpret distribution facts for Linux Mint 18\n  set_fact:\n    _docker_os_dist: \"Ubuntu\"\n    _docker_os_dist_release: \"xenial\"\n    _docker_os_dist_major_version: \"16\"\n  when:\n    _docker_os_dist == \"Linux Mint\" and\n    _docker_os_dist_major_version == \"18\"\n  tags: [\"install\", \"configure\"]\n\n# https://wiki.ubuntu.com/SystemdForUpstartUsers\n# Important! systemd is only fully supported in Ubuntu 15.04 and later releases\n- name: Determine usage of systemd\n  become: true\n  shell: \"ps -p1 | grep systemd 1>/dev/null && echo systemd || echo upstart\"\n  changed_when: no\n  check_mode: no\n  register: _determine_systemd_usage\n\n- name: Set fact to indicate systemd is used or not\n  set_fact:\n    _docker_systemd_used: \"{{ _determine_systemd_usage is defined and _determine_systemd_usage.stdout == 'systemd' }}\"\n\n- name: Temporary handling of deprecated variable docker_enable_ce_edge (#54)\n  set_fact:\n    docker_channel: edge\n  when:\n    - docker_enable_ce_edge is defined\n    - docker_enable_ce_edge\n\n- name: Compatibility and distribution checks\n  include_tasks: checks.yml\n  tags: [\"install\", \"configure\"]\n\n- name: Setup Docker package repositories\n  include_tasks: setup-repository.yml\n  tags: [\"install\"]\n\n- name: Remove Docker versions before Docker CE\n  include_tasks: remove-pre-docker-ce.yml\n  when: docker_remove_pre_ce | bool\n  tags: [\"install\"]\n\n- name: Install Docker\n  include_tasks: install-docker.yml\n  tags: [\"install\"]\n\n- name: Configure audit logging\n  include_tasks: setup-audit.yml\n  tags: [\"configure\"]\n\n- name: Apply workarounds for bugs and/or tweaks\n  include_tasks: bug-tweaks.yml\n  tags: [\"configure\"]\n\n- name: Configure systemd service\n  include_tasks: configure-systemd.yml\n  when: _docker_systemd_used | bool\n  tags: [\"configure\"]\n\n- name: Configure non-systemd service\n  include_tasks: configure-non-systemd.yml\n  when: _docker_systemd_used | bool\n  tags: [\"configure\"]\n\n- name: Configure Docker\n  include_tasks: configure-docker.yml\n  tags: [\"configure\"]\n\n- name: Postinstall tasks\n  include_tasks: postinstall.yml\n  tags: [\"install\"]"}, {"commit_sha": "1778cb1f45e31c803a0cf30edc76b55cb0c91e83", "sha": "0a23f2489b080e528bda6e529e427fb744d521e2", "filename": "roles/vpn/defaults/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\nstrongswan_enabled_plugins:\n  - aes\n  - gcm\n  - hmac\n  - kernel-netlink\n  - nonce\n  - openssl\n  - pem\n  - pgp\n  - pkcs12\n  - pkcs7\n  - pkcs8\n  - pubkey\n  - random\n  - revocation\n  - sha2\n  - socket-default\n  - stroke\n  - x509\n\nciphers:\n  defaults:\n    ike: aes128gcm16-prfsha512-ecp256!\n    esp: aes128gcm16-ecp256!\n  compat:\n    ike: aes128gcm16-prfsha512-ecp256,aes128-sha2_512-prfsha512-ecp256,aes128-sha2_512-prfsha512-modp2048!\n    esp: aes128gcm16-ecp256,aes128-sha2_512-prfsha512-ecp256,aes128-sha2_512-prfsha512-modp2048!\n"}, {"commit_sha": "f85435227eb23c6e474103286c17d7406baeff47", "sha": "038c8692755b64fd16a2d364fb4fb5f8e862d164", "filename": "roles/cadvisor/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for cadvisor\ncadvisor_enabled: true\ncadvisor_version: 'latest'\ncadvisor_restart_policy: 'always'\ncadvisor_net: 'bridge'\ncadvisor_hostname: \"{{ ansible_ssh_host }}\"\ncadvisor_image: \"google/cadvisor:{{ cadvisor_version }}\"\ncadvisor_consul_dir: /etc/consul.d\ncadvisor_consul_service_id: \"{{ ansible_hostname }}:cadvisor:8080\"\ncadvisor_rebuild_container: false\n"}, {"commit_sha": "6d10af54bdbf8e81c3d90a70ffea87b4d2c20eb2", "sha": "1dec1042836edd2615c720023ae0925eb22fa04b", "filename": "tasks/themes.yml", "repository": "Oefenweb/ansible-wordpress", "decoded_content": "# tasks file for wordpress, themes\n---\n- name: identify installation (theme)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.0.path }}' theme is-installed {{ item.1.name }}\"\n  register: check_installation_themes\n  failed_when: False\n  changed_when: False\n  with_subelements:\n    - wordpress_installs\n    - themes\n  when: item.1.name\n  tags: [configuration, wordpress, wordpress-themes, wordpress-is-installed-theme]\n\n- name: install (theme)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.item.0.path }}' theme install {{ item.item.1.name }}\"\n  with_items: check_installation_themes.results\n  when: check_installation_themes is defined and item.item.1.name and item.rc != 0\n  tags: [configuration, wordpress, wordpress-themes, wordpress-install-theme]\n\n- name: check install (theme)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.0.path }}' theme is-installed {{ item.1.name }}\"\n  changed_when: False\n  with_subelements:\n    - wordpress_installs\n    - themes\n  when: item.1.name\n  tags: [configuration, wordpress, wordpress-themes, wordpress-install-theme-check]\n\n- name: activate (theme)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.0.path }}' theme activate {{ item.1.name }}\"\n  register: check_activate_theme\n  changed_when: \"'Success: Switched to' in check_activate_theme.stdout\"\n  with_subelements:\n    - wordpress_installs\n    - themes\n  when: item.1.name and item.1.activate | default(false)\n  tags: [configuration, wordpress, wordpress-themes, wordpress-activate-theme]\n"}, {"commit_sha": "a050f0c00e18bfe3e91ea67ee6fc4461ed6e904f", "sha": "afa50280eb84f03e3a519f2b574e35f2a8f97bc1", "filename": "roles/config-clair/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# Base Configurations\nclair_name: clair\nclair_service: \"{{ clair_name }}.service\"\nclair_address: \"\"\n\n#Systemd\nsystemd_service_dir: /usr/lib/systemd/system\nsystemd_environmentfile_dir: /etc/sysconfig\n\n# Clair\nclair_image: quay.io/coreos/clair-jwt:v2.0.8\nclair_config_dir: /var/lib/clair/config\nclair_container_config_dir: /config\nclair_ssl_trust_configure: False\nclair_ssl_trust_src_file: /tmp/clair-ssl-trust.crt\nclair_ssl_trust_host_file: \"{{ clair_config_dir }}/ca.crt\"\nclair_ssl_trust_container_file: /usr/local/share/ca-certificates/ca.crt\n\n# Quay\nquay_enterprise_address: \"\"\n\n# PostgreSQL\n# External Databases\npostgresql_ssl_enabled: False\npostgresql_username: \"clair\"\npostgresql_password: \"clair\"\npostgresql_database: \"clair\"\npostgresql_host: \"\"\npostgresql_port: \"5432\"\npostgresql_db_uri: \"postgresql://{{ postgresql_username }}:{{ postgresql_password }}@{{ postgresql_host if postgresql_host is defined and postgresql_host|trim != '' else hostvars[inventory_hostname]['ansible_eth0']['ipv4']['address'] }}:{{ postgresql_port | default('5432') }}/{{ postgresql_database | default('clair') }}{{ '?sslmode=disable' if not postgresql_ssl_enabled }}\"\n\n# Ports\nclair_host_proxy_port: 6060\nclair_container_proxy_port: 6060\nclair_host_api_port: 6061\nclair_container_api_port: 6061\n\n# SSL\n#clair_ssl_enable: True\n#clair_ssl_key_file: \"\"\n#clair_ssl_cert_file: \"\"\n#clair_ssl_generate_city: Raleigh\n#clair_ssl_generate_state: NC\n#clair_ssl_generate_country: US\n#clair_ssl_generate_organization: Red Hat\n#clair_ssl_generate_organizational_unit: CoP\n#clair_ssl_generate_days_validity: 365\n"}, {"commit_sha": "71325f59a06782a99f03696780a9681a3bc0827f", "sha": "162eb980079a4fc95b19ec2ff3ef9ee17cab0769", "filename": "roles/docker/tasks/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: \"Setting Docker Facts\"\n  set_fact:\n    docker_storage_block_device: \"{{ docker_storage_block_device | default(default_docker_storage_block_device) }}\"\n    docker_storage_volume_group: \"{{ docker_storage_volume_group | default(default_docker_storage_volume_group) }}\"\n\n- name: \"Install Docker\"\n  yum:\n    name: docker\n    state: latest\n  notify:\n    - enable docker\n\n- name: \"Confige Docker\"\n  lineinfile:\n    dest: /etc/sysconfig/docker\n    regexp: '^OPTIONS=.*$'\n    line: \"OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16'\"\n\n- name: \"Check for existing Docker Storage device\"\n  command: pvs\n  register: pvs\n\n- name: \"Set Docker Storage fact if already configured\"\n  set_fact:\n    docker_storage_setup: true\n  when: pvs.stdout | search('{{ docker_storage_block_device }}.*{{ docker_storage_volume_group }}')\n\n- name: \"Configure Docker Storage Setup\"\n  template:\n    src: docker-storage-setup.j2\n    dest: /etc/sysconfig/docker-storage-setup\n  when: docker_storage_setup is undefined\n\n# - name: \"Run Docker Storage Setup\"\n#   command: docker-storage-setup\n#   when: docker_storage_setup is undefined\n#   notify:\n#   - restart docker\n#\n# - name: \"Extend the Volume Group for Docker Storage\"\n#   command: lvextend -l 90%VG /dev/{{ docker_storage_volume_group }}/docker-pool\n#   when: docker_storage_setup is undefined\n#   notify:\n#   - restart docker\n"}, {"commit_sha": "d6e8fe0dad9afc88eb88e0e34a7607042552df74", "sha": "c5d463dcd3632f0b0f9fe7cea264276b11435e81", "filename": "tasks/bug-tweaks.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Configuration to avoid 'Device or resource busy'\n  block:\n  - name: Stat /proc/sys/fs/may_detach_mounts (CentOS/RedHat)\n    stat:\n      path: /proc/sys/fs/may_detach_mounts\n    register: may_detach_mounts\n\n  - name: Ensure fs.may_detach_mounts is set to avoid 'Device or resource busy' (CentOS/RedHat)\n    become: true\n    sysctl:\n      name: fs.may_detach_mounts\n      value: 1\n      sysctl_file: /etc/sysctl.d/99-docker.conf\n      reload: yes\n    when: may_detach_mounts.stat.exists\n\n  # Keep for compatibility reasons of this role. Now everything is in the same file.\n  - name: Remove systemd drop-in for Docker Mount Flags slave configuration (CentOS/RedHat)\n    become: true\n    file:\n      path: /etc/systemd/system/docker.service.d/mountflags-slave.conf\n      state: absent\n    notify: restart docker\n\n  - name: Set systemd service MountFlags option to \"slave\" to prevent \"device busy\" errors on CentOS/RedHat 7.3 kernels (CentOS/RedHat)\n    set_fact:\n      docker_systemd_service_config_tweaks: \"{{ docker_systemd_service_config_tweaks + _systemd_service_config_tweaks }}\"\n    vars:\n      _systemd_service_config_tweaks:\n        - 'MountFlags=slave'\n  when:\n    - _docker_os_dist == \"CentOS\" or _docker_os_dist == \"RedHat\"\n    - docker_enable_mount_flag_fix | bool\n    - ansible_kernel | version_compare('4', '<')\n\n- name: Best effort handling to directlvm for Debian 8 to get uniform behavior across distributions\n  block:\n  - name: Create LVM thinpool for Docker according to Docker documentation\n    include_tasks: lvm-thinpool.yml\n    vars:\n      pool:\n        name: thinpool\n        volume_group: docker\n        physical_volumes: \"{{ docker_daemon_config['storage-opts'] | select('match', '^dm.directlvm_device.+') | list | regex_replace('dm.directlvm_device=\\\\s*(.+)', '\\\\1') }}\"\n        metadata_size: \"1%VG\"\n        data_size: \"95%VG\"\n\n  - name: Modify storage-opts to handle problems with thinpool on Debian 8\n    set_fact:\n      _modified_storage_config: \"{{ (docker_daemon_config['storage-opts'] | difference(_exclusions)) + ['dm.thinpooldev=/dev/mapper/docker-thinpool-tpool'] }}\"\n    vars:\n      _exclusions: \"{{ docker_daemon_config['storage-opts'] | select('match', '^dm.directlvm_device.+') | list }}\"\n\n  - name: Update Docker daemon configuration to handle consistency between distributions\n    set_fact:\n      docker_daemon_config: \"{{ docker_daemon_config | combine(_updated_item, recursive=true) }}\"\n    vars:\n      _updated_item: \"{ 'storage-opts': {{ _modified_storage_config }} }\"\n\n  - name: Updated Docker daemon configuration\n    debug:\n      var: docker_daemon_config\n  when:\n    - _docker_os_dist == \"Debian\"\n    - _docker_os_dist_major_version == '8'\n    - docker_daemon_config['storage-opts'] is defined\n    - docker_daemon_config['storage-opts'] | select('match', '^dm.directlvm_device.+')\n"}, {"commit_sha": "5c1cde6cffcd979d0292e988da029c0ece96054c", "sha": "5b31bb9f9f1e74b0cea57def46d7a5661996e421", "filename": "tasks/apt.yml", "repository": "openwisp/ansible-openwisp2", "decoded_content": "- name: Update APT package cache\n  apt: update_cache=yes\n  changed_when: false\n\n- name: Install system packages\n  apt:\n    name:\n      - sudo\n      - software-properties-common\n      - build-essential\n      - supervisor\n      - nginx\n      - openssl\n      - libssl-dev\n      - libffi-dev\n      - python-dev\n      - redis-server\n      - cron\n    state: latest\n  notify:\n    - reload systemd\n    - start redis-server\n\n# On the newer versions of redis, by default redis\n# binds to localhost on ipv6 address which wouldn't\n# let the service start if the server doesn't have\n# ipv6 enabled. Hence, we set redis to listen on ipv4\n- name: set redis to listen on ipv4\n  notify: start redis-server\n  lineinfile:\n    path: /etc/redis/redis.conf\n    regexp: '^bind 127\\.0\\.0\\.1 ::1'\n    line: 'bind 127.0.0.1'\n    backrefs: yes\n\n- name: Install spatialite\n  when: openwisp2_database.engine == \"django.contrib.gis.db.backends.spatialite\"\n  apt:\n    name:\n      - sqlite3\n      - gdal-bin\n      - libproj-dev\n      - libgeos-dev\n      - libspatialite-dev\n    state: latest\n  notify: reload systemd\n\n- name: Install mod-spatialite (may fail on older linux distros)\n  when: openwisp2_database.engine == \"django.contrib.gis.db.backends.spatialite\"\n  ignore_errors: yes\n  apt: name=libsqlite3-mod-spatialite state=latest\n\n# fixes issue described in https://docs.ansible.com/ansible/become.html#becoming-an-unprivileged-user\n- name: Install acl if acting as non-root user\n  apt: name=acl state=latest\n  when: ansible_user is not defined or ansible_user != 'root'\n\n- name: ensure supervisor is started\n  service: name=supervisor state=started\n\n- name: Install python2 packages\n  when: openwisp2_python in [\"python2.7\", \"python2\"]\n  apt:\n    name:\n      - python-pip\n      - python-dev\n      - python-virtualenv\n    state: latest\n\n- name: Install python3 packages\n  when: openwisp2_python == \"python3\"\n  apt:\n    name:\n      - python3\n      - python3-pip\n      - python3-dev\n      - python-virtualenv\n    state: latest\n\n- name: Install python3-pyparsing\n  when: ansible_distribution == \"Debian\" and ansible_distribution_version == \"9.0\"\n  apt:\n    name:\n      - python3-pyparsing\n    state: latest\n\n- name: Install python wheel (optional, allowed to fail)\n  ignore_errors: yes\n  apt:\n    name:\n      - python-wheel\n      - python3-wheel\n    state: latest\n\n- name: Install python3-virtualenv\n  ignore_errors: yes\n  when: ansible_distribution != 'Ubuntu'\n  apt:\n    name:\n      - python3-virtualenv\n    state: latest\n\n- name: Install ntp\n  when: openwisp2_install_ntp\n  apt:\n    name: ntp\n    state: latest\n"}, {"commit_sha": "d944da6c34c3e510a807079256632482b53f00d2", "sha": "6c5b44d3000cd44e922d580077385bb9812fc632", "filename": "tasks/configure-docker.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Ensure /etc/docker directory exists\n  become: true\n  file:\n    path: /etc/docker\n    state: directory\n    mode: 0755\n\n- name: Configure Docker daemon (file)\n  become: true\n  copy:\n    src: \"{{ docker_daemon_config_file }}\"\n    dest: /etc/docker/daemon.json\n  notify: restart docker\n  when: docker_daemon_config_file is defined\n\n- name: Configure Docker daemon (variables)\n  become: true\n  copy:\n    content: \"{{ docker_daemon_config | to_nice_json }}\"\n    dest: /etc/docker/daemon.json\n  notify: restart docker\n  when: docker_daemon_config_file is not defined and\n        docker_daemon_config is defined\n\n- name: Ensure Docker default user namespace is defined in subuid and subgid\n  become: true\n  lineinfile:\n    path: \"{{ item }}\"\n    regexp: '^dockremap'\n    line: 'dockremap:500000:65536'\n  with_items:\n    - /etc/subuid\n    - /etc/subgid\n  when: (_docker_os_dist == \"CentOS\" or _docker_os_dist == \"RedHat\") and\n        ((docker_daemon_config is defined and\n        docker_daemon_config['userns-remap'] is defined and\n        docker_daemon_config['userns-remap'] == 'default') or\n        docker_bug_usermod|bool == true)\n\n- name: Ensure Docker users are added to the docker group\n  become: true\n  user:\n    name: \"{{ item }}\"\n    groups: docker\n    append: true\n  with_items: \"{{ docker_users }}\"\n\n- name: Ensure lvm2 is installed when devicemapper is used (Fedora/Debian)\n  become: true\n  package:\n    name: lvm2\n    state: present\n  when: (_docker_os_dist == \"Fedora\" or _docker_os_dist == \"Debian\") and\n        docker_daemon_config['storage-driver'] is defined and\n        docker_daemon_config['storage-driver'] == 'devicemapper'\n\n- name: Ensure thin-provisioning-tools is installed when devicemapper is used (Ubuntu/Debian)\n  become: true\n  package:\n    name: thin-provisioning-tools\n    state: present\n  when: (_docker_os_dist == \"Ubuntu\" or _docker_os_dist == \"Debian\") and\n        docker_daemon_config['storage-driver'] is defined and\n        docker_daemon_config['storage-driver'] == 'devicemapper'\n\n- name: Enable Docker service\n  become: true\n  service:\n    name: docker\n    enabled: yes\n  notify: restart docker\n  register: docker_service\n\n- name: Trigger start/restart of Docker\n  service:\n    name: docker\n  notify: restart docker\n  changed_when: docker_service.status.SubState != \"running\"\n  when: docker_service.status is defined\n"}, {"commit_sha": "f85435227eb23c6e474103286c17d7406baeff47", "sha": "3f26c78190cf10a8d222501b8e640544f4cb1d06", "filename": "roles/zookeeper/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for zookeeper\nzookeeper_config_dir: \"/etc/zookeeper/conf\"\nzookeeper_image: \"mesosphere/mesos:0.25.0-0.2.70.ubuntu1404\"\nzookeeper_client_port: 2181\nzookeeper_leader_connect_port: 2888\nzookeeper_leader_election_port: 3888\nzookeeper_server_group: zookeeper_servers\nzookeeper_id: \"\n\t{%- if zookeeper_host_list is defined -%}\n\t\t{%- for host in zookeeper_host_list.split() -%}\n\t\t\t{%- if host == ansible_eth0.ipv4.address -%}\n\t        \t{{ loop.index }}\n\t\t\t{%- endif -%}\n\t\t{%- endfor -%}\n\t{%- else -%}\n    \t{%- for host in groups[zookeeper_server_group] -%}\n      \t\t{%- if host == 'default' or host == inventory_hostname or host == ansible_fqdn or host in ansible_all_ipv4_addresses -%}\n        \t\t{{ loop.index }}\n  \t\t\t{%- endif -%}\n    \t{%- endfor -%}\n    {%- endif -%}\n\"\nconsul_dir: /etc/consul.d\nzookeeper_rebuild_container: false\n"}, {"commit_sha": "669ef8a0840eb58b29fca67110f86c4e2d99eebc", "sha": "402f07b57386df4615209951a6456af37047403f", "filename": "meta/main.yml", "repository": "geerlingguy/ansible-role-solr", "decoded_content": "---\ndependencies: []\n\ngalaxy_info:\n  author: geerlingguy\n  description: Apache Solr for Linux.\n  company: \"Midwestern Mac, LLC\"\n  license: \"license (BSD, MIT)\"\n  min_ansible_version: 2.4\n  platforms:\n    - name: EL\n      versions:\n      - 6\n      - 7\n    - name: Debian\n      versions:\n      - all\n    - name: Ubuntu\n      versions:\n      - all\n  galaxy_tags:\n    - development\n    - solr\n    - search\n    - lucene\n    - container\n    - apache\n    - text\n"}, {"commit_sha": "9f3bd0a4feb65ecf44809ac5403bfa1b7f3f92db", "sha": "ca3176568462ba1641f603bec9c9fdf0d82e0727", "filename": "tasks/cleanup.yml", "repository": "fubarhouse/ansible-role-golang", "decoded_content": "---\n\n- name: \"Go-Lang | Removing GOROOT\"\n  file:\n    path: \"{{ GOROOT }}\"\n    state: absent\n  failed_when: false\n\n- name: \"Go-Lang | Removing GOPATH\"\n  file:\n    path: \"{{ GOPATH }}\"\n    state: absent\n  failed_when: false\n\n- name: \"Go-Lang | Removing GOBOOTSTRAP\"\n  file:\n    path: \"{{ GOROOT_BOOTSTRAP }}\"\n    state: absent\n  when:\n   - GOROOT_BOOTSTRAP is defined\n   - install_go_bootstrap|bool == true\n  failed_when: false\n\n- name: \"Go-Lang | Define shell exports\"\n  set_fact:\n    shell_exports:\n      - regex: \"export GOROOT\"\n        lineinfile: \"export GOROOT={{ GOROOT }}\"\n      - regex: \"export GOPATH\"\n        lineinfile: \"export GOPATH={{ GOPATH }}\"\n      - regex: \"PATH:{{ GOROOT }}/bin\"\n        lineinfile: \"export $PATH:{{ GOROOT }}/bin\"\n      - regex: \"PATH:{{ GOPATH }}\"\n        lineinfile: \"export $PATH=$PATH:{{ GOPATH }}\"\n      - regex: \"PATH:{{ GOPATH }}/bin\"\n        lineinfile: \"export $PATH=$PATH:{{ GOPATH }}/bin\"\n  when: shell_exports is not defined\n\n- name: \"Go-Lang | Detect configured shell profiles\"\n  stat:\n    path: \"{{ fubarhouse_user_dir }}/{{ item }}\"\n  changed_when: false\n  failed_when: false\n  with_items: \"{{ shell_profiles }}\"\n  register: stat_shell_profiles\n  when:\n  - shell_profiles is defined\n  - stat_shell_profiles is not defined\n\n- name: \"Go-Lang | Ensure shell profiles are clean\"\n  lineinfile:\n    dest: \"{{ item[0].stat.path }}\"\n    regexp: \"{{ item[1].regex }}\"\n    line: \"{{ item[1].lineinfile }}\"\n    state: absent\n  with_nested:\n  - \"{{ stat_shell_profiles.results }}\"\n  - \"{{ shell_exports }}\"\n  when:\n  - shell_profiles is defined\n  - shell_exports is defined\n  - item[0].stat.exists|bool == true\n"}, {"commit_sha": "b4e1f0b868b5b344029a2a379812bd7112534a26", "sha": "377da8f4a0e8a1455ff9eb42ced862040df4b618", "filename": "tasks/section1.yml", "repository": "florianutz/Ubuntu1604-CIS", "decoded_content": "---\n- name: \"SCORED | 1.1.1.1 | PATCH | Ensure mounting of cramfs filesystems is disabled\"\n  lineinfile:\n      dest: /etc/modprobe.d/CIS.conf\n      regexp: \"^(#)?install cramfs(\\\\s|$)\"\n      line: \"install cramfs /bin/true\"\n      state: present\n      owner: root\n      group: root\n      mode: 0644\n      create: true\n  when:\n      - ubuntu1604cis_rule_1_1_1_1\n  tags:\n      - level1\n      - scored\n      - patch\n      - cramfs\n      - filesystems\n      - rule_1.1.1.1\n\n- name: \"SCORED | 1.1.1.1 | PATCH | Remove cramfs module\"\n  modprobe:\n      name: cramfs\n      state: absent\n  when:\n      - ubuntu1604cis_rule_1_1_1_1\n  tags:\n      - level1\n      - scored\n      - patch\n      - cramfs\n      - filesystems\n      - rule_1.1.1.1\n\n- name: \"SCORED | 1.1.1.2 | PATCH | Ensure mounting of freevxfs filesystems is disabled\"\n  lineinfile:\n      dest: /etc/modprobe.d/CIS.conf\n      regexp: \"^(#)?install freevxfs\"\n      line: \"install freevxfs /bin/true\"\n      state: present\n      create: true\n  when:\n      - ubuntu1604cis_rule_1_1_1_2\n  tags:\n      - level1\n      - scored\n      - patch\n      - freevxfs\n      - filesystems\n      - rule_1.1.1.2\n\n- name: \"SCORED | 1.1.1.2 | PATCH | Remove freevxfs module\"\n  modprobe:\n      name: freevxfs\n      state: absent\n  when:\n      - ubuntu1604cis_rule_1_1_1_2\n  tags:\n      - level1\n      - scored\n      - patch\n      - freevxfs\n      - filesystems\n      - rule_1.1.1.2\n\n- name: \"SCORED | 1.1.1.3 | PATCH | Ensure mounting of jffs2 filesystems is disabled\"\n  lineinfile:\n      dest: /etc/modprobe.d/CIS.conf\n      regexp: \"^(#)?install jffs2(\\\\s|$)\"\n      line: \"install jffs2 /bin/true\"\n      state: present\n      create: true\n  when:\n      - ubuntu1604cis_rule_1_1_1_3\n  tags:\n      - level1\n      - scored\n      - patch\n      - jffs2\n      - filesystems\n      - rule_1.1.1.3\n\n- name: \"SCORED | 1.1.1.3 | PATCH | Remove jffs2 module\"\n  modprobe:\n      name: jffs2\n      state: absent\n  when:\n      - ubuntu1604cis_rule_1_1_1_3\n  tags:\n      - level1\n      - scored\n      - patch\n      - jffs2\n      - filesystems\n      - rule_1.1.1.3\n\n- name: \"SCORED | 1.1.1.4 | PATCH | Ensure mounting of hfs filesystems is disabled\"\n  lineinfile:\n      dest: /etc/modprobe.d/CIS.conf\n      regexp: \"^(#)?install hfs(\\\\s|$)\"\n      line: \"install hfs /bin/true\"\n      state: present\n      create: true\n  when:\n      - ubuntu1604cis_rule_1_1_1_4\n  tags:\n      - level1\n      - scored\n      - patch\n      - hfs\n      - filesystems\n      - rule_1.1.1.4\n\n- name: \"SCORED | 1.1.1.4 | PATCH | Remove hfs module\"\n  modprobe:\n      name: hfs\n      state: absent\n  when:\n      - ubuntu1604cis_rule_1_1_1_4\n  tags:\n      - level1\n      - scored\n      - patch\n      - hfs\n      - filesystems\n      - rule_1.1.1.4\n\n- name: \"SCORED | 1.1.1.5 | PATCH | Ensure mounting of hfsplus filesystems is disabled\"\n  lineinfile:\n      dest: /etc/modprobe.d/CIS.conf\n      regexp: \"^(#)?install hfsplus(\\\\s|$)\"\n      line: \"install hfsplus /bin/true\"\n      state: present\n      create: true\n  when:\n      - ubuntu1604cis_rule_1_1_1_5\n  tags:\n      - level1\n      - scored\n      - patch\n      - hfsplus\n      - filesystems\n      - rule_1.1.1.5\n\n- name: \"SCORED | 1.1.1.5 | PATCH | Remove hfsplus module\"\n  modprobe:\n      name: hfsplus\n      state: absent\n  when:\n      - ubuntu1604cis_rule_1_1_1_5\n  tags:\n      - level1\n      - scored\n      - patch\n      - hfsplus\n      - filesystems\n      - rule_1.1.1.5\n\n- name: \"SCORED | 1.1.1.6 | PATCH | Ensure mounting of squashfs filesystems is disabled\"\n  lineinfile:\n      dest: /etc/modprobe.d/CIS.conf\n      regexp: \"^(#)?install squashfs(\\\\s|$)\"\n      line: \"install squashfs /bin/true\"\n      state: present\n      create: true\n  when:\n      - ubuntu1604cis_rule_1_1_1_6\n  tags:\n      - level1\n      - scored\n      - patch\n      - squashfs\n      - filesystems\n      - rule_1.1.1.6\n\n- name: \"SCORED | 1.1.1.6 | PATCH | Remove squashfs module\"\n  modprobe:\n      name: squashfs\n      state: absent\n  when:\n      - ubuntu1604cis_rule_1_1_1_6\n  tags:\n      - level1\n      - scored\n      - patch\n      - squashfs\n      - filesystems\n      - rule_1.1.1.6\n\n- name: \"SCORED | 1.1.1.7 | PATCH | Ensure mounting of udf filesystems is disabled\"\n  lineinfile:\n      dest: /etc/modprobe.d/CIS.conf\n      regexp: \"^(#)?install udf(\\\\s|$)\"\n      line: \"install udf /bin/true\"\n      state: present\n      create: true\n  when:\n      - ubuntu1604cis_rule_1_1_1_7\n  tags:\n      - level1\n      - scored\n      - patch\n      - udf\n      - filesystems\n      - rule_1.1.1.7\n\n- name: \"SCORED | 1.1.1.7 | PATCH | Remove udf module\"\n  modprobe:\n      name: udf\n      state: absent\n  when:\n      - ubuntu1604cis_rule_1_1_1_7\n  tags:\n      - level1\n      - scored\n      - patch\n      - udf\n      - filesystems\n      - rule_1.1.1.7\n\n- name: \"SCORED | 1.1.1.8 | PATCH | Ensure mounting of FAT filesystems is disabled\"\n  lineinfile:\n      dest: /etc/modprobe.d/CIS.conf\n      regexp: \"^(#)?install vfat(\\\\s|$)\"\n      line: \"install vfat /bin/true\"\n      state: present\n      create: true\n  when:\n      - ubuntu1604cis_rule_1_1_1_8\n  tags:\n      - level1\n      - scored\n      - patch\n      - vfat\n      - filesystems\n      - rule_1.1.1.8\n\n- name: \"SCORED | 1.1.1.8 | PATCH | Remove FAT module\"\n  modprobe:\n      name: vfat\n      state: absent\n  when:\n      - ubuntu1604cis_rule_1_1_1_8\n  tags:\n      - level2\n      - scored\n      - patch\n      - vfat\n      - filesystems\n      - rule_1.1.1.8\n\n- name: \"SCORED | 1.1.2 | PATCH | Ensure separate partition exists for /tmp | enable and start/restart tmp.mount\"\n  copy:\n      src: \"{{ tmp_mount_file[ansible_os_family] }}\"\n      dest: /etc/systemd/system/tmp.mount\n      owner: root\n      group: root\n      mode: 0644\n      force: true\n      remote_src: true\n  notify:\n      - systemd restart tmp.mount\n  when:\n      - ubuntu1604cis_rule_1_1_2\n      - ubuntu1604cis_skip_for_travis == false\n  tags:\n      - level2\n      - scored\n      - patch\n      - rule_1.1.2\n\n- name: \"SCORED | 1.1.2 | PATCH | Ensure separate partition exists for /tmp | enable and start/restart tmp.mount\"\n  systemd:\n      name: tmp.mount\n      daemon_reload: yes\n      enabled: yes\n      masked: no\n      state: started\n  when:\n      - ubuntu1604cis_rule_1_1_2\n      - ubuntu1604cis_skip_for_travis == false\n  tags:\n      - level2\n      - scored\n      - patch\n      - rule_1.1.2\n\n- name: \"SCORED | 1.1.3 | PATCH | Ensure nodev option set on /tmp partition\\n\n         SCORED | 1.1.4 | PATCH | Ensure nosuid option set on /tmp partition\\n\n         | drop custom tmp.mount\"\n  ini_file:\n      path: \"{{ item }}\"\n      section: Mount\n      option: Options\n      value: \"{{ tmp_mount_options[ansible_os_family] }}\"\n      no_extra_spaces: true\n  with_items:\n      - \"{{ tmp_mount_file[ansible_os_family] }}\"\n      - /etc/systemd/system/tmp.mount\n  notify:\n      - systemd restart tmp.mount\n  when:\n      - ubuntu1604cis_rule_1_1_3\n      - ubuntu1604cis_rule_1_1_4\n  tags:\n      - level1\n      - scored\n      - patch\n      - rule_1.1.3\n      - rule_1.1.4\n\n- name: \"SCORED | 1.1.5 | PATCH | Ensure separate partition exists for /var\"\n  shell: mount | grep \"on /var \"\n  register: var_mounted\n  changed_when: false\n  failed_when: false\n  when:\n      - ubuntu1604cis_rule_1_1_5\n  tags:\n      - level2\n      - scored\n      - patch\n      - rule_1.1.5\n      - skip_ansible_lint\n\n- name: \"SCORED | 1.1.6 | PATCH | Ensure separate partition exists for /var/tmp\"\n  shell: mount | grep \"on /var/tmp \"\n  register: var_tmp_mounted\n  changed_when: false\n  failed_when: false\n  when:\n      - ubuntu1604cis_rule_1_1_6\n  tags:\n      - level2\n      - scored\n      - patch\n      - rule_1.1.6\n      - skip_ansible_lint\n\n- name: \"SCORED | 1.1.7  | PATCH | Ensure nodev option set on /var/tmp partition\\n\n         SCORED | 1.1.8  | PATCH | Ensure nosuid option set on /var/tmp partition\\n\n         SCORED | 1.1.9 | PATCH | Ensure noexec option set on /var/tmp partition\"\n  mount:\n      name: /var/tmp\n      src: \"{{ ubuntu1604cis_vartmp['source'] }}\"\n      state: mounted\n      fstype: \"{{ ubuntu1604cis_vartmp['fstype'] }}\"\n      opts: \"{{ ubuntu1604cis_vartmp['opts'] }}\"\n  when:\n      - ubuntu1604cis_vartmp['enabled'] == 'yes'\n      - ubuntu1604cis_rule_1_1_7\n      - ubuntu1604cis_rule_1_1_8\n      - ubuntu1604cis_rule_1_1_9\n  tags:\n      - level1\n      - scored\n      - patch\n      - rule_1.1.7\n      - rule_1.1.8\n      - rule_1.1.9\n\n- name: \"SCORED | 1.1.10 | PATCH | Ensure separate partition exists for /var/log\"\n  shell: mount | grep \"on /var/log \"\n  register: var_log_mounted\n  changed_when: false\n  failed_when: false\n  when:\n      - ubuntu1604cis_rule_1_1_10\n  tags:\n      - level2\n      - scored\n      - patch\n      - rule_1.1.10\n      - skip_ansible_lint\n\n- name: \"SCORED | 1.1.11 | PATCH | Ensure separate partition exists for /var/log/audit\"\n  shell: mount | grep \"on /var/log/audit \"\n  register: var_log_audit_mounted\n  changed_when: false\n  failed_when: false\n  when:\n      - ubuntu1604cis_rule_1_1_11\n  tags:\n      - level2\n      - scored\n      - patch\n      - rule_1.1.11\n      - skip_ansible_lint\n\n- name: \"SCORED | 1.1.12 | PATCH | Ensure separate partition exists for /home\"\n  shell: mount | grep \"on /home \"\n  register: home_mounted\n  changed_when: false\n  failed_when: false\n  when:\n      - ubuntu1604cis_rule_1_1_12\n  tags:\n      - level2\n      - scored\n      - patch\n      - rule_1.1.12\n      - skip_ansible_lint\n\n- name: \"SCORED | 1.1.13 | PATCH | Ensure nodev option set on /home partition\"\n  mount:\n      name: \"/home\"\n      src: \"{{ item.device }}\"\n      state: mounted\n      fstype: \"{{ item.fstype }}\"\n      opts: \"nodev\"\n  when:\n      - ubuntu1604cis_rule_1_1_13\n      - item.mount == \"/home\"\n  with_items:\n      - \"{{ ansible_mounts }}\"\n  tags:\n      - scored\n      - level1\n      - patch\n      - rule_1.1.13\n\n- name: \"SCORED | 1.1.14 | PATCH | Ensure nodev option set on /dev/shm partition\\n\n         SCORED | 1.1.15 | PATCH | Ensure nosuid option set on /dev/shm partition\\n\n         SCORED | 1.1.16 | PATCH | Ensure noexec option set on /dev/shm partition\"\n  mount:\n      name: /dev/shm\n      src: tmpfs\n      state: mounted\n      fstype: tmpfs\n      opts: \"defaults,nodev,nosuid,noexec\"\n  when:\n      - ubuntu1604cis_rule_1_1_14\n      - ubuntu1604cis_rule_1_1_15\n      - ubuntu1604cis_rule_1_1_16\n  tags:\n      - level1\n      - scored\n      - patch\n      - rule_1.1.14\n      - rule_1.1.15\n      - rule_1.1.16\n\n- name: \"NOTSCORED | 1.1.17 | PATCH | Ensure nodev option set on removable media partitions\"\n  command: /bin/true\n  changed_when: false\n  when:\n      - ubuntu1604cis_rule_1_1_17\n  tags:\n      - level1\n      - notscored\n      - patch\n      - rule_1.1.18\n      - notimplemented\n\n- name: \"NOTSCORED | 1.1.18 | PATCH | Ensure nosuid option set on removable media partitions\"\n  command: /bin/true\n  changed_when: false\n  when:\n      - ubuntu1604cis_rule_1_1_18\n  tags:\n      - level1\n      - notscored\n      - patch\n      - rule_1.1.18\n      - notimplemented\n\n- name: \"NOTSCORED | 1.1.19 | PATCH | Ensure noexec option set on removable media partitions\"\n  command: /bin/true\n  changed_when: false\n  when:\n      - ubuntu1604cis_rule_1_1_19\n  tags:\n      - level1\n      - notscored\n      - patch\n      - rule_1.1.19\n      - notimplemented\n\n- name: \"SCORED | 1.1.20 | PATCH | Ensure sticky bit is set on all world-writable directories\"\n  shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -type d -perm -0002 2>/dev/null | xargs chmod a+t\n  changed_when: false\n  failed_when: false\n  when:\n      - ubuntu1604cis_rule_1_1_20\n      # - sticky_bit_on_worldwritable_dirs_audit.rc == '0'\n  tags:\n      - level1\n      - scored\n      - patch\n      - rule_1.1.20\n\n- name: \"SCORED | 1.1.21 | PATCH | Disable Automounting\"\n  service:\n      name: autofs\n      enabled: false\n  when:\n      - ubuntu1604cis_allow_autofs == false and autofs_service_status.stdout == \"loaded\"\n      - ubuntu1604cis_rule_1_1_21\n  tags:\n      - level1\n      - scored\n      - patch\n      - rule_1.1.21\n\n- name: \"NOTSCORED | 1.2.1 | PATCH | Ensure package manager repositories are configured\"\n  command: /bin/true\n  changed_when: false\n  when:\n      - ubuntu1604cis_rule_1_2_1\n  tags:\n      - level1\n      - notscored\n      - patch\n      - rule_1.2.1\n\n\n- name: \"NOTSCORED | 1.2.3 | PATCH | Ensure GPG keys are configured\"\n  command: /bin/true\n  changed_when: false\n  when:\n      - ubuntu1604cis_rule_1_2_3\n  tags:\n      - level1\n      - notscored\n      - patch\n      - rule_1.2.3\n      - notimplemented\n\n- name: \"SCORED | 1.3.1 | PATCH | Ensure AIDE is installed\"\n  apt:\n      name: aide\n      state: present\n      install_recommends: false\n  when:\n      - ubuntu1604cis_rule_1_3_1\n  tags:\n      - level1\n      - scored\n      - aide\n      - patch\n      - rule_1.3.1\n\n- name: \"SCORED | 1.3.1 | PATCH | Ensure AIDE is installed\"\n  command: /usr/bin/aide --init -B 'database_out=file:/var/lib/aide/aide.db.gz'\n  args:\n      creates: /var/lib/aide/aide.db.gz\n  changed_when: false\n  failed_when: false\n  async: 45\n  poll: 0\n  when:\n      - ubuntu1604cis_config_aide\n      - ubuntu1604cis_rule_1_3_1\n  tags:\n      - level1\n      - scored\n      - aide\n      - patch\n      - rule_1.3.1\n\n- name: \"SCORED | 1.3.2 | PATCH | Ensure filesystem integrity is regularly checked\"\n  cron:\n      name: Run AIDE integrity check weekly\n      cron_file: \"{{ ubuntu1604cis_aide_cron['cron_file'] }}\"\n      user: \"{{ ubuntu1604cis_aide_cron['cron_user'] }}\"\n      minute: \"{{ ubuntu1604cis_aide_cron['aide_minute'] | default('0') }}\"\n      hour: \"{{ ubuntu1604cis_aide_cron['aide_hour'] | default('5') }}\"\n      day: \"{{ ubuntu1604cis_aide_cron['aide_day'] | default('*') }}\"\n      month: \"{{ ubuntu1604cis_aide_cron['aide_month'] | default('*') }}\"\n      weekday: \"{{ ubuntu1604cis_aide_cron['aide_weekday'] | default('*') }}\"\n      job: \"{{ ubuntu1604cis_aide_cron['aide_job'] }}\"\n  when:\n      - ubuntu1604cis_rule_1_3_2\n  tags:\n      - level1\n      - scored\n      - aide\n      - file_integrity\n      - patch\n      - rule_1.3.2\n\n- name: \"SCORED | 1.4.1 | PATCH | Ensure permissions on bootloader config are configured\"\n  file:\n      path: \"/boot/grub/grub.cfg\"\n      owner: root\n      group: root\n      mode: 0600\n  when: \n      - ansible_os_family == \"Debian\"\n      - ubuntu1604cis_rule_1_4_1\n  tags:\n      - level1\n      - scored\n      - grub\n      - patch\n      - rule_1.4.1\n\n- name: \"SCORED | 1.4.2 | PATCH | Ensure bootloader password is set\"\n  grub_crypt:\n      password: \"{{ ubuntu1604cis_bootloader_password }}\"\n  register: grub_pass\n  when:\n      - ubuntu1604cis_set_boot_pass\n      - ubuntu1604cis_rule_1_4_2\n  tags:\n      - level1\n      - scored\n      - grub\n      - patch\n      - rule_1.4.2\n      - notimplemented\n\n- name: \"NOTSCORED | 1.4.3 | PATCH | Ensure authentication required for single user mode\"\n  command: /bin/true\n  changed_when: false\n  when:\n      - ubuntu1604cis_rule_1_4_3\n  tags:\n      - level1\n      - notscored\n      - patch\n      - rule_1.4.3\n      - notimplemented\n\n- name: \"SCORED | 1.5.1 | PATCH | Ensure core dumps are restricted\"\n  lineinfile:\n      state: present\n      dest: /etc/security/limits.conf\n      regexp: '^#?\\\\*.*core'\n      line: '*                hard    core            0'\n      insertbefore: '^# End of file'\n  when:\n      - ubuntu1604cis_rule_1_5_1\n  tags:\n      - level1\n      - scored\n      - limits\n      - patch\n      - rule_1.5.1\n\n- name: \"SCORED | 1.5.1 | PATCH | Ensure core dumps are restricted\"\n  sysctl:\n      name: fs.suid_dumpable\n      value: 0\n      state: present\n      reload: true\n      sysctl_set: true\n      ignoreerrors: true\n  when:\n      - ubuntu1604cis_rule_1_5_1\n  tags:\n      - level1\n      - scored\n      - sysctl\n      - patch\n      - rule_1.5.1\n\n- name: \"NOTSCORED | 1.5.2 | PATCH | Ensure XD/NX support is enabled\"\n  shell: dmesg | grep -E \"NX|XD\" | grep \" active\"\n  changed_when: false\n  when:\n      - ubuntu1604cis_rule_1_5_2\n  tags:\n      - level1\n      - notscored\n      - patch\n      - rule_1.5.2\n      - notimplemented\n\n- name: \"SCORED | 1.5.3 | PATCH | Ensure address space layout randomization (ASLR) is enabled\"\n  sysctl:\n      name: kernel.randomize_va_space\n      value: 2\n      state: present\n      reload: true\n      sysctl_set: true\n      ignoreerrors: true\n  when:\n      - ubuntu1604cis_rule_1_5_3\n  tags:\n      - level1\n      - scored\n      - patch\n      - sysctl\n      - rule_1.5.3\n\n- name: \"SCORED | 1.5.4 | PATCH | Ensure prelink is disabled\"\n  command: prelink -ua\n  when:\n      - prelink_installed.rc == 0\n      - ubuntu1604cis_rule_1_5_4\n  tags:\n      - level1\n      - scored\n      - patch\n      - rule_1.5.4\n\n- name: \"SCORED | 1.5.4 | PATCH | Ensure prelink is disabled\"\n  apt:\n      name: prelink\n      state: absent\n  when:\n      - ubuntu1604cis_rule_1_5_4\n  tags:\n      - level1\n      - scored\n      - patch\n      - rule_1.5.4\n\n- name: \"SCORED | 1.6.1.1 | PATCH | Ensure SELinux is not disabled in bootloader configuration\"\n  replace:\n      dest: /etc/default/grub\n      regexp: '(selinux|enforcing)\\s*=\\s*0\\s*'\n      follow: true\n  register: selinux_grub_patch\n  ignore_errors: true\n  notify: generate new grub config\n  when:\n      - ubuntu1604cis_rule_1_6_1_1\n  tags:\n      - level2\n      - scored\n      - patch\n      - rule_1.6.1.1\n\n- name: \"SCORED | 1.6.1.2 | PATCH | Ensure the SELinux state is enforcing\"\n  command: /bin/true\n  changed_when: false\n  when:\n      - ubuntu1604cis_rule_1_6_1_2\n  tags:\n      - level1\n      - scored\n      - patch\n      - rule_1.6.1.2\n      - notimplemented\n\n- name: \"SCORED | 1.6.1.3 | PATCH | Ensure SELinux policy is configured\"\n  command: /bin/true\n  changed_when: false\n  when:\n      - ubuntu1604cis_rule_1_6_1_3\n  tags:\n      - level1\n      - scored\n      - patc3\n      - rule_1.6.1.2\n      - notimplemented\n\n- name: \"SCORED | 1.6.1.4 | PATCH | Ensure no unconfined daemons exist\"\n  command: /bin/true\n  changed_when: false\n  when:\n      - ubuntu1604cis_rule_1_6_1_4\n  tags:\n      - level1\n      - scored\n      - patch\n      - rule_1.6.1.4\n      - notimplemented\n\n- name: \"SCORED | 1.6.2.1 | PATCH | Ensure AppArmor is not disabled in bootloader configuration\"\n  command: /bin/true\n  changed_when: false\n  when:\n      - ubuntu1604cis_rule_1_6_2_1\n  tags:\n      - level1\n      - scored\n      - patch\n      - rule_1.6.2.1\n      - notimplemented\n\n- name: \"SCORED | 1.6.2.2 | PATCH | Ensure all AppArmor Profiles are enforcing\"\n  command: /bin/true\n  changed_when: false\n  when:\n      - ubuntu1604cis_rule_1_6_2_2\n  tags:\n      - level1\n      - scored\n      - patch\n      - rule_1.6.2.2\n      - notimplemented\n\n- name: \"SCORED | 1.6.3 | PATCH | Ensure SELinux or AppArmor are installed\"\n  command: /bin/true\n  changed_when: false\n  when:\n      - ubuntu1604cis_rule_1_6_3\n  tags:\n      - level1\n      - scored\n      - patch\n      - rule_1.6.3\n      - notimplemented\n\n\n- name: \"SCORED | 1.7.1.1 | PATCH | Ensure message of the day is configured properly\"\n  template:\n      src: etc/motd.j2\n      dest: /etc/motd\n  when:\n      - ubuntu1604cis_rule_1_7_1_1\n  tags:\n      - level1\n      - scored\n      - patch\n      - banner\n      - rule_1.7.1.1\n\n- name: \"NOTSCORED | 1.7.1.2 | PATCH | Ensure local login warning banner is configured properly\"\n  template:\n      src: etc/issue.j2\n      dest: /etc/issue\n  when:\n      - ubuntu1604cis_rule_1_7_1_2\n  tags:\n      - level1\n      - notscored\n      - patch\n      - banner\n      - rule_1.7.1.2\n\n- name: \"NOTSCORED | 1.7.1.3 | PATCH | Ensure remote login warning banner is configured properly\"\n  template:\n      src: etc/issue.net.j2\n      dest: /etc/issue.net\n  when:\n      - ubuntu1604cis_rule_1_7_1_3\n  tags:\n      - level1\n      - notscored\n      - patch\n      - banner\n      - rule_1.7.1.3\n\n- name: \"NOTSCORED | 1.7.1.4 | PATCH | Ensure permissions on /etc/motd are configured\"\n  file:\n      dest: /etc/motd\n      state: file\n      owner: root\n      group: root\n      mode: 0644\n  when:\n      - ubuntu1604cis_rule_1_7_1_4\n  tags:\n      - level1\n      - notscored\n      - patch\n      - perms\n      - rule_1.7.1.4\n\n- name: \"SCORED | 1.7.1.5 | PATCH | Ensure permissions on /etc/issue are configured\"\n  file:\n      dest: /etc/issue\n      state: file\n      owner: root\n      group: root\n      mode: 0644\n  when:\n      - ubuntu1604cis_rule_1_7_1_5\n  tags:\n      - level1\n      - scored\n      - patch\n      - perms\n      - rule_1.7.1.5\n\n- name: \"NOTSCORED | 1.7.1.6 | PATCH | Ensure permissions on /etc/issue.net are configured\"\n  file:\n      dest: /etc/issue.net\n      state: file\n      owner: root\n      group: root\n      mode: 0644\n  when:\n      - ubuntu1604cis_rule_1_7_1_6\n  tags:\n      - level1\n      - notscored\n      - patch\n      - perms\n      - rule_1.7.1.6\n\n- name: \"SCORED | 1.7.2 | PATCH | Ensure GDM login banner is configured\"\n  lineinfile:\n      dest: \"{{ item.file }}\"\n      regexp: \"{{ item.regexp }}\"\n      line: \"{{ item.line }}\"\n      state: present\n      create: true\n      owner: root\n      group: root\n      mode: 0644\n  with_items:\n      - { file: '/etc/dconf/profile/gdm', regexp: 'user-db', line: 'user-db:user' }\n      - { file: '/etc/dconf/profile/gdm', regexp: 'system-db', line: 'system-db:gdm' }\n      - { file: '/etc/dconf/profile/gdm', regexp: 'file-db', line: 'file-db:/usr/share/gdm/greeter-dconf-defaults' }\n      - { file: '/etc/dconf/db/gdm.d/01-banner-message', regexp: '\\[org\\/gnome\\/login-screen\\]', line: '[org/gnome/login-screen]' }\n      - { file: '/etc/dconf/db/gdm.d/01-banner-message', regexp: 'banner-message-enable', line: 'banner-message-enable=true' }\n      - { file: '/etc/dconf/db/gdm.d/01-banner-message', regexp: 'banner-message-text', line: \"banner-message-text='{{ ubuntu1604cis_warning_banner }}' \" }\n  when:\n      - ubuntu1604cis_gui\n      - ubuntu1604cis_rule_1_7_2\n  tags:\n      - level1\n      - scored\n      - patch\n      - banner\n      - rule_1.7.2\n\n- name: \"NOTSCORED | 1.8 | PATCH | Ensure updates, patches, and additional security software are installed\"\n  apt:\n      upgrade: dist\n  tags:\n      - level1\n      - notscored\n      - patch\n      - rule_1.8\n      - skip_ansible_lint\n"}, {"commit_sha": "f16d960feb476f28b8e219a7c9aee6f89f1a77ef", "sha": "eb7a695b05f3b6997f7f863c7b32ce2561c6b910", "filename": "playbooks/common.yml", "repository": "trailofbits/algo", "decoded_content": "- name: Install prerequisites\n  raw: sudo apt-get update -qq && sudo apt-get install -qq -y python2.7\n\n- name: Configure defaults\n  raw: sudo update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1\n  tags:\n    - update-alternatives\n"}, {"commit_sha": "f85435227eb23c6e474103286c17d7406baeff47", "sha": "ca9f04f1f431bf43ccab863e91b865c3cf7b2a82", "filename": "roles/dnsmasq/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for dnsmasq\ndnsmasq_config_folder: \"/etc/dnsmasq.d\"\ndnsmasq_resolvconf_file: \"{{ dnsmasq_config_folder }}/resolv.conf~\"\ndnsmasq_rebuild_container: false\ndnsmasq_image: \"andyshinn/dnsmasq\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "d7b1e862aedad429ccd8a486cf9e01ff065a0ef3", "filename": "roles/config-lvm/tasks/lvm.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Setup and create PV & VG\"\n  lvg:\n    vg: \"{{ item.vg_name }}\"\n    pvs: \"{{ item.storage_device }}\"\n\n- name: \"Setup LV\"\n  lvol: \n    vg: \"{{ item.vg_name }}\"\n    lv: \"{{ item.lv_name }}\"\n    size: \"{{ item.lv_size | default(default_lv_size) }}\"\n\n- name: \"Create file system on share\"\n  filesystem:\n    fstype: \"{{ item.lv_fstype | default(lvm_fstype) }}\"\n    dev: \"/dev/mapper/{{ item.vg_name }}-{{ item.lv_name }}\"\n\n- name: \"Ensure the mount dir exists\" \n  file:\n    path: \"{{ item.mount_path }}\"\n    state: directory\n\n- name: \"Mount LVM to directory\"\n  mount:\n    src: \"/dev/mapper/{{ item.vg_name }}-{{ item.lv_name }}\"\n    path: \"{{ item.mount_path }}\"\n    fstype: \"{{ item.lv_fstype | default(lvm_fstype) }}\"\n    state: mounted\n"}, {"commit_sha": "29c99df6be1f12167300ee9cdc7298ff44f1fe21", "sha": "e3484d8dc5ed6e761d71517742a13dd3ea5a40f7", "filename": "tasks/main.yml", "repository": "dev-sec/ansible-nginx-hardening", "decoded_content": "---\n- name: add the OS specific variables\n  include_vars: \"{{ ansible_os_family }}.yml\"\n\n- name: config should not be worldwide read- or writeable\n  file: path=\"/etc/nginx\" mode=\"o-rw\" owner=\"root\" group=\"root\" recurse=yes\n\n- name: create additional configuration\n  template: src=\"hardening.conf.j2\" dest=\"{{nginx_config_conf_dir}}/90.hardening.conf\" owner=\"root\" group=\"root\"\n  notify: reload nginx\n\n- name: change configuration in main nginx.conf\n  lineinfile: dest=\"/etc/nginx/nginx.conf\" regexp=\"^\\s*server_tokens\" line=\"server_tokens {{nginx_server_tokens}};\" insertafter=\"http {\"\n  notify: reload nginx\n\n- name: change client_max_body_size in main nginx.conf\n  lineinfile: dest=\"/etc/nginx/nginx.conf\" regexp=\"^\\s*client_max_body_size\" line=\"client_max_body_size {{nginx_client_max_body_size}};\" insertafter=\"http {\"\n  notify: reload nginx\n\n- name: change client_body_buffer_size in main nginx.conf\n  lineinfile: dest=\"/etc/nginx/nginx.conf\" regexp=\"^\\s*client_body_buffer_size\" line=\"client_body_buffer_size {{nginx_client_body_buffer_size}};\" insertafter=\"http {\"\n  notify: reload nginx\n\n- name: change keepalive_timeout in main nginx.conf\n  lineinfile: dest=\"/etc/nginx/nginx.conf\" regexp=\"^\\s*keepalive_timeout\" line=\"keepalive_timeout {{nginx_keepalive_timeout}};\" insertafter=\"http {\"\n  notify: reload nginx\n\n- name: remove default.conf\n  file: path=\"{{nginx_default_conf}}\" state=absent\n  when: nginx_remove_default_site\n  notify: reload nginx\n\n- name: generate dh group\n  command: openssl dhparam -out {{nginx_dh_param}} {{nginx_dh_size}} creates={{nginx_dh_param}}\n  notify: reload nginx\n"}, {"commit_sha": "f3dd45a61b08de1b3351140cc442a705cb2fad82", "sha": "bb4e615c2f2efd283132f1c3cd606942e1d5a60d", "filename": "tasks/autoupdate-RedHat.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\n- name: Set correct automatic update utility vars (RHEL 8).\n  set_fact:\n    update_utility: dnf-automatic\n    update_service: dnf-automatic-install.timer\n    update_conf_path: /etc/dnf/automatic.conf\n  when: ansible_distribution_major_version | int == 8\n\n- name: Set correct automatic update utility vars (RHEL <= 7).\n  set_fact:\n    update_utility: yum-cron\n    update_service: yum-cron\n    update_conf_path: /etc/yum/yum-cron.conf\n  when: ansible_distribution_major_version | int <= 7\n\n- name: Install automatic update utility.\n  package:\n    name: '{{ update_utility }}'\n    state: present\n\n- name: Ensure automatic update utility is running and enabled on boot.\n  service:\n    name: '{{ update_service }}'\n    state: started\n    enabled: true\n\n- name: Configure autoupdates.\n  lineinfile:\n    dest: '{{ update_conf_path }}'\n    regexp: '^apply_updates = .+'\n    line: 'apply_updates = yes'\n  when:\n    - security_autoupdate_enabled\n    - ansible_distribution_major_version | int in [7, 8]\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "331bf8ce154d36a089592f479f561be6a5d0c7fa", "filename": "playbooks/openshift/cinder-registry.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n# Create a volume for the registry\n- hosts: localhost\n  vars:\n    osp_volumes:\n    - name: \"{{ full_dns_domain }}-registry\"\n      display_description: \"Registry Vol\"\n      display_name: \"{{ full_dns_domain }}-registry\"\n      size: \"{{ openshift_hosted_registry_storage_volume_size | default(20) }}\"\n      purpose: \"ose_registry\"\n  roles:\n  - infra-ansible/roles/osp/admin-volume\n\n- hosts: OSEv3\n  tasks:\n  - name: \"set cinder registry storage facts\"\n    set_fact:\n      openshift_hosted_registry_storage_openstack_volumeID: \"{{ disk.volume.id }}\"\n      openshift_hosted_registry_storage_volume_size: \"{{ disk.volume.size }}Gi\"\n      openshift_hosted_registry_storage_access_modes: \"{{ openshift_hosted_registry_storage_access_modes | default(\\\"['ReadWriteOnce']\\\") }}\"\n      openshift_hosted_registry_storage_openstack_filesystem: \"{{ openshift_hosted_registry_storage_openstack_filesystem | default('ext4')}}\"\n    with_items:\n    -  \"{{ hostvars['localhost'].os_volumes.results }}\"\n    when:\n    - disk.item.purpose  == \"ose_registry\"\n    loop_control:\n      loop_var: disk\n"}, {"commit_sha": "8c4af8da901c68ce8c4bdd21c62e08cec5d3c23a", "sha": "132889d3222bdac5d6233bcf6d0930cc792b7044", "filename": "tasks/configure-docker.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Ensure /etc/docker directory exists\n  become: true\n  file:\n    path: /etc/docker\n    state: directory\n    mode: 0755\n\n- name: Configure Docker daemon (file)\n  become: true\n  copy:\n    src: \"{{ docker_daemon_config_file }}\"\n    dest: /etc/docker/daemon.json\n  notify: restart docker\n  when: docker_daemon_config_file is defined\n\n- name: Configure Docker daemon (variables)\n  become: true\n  copy:\n    content: \"{{ docker_daemon_config | to_nice_json }}\"\n    dest: /etc/docker/daemon.json\n  notify: restart docker\n  when: docker_daemon_config_file is not defined and\n        docker_daemon_config is defined\n\n- name: Ensure Docker default user namespace is defined in subuid and subgid\n  become: true\n  lineinfile:\n    path: \"{{ item }}\"\n    regexp: '^dockremap'\n    line: 'dockremap:500000:65536'\n  with_items:\n    - /etc/subuid\n    - /etc/subgid\n  when: (_docker_os_dist == \"CentOS\" or _docker_os_dist == \"RedHat\") and\n        ((docker_daemon_config is defined and\n        docker_daemon_config['userns-remap'] is defined and\n        docker_daemon_config['userns-remap'] == 'default') or\n        docker_bug_usermod | bool)\n\n- name: Ensure Docker users are added to the docker group\n  become: true\n  user:\n    name: \"{{ item }}\"\n    groups: docker\n    append: true\n  with_items: \"{{ docker_users }}\"\n\n- name: Ensure devicemapper prerequisites are fulfilled\n  block:\n    - name: Ensure lvm2 is installed\n      become: true\n      package:\n        name: lvm2\n        state: present\n\n    - name: Ensure thin-provisioning-tools is installed\n      become: true\n      package:\n        name: thin-provisioning-tools\n        state: present\n      when: (_docker_os_dist == \"Ubuntu\" or (_docker_os_dist == \"Debian\" and _docker_os_dist_major_version > '7'))\n  when:\n    - docker_daemon_config['storage-driver'] is defined\n    - docker_daemon_config['storage-driver'] == 'devicemapper'\n\n\n- name: Enable Docker service\n  become: true\n  service:\n    name: docker\n    enabled: yes\n  notify: restart docker\n  register: _docker_service\n\n- name: Trigger start/restart of Docker\n  service:\n    name: docker\n  notify: restart docker\n  changed_when: _docker_service.status.SubState != \"running\"\n  when: _docker_service.status is defined and _docker_service.status.SubState is defined\n"}, {"commit_sha": "fbecfb07716ea9536ad2b97bf0363ba0007415e1", "sha": "6e9e23b8c46ddccafe24dac6b09e2b6c536de6fa", "filename": "tasks/Linux/security_policy.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: 'Fetch oracle security policy with {{ java_unlimited_policy_transport }} transport'\n  include_tasks: '{{ transport_driver }}'\n  with_first_found:\n    - 'fetch/security-fetch/security-fetch-{{ java_unlimited_policy_transport }}.yml'\n    - 'fetch/unknown-transport.yml'\n  loop_control:\n    loop_var: transport_driver\n  when:\n    - java_unlimited_policy_enabled\n    - java_distribution == 'oracle_java'\n    - java_full_version is version('8.151', '<')\n\n- name: Become block\n  block:\n    - name: Unzip patch file\n      unarchive:\n        src: '{{ security_policy_java_artifact }}'\n        dest: '{{ security_patch_path }}'\n        remote_src: true\n        owner: root\n        group: root\n        mode: 0755\n\n    - name: Apply patch file\n      copy:\n        src: \"{{ java_path }}/{{ java_folder }}/jre/lib/security/\\\n          {{ security_patch_folders[java_major_version|int] }}/{{ policy_item }}\"\n        dest: '{{ security_patch_path }}/'\n        remote_src: true\n        directory_mode: true\n        owner: root\n        group: root\n        mode: 0644\n      loop:\n        - local_policy.jar\n        - US_export_policy.jar\n        - README.txt\n      loop_control:\n        loop_var: policy_item\n  when: java_full_version is version('8.151', '<')\n  become: true\n\n- name: Apply setting\n  replace:\n    path: '{{ security_patch_path }}/java.security'\n    regexp: '#crypto.policy=unlimited'\n    replace: 'crypto.policy=unlimited'\n  when: java_major_version | int < 9\n  become: true\n"}, {"commit_sha": "f85435227eb23c6e474103286c17d7406baeff47", "sha": "92e1eb4cdb4a749ac4b36dd16b0230db621cba10", "filename": "roles/mesos/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# Common\nconsul_dir: /etc/consul.d\nmesos_executor_registration_timeout: 10mins\nmesos_cluster_name: \"Cluster01\"\nmesos_ip: \"{{ ansible_default_ipv4.address }}\"\nmesos_hostname: \"{{ ansible_ssh_host }}\"\nmesos_docker_socket: \"/var/run/weave/weave.sock\"\nmesos_version: \"0.25.0-0.2.70.ubuntu1404\"\n\n# Defaults file for mesos-salve\nmesos_slave_port: 5051\nmesos_containerizers: \"docker,mesos\"\nmesos_resources: \"ports(*):[31000-32000]\"\nmesos_slave_work_dir: \"/tmp/mesos\"\nmesos_slave_image: \"mesosphere/mesos-slave:{{ mesos_version }}\"\nmesos_slave_rebuild_container: false\n\n# Defaults file for mesos-master\nmesos_zookeeper_group: zookeeper_servers\nmesos_master_port: 5050\nmesos_master_work_dir: \"/var/lib/mesos\"\nmesos_master_image: \"mesosphere/mesos-master:{{ mesos_version }}\"\nmesos_master_rebuild_container: false\n\nprometheus_mesos_exporter_image: \"prom/mesos-exporter:latest\"\nprometheus_mesos_exporter_port: 9105\nprometheus_mesos_exporter_consul_service_id: \"{{ ansible_hostname }}:mesos-exporter:{{ prometheus_mesos_exporter_port }}\"\n\n# The Mesos quorum value is based on the number of Mesos Masters. Take the\n# number of masters, divide by 2, and round-up to nearest integer. For example,\n# if there are 1 or 2 masters the quorum count is 1. If there are 3 or 4\n# masters then the quorum count is 2. For 5 or 6 masters it's 3 and so on.\nmesos_quorum: \"\n{% if mesos_master_quorum is defined %}\n{{ mesos_master_quorum }}\n{% else %}\n{{ ( groups.mesos_masters|count / 2) | round(0, 'ceil') | int }}\n{%- endif -%}\n\"\n"}, {"commit_sha": "b2591b9333f6e7e70f6b9d99e55356b30d7e173c", "sha": "7fdac055073a00e9427cdad7247af68ec5886cf2", "filename": "tasks/configure.yml", "repository": "inkatze/wildfly", "decoded_content": "---\n# task file for wildfly\n\n- name: Create wildfly etc directory\n  file:\n    path: '{{ wildfly_conf_dir }}'\n    state: directory\n    owner: '{{ wildfly_user }}'\n    group: '{{ wildfly_group }}'\n    mode: '0750'\n\n- name: Copy wildfly configuration\n  template:\n    src: wildfly.conf.j2\n    dest: '{{ wildfly_conf_dir }}/wildfly.conf'\n    owner: root\n    group: root\n    mode: '0640'\n  notify:\n    - restart wildfly\n    - change standalone data mode\n\n- name: Copy wildfly properties file\n  template:\n    src: wildfly.properties.j2\n    dest: '{{ wildfly_conf_dir }}/wildfly.properties'\n    owner: '{{ wildfly_user }}'\n    group: '{{ wildfly_group }}'\n    mode: '0640'\n  notify:\n    - restart wildfly\n    - change standalone data mode\n\n- name: Copy wildfly init script\n  template: src=wildfly.init.j2 dest={{ wildfly_init_dir }}/wildfly owner=root\n            group=root mode=0750\n  when: ansible_service_mgr == 'init'\n  notify:\n    - restart wildfly\n    - change standalone data mode\n\n- name: Copy wildfly systemd unit file\n  template: src=wildfly.service.j2 dest={{ wildfly_systemd_dir }}/wildfly owner=root\n            group=root mode=0640\n  when: ansible_service_mgr == 'systemd'\n  notify:\n    - restart wildfly\n    - change standalone data mode\n\n- name: Open wildfly management http tcp port\n  firewalld:\n    port: '{{ wildfly_manage_http_port }}/tcp'\n    permanent: yes\n    immediate: yes\n    state: enabled\n\n- name: Open wildfly management https tcp port\n  firewalld:\n    port: '{{ wildfly_manage_https_port }}/tcp'\n    permanent: yes\n    immediate: yes\n    state: enabled\n\n- name: Open wildfly http tcp port\n  firewalld:\n    port: '{{ wildfly_http_port }}/tcp'\n    permanent: yes\n    immediate: yes\n    state: enabled\n\n- name: Open wildfly https tcp port\n  firewalld:\n    port: '{{ wildfly_https_port }}/tcp'\n    permanent: yes\n    immediate: yes\n    state: enabled\n\n- name: Enable and start the service\n  service:\n    name: wildfly\n    state: started\n    enabled: yes\n\n- name: Change standalone data mode\n  file:\n    path: '{{ wildfly_dir }}/standalone/data'\n    owner: '{{ wildfly_user }}'\n    group: '{{ wildfly_group }}'\n    mode: '0750'\n    recurse: yes\n\n- name: Delete wildfly tar file\n  file:\n    path: '{{ wildfly_download_dir }}/{{ wildfly_download_file }}'\n    state: absent\n\n- name: Create a version file\n  template:\n    src: version.j2\n    dest: '{{ wildfly_version_file }}'\n    owner: '{{ wildfly_user }}'\n    group: '{{ wildfly_group }}'\n    mode: '0750'\n"}, {"commit_sha": "f5364b57f100ae9fa898af59b7812ec0c447ac42", "sha": "1988f01ef9d24891230f86caf9f6bb0c5d99b828", "filename": "tasks/ip-list.yml", "repository": "nusenu/ansible-relayor", "decoded_content": "---\n\n- name: Use a single private IPv4 address if we have no public IPv4 address\n  include_vars: private_IPv4_only.yml\n  when: tor_v4ips == []\n\n# workaround for this ansible IPv6 filter bug\n# https://github.com/ansible/ansible/issues/14829\n# we simply convert False to empty lists\n- name: workaround for ansible bug 14829 (1/3)\n  set_fact:\n    v6tmp: []\n  when: v6tmp == False\n\n- name: workaround for ansible bug 14829 (2/3)\n  set_fact:\n    tor_v6ips: \"{{ v6tmp[0:ipv4_count|int]|ipv6('address') }}\"\n\n- name: workaround for ansible bug 14829 (3/3)\n  set_fact:\n    tor_v6ips: []\n  when: tor_v6ips == False\n\n- name: setup IP list (1/2)\n  set_fact:\n    ips:\n        ipv4: \"{{ item.0 }}\"\n        ipv6: \"{{ item.1 }}\"\n  with_together:\n        - \"{{ tor_v4ips }}\"\n        - \"{{ tor_v6ips }}\"\n  register: ipsinterm\n\n- name: setup IP list (2/2)\n  set_fact:\n    tor_ips: \"{{ ipsinterm.results | map(attribute='ansible_facts.ips')|list}}\"\n"}, {"commit_sha": "cd0410a521d2abc0b0e57f40c951982403b5f6d9", "sha": "d7f1291b46cc7268701de4e81cc65dc64fc8e6fe", "filename": "roles/network_interface/tasks/main.yml", "repository": "CSCfi/fgci-ansible", "decoded_content": "---\n\n- name: Add the OS specific varibles\n  include_vars: \"{{ ansible_os_family }}.yml\"\n\n- name: Install the required  packages in Redhat derivatives\n  yum: name={{ item }} state=installed\n  with_items: network_pkgs\n  when: ansible_os_family == 'RedHat'\n\n- name: Install the required packages in Debian derivatives\n  apt: name={{ item }} state=installed update_cache=yes\n  with_items: network_pkgs\n  environment: env\n  when: ansible_os_family == 'Debian'\n\n- name: Make sure the include line is there in interfaces file\n  lineinfile: >\n     regexp=\"^source\\ \\/etc\\/network\\/interfaces.d\\/\\*\"\n     line=\"source /etc/network/interfaces.d/*\"\n     dest=/etc/network/interfaces\n     state=present\n     insertafter=EOF\n  when: ansible_os_family == \"Debian\"\n\n- name: Create the directory for interface cfg files\n  file: path=/etc/network/interfaces.d  state=directory\n  when: ansible_os_family == \"Debian\"\n\n- name: Create the network configuration file for ethernet devices\n  template: src=ethernet_{{ ansible_os_family }}.j2 dest={{ net_path }}/ifcfg-{{ item.device }}\n  with_items: network_ether_interfaces\n  when: network_ether_interfaces is defined\n  register: ether_result\n  notify:\n     - restart network\n\n- name: Write configuration files for rhel route configuration\n  template: src=route_{{ ansible_os_family }}.j2 dest={{ net_path }}/route-{{ item.device }} \n  with_items: network_ether_interfaces\n  when: network_ether_interfaces is defined and item.route is defined and ansible_os_family == 'RedHat'\n  notify:\n     - restart network\n\n#- shell: ifdown {{ item.item.device }}; ifup {{ item.item.device }}\n#  with_items: ether_result.results\n#  when: ether_result is defined and item.changed\n\n- name: Create the network configuration file for bond devices\n  template: src=bond_{{ ansible_os_family }}.j2 dest={{ net_path }}/ifcfg-{{ item.device }}\n  with_items: network_bond_interfaces\n  when: network_bond_interfaces is defined\n  register: bond_result\n  notify:\n     - restart network\n\n- name: Make sure the bonding module is loaded\n  modprobe: name=bonding state=present\n  when: bond_result|changed\n\n- name: Write configuration files for route configuration\n  template: src=route_{{ ansible_os_family }}.j2 dest={{ net_path }}/route-{{ item.device }} \n  with_items: network_bond_interfaces\n  when: network_bond_interfaces is defined and item.route is defined and ansible_os_family == 'RedHat'\n  notify:\n     - restart network\n\n#- shell: ifdown {{ item.item.device }}; ifup {{ item.item.device }}\n#  with_items: bond_result.results\n#  when: bond_result is defined and item.changed\n\n- name: Create the network configuration file for slave in the bond devices\n  template: src=bond_slave_{{ ansible_os_family }}.j2 dest={{ net_path }}/ifcfg-{{ item.1 }}\n  with_subelements:\n   - network_bond_interfaces\n   - bond_slaves\n  when: network_bond_interfaces is defined\n  register: bond_port_result\n  notify:\n     - restart network\n\n#- shell: ifdown {{ item.item.1 }}; ifup {{ item.item.1 }}\n#  with_items: bond_port_result.results\n#  when: bond_port_result is defined and item.changed\n\n#- shell: ifdown {{ item.item.device }}; ifup {{ item.item.device }}\n#  with_items: bond_result.results\n#  when: bond_result is defined and item.changed and ansible_os_family == 'RedHat'\n\n- name: Create the network configuration file for vlan devices\n  template: src=ethernet_{{ ansible_os_family }}.j2 dest={{ net_path }}/ifcfg-{{ item.device }}\n  with_items: network_vlan_interfaces\n  when: network_vlan_interfaces is defined\n  register: vlan_result\n  notify:\n     - restart network\n\n- name: Write configuration files for rhel route configuration with vlan\n  template: src=route_{{ ansible_os_family }}.j2 dest={{ net_path }}/route-{{ item.device }} \n  with_items: network_vlan_interfaces\n  when: network_vlan_interfaces is defined and item.route is defined and ansible_os_family == 'RedHat'\n  notify:\n     - restart network\n\n#- shell: ifdown {{ item.item.device }}; ifup {{ item.item.device }}\n#  with_items: vlan_result.results\n#  when: vlan_result is defined and item.changed\n\n- name: Create the network configuration file for bridge devices\n  template: src=bridge_{{ ansible_os_family }}.j2 dest={{ net_path }}/ifcfg-{{ item.device }}\n  with_items: network_bridge_interfaces\n  when: network_bridge_interfaces is defined\n  register: bridge_result\n  notify:\n     - restart network\n\n- name: Write configuration files for rhel route configuration\n  template: src=route_{{ ansible_os_family }}.j2 dest={{ net_path }}/route-{{ item.device }}\n  with_items: network_bridge_interfaces\n  when: network_bridge_interfaces is defined and item.route is defined and ansible_os_family == 'RedHat'\n  notify:\n     - restart network\n\n#- shell: ifdown {{ item.item.device }}; ifup {{ item.item.device }}\n#  with_items: bridge_result.results\n#  when: bridge_result is defined and item.changed\n\n- name: Create the network configuration file for port on the bridge devices\n  lineinfile: dest={{ net_path }}/ifcfg-{{ item.1 }} regexp='^BRIDGE=' line=BRIDGE={{ item.0.device }}\n  with_subelements:\n    - network_bridge_interfaces\n    - ports\n  when: network_bridge_interfaces is defined\n  register: bridge_port_result\n  notify:\n     - restart network\n"}, {"commit_sha": "b5191ae436b8a997ef5f322b35043af64f087623", "sha": "960b74c7d0af6ee7e3de75e7bcb992b17d5c7c73", "filename": "tasks/setup.yml", "repository": "fubarhouse/ansible-role-golang", "decoded_content": "---\n\n# Define system-specific variables for fubarhouse.golang.\n\n- name: \"Go-Lang | Define user variable for ssh use\"\n  set_fact:\n    fubarhouse_user: \"{{ ansible_ssh_user }}\"\n  when: ansible_ssh_user is defined and fubarhouse_user is not defined\n\n- name: \"Go-Lang | Define user variable for non-ssh use\"\n  set_fact:\n    fubarhouse_user: \"{{ ansible_user_id }}\"\n  when: ansible_ssh_user is not defined and fubarhouse_user is not defined\n\n- name: \"Go-Lang | Get $HOME\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell: \"echo $HOME\"\n  register: shell_home_dir\n  changed_when: false\n  when: fubarhouse_user_dir is not defined\n\n- name: \"Go-Lang | Set $HOME\"\n  set_fact:\n    fubarhouse_user_dir: \"{{ shell_home_dir.stdout }}\"\n  when: fubarhouse_user_dir is not defined\n\n- name: \"Go-Lang | Define GOROOT\"\n  set_fact:\n    GOROOT: \"{{ fubarhouse_user_dir }}/go\"\n  when:\n    - GOROOT is not defined\n\n- name: \"Go-Lang | Define GOPATH\"\n  set_fact:\n    GOPATH: \"{{ GOROOT }}/bin\"\n  when:\n    - GOROOT is defined\n    - GOPATH is not defined\n\n- name: \"Go-Lang | Define GOROOT_BOOTSTRAP\"\n  set_fact:\n    GOROOT_BOOTSTRAP: \"{{ fubarhouse_user_dir }}/go1.4\"\n  when:\n   - fubarhouse_user_dir is defined\n   - GOROOT_BOOTSTRAP is not defined\n   - build_go_from_source|bool == true\n\n- name: \"Go-Lang | Looking for compiled binary in GOROOT_BOOTSTRAP installation\"\n  stat:\n    path: \"{{ GOROOT_BOOTSTRAP }}/bin/go\"\n  register: go_binary_bootstrap\n  failed_when: false\n  when:\n   - GOROOT_BOOTSTRAP is defined\n   - build_go_from_source|bool == true\n\n- name: \"Go-Lang | Define GOARCH for 32-bit systems\"\n  set_fact:\n    GOARCH: \"386\"\n  when:\n    - '\"386\" in ansible_architecture'\n    - GOARCH is not defined\n\n- name: \"Go-Lang | Define GOARCH for 64-bit systems\"\n  set_fact:\n    GOARCH: \"amd64\"\n  when:\n    - ansible_architecture == 'x86_64' or ansible_distribution == 'CentOS' or ansible_distribution == 'Debian' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat' or ansible_distribution == 'Ubuntu' or ansible_distribution == 'Ubuntu'\n    - GOARCH is not defined\n\n- name: \"Go-Lang | Define GOOS for Darwin systems\"\n  set_fact:\n    GOOS: \"darwin\"\n  when:\n    - ansible_os_family == 'Darwin'\n    - ansible_distribution == 'MacOSX'\n    - GOOS is not defined\n\n- name: \"Go-Lang | Define GOOS for FreeBSD systems\"\n  set_fact:\n    GOOS: \"freebsd\"\n  when:\n    - ansible_distribution == 'FreeBSD'\n    - GOOS is not defined\n\n- name: \"Go-Lang | Define GOOS for linux systems\"\n  set_fact:\n    GOOS: \"linux\"\n  when:\n    - ansible_architecture == 'x86_64' or ansible_distribution == 'CentOS' or ansible_distribution == 'Debian' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'RedHat' or ansible_distribution == 'Ubuntu' or ansible_distribution == 'Ubuntu'\n    - GOOS is not defined\n\n- name: \"Go-Lang | Define URL for distribution\"\n  set_fact:\n    go_distribution_filename: \"go{{ go_version }}.{{ GOOS }}-{{ GOARCH }}\"\n  when: build_go_from_source|bool == false\n\n- name: \"Go-Lang | Define URL for source\"\n  set_fact:\n    go_distribution_filename: \"go{{ go_version }}.src\"\n  when: build_go_from_source|bool == true\n\n- name: \"Go-Lang | Looking for existing installation\"\n  stat:\n    path: \"{{ GOROOT }}/bin/go\"\n  register: go_binary\n  failed_when: false\n\n- name: \"Go-Lang | Define GOROOT\"\n  set_fact:\n    GOROOT: \"{{ GOROOT }}\"\n  when: GOROOT is defined\n\n- name: \"Go-Lang | Define GOPATH\"\n  set_fact:\n    GOPATH: \"{{ GOROOT }}/bin\"\n  when:\n    - GOPATH is not defined\n    - GOROOT is defined\n\n- name: \"Go-Lang | Getting version information\"\n  become: yes\n  become_user: \"root\"\n  shell: \"{{ GOPATH }}/go version\"\n  environment:\n    GOPATH: \"{{ GOPATH }}\"\n    GOROOT: \"{{ GOROOT }}\"\n  register: current_go_version\n  changed_when: false\n  when: go_binary.stat.exists|bool == true\n\n- name: \"Go-Lang | Define expected version output\"\n  set_fact:\n    expected_go_version_output: \"go version go{{ go_version }} {{ GOOS }}/{{ GOARCH }}\"\n  when: expected_go_version_output is not defined"}, {"commit_sha": "323de2d9e686babb9557b49848180eda05834f5c", "sha": "99d09c0fe718eb07c5de1d5d352706235263504a", "filename": "tasks/deploy_netbox.yml", "repository": "lae/ansible-role-netbox", "decoded_content": "---\n- name: Create NetBox application directories\n  file:\n    path: \"{{ item }}\"\n    state: directory\n  with_items:\n    - \"{{ netbox_releases_path }}\"\n    - \"{{ netbox_shared_path }}\"\n    - \"{{ netbox_shared_path }}/media/image-attachments\"\n\n- include: \"install_via_{{ 'git' if netbox_git else 'stable' }}.yml\"\n\n- include: generate_secret_key.yml\n  when:\n    - netbox_config.SECRET_KEY is not defined\n\n- name: Create NetBox virtualenv and install needed Python dependencies\n  pip:\n    requirements: \"{{ netbox_current_path }}/requirements.txt\"\n    virtualenv: \"{{ netbox_virtualenv_path }}\"\n    virtualenv_python: \"{{ netbox_python3_binary if (netbox_python == 3) else netbox_python2_binary }}\"\n\n- name: Generate NetBox configuration file\n  template:\n    src: templates/configuration.py.j2\n    dest: \"{{ netbox_shared_path }}/configuration.py\"\n    mode: 0640\n  notify:\n    - reload netbox.service\n\n- block:\n  - name: Install django-auth-ldap if LDAP is enabled\n    pip:\n      name: django-auth-ldap\n      virtualenv: \"{{ netbox_virtualenv_path }}\"\n\n  - name: Generate LDAP configuration for NetBox if enabled\n    template:\n      src: \"{{ netbox_ldap_config_template }}\"\n      dest: \"{{ netbox_shared_path }}/ldap_config.py\"\n      mode: 0640\n    notify:\n      - reload netbox.service\n  when:\n    - netbox_ldap_enabled\n\n- name: Symlink NetBox configuration file into the active NetBox release\n  file:\n    src: \"{{ netbox_shared_path }}/configuration.py\"\n    dest: \"{{ netbox_config_path }}/configuration.py\"\n    state: link\n\n- name: Symlink/Remove NetBox LDAP configuration file into/from the active NetBox release\n  file:\n    src: \"{{ netbox_shared_path }}/ldap_config.py\"\n    dest: \"{{ netbox_config_path }}/ldap_config.py\"\n    state: \"{{ 'link' if netbox_ldap_enabled else 'absent' }}\"\n  notify:\n    - reload netbox.service\n\n- name: Set MEDIA_ROOT to shared media directory\n  lineinfile:\n    path: \"{{ netbox_config_path }}/settings.py\"\n    regexp: '^MEDIA_ROOT\\s*='\n    line: 'MEDIA_ROOT = \"{{ netbox_shared_path }}/media/\"'\n    state: present\n  notify:\n    - reload netbox.service\n\n- name: Run database migrations for NetBox\n  django_manage:\n    command: migrate\n    app_path: \"{{ netbox_current_path }}/netbox\"\n    virtualenv: \"{{ netbox_virtualenv_path }}\"\n\n- name: Create a super user for NetBox\n  shell: \"printf '{{ netbox_superuser_script }}' | {{ netbox_virtualenv_path }}/bin/python {{ netbox_current_path }}/netbox/manage.py shell\"\n  register: __netbox_superuser_result\n  changed_when: \"'changed' in __netbox_superuser_result.stdout\"\n  when:\n    - not netbox_ldap_enabled\n\n- name: Generate static assets for NetBox\n  django_manage:\n    command: collectstatic\n    app_path: \"{{ netbox_current_path }}/netbox\"\n    virtualenv: \"{{ netbox_virtualenv_path }}\"\n\n- name: Populate NetBox with initial data\n  django_manage:\n    command: loaddata\n    fixtures: initial_data\n    app_path: \"{{ netbox_current_path }}/netbox\"\n    virtualenv: \"{{ netbox_virtualenv_path }}\"\n  when:\n    - netbox_load_initial_data\n"}, {"commit_sha": "5404962535c1adcfe95bff1536d60dc05fa32e0c", "sha": "cc88f95da826ef0c94fad4097d41511be3143510", "filename": "tasks/nexus_install.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n\n- name: No version given =>> calculate latest available nexus version\n  block:\n\n    - name: Call latest nexus uri to get redirection\n      uri:\n        url: \"{{ nexus_download_url }}/latest-unix.tar.gz\"\n        method: CONNECT\n        status_code: 302\n      register: nexus_latest_uri_call\n      # No changes made, we only need the target uri. Safe for check mode and needed for next operations\n      check_mode: no\n\n    - name: Register latest nexus version from redirection\n      set_fact:\n        nexus_version: \"{{ nexus_latest_uri_call.location | regex_replace('^https://.*nexus-(\\\\d*\\\\.\\\\d*\\\\.\\\\d*-\\\\d*)-unix.tar.gz', '\\\\1') }}\"\n\n  when: nexus_version == ''\n\n- name: Broken compatibility => nexus_package is now dynamically calculated\n  fail:\n    msg: >-\n      You have set the variable nexus_package in your playbook.\n      Starting from version 2.2.0 of this role, this is not compatible\n      with the new nexus latest version detection feature and is not\n      supported anymore. Please use the nexus_version variable only.\n  when: nexus_package is defined\n\n- name: Register nexus package name\n  set_fact:\n    nexus_package: \"nexus-{{ nexus_version }}-unix.tar.gz\"\n\n- name: Download nexus_package\n  get_url:\n    url: \"{{ nexus_download_url }}/{{ nexus_package }}\"\n    dest: \"{{ nexus_download_dir }}/{{ nexus_package }}\"\n    force: no\n  notify:\n    - nexus-service-stop\n\n- name: Ensure Nexus o/s group exists\n  group:\n    name: \"{{ nexus_os_group }}\"\n    state: present\n\n- name: Ensure Nexus o/s user exists\n  user:\n    name: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n    shell: \"/bin/bash\"\n    state: present\n\n- name: Ensure Nexus installation directory exists\n  file:\n    path: \"{{ nexus_installation_dir }}\"\n    state: \"directory\"\n\n- name: Unpack Nexus download\n  unarchive:\n    src: \"{{ nexus_download_dir }}/{{ nexus_package }}\"\n    dest: \"{{ nexus_installation_dir }}\"\n    creates: \"{{ nexus_installation_dir }}/nexus-{{ nexus_version }}\"\n    force: no\n    copy: false\n  notify:\n    - nexus-service-stop\n\n- name: Update symlink nexus-latest\n  file:\n    path: \"{{ nexus_installation_dir }}/nexus-latest\"\n    src: \"{{ nexus_installation_dir }}/nexus-{{ nexus_version }}\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n    state: link\n    follow: false\n  register: nexus_latest_version\n  notify:\n    - nexus-service-stop\n\n- meta: flush_handlers\n\n- name: Delete unpacked data directory\n  file:\n    path: \"{{ nexus_installation_dir }}/nexus-latest/data\"\n    state: absent\n\n- name: Get path to default settings\n  set_fact:\n    nexus_default_settings_file: \"{{ nexus_installation_dir }}/nexus-latest/etc/org.sonatype.nexus.cfg\"\n  when: nexus_version is version_compare('3.1.0', '<')\n\n- name: Get path to default settings\n  set_fact:\n    nexus_default_settings_file: \"{{ nexus_installation_dir }}/nexus-latest/etc/nexus-default.properties\"\n  when: nexus_version is version_compare('3.1.0', '>=')\n\n- name: Get application settings directories\n  set_fact:\n    nexus_app_dir_settings_dirs:\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc\"\n  when: nexus_version is version_compare('3.1.0', '<')\n\n- name: Get application settings directories\n  set_fact:\n    nexus_app_dir_settings_dirs:\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/karaf\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/jetty\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/fabric\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/logback\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/scripts\"\n  when: nexus_version is version_compare('3.1.0', '>=')\n\n- name: Get rest API endpoint (v < 3.8.0)\n  set_fact:\n    nexus_rest_api_endpoint: \"service/siesta/rest/v1/script\"\n  when: nexus_version is version_compare('3.8.0', '<')\n\n- name: Get rest API endpoint (v >= 3.8.0)\n  set_fact:\n    nexus_rest_api_endpoint: \"service/rest/v1/script\"\n  when: nexus_version is version_compare('3.8.0', '>=')\n\n- name: Get path to database restore dir (v < 3.11.0)\n  set_fact:\n    nexus_db_restore_dir: \"{{ nexus_data_dir }}/backup\"\n  when: nexus_version is version_compare('3.11.0', '<')\n\n- name: Get path to database restore dir (v >= 3.11.0)\n  set_fact:\n    nexus_db_restore_dir: \"{{ nexus_data_dir }}/restore-from-backup\"\n  when: nexus_version is version_compare('3.11.0', '>=')\n\n- name: Allow nexus to create first-time install configuration files in  {{ nexus_installation_dir }}/nexus-latest/etc\n  file:\n    path: \"{{ item }}\"\n    state: \"directory\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n    mode: \"0755\"\n    recurse: false\n  with_items: \"{{ nexus_app_dir_settings_dirs }}\"\n  when: nexus_latest_version.changed\n  register: chown_config_first_time\n  tags:\n    # hard to run as a handler for time being\n    - skip_ansible_lint\n\n- name: Create Nexus data directory\n  file:\n    path: \"{{ nexus_data_dir }}\"\n    state: \"directory\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n\n- name: Setup Nexus data directory\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Dkaraf.data=.*\"\n    line: \"-Dkaraf.data={{ nexus_data_dir }}\"\n  notify:\n    - nexus-service-stop\n\n- name: Setup JVM logfile directory\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-XX:LogFile=.*\"\n    line: \"-XX:LogFile={{ nexus_data_dir }}/log/jvm.log\"\n  notify:\n    - nexus-service-stop\n\n- name: Setup Nexus default timezone\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Duser.timezone=.*\"\n    line: \"-Duser.timezone={{ nexus_timezone }}\"\n  notify:\n    - nexus-service-stop\n\n- name: Setup Nexus JVM min heap size\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Xms.*\"\n    line: \"-Xms{{ nexus_min_heap_size }}\"\n  notify: nexus-service-stop\n\n- name: Setup Nexus JVM max heap size\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Xmx.*\"\n    line: \"-Xmx{{ nexus_max_heap_size }}\"\n  notify: nexus-service-stop\n\n- name: Setup Nexus JVM max direct memory\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-XX:MaxDirectMemorySize=.*\"\n    line: \"-XX:MaxDirectMemorySize={{ nexus_max_direct_memory }}\"\n  notify: nexus-service-stop\n\n- name: Create Nexus tmp/backup directory\n  file:\n    path: \"{{ item }}\"\n    state: \"directory\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n  with_items:\n    - \"{{ nexus_tmp_dir }}\"\n    - \"{{ nexus_backup_dir }}\"\n\n- name: Setup Nexus tmp directory\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Djava.io.tmpdir=.*\"\n    line: \"-Djava.io.tmpdir={{ nexus_tmp_dir }}\"\n  notify:\n    - nexus-service-stop\n\n- name: Set NEXUS_HOME for the service user\n  lineinfile:\n    dest: \"/home/{{ nexus_os_user }}/.bashrc\"\n    regexp: \"^export NEXUS_HOME=.*\"\n    line: \"export NEXUS_HOME={{ nexus_installation_dir }}/nexus-latest\"\n  notify:\n    - nexus-service-stop\n\n- name: Set nexus user\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.rc\"\n    regexp: \".*run_as_user=.*\"\n    line: \"run_as_user=\\\"{{ nexus_os_user }}\\\"\"\n  notify:\n    - nexus-service-stop\n\n- name: Set nexus port\n  lineinfile:\n    dest: \"{{ nexus_default_settings_file }}\"\n    regexp: \"^application-port=.*\"\n    line: \"application-port={{ nexus_default_port }}\"\n  notify:\n    - nexus-service-stop\n\n- name: Set nexus context path\n  lineinfile:\n    dest: \"{{ nexus_default_settings_file }}\"\n    regexp: \"^nexus-context-path=.*\"\n    line: \"nexus-context-path={{ nexus_default_context_path }}\"\n  notify:\n    - nexus-service-stop\n\n- name: Bind nexus service to 127.0.0.1 only\n  lineinfile:\n    dest: \"{{ nexus_default_settings_file }}\"\n    regexp: \"^application-host=.*\"\n    line: \"application-host=127.0.0.1\"\n  when: httpd_setup_enable\n  notify:\n    - nexus-service-stop\n\n- name: Create systemd service configuration\n  template:\n    src: \"nexus.service\"\n    dest: \"/etc/systemd/system\"\n  notify:\n    - systemd-reload\n\n- block:\n    - name: \"Deploy backup restore script\"\n      template:\n        src: \"nexus-blob-restore.sh.j2\"\n        dest: \"{{ nexus_script_dir }}/nexus-blob-restore.sh\"\n        mode: 0755\n    - name: \"Symlink backup restore script to /sbin\"\n      file:\n        src: \"{{ nexus_script_dir }}/nexus-blob-restore.sh\"\n        dest: \"/sbin/nexus-blob-restore.sh\"\n        state: link\n  when: nexus_backup_configure | bool\n\n- name: 'Check if data directory is empty (first-time install)'\n  command: \"ls {{ nexus_data_dir }}\"\n  register: nexus_data_dir_contents\n  check_mode: no\n  changed_when: false\n\n- name: Clean cache for upgrade process\n  file:\n    path: \"{{ nexus_data_dir }}/clean_cache\"\n    state: touch\n  when: nexus_latest_version.changed and nexus_data_dir_contents.stdout != \"\"\n  tags:\n    # hard to run as a handler for time being\n    - skip_ansible_lint\n\n- meta: flush_handlers\n\n- name: Enable nexus service and make sure it is started\n  systemd:\n    name: nexus.service\n    enabled: yes\n    state: started\n    no_block: yes\n  notify:\n    - wait-for-nexus\n    - wait-for-nexus-port\n\n- meta: flush_handlers\n\n- name: Chown configuration files from {{ nexus_installation_dir }}/nexus-latest/etc back to root\n  file:\n    path: \"{{ nexus_installation_dir }}/nexus-latest/etc\"\n    owner: \"root\"\n    group: \"root\"\n    mode: a=rX,u+w\n    recurse: true\n  when: chown_config_first_time.changed\n  tags:\n    # hard to run as a handler for time being\n    - skip_ansible_lint\n\n- name: Prevent nexus to create any new configuration files in  {{ nexus_installation_dir }}/nexus-latest/etc\n  file:\n    path: \"{{ item }}\"\n    state: \"directory\"\n    owner: \"root\"\n    group: \"root\"\n    mode: \"0755\"\n    recurse: false\n  with_items: \"{{ nexus_app_dir_settings_dirs }}\"\n\n- name: First-time install admin password\n  set_fact:\n    current_nexus_admin_password: 'admin123'\n  when: nexus_data_dir_contents.stdout == \"\"\n\n- name: Subsequent re-provision admin password\n  set_fact:\n    current_nexus_admin_password: \"{{ nexus_admin_password }}\"\n  when: nexus_data_dir_contents.stdout != \"\"\n  no_log: true\n\n- name: Create directory to hold current groovy scripts for reference\n  file:\n    path: \"{{ nexus_data_dir }}/groovy-raw-scripts/current\"\n    state: directory\n    owner: root\n    group: root\n\n- name: Upload new scripts\n  synchronize:\n    archive: no\n    checksum: yes\n    recursive: yes\n    delete: yes\n    mode: push\n    use_ssh_args: yes\n    src: \"files/groovy/\"\n    dest: \"{{ nexus_data_dir }}/groovy-raw-scripts/new/\"\n\n- name: Sync new scripts to old and get differences\n  shell: 'rsync -ric {{ nexus_data_dir }}/groovy-raw-scripts/new/ {{ nexus_data_dir }}/groovy-raw-scripts/current/ | cut -d\" \" -f 2 | sed \"s/\\.groovy//g\"'\n  register: nexus_groovy_files_changed\n  check_mode: no\n  changed_when: false\n  # simple check on changed files kept on host\n  # skip ansible lint (we don't want to use synchronize module for this)\n  args:\n    warn: false\n\n- name: Declare new or changed groovy scripts in nexus\n  include_tasks: declare_script_each.yml\n  with_items: \"{{ nexus_groovy_files_changed.stdout_lines}}\"\n"}, {"commit_sha": "f4408034296e2e7cf358226617b514c0a1101b93", "sha": "ed0423a016b37fa31f3fb5bd160c11e90a007c5b", "filename": "tasks/pam.yml", "repository": "dev-sec/ansible-os-hardening", "decoded_content": "---\n- name: update pam on Debian systems\n  command: 'pam-auth-update --package'\n  when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'\n  changed_when: False\n  environment:\n    DEBIAN_FRONTEND: noninteractive\n\n- name: remove pam ccreds on Debian systems\n  apt:\n    name: '{{ os_packages_pam_ccreds }}'\n    state: 'absent'\n  when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'\n\n- name: remove pam ccreds on Redhat systems\n  yum:\n    name: '{{ os_packages_pam_ccreds }}'\n    state: 'absent'\n  when: ansible_os_family == 'RedHat'\n\n- name: remove pam_cracklib, because it does not play nice with passwdqc\n  apt:\n    name: '{{ os_packages_pam_cracklib }}'\n    state: 'absent'\n  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu') and os_auth_pam_passwdqc_enable\n\n- name: install the package for strong password checking\n  apt:\n    name: '{{ os_packages_pam_passwdqc }}'\n    state: 'present'\n    update_cache: 'yes'\n  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu') and os_auth_pam_passwdqc_enable\n\n- name: configure passwdqc\n  template:\n    src: 'pam_passwdqc.j2'\n    dest: '{{ passwdqc_path }}'\n    mode: '0644'\n    owner: 'root'\n    group: 'root'\n  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu') and os_auth_pam_passwdqc_enable\n\n- name: remove passwdqc\n  apt:\n    name: '{{ os_packages_pam_passwdqc }}'\n    state: 'absent'\n  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu') and not os_auth_pam_passwdqc_enable\n\n- name: install tally2\n  apt:\n    name: 'libpam-modules'\n    state: 'present'\n  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu') and not os_auth_pam_passwdqc_enable and os_auth_retries > 0\n\n- name: configure tally2\n  template:\n    src: 'pam_tally2.j2'\n    dest: '{{ tally2_path }}'\n    mode: '0644'\n    owner: 'root'\n    group: 'root'\n  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu') and not os_auth_pam_passwdqc_enable and os_auth_retries > 0\n\n- name: delete tally2 when retries is 0\n  file:\n    path: '{{ tally2_path }}'\n    state: 'absent'\n  when: (ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu') and not os_auth_pam_passwdqc_enable and os_auth_retries == 0\n\n- name: remove pam_cracklib, because it does not play nice with passwdqc\n  yum:\n    name: '{{ os_packages_pam_cracklib }}'\n    state: 'absent'\n  when: ((ansible_os_family == 'RedHat' and ansible_distribution_version < '7') or ansible_distribution == 'Amazon') and os_auth_pam_passwdqc_enable\n\n- name: install the package for strong password checking\n  yum:\n    name: '{{ os_packages_pam_passwdqc }}'\n    state: 'present'\n  when: ((ansible_os_family == 'RedHat' and ansible_distribution_version < '7') or ansible_distribution == 'Amazon') and os_auth_pam_passwdqc_enable\n\n- name: remove passwdqc\n  yum:\n    name: '{{ os_packages_pam_passwdqc }}'\n    state: 'absent'\n  when: ansible_os_family == 'RedHat' and not os_auth_pam_passwdqc_enable\n\n- name: configure passwdqc and tally via central system-auth confic\n  template:\n    src: 'rhel_system_auth.j2'\n    dest: '/etc/pam.d/system-auth-ac'\n    mode: '0640'\n    owner: 'root'\n    group: 'root'\n\n- name: NSA 2.3.3.5 Upgrade Password Hashing Algorithm to SHA-512\n  template:\n    src: 'rhel_libuser.conf.j2'\n    dest: '/etc/libuser.conf'\n    mode: '0640'\n    owner: 'root'\n    group: 'root'\n"}, {"commit_sha": "3331ceddd3788b3f43d1f6c7ab4ff27d68ceb2c2", "sha": "4f867a135244478feb97e2e12501e812a322b063", "filename": "roles/ovirt-common/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# check variables setting ovirt source\n- name: complain if no ovirt source is specified\n  fail:\n    msg: \"At least one of: 'ovirt_repo_file', 'ovirt_rpm_repo' or 'ovirt_repo'\n      must be specified. More information in the README\"\n  when: not (ovirt_repo_file or ovirt_rpm_repo is defined or ovirt_repo)\n\n# install libselinux-python on machine - selinux policy\n- name: install libselinux-python for ansible\n  yum:\n    name: libselinux-python\n    state: \"present\"\n\n# backup repos\n- name: creating directory repo-backup in yum.repos.d\n  file:\n    path: /tmp/repo-backup\n    state: directory\n\n- name: create repository backup\n  shell: 'cp /etc/yum.repos.d/*.repo /tmp/repo-backup'\n  tags:\n    - skip_ansible_lint\n\n## OPTIONS\n# 1) get repository files\n- name: copy repository files\n  get_url:\n    url: \"{{ item.url }}\"\n    dest: \"/etc/yum.repos.d/{{ item.name | default('') }}\"\n    force_basic_auth: yes\n    force: \"{{ item.force | default('no') }}\"\n  with_items: \"{{ ovirt_repo_file }}\"\n\n# 2) install from rpm\n- name: install rpm repository package\n  yum:\n    name: \"{{ ovirt_rpm_repo }}\"\n    state: present\n  when: ovirt_rpm_repo is defined\n\n# 3) create repository files\n- name: create repository files\n  yum_repository:\n    name: \"{{ item.name }}\"\n    description: \"{{ item.name }}\"\n    baseurl: \"{{ item.url }}\"\n    enabled: \"{{ item.enabled | default('yes') }}\"\n  with_items: \"{{ ovirt_repo }}\"\n"}, {"commit_sha": "9925a1fe9dadb982563155b57b6001182f7fe617", "sha": "2b9e64f1609e7cfa8b2fcd76c1758b7b0025a76d", "filename": "roles/manage-jira/tasks/prepare_vars.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Configure Jira Variables\n  set_fact:\n    jira_url: \"{{ atlassian.jira.url | default(atlassian.url) }}\"\n    jira_username: \"{{ atlassian.jira.username | default(atlassian.username) }}\"\n    jira_password: \"{{ atlassian.jira.password | default(atlassian.password) }}\"\n\n- name: Build permission list variable\n  include_tasks: build_permissions_list.yml\n  loop: \"{{ atlassian.jira.groups | default([]) }}\"\n  loop_control:\n    loop_var: group\n"}, {"commit_sha": "06debddee35d8fefbe5e636c03cbe6c619befb00", "sha": "1cdcad3eac647a1b95eb8defe0e622620e4c1730", "filename": "meta/main.yml", "repository": "cytopia/ansible-role-cloudformation", "decoded_content": "---\n\ngalaxy_info:\n  author: cytopia\n  company: cytopia\n  license: MIT\n  description: Ansible role to render an arbitrary number of Jinja2 templates into cloudformation files and create any number of stacks.\n  min_ansible_version: 2.5\n  platforms:\n    - name: Amazon\n      versions:\n        - all\n    - name: EL\n      versions:\n        - 5\n        - 6\n    - name: Fedora\n      versions:\n        - 16\n        - 17\n        - 18\n    - name: Ubuntu\n      versions:\n        - precise\n        - quantal\n        - raring\n        - saucy\n  galaxy_tags:\n    - amazon\n    - aws\n    - cfn\n    - cloud\n    - cloudformation\n    - deployment\n    - diff\n    - jinja2\n    - template\ndependencies: []\n"}, {"commit_sha": "5c1cde6cffcd979d0292e988da029c0ece96054c", "sha": "4dae88e8ed704b16bc1be9fab20de250e26789d7", "filename": "meta/main.yml", "repository": "openwisp/ansible-openwisp2", "decoded_content": "---\n\ndependencies:\n  - src: https://github.com/nemesisdesign/Stouts.postfix\n    version: origin/develop\n    name: Stouts.postfix\n    postfix_smtp_sasl_auth_enable: \"{{ postfix_smtp_sasl_auth_enable_override | default(false) }}\"\n\ngalaxy_info:\n  author: Federico Capoano\n  company: Cineca\n  description: Official role to install and upgrade openwisp2 controller\n  license: BSD\n  min_ansible_version: 2.5\n  issue_tracker_url: https://github.com/openwisp/ansible-openwisp2/issues\n  platforms:\n  - name: Debian\n    versions:\n    - wheezy\n    - jessie\n    - stretch\n  - name: Ubuntu\n    versions:\n    - trusty\n    - xenial\n    - bionic\n  - name: Fedora\n    versions:\n    - 25\n    - 27\n    - 28\n  - name: EL\n    versions:\n    - 7\n  galaxy_tags:\n    - system\n    - networking\n"}, {"commit_sha": "16f21c5c7c51314eea1d023a46bcaad8c889cd46", "sha": "be74eb98fd7d419ba65eff68e0a89ea8c4a88630", "filename": "tasks/plus/install-plus.yml", "repository": "nginxinc/ansible-role-nginx", "decoded_content": "---\n- import_tasks: setup-license.yml\n\n- import_tasks: setup-alpine.yml\n  when: ansible_os_family == \"Alpine\"\n\n- import_tasks: setup-debian.yml\n  when: ansible_os_family == \"Debian\"\n\n- import_tasks: setup-freebsd.yml\n  when: ansible_os_family == \"FreeBSD\"\n\n- import_tasks: setup-redhat.yml\n  when: ansible_os_family == \"RedHat\"\n\n- import_tasks: setup-suse.yml\n  when: ansible_os_family == \"Suse\"\n\n- name: \"(Install: All OSs) Install NGINX Plus\"\n  package:\n    name: \"nginx-plus{{ nginx_version | default('') }}\"\n    state: present\n  notify: \"(Handler: All OSs) Start NGINX\"\n"}, {"commit_sha": "06debddee35d8fefbe5e636c03cbe6c619befb00", "sha": "fba1bb350bc6e088d98c3f3b0b0ff999c5725eb2", "filename": "tasks/run.yml", "repository": "cytopia/ansible-role-cloudformation", "decoded_content": "---\n\n###\n### Set a sane name for the rendered file\n###\n- name: register a sane template name\n  set_fact:\n    cloudformation_file_name: \"{{ cloudformation.template | basename }}-{{ cloudformation.stack_name }}.yml\"\n  check_mode: False\n\n\n###\n### Render cloudformation jinja2 template into the build directory\n###\n- name: ensure cloudformation template is rendered\n  template:\n    src: \"{{ cloudformation.template }}\"\n    dest: \"{{ cloudformation_build_dir_path }}/{{ cloudformation_file_name }}\"\n    mode: 0644\n    force: True\n  changed_when: False\n  check_mode: False\n\n\n###\n### Diff rendered cloudformation template (check mode only)\n###\n- name: diff cloudformation template parameters\n  cloudformation_diff:\n    ignore_hidden_params: True\n    ignore_final_newline: False\n    output_format: \"{{ cloudformation_diff_output }}\"\n    output_choice: parameter\n    stack_name: \"{{ cloudformation.stack_name }}\"\n    template: \"{{ cloudformation_build_dir_path }}/{{ cloudformation_file_name }}\"\n    template_tags: \"{{ cloudformation.tags | default(omit) }}\"\n    template_parameters: \"{{ cloudformation.template_parameters | default(omit) }}\"\n    aws_access_key: \"{{ cloudformation.aws_access_key | default(cloudformation_defaults.aws_access_key | default(omit)) }}\"\n    aws_secret_key: \"{{ cloudformation.aws_secret_key | default(cloudformation_defaults.aws_secret_key | default(omit)) }}\"\n    security_token: \"{{ cloudformation.security_token | default(cloudformation_defaults.security_token | default(omit)) }}\"\n    profile: \"{{ cloudformation.profile | default(cloudformation_defaults.profile | default(omit)) }}\"\n    region: \"{{ cloudformation.region | default(cloudformation_defaults.region | default(omit)) }}\"\n  when: not cloudformation_generate_only and cloudformation_run_diff\n  check_mode: False\n\n- name: diff cloudformation template file\n  cloudformation_diff:\n    ignore_hidden_params: True\n    ignore_final_newline: False\n    output_format: \"{{ cloudformation_diff_output }}\"\n    output_choice: template\n    stack_name: \"{{ cloudformation.stack_name }}\"\n    template: \"{{ cloudformation_build_dir_path }}/{{ cloudformation_file_name }}\"\n    template_tags: \"{{ cloudformation.tags | default(omit) }}\"\n    template_parameters: \"{{ cloudformation.template_parameters | default(omit) }}\"\n    aws_access_key: \"{{ cloudformation.aws_access_key | default(cloudformation_defaults.aws_access_key | default(omit)) }}\"\n    aws_secret_key: \"{{ cloudformation.aws_secret_key | default(cloudformation_defaults.aws_secret_key | default(omit)) }}\"\n    security_token: \"{{ cloudformation.security_token | default(cloudformation_defaults.security_token | default(omit)) }}\"\n    profile: \"{{ cloudformation.profile | default(cloudformation_defaults.profile | default(omit)) }}\"\n    region: \"{{ cloudformation.region | default(cloudformation_defaults.region | default(omit)) }}\"\n  when: not cloudformation_generate_only and cloudformation_run_diff\n  check_mode: False\n\n\n###\n### Run rendered cloudformation template\n###\n- name: ensure cloudformation stack is present\n  cloudformation:\n    stack_name: \"{{ cloudformation.stack_name }}\"\n    state: present\n    template: \"{{ cloudformation_build_dir_path }}/{{ cloudformation_file_name }}\"\n\n    aws_access_key: \"{{ cloudformation.aws_access_key | default(cloudformation_defaults.aws_access_key | default(omit)) }}\"\n    aws_secret_key: \"{{ cloudformation.aws_secret_key | default(cloudformation_defaults.aws_secret_key | default(omit)) }}\"\n    security_token: \"{{ cloudformation.security_token | default(cloudformation_defaults.security_token | default(omit)) }}\"\n    profile: \"{{ cloudformation.profile | default(cloudformation_defaults.profile | default(omit)) }}\"\n    region: \"{{ cloudformation.region | default(cloudformation_defaults.region | default(omit)) }}\"\n    notification_arns: \"{{ cloudformation.notification_arns | default(cloudformation_defaults.notification_arns | default(omit)) }}\"\n    termination_protection: \"{{ cloudformation.termination_protextion | default(cloudformation_defaults.termination_protection | default(omit)) }}\"\n    template_parameters: \"{{ cloudformation.template_parameters | default(omit) }}\"\n    tags: \"{{ cloudformation.tags | default(omit) }}\"\n  when: not cloudformation_generate_only\n"}, {"commit_sha": "a5a5c4d74b7b0fd14f534dda1f41f903d1a58abf", "sha": "3661751ce262940d84aba6706c6d078870891112", "filename": "tasks/nvm.yml", "repository": "fubarhouse/ansible-role-nodejs", "decoded_content": "---\n# Tasks file for NVM\n\n- name: \"NVM | Clean-up\"\n  become: yes\n  become_user: \"root\"\n  file:\n    path: \"{{ fubarhouse_npm.nvm_install_dir }}\"\n    state: absent\n  when: fubarhouse_npm.clean_install\n\n- name: NVM | Clean-up default version from shell profiles\n  become: yes\n  become_user: \"root\"\n  lineinfile:\n    dest: \"{{ fubarhouse_npm.user_dir }}/{{ item.filename }}\"\n    regexp: '.nvm/v{{ node_version }}/bin'\n    line:  'export PATH=$PATH:{{ fubarhouse_npm.user_dir }}/.nvm/v{{ node_version }}/bin;'\n    state: absent\n  with_items:\n    - \"{{ fubarhouse_npm.shell_profiles }}\"\n  ignore_errors: yes\n  when: fubarhouse_npm.clean_install\n\n- name: NVM | Clean-up other versions from shell profiles\n  become: yes\n  become_user: \"root\"\n  lineinfile:\n    dest: \"{{ fubarhouse_npm.user_dir }}/{{ item[0].filename }}\"\n    regexp: '.nvm/v{{ item[1] }}/bin'\n    line:  'export PATH=$PATH:{{ fubarhouse_npm.user_dir }}/.nvm/v{{ item[1] }}/bin;'\n    state: absent\n  with_nested:\n    - \"{{ fubarhouse_npm.shell_profiles }}\"\n    - \"{{ node_versions }}\"\n  ignore_errors: yes\n  when: fubarhouse_npm.clean_install\n\n- name: \"NodeJS | Remove imported exports not associated to specific versions\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  lineinfile:\n    dest: \"{{ fubarhouse_npm.user_dir }}/{{ item.filename }}\"\n    line: \"export PATH=$PATH:$(npm config --global get prefix)/bin\"\n    state: absent\n  with_items: \"{{ fubarhouse_npm.shell_profiles }}\"\n  when: fubarhouse_npm.clean_install\n\n- name: \"NVM | Check\"\n  stat:\n    path: \"{{ fubarhouse_npm.nvm_install_dir }}\"\n  register: fubarhouse_npm_nvm_installed\n\n- name: \"NVM | Ensure permissions are set\"\n  become: yes\n  become_user: \"root\"\n  file:\n    path: \"{{ item.path }}\"\n    state: directory\n    mode: 0777\n    owner: \"{{ fubarhouse_user }}\"\n    recurse: yes\n  with_items: \"{{ fubarhouse_npm.folder_paths }}\"\n  changed_when: false\n\n- name: \"NVM | Clone/Update\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  git:\n    repo: \"{{ nvm_repo }}\"\n    dest: \"{{ fubarhouse_npm.nvm_install_dir }}\"\n    clone: yes\n    update: no\n    version: master\n    recursive: false\n\n- name: \"NVM | Install\"\n  shell: \"{{ fubarhouse_npm.nvm_install_dir }}/install.sh\"\n  when: fubarhouse_npm_nvm_installed.stat.exists == false\n\n- name: \"NVM | Create an executable\"\n  template:\n    src: \"nvm.sh\"\n    dest: \"{{ fubarhouse_npm.nvm_symlink_exec }}\"\n    owner: \"{{ fubarhouse_user }}\"\n    mode: 0755\n  when: fubarhouse_npm_nvm_installed.stat.exists == false\n\n- name: \"NVM | Get versions\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell: \"{{ fubarhouse_npm.nvm_symlink_exec }} ls-remote\"\n  register: nodejs_available_versions\n  changed_when: false\n\n- name: NVM | Ensure shell profiles are available\n  become: yes\n  become_user: \"root\"\n  file:\n    path: \"{{ fubarhouse_npm.user_dir }}/{{ item.filename }}\"\n    state: touch\n  with_items: \"{{ fubarhouse_npm.shell_profiles }}\"\n  ignore_errors: yes\n  changed_when: false\n\n- name: NVM | Ensure shell profiles are configured for default version\n  become: yes\n  become_user: \"root\"\n  lineinfile:\n    dest: \"{{ fubarhouse_npm.user_dir }}/{{ item.filename }}\"\n    regexp: '.nvm/v{{ node_version }}/bin'\n    line:  'export PATH=$PATH:{{ fubarhouse_npm.user_dir }}/.nvm/v{{ node_version }}/bin;'\n    state: present\n  with_items:\n    - \"{{ fubarhouse_npm.shell_profiles }}\"\n  when: '\"{{ node_version }}\" in nodejs_available_versions.stdout'\n  ignore_errors: yes\n\n- name: NVM | Ensure shell profiles are configured for other versions\n  become: yes\n  become_user: \"root\"\n  lineinfile:\n    dest: \"{{ fubarhouse_npm.user_dir }}/{{ item[0].filename }}\"\n    regexp: '.nvm/v{{ item[1] }}/bin'\n    line:  'export PATH=$PATH:{{ fubarhouse_npm.user_dir }}/.nvm/v{{ item[1] }}/bin;'\n    state: present\n  when: '\"{{ item[1] }}\" in nodejs_available_versions.stdout'\n  with_nested:\n    - \"{{ fubarhouse_npm.shell_profiles }}\"\n    - \"{{ node_versions }}\"\n  ignore_errors: yes"}, {"commit_sha": "526b8a4e7bc81c6ee7da4150a0757c626a4f6578", "sha": "fb0da0d11692ab5ebcd87e61f493fe01ba5d36af", "filename": "playbooks/deploy-rock.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- hosts: all\n  vars:\n    rock_debug: \"{{ lookup('env', 'DEBUG') }}\"\n    http_proxy: \"{{ lookup('env','http_proxy') }}\"\n    https_proxy: \"{{ lookup('env', 'https_proxy') }}\"\n  tasks:\n  - name: Get default settings\n    include_vars: rocknsm_config.dist.yml\n  - name: Apply override settings, if available\n    include_vars: /etc/rocknsm/config.yml\n    ignore_errors: true\n    failed_when: false\n  - name: Debug variables\n    include: debug.yml\n    when: rock_debug is defined and rock_debug\n\n    ######################################################\n    ################# Data Directory #####################\n    ######################################################\n    ###############\n    ##### NOTE ####\n    ###############\n    # You will want to remount this to your \"good\" storage after the build.\n    # This is just to make sure all the paths in the configs are proper.\n    ###############  - file:\n  - name: Create ROCK data dir\n    file:\n      path: \"{{ rock_data_dir }}\"\n      mode: 0755\n      owner: \"{{ rock_data_user }}\"\n      group: \"{{ rock_data_group }}\"\n      state: directory\n\n  - name: Create ROCK NSM directory\n    file:\n      path: \"{{ rocknsm_dir }}\"\n      mode: 0755\n      owner: root\n      group: root\n      state: directory\n\n    ######################################################\n    ######### Configure the monitoring interface #########\n    ######################################################\n  - name: Set monitor interface config\n    template:\n      src: templates/ifcfg-monif.j2\n      dest: /etc/sysconfig/network-scripts/ifcfg-{{ item }}\n      mode: 0644\n      owner: root\n      group: root\n      force: yes\n    with_items: \"{{ rock_monifs }}\"\n\n  - name: Configure local ifup script\n    template:\n      src: templates/ifup-local.j2\n      dest: /sbin/ifup-local\n      mode: 0755\n      owner: root\n      group: root\n      force: yes\n    notify: configure monitor interfaces\n\n    #######################################################\n    #################### Disable IPv6 #####################\n    #######################################################\n  - name: Disable IPv6 for all interfaces\n    sysctl:\n      name: net.ipv6.conf.all.disable_ipv6\n      value: 1l\n      sysctl_file: \"{{ rock_sysctl_file }}\"\n\n  - name: Disable IPv6 for default interfaces\n    sysctl:\n      name: net.ipv6.conf.default.disable_ipv6\n      value: 1\n      sysctl_file: \"{{ rock_sysctl_file }}\"\n\n  - name: Disable IPv6 in SSHD\n    lineinfile:\n      dest: /etc/ssh/sshd_config\n      regexp: AddressFamily\n      line: AddressFamily inet\n    notify:\n    - sshd restart\n\n  - name: Remove localhost6 from hosts file\n    lineinfile:\n      dest: /etc/hosts\n      regexp: localhost6\n      state: absent\n\n    #######################################################\n    #################### DNS Changes ######################\n    #######################################################\n  - name: Set hostname in hosts file\n    lineinfile:\n      dest: /etc/hosts\n      insertafter: 127.0.0.1\n      line: 127.0.0.2  {{ rock_fqdn }}  {{ rock_hostname }}\n\n  - name: Set system hostname\n    hostname:\n      name: \"{{ rock_fqdn }}\"\n\n    #######################################################\n    ################## Setup Yum Repos ####################\n    #######################################################\n  - name: Setup EPEL repo\n    yum_repository:\n      name: epel\n      description: EPEL YUM repo\n      baseurl: \"{{ epel_baseurl }}\"\n      gpgkey:  \"{{ epel_gpgurl }}\"\n      gpgcheck: yes\n    when: rock_online_install\n\n  - name: Manually trust CentOS GPG key\n    shell: >\n      rpm --import http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-7\n    when: rock_online_install\n\n  - name: Setup ELrepo Kernel repo\n    yum_repository:\n      name: elrepo-kernel\n      description: ELrepo Kernel YUM repo\n      baseurl: \"{{ elrepo_baseurl }}\"\n      gpgkey:  \"{{ elrepo_gpgurl }}\"\n      gpgcheck: yes\n    when: rock_online_install\n\n  - name: Setup Elastic repo\n    yum_repository:\n      name: elastic-5.x\n      description: Elastic Stack repository for 5.x\n      baseurl: \"{{ elastic_baseurl }}\"\n      gpgkey:  \"{{ elastic_gpgurl }}\"\n      gpgcheck: no\n    when: rock_online_install\n\n  - name: Setup ROCK NSM repo\n    yum_repository:\n      name: rocknsm\n      description: ROCK NSM repository for devel\n      baseurl: \"{{ rocknsm_baseurl }}\"\n      gpgkey:  \"{{ rocknsm_gpgurl }}\"\n      gpgcheck: no\n      cost: 750\n    when: rock_online_install\n\n  - name: Setup local offline repo\n    yum_repository:\n      name: rocknsm-local\n      description: ROCKNSM Local Repository\n      baseurl: \"{{ rocknsm_local_baseurl }}\"\n      gpgcheck: no\n      cost: 500\n    when: not rock_online_install\n\n  - name: Configure default CentOS online repos\n    yum_repository:\n      name: \"{{ item.name }}\"\n      enabled: \"{{ rock_online_install }}\"\n      description: \"CentOS-$releasever - {{ item.name | title }}\"\n      mirrorlist: \"{{ item.mirror }}\"\n      file:  CentOS-Base\n    with_items:\n      - { name: base, mirror: \"http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os&infra=$infra\" }\n      - { name: updates, mirror: \"http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=updates&infra=$infra\" }\n      - { name: extras, mirror: \"http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras&infra=$infra\"}\n\n    #######################################################\n    ############# Install/Remove Packages #################\n    #######################################################\n  - name: Configure packages\n    set_fact:\n      rocknsm_package_list: \"{{ rocknsm_package_list }} + [ '{{ item.pkg }}']\"\n    when: (item.test is undefined) or (item.test)\n    with_items:\n      - { pkg: elasticsearch, test: \"{{with_elasticsearch}}\", state: installed }\n      - { pkg: logstash, test: \"{{with_logstash}}\", state: installed }\n      - { pkg: kibana, test: \"{{with_kibana}}\", state: installed }\n      - { pkg: filebeat, test: \"{{with_suricata or with_fsf}}\", state: installed }\n      - { pkg: nginx, test: \"{{with_nginx}}\", state: installed }\n      - { pkg: bro, test: \"{{with_bro}}\", state: installed }\n      - { pkg: bro-plugin-af_packet, test: \"{{with_bro}}\", state: installed }\n      - { pkg: bro-plugin-kafka, test: \"{{(with_bro and with_kafka)}}\", state: installed }\n      - { pkg: stenographer, test: \"{{with_stenographer}}\", state: installed }\n      - { pkg: suricata, test: \"{{with_suricata}}\", state: installed }\n      - { pkg: snort, test: \"{{with_snort}}\", state: installed }\n      - { pkg: daq, test: \"{{with_snort}}\", state: installed }\n      - { pkg: zookeeper, test: \"{{with_zookeeper}}\", state: installed }\n      - { pkg: kafka, test: \"{{with_kafka}}\", state: installed }\n      - { pkg: kafkacat, test: \"{{with_kafka}}\", state: installed }\n      - { pkg: fsf, test: \"{{with_fsf}}\", state: installed }\n      - { pkg: chrony, state: installed }\n      - { pkg: firewalld, state: installed }\n      - { pkg: postfix, state: installed }\n\n  - name: Install packages\n    yum:\n      name: \"{{ item.pkg }}\"\n      state: \"{{ item.state }}\"\n    when: (item.test is undefined) or (item.test)\n    with_items:\n      - { pkg: \"{{ rocknsm_package_list }}\", state: installed }\n\n  - name: Ensure cache directory exists\n    file:\n      dest: \"{{ rock_cache_dir }}\"\n      state: directory\n      mode: 0755\n\n  - name: Download Pulled Pork\n    get_url:\n      url: \"{{ pulledpork_url }}\"\n      dest: \"{{ rock_cache_dir }}/{{ pulledpork_filename }}\"\n      mode: 0644\n    when: rock_online_install\n\n  - name: Install Pulled Pork\n    unarchive:\n      src: \"{{ rock_cache_dir }}/{{ pulledpork_filename }}\"\n      dest: /opt\n      owner: root\n      group: root\n      creates: \"/opt/pulledpork-{{ pulledpork_release }}\"\n      remote_src: yes\n    when: \"{{ with_pulledpork }}\"\n\n    ######################################################\n    ################### Configure Time ###################\n    ######################################################\n  - name: Enable and start chrony\n    service:\n      name: chronyd\n      enabled: yes\n      state: started\n\n  - name: Set system timezone\n    command: /usr/bin/timedatectl set-timezone UTC\n    when: ansible_date_time.tz != \"UTC\"\n\n  - name: Check if RTC set to UTC\n    shell: timedatectl | awk '/RTC in local/ { print $5 }'\n    changed_when: false\n    register: chrony_local_utc\n\n  - name: Set system hardware clock to UTC\n    command: /usr/bin/timedatectl set-local-rtc no\n    when: chrony_local_utc == 'yes'\n\n    #######################################################\n    ################ Configure firewall ###################\n    #######################################################\n  - name: Enable and start firewalld\n    service:\n      name: firewalld\n      enabled: yes\n      state: started\n\n  - name: Configure firewalld\n    firewalld:\n      port: \"{{ item[1].port }}\"\n      source: \"{{ item[0] }}\"\n      permanent: yes\n      state: enabled\n      immediate: yes\n    when: (item[1].test is undefined) or item[1].test\n    with_nested:\n      - \"{{ rock_mgmt_nets }}\"\n      -\n        - { port: \"22/tcp\" }\n        - { port: \"443/tcp\",  test: \"{{ with_kibana }}\" }\n\n    ######################################################\n    ############## Configure GeoIP Databases #############\n    ######################################################\n  - name: Configure GeoIP Update\n    copy: src=GeoIP.conf dest=/etc/GeoIP.conf\n\n    # There's an issue w/ geoipupdate when env is empty\n  - name: Update GeoIP\n    shell: >\n      if [ \"x$HTTP_PROXY\" == \"x\" ]; then\n          unset HTTP_PROXY;\n      fi\n      if [ \"x$http_proxy\" == \"x\" ]; then\n          unset http_proxy;\n      fi\n      if [ \"x$HTTPS_PROXY\" == \"x\" ]; then\n          unset HTTPS_PROXY;\n      fi\n      if [ \"x$https_proxy\" == \"x\" ]; then\n          unset https_proxy;\n      fi\n      /usr/bin/geoipupdate\n    args:\n      creates: /usr/share/GeoIP/GeoLiteASNum.dat\n    register: result\n    failed_when: (result.rc != 0) and (result.rc != 1)\n\n  - name: Create GeoIP symlinks\n    file:\n      src: \"/usr/share/GeoIP/{{ item.src }}\"\n      dest: \"/usr/share/GeoIP/{{ item.dest }}\"\n      force: yes\n      state: link\n    with_items:\n      - { src: 'GeoLiteCity.dat', dest: 'GeoIPCity.dat' }\n      - { src: 'GeoLiteCountry.dat', dest: 'GeoIPCountry.dat' }\n      - { src: 'GeoLiteASNum.dat', dest: 'GeoIPASNum.dat' }\n      - { src: 'GeoLiteCityv6.dat', dest: 'GeoIPCityv6.dat' }\n\n    ######################################################\n    ################### Setup Zookeeper ##################\n    ######################################################\n  - name: Install zookeeper service file\n    copy:\n      src: zookeeper.service\n      dest: /etc/systemd/system/zookeeper.service\n      mode: 0644\n      owner: root\n      group: root\n    when: with_zookeeper\n\n  - name: Enable and start zookeeper\n    service:\n      name: zookeeper\n      state: \"{{ 'started' if enable_zookeeper else 'stopped' }}\"\n      enabled: \"{{ enable_zookeeper }}\"\n    when: with_zookeeper\n\n    ######################################################\n    ##################### Setup Kafka ####################\n    ######################################################\n  - name: Create Kafka data dir\n    file:\n      path: \"{{ kafka_data_dir }}\"\n      mode: 0755\n      owner: \"{{ kafka_user }}\"\n      group: \"{{ kafka_group }}\"\n      state: directory\n    when: with_kafka\n\n  - name: Set kafka retention\n    lineinfile:\n      dest: \"{{ kafka_config_path }}\"\n      regexp: \"log.retention.hours=\"\n      line:  \"log.retention.hours={{ kafka_retention }}\"\n      state: present\n    when: with_kafka\n\n  - name: Set kafka data dir\n    lineinfile:\n      dest: \"{{ kafka_config_path }}\"\n      regexp: \"log.dirs=\"\n      line: \"log.dirs={{ kafka_data_dir }}\"\n    when: with_kafka\n\n  - name: Enable and start kafka\n    service:\n      name: kafka\n      state: \"{{ 'started' if enable_kafka else 'stopped' }}\"\n      enabled: \"{{ enable_kafka }}\"\n    when: with_kafka\n\n    ######################################################\n    ################# Setup Elasticsearch ################\n    ######################################################\n  - name: Create Elasticsearch directory\n    file:\n      path: \"{{ es_data_dir }}\"\n      mode: 0755\n      owner: \"{{ es_user }}\"\n      group: \"{{ es_group }}\"\n      state: directory\n    when: with_elasticsearch\n\n  - name: Setup elasticsearch config\n    template:\n      src: templates/elasticsearch.yml.j2\n      dest: /etc/elasticsearch/elasticsearch.yml\n      owner: root\n      group: \"{{ es_group }}\"\n      mode: 0640\n    when: with_elasticsearch\n\n  - name: Create elasticsearch systemd override dir\n    file:\n      path: /etc/systemd/system/elasticsearch.service.d\n      owner: root\n      group: root\n      mode: 0755\n      state: directory\n    when: with_elasticsearch\n\n  - name: Enable elasticsearch memlock in service override\n    copy:\n      content: \"{{ es_memlock_override }}\"\n      dest: /etc/systemd/system/elasticsearch.service.d/override.conf\n      mode: 0644\n      owner: root\n      group: root\n    when: with_elasticsearch\n\n  - name: Setup elasticsearch data dir in sysconfig\n    lineinfile:\n      dest: /etc/sysconfig/elasticsearch\n      regexp: \"DATA_DIR=\"\n      line: \"DATA_DIR={{ es_data_dir }}\"\n    when: with_elasticsearch\n\n  - name: Setup elasticsearch jvm options\n    template:\n      src: templates/es-jvm.options.j2\n      dest: /etc/elasticsearch/jvm.options\n      mode: 0640\n      owner: root\n      group: \"{{ es_group }}\"\n    when: with_elasticsearch\n\n  - name: Install ROCK Elasticsearch cleanup script\n    template:\n      src: templates/es_cleanup.sh.j2\n      dest: /usr/local/bin/es_cleanup.sh\n      mode: 0755\n      owner: root\n      group: root\n    when: with_elasticsearch\n\n  - name: Set elasticsearch cleanup cron job\n    cron:\n      name: \"ES maintenance\"\n      cron_file: rocknsm_es_maintenance\n      hour: 0\n      minute: 1\n      user: root\n      job: /usr/local/bin/es_cleanup.sh > /dev/null 2>&1\n    when: with_elasticsearch\n\n  - name: Enable and start Elasticsearch\n    service:\n      name: elasticsearch\n      state: \"{{ 'started' if enable_elasticsearch else 'stopped' }}\"\n      enabled: \"{{ enable_elasticsearch }}\"\n    when: with_elasticsearch\n    notify:\n      - es maintenance\n\n  - name: Wait for Elasticsearch to become ready\n    wait_for: host=localhost port=9200\n    when: with_elasticsearch\n\n  - name: Check for Bro mapping templates\n    uri:\n      method: \"GET\"\n      url: http://localhost:9200/_template/bro_index\n    failed_when: False\n    register: bro_mapping\n    when: (with_elasticsearch and with_bro)\n\n  - name: Load Bro Elasticsearch mapping templates\n    uri:\n      method: PUT\n      url: http://localhost:9200/_template/bro_index\n      body: \"{{ lookup('file', 'es-bro-mappings.json')}}\"\n      body_format: json\n    when: (with_elasticsearch and with_bro) and bro_mapping.status == 404\n\n    ######################################################\n    ################### Setup Logstash ###################\n    ######################################################\n  - name: Install Bro-Kafka configuration for Logstash\n    copy:\n      src: logstash-kafka-bro.conf\n      dest: /etc/logstash/conf.d/kafka-bro.conf\n      mode: 0640\n      owner: \"{{ logstash_user }}\"\n      group: \"{{ logstash_group }}\"\n    when: with_logstash and with_bro and with_kafka\n    notify: Restart Logstash\n\n  - name: Install Suricata-Kafka configuration for Logstash\n    copy:\n      src: logstash-kafka-suricata.conf\n      dest: /etc/logstash/conf.d/kafka-suricata.conf\n      mode: 0640\n      owner: \"{{ logstash_user }}\"\n      group: \"{{ logstash_group }}\"\n    when: with_logstash and with_suricata and with_kafka\n    notify: Restart Logstash\n\n  - name: Install FSF-Kafka configuration for Logstash\n    copy:\n      src: logstash-kafka-fsf.conf\n      dest: \"/etc/logstash/conf.d/kafka-fsf.conf\"\n      mode: 0640\n      owner: \"{{ logstash_user }}\"\n      group: \"{{ logstash_group }}\"\n    when: with_logstash and with_fsf and with_kafka\n    notify: Restart Logstash\n\n  - name: Enable and start Logstash\n    service:\n      name: logstash\n      state: \"{{ 'started' if enable_logstash else 'stopped' }}\"\n      enabled: \"{{ enable_logstash }}\"\n    when: with_logstash\n\n    ######################################################\n    ################### Setup Filebeat ###################\n    ######################################################\n  - name: Add Filebeat configuration file\n    template:\n      src: filebeat.yml.j2\n      dest: /etc/filebeat/filebeat.yml\n    notify: Restart Filebeat\n\n  - name: Enable and start Filebeat\n    service:\n      name: filebeat\n      state: \"{{ 'started' if enable_filebeat else 'stopped' }}\"\n      enabled: \"{{ enable_filebeat }}\"\n    when: with_filebeat\n\n    #######################################################\n    ###################### Setup Bro  #####################\n    #######################################################\n  - name: Create bro group\n    group:\n      name: \"{{ bro_group }}\"\n      state: present\n      system: yes\n    when: with_bro\n\n  - name: Create bro user and group\n    user:\n      name: \"{{ bro_user }}\"\n      comment: \"bro service account\"\n      createhome: no\n      group: \"{{ bro_group }}\"\n      home: /opt/bro\n      shell: /sbin/nologin\n      system: yes\n      state: present\n    when: with_bro\n\n  - name: Create Bro directories\n    file:\n      path: \"{{ item }}\"\n      mode: 0755\n      owner: \"{{ bro_user }}\"\n      group: \"{{ bro_group }}\"\n      state: directory\n    with_items:\n      - \"{{ bro_data_dir }}\"\n      - \"{{ bro_data_dir }}/logs\"\n      - \"{{ bro_data_dir }}/spool\"\n    when: with_bro\n\n  - name: Create symlinks for wandering analysts\n    file:\n      dest: \"/opt/bro/{{ item }}\"\n      src:  \"{{ bro_data_dir }}/{{ item }}\"\n      state: link\n      force: yes\n    with_items:\n      - logs\n    when: with_bro\n\n  - name: Install broctl service file\n    template:\n      src: templates/broctl.service.j2\n      dest: /etc/systemd/system/broctl.service\n      owner: root\n      group: root\n      mode: 0644\n    when: with_bro\n    notify: reload systemd\n\n  - name: Create Bro node.cfg\n    template:\n      src: templates/bro-node.cfg.j2\n      dest: /opt/bro/etc/node.cfg\n      mode: 0644\n      owner: root\n      group: root\n    when: with_bro\n    notify: reload broctl\n\n\n  - name: Create broctl.cfg\n    template:\n      src: templates/bro-broctl.cfg.j2\n      dest: /opt/bro/etc/broctl.cfg\n      mode: 0644\n      owner: root\n      group: root\n    when: with_bro\n    notify: reload broctl\n\n  - name: Create bro networks.cfg\n    copy:\n      src: bro-networks.cfg\n      dest: /opt/bro/etc/networks.cfg\n      mode: 0644\n      owner: root\n      group: root\n    when: with_bro\n    notify: reload broctl\n\n  - name: Add bro custom scripts dir\n    file:\n      path: /opt/bro/share/bro/site/scripts\n      owner: root\n      group: root\n      mode: 0755\n      state: directory\n    when: with_bro\n\n  - name: Set permissions on broctl scripts dir\n    file:\n      path: /opt/bro/share/broctl/scripts\n      owner: \"{{ bro_user }}\"\n      group: \"{{ bro_user }}\"\n      mode: 0755\n      state: directory\n    when: with_bro\n\n  - name: Add README to scripts dir\n    copy:\n      src: bro-scripts-readme.txt\n      dest: /opt/bro/share/bro/site/scripts/README.txt\n      mode: 0644\n      owner: root\n      group: root\n    when: with_bro\n\n  - name: Checkout ROCK Bro scripts\n    git:\n      repo: \"{{ bro_rockscripts_repo }}\"\n      dest: /opt/bro/share/bro/site/scripts/rock\n      version: \"{{ bro_rockscripts_branch }}\"\n    when: with_bro and rock_online_install\n\n  - name: Deploy offline ROCK Bro scripts\n    unarchive:\n      src: \"{{ rock_cache_dir }}/{{ bro_rockscripts_filename }}\"\n      dest: /opt/bro/share/bro/site/scripts/\n      owner: root\n      group: root\n      creates: \"/opt/bro/share/bro/site/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}\"\n      remote_src: yes\n    when: with_bro and not rock_online_install\n\n  - name: Symlink offline ROCK bro scripts\n    file:\n      src: \"/opt/bro/share/bro/site/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}\"\n      dest: \"/opt/bro/share/bro/site/scripts/rock\"\n      state: link\n      force: yes\n    when: with_bro and not rock_online_install\n\n  - name: Update owner for ROCK NSM Bro scripts\n    file:\n      path: /opt/bro/share/bro/site/scripts/rock\n      owner: \"{{ bro_user }}\"\n      group: \"{{ bro_group }}\"\n      state: directory\n      recurse: yes\n      follow: yes\n    tags:\n      - bro_scripts\n    when: with_bro\n\n  - name: Add ROCK scripts to local.bro\n    lineinfile:\n      dest: /opt/bro/share/bro/site/local.bro\n      line: \"@load scripts/rock # ROCK NSM customizations\"\n      state: present\n    when: with_bro\n\n  - name: Add AF_PACKET workaround to local.bro\n    lineinfile:\n      dest: /opt/bro/share/bro/site/local.bro\n      line: \"@load scripts/rock/plugins/afpacket\"\n      state: present\n    when: with_bro\n\n  - name: Enable Bro Kafka output to local.bro\n    lineinfile:\n      dest: /opt/bro/share/bro/site/local.bro\n      line: \"@load scripts/rock/plugins/kafka\"\n      state: present\n    when: with_bro and with_kafka\n\n  - name: Enable the SMB Analyzer in local.bro\n    lineinfile:\n      dest: /opt/bro/share/bro/site/local.bro\n      line: \"@load policy/protocols/smb # Enable Bro SMB Analyzer\"\n      state: present\n    when: with_bro\n\n  - name: Add bro to path and aliases\n    copy:\n      src: profile.d-bro.sh\n      dest: /etc/profile.d/bro.sh\n      mode: 0644\n      owner: root\n      group: root\n    when: with_bro\n\n  - name: Add broctl wrapper for admin use\n    copy:\n      src: broctl.sh\n      dest: /usr/sbin/broctl\n      mode: 0754\n      owner: root\n      group: root\n    when: with_bro\n\n  - name: Create bro utility symlinks\n    file:\n      src: \"/opt/bro/bin/{{ item.src }}\"\n      dest: \"/usr/bin/{{ item.dest }}\"\n      force: yes\n      state: link\n    with_items:\n      - { src: 'bro', dest: 'bro' }\n      - { src: 'bro-cut', dest: 'bro-cut' }\n\n  - name: Set bro capabilities\n    capabilities:\n      path: /opt/bro/bin/bro\n      capability: \"{{ item }}\"\n      state: present\n    with_items:\n      - \"cap_net_raw+eip\"\n      - \"cap_net_admin+eip\"\n    when: with_bro\n\n  - name: Set capstats capabilities\n    capabilities:\n      path: /opt/bro/bin/capstats\n      capability: \"{{ item }}\"\n      state: present\n    with_items:\n      - \"cap_net_raw+eip\"\n      - \"cap_net_admin+eip\"\n    when: with_bro\n\n  - name: Set broctl cron\n    cron:\n      name: \"broctl maintenance\"\n      minute: \"*/5\"\n      cron_file: rocknsm_broctl\n      user: \"{{ bro_user }}\"\n      job: \"/opt/bro/bin/broctl cron >/dev/null 2>&1\"\n    when: with_bro\n\n  - name: Initialize bro scripts for workers\n    command: /opt/bro/bin/broctl install\n    args:\n      creates: \"{{ bro_data_dir }}/spool/broctl-config.sh\"\n    become: yes\n    become_user: \"{{ bro_user }}\"\n    when: with_bro\n\n  - name: Enable and start broctl\n    service:\n      name: broctl\n      enabled: \"{{ enable_bro }}\"\n      state: \"{{ 'started' if enable_bro else 'stopped' }}\"\n    when: with_bro\n\n    ######################################################\n    ################# Setup Stenographer #################\n    ######################################################\n  - name: Set stenographer config\n    template:\n      src: templates/stenographer-config.j2\n      dest: \"/etc/stenographer/config.{{ item.1 }}\"\n    with_indexed_items: \"{{ rock_monifs }}\"\n    when: with_stenographer\n\n  - name: Create Stenographer directories\n    file:\n      path: \"{{ stenographer_data_dir }}/{{ item[0] }}/{{ item[1] }}\"\n      mode: 0755\n      owner: \"{{ stenographer_user }}\"\n      group: \"{{ stenographer_group }}\"\n      state: directory\n    with_nested:\n      - \"{{ rock_monifs }}\"\n      - [ 'index', 'packets' ]\n    when: with_stenographer\n\n  - name: Install stenographer service files\n    copy:\n      src: \"{{ item }}\"\n      dest: \"/etc/systemd/system/{{ item }}\"\n      mode: 0644\n      owner: root\n      group: root\n    with_items:\n      - stenographer.service\n      - stenographer@.service\n    when: with_stenographer\n\n  - name: Generate stenographer keys\n    command: >\n      /usr/bin/stenokeys.sh {{ stenographer_user }} {{ stenographer_group }}\n    args:\n      creates: /etc/stenographer/certs/client_key.pem\n    when: with_stenographer\n\n  - name: Configure Stenographer service\n    service:\n      name: stenographer\n      enabled: \"{{ enable_stenographer }}\"\n      state: \"{{ 'started' if enable_stenographer else 'stopped' }}\"\n    when: with_stenographer\n\n  - name: Configure Stenographer per-interface\n    service:\n      name: \"stenographer@{{ item }}\"\n      enabled: \"{{ enable_stenographer }}\"\n      state: \"{{ 'started' if enable_stenographer else 'stopped' }}\"\n    with_items: \"{{ rock_monifs }}\"\n    when: with_stenographer\n\n    ######################################################\n    ################## Setup Suricata ####################\n    ######################################################\n  - name: Create Suricata directories\n    file:\n      path: \"{{ suricata_data_dir }}/\"\n      mode: 0755\n      owner: \"{{ suricata_user }}\"\n      group: \"{{ suricata_group }}\"\n      state: directory\n    when: with_suricata\n\n  - name: Set suricata capabilities\n    capabilities:\n      path: /usr/sbin/suricata\n      capability: \"{{ item }}\"\n      state: present\n    with_items:\n      - \"cap_net_raw+eip\"\n      - \"cap_net_admin+eip\"\n      - \"cap_ipc_lock+eip\"\n    when: with_suricata\n\n  - name: Remove suricata sysconfig file\n    file:\n      path: /etc/sysconfig/suricata\n      state: absent\n    when: with_suricata\n\n  - name: Install suricata service files\n    copy:\n      src: \"suricata.service\"\n      dest: \"/etc/systemd/system/suricata.service\"\n      mode: 0644\n      owner: root\n      group: root\n    when: with_suricata\n\n  - name: Install suricata overrides\n    template:\n      src: templates/suricata_overrides.yaml.j2\n      dest: /etc/suricata/rocknsm-overrides.yaml\n      mode: 0644\n      owner: \"{{ suricata_user }}\"\n      group: root\n    when: with_suricata\n\n  - name: Create IP reputation config dir\n    file:\n      path: /etc/suricata/rules/iplists\n      state: directory\n      owner: root\n      group: root\n      mode: 0755\n    when: with_suricata\n\n  - name: Set suricata overrides include in main config\n    lineinfile:\n      dest: /etc/suricata/suricata.yaml\n      line: \"include: rocknsm-overrides.yaml\"\n      state: present\n    when: with_suricata\n\n  - name: Enable and start suricata\n    service:\n      name: suricata\n      enabled: \"{{ enable_suricata }}\"\n      state: \"{{ 'started' if enable_suricata else 'stopped' }}\"\n    when: with_suricata\n\n  - name: Configure logrotate for suricata logs\n    template:\n      src: templates/logrotate-suricata.conf.j2\n      dest: /etc/logrotate.d/suricata.conf\n      mode: 0644\n      owner: root\n      group: root\n    when: with_suricata\n\n    ######################################################\n    ################# Setup PulledPork  ##################\n    ######################################################\n  - name: Create pulledpork directory symlink\n    file:\n      src: \"/opt/pulledpork-{{ pulledpork_release }}\"\n      dest: \"/opt/pulledpork\"\n      state: link\n      force: yes\n    when: with_pulledpork\n\n  - name: Set pulledpork executable\n    file:\n      path: /opt/pulledpork/pulledpork.pl\n      mode: 0755\n      owner: root\n      group: root\n    when: with_pulledpork\n\n  - name: Create pulledpork config dir\n    file:\n      path: /etc/pulledpork\n      mode: 0755\n      owner: root\n      group: root\n      state: directory\n    when: with_pulledpork\n\n  - name: Configure pulledpork\n    template:\n      src: templates/pulledpork.conf.j2\n      dest: /etc/pulledpork/pulledpork.conf\n      owner: root\n      group: root\n      mode: 0644\n    when: with_pulledpork\n\n  - name: Check stats of rules files\n    stat:\n      path: \"{{ pulledpork_engine_basepath }}/rules/pulledpork.rules\"\n    register: rules_file\n    when: with_pulledpork\n\n  - name: Create initial pulledpork rules-related files\n    file:\n      path: \"{{ pulledpork_engine_basepath }}/rules/pulledpork.rules\"\n      owner: root\n      group: root\n      mode: 0644\n      state: touch\n    when: with_pulledpork and not rules_file.stat.exists\n\n  - name: Schedule pulledpork to run daily\n    cron:\n      name: \"pulledpork update\"\n      cron_file: rocknsm_pulledpork\n      user: root\n      hour: \"12\"\n      minute: \"0\"\n      job: /opt/pulledpork/pulledpork.pl\n        -c /etc/pulledpork/pulledpork.conf\n        -l > /var/log/pulledpork.log 2>&1;\n        {{ \"/usr/bin/systemctl kill -s USR2 suricata;\" if with_suricata else None }}\n        {{ \"/usr/bin/systemctl restart snortd;\" if with_snort else None }}\n    when: with_pulledpork\n\n    #######################################################\n    ######################## FSF ##########################\n    #######################################################\n  - name: Create FSF data dir\n    file:\n      path: \"{{ fsf_data_dir }}\"\n      mode: 0755\n      owner: \"{{ fsf_user }}\"\n      group: \"{{ fsf_group }}\"\n      state: directory\n    when: with_fsf\n\n  - name: Create FSF archive dir\n    file:\n      path: \"{{ fsf_archive_dir }}\"\n      mode: 0755\n      owner: \"{{ fsf_user }}\"\n      group: \"{{ fsf_group }}\"\n      state: directory\n    when: with_fsf\n\n  - name: Configure logrotate for FSF logs\n    copy:\n      src: logrotate-fsf.conf\n      dest: /etc/logrotate.d/fsf.conf\n      mode: 0644\n      owner: root\n      group: root\n    when: with_fsf\n\n  - name: Configure fsf-server\n    template:\n      src: templates/fsf-server-config.j2\n      dest: /opt/fsf/fsf-server/conf/config.py\n      owner: \"{{ fsf_user }}\"\n      group: \"{{ fsf_group }}\"\n      mode: 0644\n    when: with_fsf\n\n  - name: Configure fsf-client\n    template:\n      src: templates/fsf-client-config.j2\n      dest: /opt/fsf/fsf-client/conf/config.py\n      owner: \"{{ fsf_user }}\"\n      group: \"{{ fsf_group }}\"\n      mode: 0644\n    when: with_fsf\n\n  - name: Enable and start FSF\n    service:\n      name: fsf\n      state: \"{{ 'started' if enable_fsf else 'stopped' }}\"\n      enabled: \"{{ enable_fsf }}\"\n    when: with_fsf\n\n    ######################################################\n    ################### Setup Kibana #####################\n    ######################################################\n  - name: Enable and start Kibana\n    service:\n      name: kibana\n      state: \"{{ 'started' if enable_kibana else 'stopped' }}\"\n      enabled: \"{{ enable_kibana }}\"\n    when: with_kibana\n\n  - name: Download ROCK Dashboards\n    get_url:\n      url: \"{{ rock_dashboards_url }}\"\n      dest: \"{{ rock_cache_dir }}/{{ rock_dashboards_filename }}\"\n      mode: 0644\n    when: with_kibana and rock_online_install\n\n  - name: Extract ROCK Dashboards\n    unarchive:\n      src: \"{{ rock_cache_dir }}/{{ rock_dashboards_filename }}\"\n      dest: /opt/rocknsm\n      owner: root\n      group: root\n      creates: \"/opt/rocknsm/rock-dashboards-{{ rock_dashboards_branch }}\"\n      remote_src: yes\n    when: with_kibana\n\n  - name: Query Kibana package info\n    yum:\n      list: kibana\n    register: kibana_pkg\n    when: with_kibana\n\n  - name: Store installed kibana pkg info\n    set_fact:\n      kibana_info: \"{{ kibana_pkg.results | selectattr('repo', 'match', 'installed') | first }}\"\n    when: with_kibana\n\n  - name: Check current Kibana config\n    uri:\n      method: \"GET\"\n      url: \"{{ es_url }}/.kibana/config/{{ kibana_info.version }}/_source\"\n      return_content: true\n    register: kibana_cfg\n    changed_when: false\n    until: kibana_cfg.status == 200\n    retries: 10\n    delay: 3\n    when: with_kibana\n\n  - name: Store Kibana config dict\n    set_fact:\n      kibana_config: \"{{ kibana_cfg.json }}\"\n    when: with_kibana\n\n  - name: Configure Kibana templates\n    uri:\n      method: PUT\n      url: http://localhost:9200/_template/kibana-config\n      body: >\n        { \"order\" : 0, \"template\" : \".kibana\",\n          \"settings\" :\n            { \"index.number_of_replicas\" : \"0\",\n              \"index.number_of_shards\" : \"1\" },\n          \"mappings\" : { }, \"aliases\" : { } }\n      status_code: 200,201\n    when: with_kibana\n\n  - name: Push Kibana dashboard config\n    command: >\n      /opt/rocknsm/rock-dashboards-{{ rock_dashboards_branch }}/load.sh\n        -url {{ es_url }}\n    args:\n      chdir: /opt/rocknsm/rock-dashboards-{{ rock_dashboards_branch }}/\n    when: with_kibana and (kibana_config.rock_config is undefined or kibana_config.rock_config != rock_dashboards_version)\n\n  - name: Store default Kibana index to Bro\n    set_fact:\n      kibana_config: \"{{ kibana_config | combine({'defaultIndex': 'bro-*' })}}\"\n    when: with_kibana and with_bro\n\n  - name: Store default Kibana index to Suricata\n    set_fact:\n      kibana_config: \"{{ kibana_config | combine({'defaultIndex': 'suricata-*' })}}\"\n    when: with_kibana and with_suricata and not with_bro\n\n  - name: Update Kibana config dict w/ rock_config version\n    set_fact:\n      kibana_config: \"{{ kibana_config | combine({'rock_config': rock_dashboards_version }) }}\"\n    when: with_kibana\n\n  - name: Push Kibana settings for index and rock_version\n    uri:\n      method: PUT\n      url: \"{{ es_url }}/.kibana/config/{{ kibana_info.version }}\"\n      body: \"{{ kibana_config }}\"\n      body_format: \"json\"\n      status_code: 200,201\n    when: with_kibana\n\n  - name: Add the kibanapw shell function\n    copy:\n      src: profile.d-kibanapw.sh\n      dest: /etc/profile.d/kibanapw.sh\n      mode: 0644\n      owner: root\n      group: root\n    when: with_kibana\n\n  - name: Set initial Kibana credentials\n    shell: >\n      kibuser=$(getent passwd 1000 | awk -F: '{print $1}')\n      kibpw=$(xkcdpass -a rock)\n      echo -e \"U: ${kibuser}\\nP: ${kibpw}\" > /home/${kibuser}/KIBANA_CREDS.README\n      printf \"${kibuser}:$(echo ${kibpw} | openssl passwd -apr1 -stdin)\\n\" | sudo tee -a /etc/nginx/.htpasswd > /dev/null 2>&1\n    args:\n      creates: /etc/nginx/.htpasswd\n    when: with_kibana\n\n\n    ######################################################\n    ################### Setup nginx ######################\n    ######################################################\n  - name: Install ROCK nginx configuration\n    template:\n      src: templates/nginx-rock.conf.j2\n      dest: /etc/nginx/conf.d/rock.conf\n      mode: 0644\n      owner: root\n      group: root\n    when: with_nginx and with_kibana\n\n  - name: Install nginx base configuration\n    copy:\n      src: nginx.conf\n      dest: /etc/nginx/nginx.conf\n      mode: 0644\n      owner: root\n      group: root\n    when: with_nginx\n\n  - name: Enable nginx to perform proxy connect\n    seboolean: \n      name: httpd_can_network_connect\n      state: yes \n      persistent: yes\n    when: with_nginx and with_kibana\n\n  - name: Enable and start nginx\n    service:\n      name: nginx\n      state: \"{{ 'started' if enable_nginx else 'stopped' }}\"\n      enabled: \"{{ enable_nginx }}\"\n    when: with_nginx\n\n  - name: Create easy-rsa working dir\n    file:\n      path: /opt/easy-rsa\n      state: directory\n      mode: 0750\n      owner: root\n      group: root\n    when: with_kibana and with_nginx\n\n  - name: Create working copy of easy-rsa\n    copy:\n      src: /usr/share/easy-rsa/2.0/\n      dest: /opt/easy-rsa/\n      mode: 0750\n      owner: root\n      group: root\n    when: with_kibana and with_nginx\n\n  - name: Apply easy-rsa vars template\n    template:\n      src: templates/easy-rsa-vars.j2\n      dest: /opt/easy-rsa/vars\n      mode: 0644\n    when: with_kibana and with_nginx\n\n  - name: Generate and copy Kibana keys\n    shell: >\n      cd /opt/easy-rsa\n      ./build-ca --batch nopass\n      ./build-dh --batch\n      ./build-key-server --batch {{ rock_hostname }}\n      cat keys/{{{ rock_hostname }}.crt,ca.crt} >> /etc/pki/bundle.crt\n      cp keys/{{ rock_hostname }}.key /etc/pki/bundle.key\n      cp keys/dh2048.pem /etc/pki/\n    args:\n      chdir: /opt/easy-rsa/\n      creates: /etc/pki/dh2048.pem\n    when: with_kibana and with_nginx\n\n    ######################################################\n    ############### Setup ROCKNSM Scripts ################\n    ######################################################\n  - name: Install rock start script\n    copy:\n      src: rock_start\n      dest: /usr/local/bin/rock_start\n      mode: 0700\n      owner: root\n      group: root\n\n  - name: Install rock stop script\n    copy:\n      src: rock_stop\n      dest: /usr/local/bin/rock_stop\n      mode: 0700\n      owner: root\n      group: root\n\n  - name: Install rock status script\n    copy:\n      src: rock_status\n      dest: /usr/local/bin/rock_status\n      mode: 0755\n      owner: root\n      group: root\n\n  - name: Create rock script symlinks\n    file:\n      src: \"/usr/local/bin/{{ item.src }}\"\n      dest: \"/usr/sbin/{{ item.dest }}\"\n      force: yes\n      state: link\n    with_items:\n      - { src: 'rock_start', dest: 'rock_start' }\n      - { src: 'rock_stop', dest: 'rock_stop' }\n      - { src: 'rock_status', dest: 'rock_status' }\n\n  # Training mode / Service mode not needed for AF_PACKET\n  ######################################################\n  ############### ROCKNSM Customization ################\n  ######################################################\n  - name: Install ROCK NSM /etc/issue\n    copy:\n      src: etc-issue.in\n      dest: /etc/issue.in\n      mode: 0644\n      owner: root\n      group: root\n\n  - name: NetworkManager ROCK NSM hook\n    copy:\n      src: nm-issue-update\n      dest: /etc/NetworkManager/dispatcher.d/50-rocknsm-issue-update\n      mode: 0755\n      owner: root\n      group: root\n\n  #######################################################\n  #####################  Handlers  ######################\n  #######################################################\n  handlers:\n    - name: force sync time\n      command: >\n        chronyc -a 'burst 3/4'; sleep 5; chronyc -a makestep\n\n    - name: configure monitor interfaces\n      shell: >\n        for intf in {{ rock_monifs | join(' ') }}; do\n          /sbin/ifup ${intf};\n        done\n\n    - name: sshd restart\n      service: name=sshd state=restarted\n\n    - name: es maintenance\n      command: /usr/local/bin/es_cleanup.sh\n\n    - name: reload broctl\n      service: name=broctl state=restarted\n\n    - name: create kafka bro topic\n      command: >\n        /opt/kafka/bin/kafka-topics.sh\n           --zookeeper 127.0.0.1:2181\n           --create\n           --replication-factor 1\n           --topic bro-raw\n           --partitions 1\n\n    - name: create kafka suricata topic\n      command: >\n        /opt/kafka/bin/kafka-topics.sh\n           --zookeeper 127.0.0.1:2181\n           --create\n           --replication-factor 1\n           --topic suricata-raw\n           --partitions 1\n\n    - name: create kafka fsf topic\n      command: >\n        /opt/kafka/bin/kafka-topics.sh\n           --zookeeper 127.0.0.1:2181\n           --create\n           --replication-factor 1\n           --topic fsf-raw\n           --partitions 1\n\n    - name: reload systemd\n      command: systemctl daemon-reload\n\n    - name: Restart Logstash\n      service:\n        name: logstash\n        state: restarted\n\n    - name: Restart Filebeat\n      service:\n        name: filebeat\n        state: restarted\n\n  environment:\n   http_proxy:  \"{{ http_proxy }}\"\n   https_proxy: \"{{ https_proxy }}\"\n   HTTP_PROXY:  \"{{ http_proxy }}\"\n   HTTPS_PROXY: \"{{ https_proxy }}\"\n"}, {"commit_sha": "8c4af8da901c68ce8c4bdd21c62e08cec5d3c23a", "sha": "9f2d9bd09665de348e60f3c9089351b83c6b6895", "filename": "tasks/remove-pre-docker-ce.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Determine Docker version\n  command: bash -c \"docker version | grep Version | awk '{print $2}'\"\n  ignore_errors: yes\n  changed_when: false\n  register: _cmd_docker_version\n\n- name: Set fact if old Docker installation shall be removed\n  set_fact:\n    _remove_old_docker: \"{{ docker_remove_pre_ce | bool }} and not \\\n      {{ _cmd_docker_version.stdout_lines[0] | search('-ce') }}\"\n  when: _cmd_docker_version.stdout_lines is defined and _cmd_docker_version.stdout_lines[0] is defined\n\n- name: Check if Docker is running\n  become: true\n  systemd:\n    name: docker\n  ignore_errors: yes\n  register: _service_docker_status\n  when: _remove_old_docker | default(False) | bool\n\n- name: Stop Docker service\n  service:\n    name: docker\n    state: stopped\n  when: \"_service_docker_status.rc | default(1) == 0\"\n\n- name: Remove old Docker installation before Docker CE\n  become: true\n  package:\n    name: \"{{ item }}\"\n    state: absent\n  when: _remove_old_docker | default(False) | bool\n  with_items:\n    - \"{{ docker_old_packages[_docker_os_dist] }}\"\n"}, {"commit_sha": "95a0ffb87f5a5ecbf178ee4a5b4f890acaba6cbe", "sha": "47d0b3c467647d4a309f7231d0a976906113bee3", "filename": "tasks/configure.yml", "repository": "nusenu/ansible-relayor", "decoded_content": "---\n\n- name: Ensure local key folders exist (LOCAL)\n  file: path={{ offline_masterkey_dir }}/{{ item[0] }}_{{ item.1.orport }}/keys\n    state=directory mode=700\n  delegate_to: 127.0.0.1\n  with_nested:\n   - \"{{ tor_ips }}\"\n   - \"{{ tor_ports }}\"\n  tags:\n   - createdir\n\n- name: Ensure all relay keys exist (LOCAL)\n  local_action: command tor --PublishServerDescriptor 0 --orport 1234 --list-fingerprint --datadirectory \"{{ offline_masterkey_dir }}/{{ item[0] }}_{{ item.1.orport }}\" --Log \"err stdout\"\n  with_nested:\n   - \"{{ tor_ips }}\"\n   - \"{{ tor_ports }}\"\n\n- name: Generate new Ed25519 signing keys\n  local_action: command tor --keygen --SigningKeyLifetime {{ tor_signingkeylifetime_days}}\\ days --datadirectory \"{{ offline_masterkey_dir }}/{{ item[0] }}_{{ item.1.orport }}\" --Log \"err stdout\"\n  with_nested:\n   - \"{{ tor_ips }}\"\n   - \"{{ tor_ports }}\"\n  tags:\n   - renewkey\n\n- name: Detect duplicate relay keys across relays (LOCAL)\n  shell: sha1sum {{ offline_masterkey_dir }}/*/keys/secret_id_key {{ offline_masterkey_dir }}/*/keys/ed25519_master_id_secret_key|cut -d/ -f1|sort|uniq -d|wc -l\n  delegate_to: 127.0.0.1\n  register: dupcount\n\n- name: Abort on duplicate relay keys\n  fail: msg=\"Duplicate relay key detected! Aborting.\"\n  when: dupcount.stdout != \"0\"\n\n- name: Detect if Ed25519 master keys are on the relay\n  stat: path={{ tor_DataDir }}/{{ item[0] }}_{{ item.1.orport }}/keys/ed25519_master_id_secret_key\n  become: yes\n  register: masterkeycheck\n  with_nested:\n   - \"{{ tor_ips }}\"\n   - \"{{ tor_ports }}\"\n\n- name: Abort if Ed25519 master keys are on the relay\n  fail: msg=\"\n\n            Ed25519 MASTER KEY detected on the relay - it is NOT supposed to be there! Aborting.\"\n  when: item.stat.exists == True\n  with_items: masterkeycheck.results\n\n- name: Collect fingerprints for MyFamily (LOCAL)\n  shell: cut {{ offline_masterkey_dir }}/*/fingerprint -d\" \" -f2|xargs|sed -e 's/ /,/g'\n  delegate_to: 127.0.0.1\n  register: family\n  tags:\n   - reconfigure\n\n- name: Ensure per-instance tor users exist\n  become: yes\n  user: name=_tor-{{ item[0] }}_{{ item.1.orport }} system=yes shell=/bin/false createhome=no home={{ tor_DataDir }}/{{ item[0] }}_{{ item.1.orport }}\n  with_nested:\n   - \"{{ tor_ips }}\"\n   - \"{{ tor_ports }}\"\n  tags:\n   - createdir\n\n- name: Ensure per-instance config folders exist (Debian only)\n  become: yes\n  file: path={{ tor_ConfDir }}/{{ item[0] }}_{{ item.1.orport }} state=directory mode=755\n  with_nested:\n   - tor_ips\n   - tor_ports\n  when: ansible_pkg_mgr == 'apt'\n  tags:\n   - createdir\n\n- name: Ensure DataDir exists\n  become: yes\n  file: path={{ tor_DataDir }}\n    state=directory\n    owner=root\n    mode=0755\n  tags:\n   - createdir\n\n- name: Ensure \"keys\" subfolder exists\n  become: yes\n  file: path={{ tor_DataDir }}/{{ item[0] }}_{{ item.1.orport }}/keys\n    state=directory\n    owner=\"_tor-{{ item[0] }}_{{ item.1.orport }}\"\n    group=\"_tor-{{ item[0] }}_{{ item.1.orport }}\"\n    mode=0700\n    recurse=yes\n  with_nested:\n   - \"{{ tor_ips }}\"\n   - \"{{ tor_ports }}\"\n\n- name: Ensure RSA key is in place (without overriding existing keys)\n  become: yes\n  copy: src={{ offline_masterkey_dir }}/{{ item[0] }}_{{ item.1.orport }}/keys/{{ item[2] }}\n   dest={{ tor_DataDir }}/{{ item[0] }}_{{ item.1.orport }}/keys/{{ item[2] }}\n   owner=\"_tor-{{ item[0] }}_{{ item.1.orport }}\"\n   mode=700 force=no\n  with_nested:\n   - \"{{ tor_ips }}\"\n   - \"{{ tor_ports }}\"\n   - [ 'secret_id_key' ]\n\n- name: Fetch RSA key for comparision\n  become: yes\n  fetch: src={{ tor_DataDir }}/{{ item[0] }}_{{ item.1.orport }}/keys/{{ item[2] }}\n    dest={{ offline_masterkey_dir }}/{{ item[0] }}_{{ item.1.orport }}/keys/{{ item[2] }}.untrustedremotekey\n    flat=yes\n  with_nested:\n   - \"{{ tor_ips }}\"\n   - \"{{ tor_ports }}\"\n   - [ 'secret_id_key' ]\n\n- name: Compare local vs. remote RSA key (secret_id_key)\n  local_action: shell sha1sum {{ offline_masterkey_dir }}/\"{{ item[0] }}_{{ item.1.orport }}\"/keys/secret_id_key*|cut -d/ -f1|uniq -d|wc -l\n  with_nested:\n   - \"{{ tor_ips }}\"\n   - \"{{ tor_ports }}\"\n  register: rsakey\n\n- name: Abort if local and remote RSA keys do not match\n  fail: 'msg=\"\n\n\n   Key MISMATCH detected: Remote RSA key does not match local key - manual intervention required.\n\n   We deteted that the remote host uses an RSA key that was not generated by us.\n   We will not override it with our locally generated key.\n\n   If you want to make use of the remote RSA key you have to override the local key manually:\n\n\n   cd ~/.tor/offlinemasterkeys/<IP_port>/keys\n\n   mv secret_id_key.untrustedremotekey secret_id_key\"'\n  when: item.stdout != \"1\"\n  with_items: rsakey.results\n\n# this task is separated from the task named \"Ensure RSA key is in place\" because it is not run with 'force=no'\n- name: Renew Ed25519 signing keys\n  become: yes\n  copy: src={{ offline_masterkey_dir }}/{{ item[0] }}_{{ item.1.orport }}/keys/{{ item[2] }}\n   dest={{ tor_DataDir }}/{{ item[0] }}_{{ item.1.orport }}/keys/{{ item[2] }}\n   owner=\"_tor-{{ item[0] }}_{{ item.1.orport }}\"\n   mode=700\n   setype=tor_var_lib_t\n  with_nested:\n   - \"{{ tor_ips }}\"\n   - \"{{ tor_ports }}\"\n   - [ 'ed25519_master_id_public_key', 'ed25519_signing_cert', 'ed25519_signing_secret_key' ]\n  tags:\n   - renewkey\n\n# This needs to be at the end to fix SELinux contexts recursively\n- name: Ensure per-instance DataDir exists\n  become: yes\n  file: path={{ tor_DataDir }}/{{ item[0] }}_{{ item.1.orport }}\n    state=directory\n    owner=\"_tor-{{ item[0] }}_{{ item.1.orport }}\"\n    group=\"_tor-{{ item[0] }}_{{ item.1.orport }}\"\n    mode=0700\n    recurse=yes\n    setype=tor_var_lib_t\n  with_nested:\n   - \"{{ tor_ips }}\"\n   - \"{{ tor_ports }}\"\n  tags:\n   - createdir\n\n- name: Ensure Tor config directory exists\n  become: yes\n  file: path={{ tor_ConfDir }}\n    state=directory\n    owner=root\n    group={{ tor_user }}\n    mode=755\n\n- name: Ensure LogDir exists\n  become: yes\n  file: path={{ tor_LogDir }}\n    state=directory\n    owner=root\n    mode=755\n  when: ansible_system != 'Linux'\n\n# we only use distinct logfiles on systems that have no SyslogIdentityTag support yet (all non-Debian plaforms)\n# otherwise we log to syslog with SyslogIdentityTag to avoid the filesystem permissions troubles with logrotate.\n# We aim to log to syslog+SyslogIdentityTag for all platforms eventually.\n# This is a medium-term workaround until all platform get SyslogIdentityTag support\n# without this workaround tor will fail to start after logrotate created new logfiles because\n# logrotate is not not aware that every tor instance runs under a distinct user.\n# This effectively disables logrotate.\n- name: Ensure per-instance LogDir exists\n  become: yes\n  file: path={{ tor_LogDir }}/{{ item[0] }}_{{ item.1.orport }}\n    state=directory\n    owner=_tor-{{ item[0] }}_{{ item.1.orport }}\n    group=_tor-{{ item[0] }}_{{ item.1.orport }}\n    mode=700\n  with_nested:\n   - tor_ips\n   - tor_ports\n  when: ansible_system != 'Linux'\n\n- name: Generating torrc file(s)\n  become: yes\n  template: >\n    src=torrc\n    dest=\"{{ (ansible_pkg_mgr != 'apt')| ternary(tor_ConfDir ~ '/' ~ item[0] ~ '_' ~ item.1.orport ~ '.torrc', tor_ConfDir ~ '/' ~ item[0] ~ '_' ~ item.1.orport ~ '/torrc') }}\"\n    owner=root\n    mode=0644\n    backup=yes\n    validate=\"tor --verify-config -f %s\"\n  with_nested:\n   - tor_ips\n   - tor_ports\n  register: instances\n  tags:\n   - reconfigure\n"}, {"commit_sha": "157ca3c14314ff91b6450c98dd50f073d1427cee", "sha": "98b942af28537b801abdb40103ee8c647df7c43c", "filename": "playbooks/openshift/openstack/post-provision.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n# Assign hostnames\n- hosts: cluster_hosts\n  pre_tasks:\n    - import_tasks: ../prep-inventory.yml\n  roles:\n    - role: ../../../galaxy/openshift-ansible-contrib/roles/hostnames\n\n# Build and process DNS Records\n- hosts: localhost\n  pre_tasks:\n    - import_tasks: ../prep-inventory.yml\n    - import_tasks: dns.yml\n  roles:\n    - role: ../../../galaxy/infra-ansible/roles/dns\n\n# provision cinder volume\n- import_playbook: cinder-registry.yml\n  when:\n    - openshift_hosted_registry_storage_kind is defined\n    - openshift_hosted_registry_storage_kind == \"openstack\"\n    - openshift_hosted_registry_storage_openstack_volumeID is not defined\n  \n"}, {"commit_sha": "b11c4477d973b0cc87a296f6b028eaf9abab4686", "sha": "9600c1d1c7b551beb3dc0b826c32ba290854a471", "filename": "tasks/main-CentOS.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# tasks file for ansible-role-docker-ce\n\n- name: Ensure yum-utils is installed\n  package:\n    name: yum-utils\n    state: present\n  become: true\n\n- name: Add Docker CE repository\n  shell: yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n  args:\n    creates: /etc/yum.repos.d/docker-ce.repo\n  become: true\n  register: yum_repo\n\n- name: Update yum cache\n  shell: yum makecache fast\n  become: true\n  when: yum_repo.changed\n\n- include: main-Storage.yml\n  when: docker_setup_devicemapper == true\n\n- include: main-Generic.yml\n"}, {"commit_sha": "8c4af8da901c68ce8c4bdd21c62e08cec5d3c23a", "sha": "79908bc1b9ecbed3c05c60f6b5301e51ecf6dedd", "filename": "tasks/bug-tweaks.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Configuration to avoid 'Device or resource busy'\n  block:\n  - name: Stat /proc/sys/fs/may_detach_mounts (CentOS/RedHat)\n    stat:\n      path: /proc/sys/fs/may_detach_mounts\n    register: may_detach_mounts\n\n  - name: Ensure fs.may_detach_mounts is set to avoid 'Device or resource busy' (CentOS/RedHat)\n    become: true\n    sysctl:\n      name: fs.may_detach_mounts\n      value: 1\n      sysctl_file: /etc/sysctl.d/99-docker.conf\n      reload: yes\n    when: may_detach_mounts.stat.exists\n\n  # Keep for compatibility reasons of this role. Now everything is in the same file.\n  - name: Remove systemd drop-in for Docker Mount Flags slave configuration (CentOS/RedHat)\n    become: true\n    file:\n      path: /etc/systemd/system/docker.service.d/mountflags-slave.conf\n      state: absent\n    notify: restart docker\n\n  - name: Set MountFlags option to \"slave\" to prevent \"device busy\" errors on CentOS/RedHat 7.3 kernels (CentOS/RedHat)\n    set_fact:\n      docker_systemd_service_config_tweaks: \"{{ docker_systemd_service_config_tweaks + \\\n        _systemd_service_config_tweaks }}\"\n    vars:\n      _systemd_service_config_tweaks:\n        - 'MountFlags=slave'\n  when:\n    - _docker_os_dist == \"CentOS\" or _docker_os_dist == \"RedHat\"\n    - docker_enable_mount_flag_fix | bool\n    - ansible_kernel | version_compare('4', '<')\n\n- name: Best effort handling to directlvm for Debian 8 to get uniform behavior across distributions\n  block:\n  - name: Create LVM thinpool for Docker according to Docker documentation\n    include_tasks: lvm-thinpool.yml\n    vars:\n      pool:\n        name: thinpool\n        volume_group: docker\n        physical_volumes: \"{{ docker_daemon_config['storage-opts'] | select('match', '^dm.directlvm_device.+') \\\n          | list | regex_replace('dm.directlvm_device=\\\\s*(.+)', '\\\\1') }}\"\n        metadata_size: \"1%VG\"\n        data_size: \"95%VG\"\n\n  - name: Modify storage-opts to handle problems with thinpool on Debian 8\n    set_fact:\n      _modified_storage_config: \"{{ (docker_daemon_config['storage-opts'] | difference(_exclusions)) + \\\n        ['dm.thinpooldev=/dev/mapper/docker-thinpool-tpool'] }}\"\n    vars:\n      _exclusions: \"{{ docker_daemon_config['storage-opts'] | select('match', '^dm.directlvm_device.+') | list }}\"\n\n  - name: Update Docker daemon configuration to handle consistency between distributions\n    set_fact:\n      docker_daemon_config: \"{{ docker_daemon_config | combine(_updated_item, recursive=true) }}\"\n    vars:\n      _updated_item: \"{ 'storage-opts': {{ _modified_storage_config }} }\"\n\n  - name: Updated Docker daemon configuration\n    debug:\n      var: docker_daemon_config\n  when:\n    - _docker_os_dist == \"Debian\"\n    - _docker_os_dist_major_version == '8'\n    - docker_daemon_config['storage-opts'] is defined\n    - docker_daemon_config['storage-opts'] | select('match', '^dm.directlvm_device.+')\n"}, {"commit_sha": "a8cdb8bd610db76f4f3afa4831d7db624d22e3fc", "sha": "017bef2cc525db952684ab0ab74406ef4b0a6ab6", "filename": "roles/deploy/hooks/finalize-after.yml", "repository": "roots/trellis", "decoded_content": "---\n- name: WordPress Installed?\n  command: wp core is-installed {{ project.multisite.enabled | default(false) | ternary('--network', '') }}\n  args:\n    chdir: \"{{ deploy_helper.new_release_path }}\"\n  register: wp_installed\n  changed_when: false\n  failed_when: wp_installed.stderr != \"\"\n\n- name: Update WP theme paths\n  command: wp eval 'wp_clean_themes_cache(); switch_theme(get_stylesheet());'\n  args:\n    chdir: \"{{ deploy_helper.new_release_path }}\"\n  when: wp_installed | success\n\n- name: Update WP database\n  command: wp core update-db\n  args:\n    chdir: \"{{ deploy_helper.new_release_path }}\"\n  when: wp_installed | success and not project.multisite.enabled | default(false)\n\n- name: Warn about updating network database.\n  debug:\n    msg: \"Updating the network database could take a long time with a large number of sites.\"\n  when: wp_installed | success and project.multisite.enabled | default(false)\n\n- name: Update WP network database\n  command: wp core update-db --network\n  args:\n    chdir: \"{{ deploy_helper.new_release_path }}\"\n  when: wp_installed | success and project.multisite.enabled | default(false)\n\n- name: Reload php-fpm\n  shell: sudo service php7.0-fpm reload\n  args:\n    chdir: \"{{ deploy_helper.new_release_path }}\"\n"}, {"commit_sha": "e9fb46dc84b9c815a69f6de1347c9ece5db01cc8", "sha": "810143b5731381c790826699d724960bb2252094", "filename": "tasks/npm.yml", "repository": "fubarhouse/ansible-role-nodejs", "decoded_content": "---\n# Tasks file for NPM\n\n- name: \"NPM | Make sure NPM can be found\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell: which npm | cat\n  failed_when: false\n  changed_when: false\n  register: which_npm\n\n- name: \"NPM | Configure\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell: \"{{ which_npm.stdout }} config set prefix /usr/local\"\n  when: '\"npm\" in \"{{ which_npm.stdout }}\"'\n  changed_when: false\n  failed_when: false\n  when: '\"npm\" in which_npm.stdout'\n\n- name: \"NPM | Ensure installed and updated\"\n  become: yes\n  become_user: root\n  npm:\n    name: \"{{ item }}\"\n    executable: \"{{ which_npm.stdout }}\"\n    global: yes\n  with_items:\n    - \"{{ node_packages }}\"\n  when: '\"npm\" in which_npm.stdout'\n"}, {"commit_sha": "8c4af8da901c68ce8c4bdd21c62e08cec5d3c23a", "sha": "5ce912fb8b7fe41f67104f36263310c8c0b6e2c2", "filename": "tasks/postinstall.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Reset internal variables for additional packages to be installed\n  set_fact:\n    _docker_additional_packages_os: []\n    _docker_additional_packages_pip: []\n    _docker_python_system: false\n\n- name: Do best effort detection and set fact to indicate system Python environment is used\n  set_fact:\n    _docker_python_system: true\n  when: ansible_python.executable | regex_search('^/bin') or ansible_python.executable | regex_search('^/usr/bin')\n\n- name: Set facts to install Docker SDK for Python\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + \\\n      docker_predefined_packages_pip[_docker_os_dist]['sdk'] }}\"\n  when:\n    - docker_sdk\n\n- name: Set facts to install Docker Compose\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + \\\n      docker_predefined_packages_pip[_docker_os_dist]['compose'] }}\"\n  when:\n    - docker_compose\n\n- name: Set facts to install Docker Stack dependencies ('docker_stack')\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + \\\n      docker_predefined_packages_pip[_docker_os_dist]['stack'] }}\"\n  when:\n    - docker_stack\n\n- name: Set facts with additional package to be installed\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + docker_additional_packages_pip }}\"\n    _docker_additional_packages_os: \"{{ _docker_additional_packages_os + docker_additional_packages_os }}\"\n\n- name: Ensure EPEL release repository is installed\n  become: true\n  package:\n    name: \"epel-release\"\n    state: present\n  when:\n    - _docker_os_dist == \"CentOS\"\n    - _docker_additional_packages_os | length > 0\n\n- name: Install additional packages (OS package manager)\n  become: true\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - \"{{ _docker_additional_packages_os }}\"\n  when: _docker_additional_packages_os | length > 0\n\n- name: Upgrade PiP\n  become: true\n  pip:\n    name: pip\n    state: forcereinstall\n  when: docker_pip_upgrade\n\n- name: Install additional packages (PiP)\n  become: true\n  pip:\n    name: \"{{ item }}\"\n    state: present\n    extra_args: --user\n  with_items:\n    - \"{{ _docker_additional_packages_pip }}\"\n  when: _docker_additional_packages_pip | length > 0\n  environment:\n    PYTHONWARNINGS: ignore\n\n# Not using github_release:  https://github.com/ansible/ansible/issues/45391\n- name: Get latest release of docker-compose\n  uri:\n    url: https://api.github.com/repos/docker/compose/releases/latest\n    body_format: json\n  register: _github_docker_compose\n  when:\n    - docker_compose_no_pip\n\n# Official installation of docker-compose (Linux): https://docs.docker.com/compose/install/#install-compose\n- name: Install docker-compose (Linux)\n  become: true\n  get_url:\n    url: \"https://github.com/docker/compose/releases/download/{{ _github_docker_compose.json.tag_name }}/\\\n      docker-compose-{{ ansible_system }}-{{ ansible_architecture }}\"\n    checksum: \"sha256:https://github.com/docker/compose/releases/download/{{ _github_docker_compose.json.tag_name }}/\\\n      docker-compose-{{ ansible_system }}-{{ ansible_architecture }}.sha256\"\n    dest: /usr/local/bin/docker-compose\n    mode: 0755\n  when:\n    - docker_compose_no_pip\n"}, {"commit_sha": "8c4af8da901c68ce8c4bdd21c62e08cec5d3c23a", "sha": "38fd3d7d18624a4361694f5d1297ff1106282d4e", "filename": "tasks/setup-repository.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Ensure python and deps for Ansible modules\n  become: true\n  raw: dnf install -y python2 python2-dnf libselinux-python\n  changed_when: false\n  when: _docker_os_dist == \"Fedora\"\n\n- name: Update APT cache\n  become: true\n  apt:\n    update_cache: yes\n  changed_when: false\n  when: _docker_os_dist == \"Ubuntu\" or\n        _docker_os_dist == \"Debian\"\n\n- name: Ensure packages are installed for repository setup\n  become: true\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - \"{{ docker_repository_related_packages[_docker_os_dist] }}\"\n  when: _docker_os_dist == \"Ubuntu\" or\n        _docker_os_dist == \"Debian\" or\n        _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"RedHat\"\n\n- name: Add Docker official GPG key\n  become: true\n  apt_key:\n    url: https://download.docker.com/linux/{{ _docker_os_dist|lower }}/gpg\n    state: present\n  when: (_docker_os_dist == \"Ubuntu\" and _docker_os_dist_major_version > '14') or\n        (_docker_os_dist == \"Debian\" and _docker_os_dist_major_version > '7')\n\n- name: Add Docker APT key (alternative for older Ubuntu systems without SNI).\n  become: true\n  shell: \"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\"\n  args:\n    warn: false\n  changed_when: false\n  when: (_docker_os_dist == \"Ubuntu\" and _docker_os_dist_major_version == '14') or (_docker_os_dist == \"Debian\" and\n    _docker_os_dist_major_version == '7')\n\n- name: Determine channels to be enabled and/or disabled\n  set_fact:\n    _docker_disable_channels: \"{{ docker_channels | difference(_docker_merged_channels) }}\"\n    _docker_enable_channels: \"{{ docker_channels | intersect(_docker_merged_channels) }}\"\n  vars:\n    _docker_mandatory_channel: [ 'stable' ]\n    _docker_merged_channels: \"{{ _docker_mandatory_channel }} + [ '{{ docker_channel }}' ]\"\n\n- name: Add Docker CE repository with correct channels (Ubuntu/Debian)\n  become: true\n  apt_repository:\n    repo: \"deb [arch=amd64] https://download.docker.com/linux/{{ _docker_os_dist|lower }} \\\n      {{ _docker_os_dist_release }} {{ _docker_enable_channels | join(' ') }}\"\n    state: present\n    filename: 'docker-ce'\n  when: _docker_os_dist == \"Ubuntu\" or\n        _docker_os_dist == \"Debian\"\n\n# Backport is required but not documented by Docker: https://github.com/moby/moby/issues/16878\n- name: Add backport repository for Debian Wheezy\n  become: true\n  apt_repository:\n    repo: deb [arch=amd64] http://ftp.debian.org/debian wheezy-backports main\n    state: present\n    filename: 'debian-backport'\n  when: _docker_os_dist == \"Debian\" and _docker_os_dist_major_version == '7'\n\n- name: Add Docker CE repository (Fedora/CentOS/RedHat)\n  become: true\n  get_url:\n    url: \"{{ docker_repository_url[_docker_os_dist] }}\"\n    dest: /etc/yum.repos.d/docker-ce.repo\n    mode: 0644\n  register: _docker_repo\n  when: _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"Fedora\" or\n        _docker_os_dist == \"RedHat\"\n\n- name: Disable Docker CE repository channels (Fedora/CentOS/RedHat)\n  become: true\n  shell: \"{{ docker_cmd_enable_disable_edge_repo[_docker_os_dist] }}\"\n  with_items: \"{{ _docker_disable_channels }}\"\n  changed_when: false\n  vars:\n    _item_enabled: false\n  when: _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"Fedora\" or\n        _docker_os_dist == \"RedHat\"\n  tags:\n    - skip_ansible_lint\n\n- name: Enable Docker CE repository channels (Fedora/CentOS/RedHat)\n  become: true\n  shell: \"{{ docker_cmd_enable_disable_edge_repo[_docker_os_dist] }}\"\n  with_items: \"{{ _docker_enable_channels }}\"\n  changed_when: false\n  vars:\n    _item_enabled: true\n  when: _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"Fedora\" or\n        _docker_os_dist == \"RedHat\"\n  tags:\n    - skip_ansible_lint\n\n# disable rt-beta so we don't get a 403 error retrieving repomd.xml\n- name: Check if rhel-7-server-rt-beta-rpms Repository is enabled (RedHat)\n  become: true\n  shell: \"subscription-manager repos --list-enabled | grep rhel-7-server-rt-beta-rpms\"\n  register: cmd_rhel_rt_beta_repo_enabled\n  when: _docker_os_dist == \"RedHat\"\n  changed_when: false\n  failed_when: cmd_rhel_rt_beta_repo_enabled.rc not in [ 0, 1 ]\n\n- name: Disable rhel-7-server-rt-beta-rpms Repository (RedHat)\n  become: true\n  shell: \"subscription-manager repos --disable=rhel-7-server-rt-beta-rpms\"\n  when: _docker_os_dist == \"RedHat\" and cmd_rhel_rt_beta_repo_enabled.rc == 0\n  tags:\n    - skip_ansible_lint\n\n# container-selinux package wants this\n- name: Check if rhel-7-server-extras-rpms Repository is enabled (RedHat)\n  become: true\n  shell: \"subscription-manager repos --list-enabled | grep rhel-7-server-extras-rpms\"\n  register: cmd_rhel_extras_repo_enabled\n  when: _docker_os_dist == \"RedHat\"\n  changed_when: false\n  failed_when: cmd_rhel_extras_repo_enabled.rc not in [ 0, 1 ]\n\n- name: Enable rhel-7-server-extras-rpms Repository (RedHat)\n  become: true\n  shell: \"subscription-manager repos --enable=rhel-7-server-extras-rpms\"\n  when: _docker_os_dist == \"RedHat\" and cmd_rhel_extras_repo_enabled.rc == 1\n  tags:\n    - skip_ansible_lint\n\n- name: Update repository cache\n  become: true\n  shell: \"{{ docker_cmd_update_repo_cache[_docker_os_dist] }}\"\n  args:\n    warn: false\n  changed_when: false\n  tags:\n    - skip_ansible_lint"}, {"commit_sha": "e75cabae2d692a152ca21c185a313528b8bac3c8", "sha": "a107b04279d6228407950dc975e3daa94ead9dfd", "filename": "roles/ansible/tower/manage-job-templates/tasks/set-permissions.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Obtain team id based on the team name\"\n  set_fact:\n    object_id: \"{{ item.id }}\" \n  when:\n  - permissions_object == \"teams\"\n  - item.name|trim == permissions_value.name|trim\n  with_items:\n  - \"{{ existing_teams_output.rest_output }}\"\n    \n- name: \"Obtain user id based on the username\"\n  set_fact:\n    object_id: \"{{ item.id }}\" \n  when:\n  - permissions_object == \"users\"\n  - item.username|trim == permissions_value.name|trim\n  with_items:\n  - \"{{ existing_users_output.rest_output }}\"\n\n- name: \"Obtain role id based on the job_template name + role name\" \n  set_fact:\n    role_id: \"{{ item.id }}\" \n  when:\n  - item.summary_fields is defined\n  - item.summary_fields.resource_name is defined\n  - item.summary_fields.resource_name|trim == job_template.name|trim\n  - item.name|trim == permissions_value.role|trim\n  with_items:\n  - \"{{ existing_roles_output.rest_output }}\"\n\n- name: \"Set Permission\"\n  uri:\n    url: \"https://localhost/api/v2/{{ permissions_object }}/{{ object_id }}/roles/\"\n    method: POST\n    body: \"{{ { 'id': role_id|int } }}\"\n    body_format: 'json'\n    headers:\n      Content-Type: \"application/json\"\n      Accept: \"application/json\"\n    user: admin\n    password: \"{{ tower_admin_password }}\"\n    validate_certs: no\n    status_code: 200,204\n  when:\n  - object_id is defined\n  - object_id|trim != ''\n  - role_id is defined\n  - role_id|trim != ''\n\n"}, {"commit_sha": "691072e55614ca50cb35b7bd0a2772dddea7fc24", "sha": "a042017296dec7408a87eea601b185e985557656", "filename": "tasks/install.yml", "repository": "geerlingguy/ansible-role-solr", "decoded_content": "---\n- name: Ensure lsof is present (RedHat).\n  yum: name=lsof state=present\n  when: ansible_os_family == \"RedHat\"\n\n- name: Run Solr installation script.\n  shell: >\n    {{ solr_workspace }}/{{ solr_filename }}/bin/install_solr_service.sh\n    {{ solr_workspace }}/{{ solr_filename }}.tgz\n    -i {{ solr_install_dir }}\n    -d {{ solr_home }}\n    -u {{ solr_user }}\n    -s {{ solr_service_name }}\n    -p {{ solr_port }}\n    creates={{ solr_install_dir }}/solr/bin/solr\n"}, {"commit_sha": "8c4af8da901c68ce8c4bdd21c62e08cec5d3c23a", "sha": "b2f928e721d66caebfcddf15dee782843cbee8c5", "filename": "meta/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "galaxy_info:\n  author: Bjorn Oscarsson\n  company: none\n  description: \"Installs and configures Docker Community Edition (CE)\"\n  min_ansible_version: 2.4\n  license: MIT\n  platforms:\n  - name: Fedora\n    versions:\n      - 24\n      - 25\n      - 26\n      - 27\n      - 28\n\n  - name: EL\n    versions:\n      - 7\n\n  - name: Debian\n    versions:\n      - wheezy\n      - jessie\n      - stretch\n\n  - name: Ubuntu\n    versions:\n      - trusty\n      - xenial\n      - bionic\n\n  galaxy_tags:\n    - docker\n    - containers\n    - system\n\ndependencies: []\n"}, {"commit_sha": "3331ceddd3788b3f43d1f6c7ab4ff27d68ceb2c2", "sha": "3cf97c6bfb58f52a53b407f2ce325122277d8395", "filename": "roles/ovirt-engine-setup/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# health page\n- name: check if ovirt-engine running (health page)\n  uri:\n    url: \"http://{{ ansible_fqdn }}/ovirt-engine/services/health\"\n    status_code: 200\n  register: ovirt_engine_status\n  retries: 2\n  delay: 5\n  until: ovirt_engine_status|success\n  ignore_errors: True\n\n# copy default answer file\n- name: copy default answerfile\n  template:\n    src: answerfile_{{ ovirt_engine_version }}_basic.txt.j2\n    dest: /tmp/answerfile.txt\n    mode: 0644\n    owner: root\n    group: root\n  when: ovirt_engine_answer_file_path is undefined\n\n# copy custom answer file\n- name: copy custom answer file\n  template:\n    src: \"{{ ovirt_engine_answer_file_path }}\"\n    dest: /tmp/answerfile.txt\n    mode: 0644\n    owner: root\n    group: root\n  when: ovirt_engine_answer_file_path is defined\n\n- name: run engine-setup with answerfile\n  shell: 'engine-setup --config-append=/tmp/answerfile.txt'\n  when: ovirt_engine_status|failed\n  tags:\n    - skip_ansible_lint\n\n- name: check state of database\n  service:\n    name: postgresql\n    state: started\n  when: (ovirt_engine_dwh_db_host == 'localhost' and ovirt_engine_dwh == True) or ovirt_engine_db_host == 'localhost'\n\n- name: check state of engine\n  service:\n    name: ovirt-engine\n    state: started\n\n- name: restart of ovirt-engine service\n  service:\n    name: ovirt-engine\n    state: restarted\n\n- name: check health status of page\n  uri:\n    url: \"http://{{ ansible_fqdn }}/ovirt-engine/services/health\"\n    status_code: 200\n  register: health_page\n  retries: 12\n  delay: 10\n  until: health_page|success\n\n- name: clean tmp files\n  file:\n    path: '/tmp/answerfile.txt'\n    state: 'absent'\n"}, {"commit_sha": "679a956a08bc9dd352f5e397a6b29339667aa7db", "sha": "f9f2b10203ba12b1306b604134526ba44773f9e3", "filename": "tasks/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Set distribution facts\n  set_fact:\n    _docker_os_dist: \"{{ ansible_distribution }}\"\n    _docker_os_dist_release: \"{{ ansible_distribution_release }}\"\n    _docker_os_dist_major_version: \"{{ ansible_distribution_major_version }}\"\n    _docker_os_dist_check: yes\n  tags: [\"always\"]\n\n- name: Reinterpret distribution facts for Linux Mint 18\n  set_fact:\n    _docker_os_dist: \"Ubuntu\"\n    _docker_os_dist_release: \"xenial\"\n    _docker_os_dist_major_version: \"16\"\n  when:\n    _docker_os_dist == \"Linux Mint\" and\n    _docker_os_dist_major_version == \"18\"\n  tags: [\"always\"]\n\n- name: Reset role variables\n  set_fact:\n    docker_systemd_service_config_tweaks: []\n    docker_service_envs: {}\n  tags: [\"always\"]\n\n- name: Temporary handling of deprecated variable docker_enable_ce_edge (#54)\n  set_fact:\n    docker_channel: edge\n  when:\n    - docker_enable_ce_edge is defined\n    - docker_enable_ce_edge\n  tags: [\"always\"]\n\n- name: Temporary handling of deprecated variable docker_pkg_name\n  set_fact:\n    docker_version: \"{{ docker_pkg_name | regex_replace('^docker-ce.(.+)$', '\\\\1') }}\"\n  when: docker_pkg_name is match('docker-ce' + docker_os_pkg_version_separator[_docker_os_dist])\n  tags: [\"always\"]\n\n- name: Compatibility and distribution checks\n  include_tasks: checks.yml\n  when: docker_do_checks | bool\n  tags: [\"always\"]\n\n- name: Install and configure Docker CE\n  block:\n    - name: Network access disabled\n      debug:\n        msg: \"Tasks requiring network access will be skipped!\"\n      when: not docker_network_access\n      \n    - name: Remove Docker versions before Docker CE\n      include_tasks: remove-pre-docker-ce.yml\n      when: \n        - docker_remove_pre_ce | bool\n        - docker_network_access\n      tags: [\"install\"]\n\n    - name: Setup Docker package repositories\n      include_tasks: setup-repository.yml\n      tags: [\"install\"]\n\n    - name: Install Docker\n      include_tasks: install-docker.yml\n      when: docker_network_access\n      tags: [\"install\"]\n\n    - name: Configure audit logging\n      include_tasks: setup-audit.yml\n      tags: [\"configure\"]\n\n    - name: Apply workarounds for bugs and/or tweaks\n      include_tasks: bug-tweaks.yml\n      tags: [\"configure\"]\n\n    - name: Configure Docker\n      include_tasks: configure-docker.yml\n      tags: [\"configure\"]\n\n    - name: Postinstall tasks\n      include_tasks: postinstall.yml\n      when: docker_network_access\n      tags: [\"install\", \"postinstall\"]\n  when: not docker_remove | bool\n\n- name: Remove Docker CE and related configuration\n  include_tasks: remove-docker.yml\n  when: docker_remove | bool\n"}, {"commit_sha": "ec4de12ae75f7191a5f71aa775e0344679ea1477", "sha": "a78702622a74ad0fde534669f16be4af249741ff", "filename": "tasks/auth_initialization.yml", "repository": "UnderGreen/ansible-role-mongodb", "decoded_content": "---\n\n- include: auth_initialization_ald.yml\n  when: ansible_local.mongodb.mongodb.mongodb_login_port is defined\n\n- name: create administrative user siteUserAdmin\n  mongodb_user:\n    database: admin\n    name: \"{{ item.name }}\"\n    password: \"{{ item.password }}\"\n    roles: \"{{ item.roles }}\"\n    login_host: 127.0.0.1\n  with_items:\n    - {\n      name: \"{{ mongodb_user_admin_name }}\",\n      password: \"{{ mongodb_user_admin_password }}\",\n      roles: \"userAdminAnyDatabase\"\n      }\n  register: useradmin_user_result\n  when: ansible_local.mongodb.mongodb.mongodb_login_port is undefined\n\n- name: create administrative user siteRootAdmin\n  mongodb_user:\n    database: admin\n    name: \"{{ item.name }}\"\n    password: \"{{ item.password }}\"\n    roles: \"{{ item.roles }}\"\n    login_host: 127.0.0.1\n  with_items:\n    - {\n      name: \"{{ mongodb_root_admin_name }}\",\n      password: \"{{ mongodb_root_admin_password }}\",\n      roles: \"root\"\n      }\n  register: rootadmin_user_result\n  when: ansible_local.mongodb.mongodb.mongodb_login_port is undefined\n\n- name: create normal users\n  mongodb_user:\n    database: \"{{ item.database }}\"\n    name: \"{{ item.name }}\"\n    password: \"{{ item.password }}\"\n    roles: \"{{ item.roles }}\"\n    replica_set: \"{{ mongodb_conf_replSet }}\"\n    login_host: 127.0.0.1\n    login_user: \"{{ mongodb_user_admin_name }}\"\n    login_password: \"{{ mongodb_user_admin_password }}\"\n  with_items:\n    - \"{{ mongodb_users }}\"\n  when: mongodb_users is defined and ansible_local.mongodb.mongodb.mongodb_login_port is undefined\n\n- name: Create facts.d directory\n  file:\n    state: directory\n    recurse: yes\n    path: /etc/ansible/facts.d\n  when: rootadmin_user_result|changed or useradmin_user_result|changed\n\n- name: Create facts file for mongodb\n  copy:\n    dest: /etc/ansible/facts.d/mongodb.fact\n    content: \"[mongodb]\\nmongodb_login_port={{ mongodb_conf_port }}\\n\\n\"\n  when: rootadmin_user_result|changed or useradmin_user_result|changed\n"}, {"commit_sha": "16f21c5c7c51314eea1d023a46bcaad8c889cd46", "sha": "86e4d001ad3be37722b87d1eb25e81a6f1ae1885", "filename": "tasks/opensource/setup-alpine.yml", "repository": "nginxinc/ansible-role-nginx", "decoded_content": "---\n- name: \"(Install: Alpine) Set Default APK NGINX Repository\"\n  set_fact:\n    default_repository: >-\n      https://nginx.org/packages/{{ (nginx_branch == 'mainline')\n      | ternary('mainline/', '') }}alpine/v{{ ansible_distribution_version.split('.')[0] }}.{{ ansible_distribution_version.split('.')[1] }}/main\n\n- name: \"(Install: Alpine) Set APK NGINX Repository\"\n  set_fact:\n    repository: \"{{ nginx_repository | default(default_repository) }}\"\n\n- name: \"(Install: Alpine) Add NGINX Repository\"\n  lineinfile:\n    path: /etc/apk/repositories\n    insertafter: EOF\n    line: \"{{ repository }}\"\n"}, {"commit_sha": "157ca3c14314ff91b6450c98dd50f073d1427cee", "sha": "67caf028977c383f52f4928f19060f81395c7eb9", "filename": "playbooks/openshift/install.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- import_playbook: \"{{ openshift_ansible_path | default('../../galaxy/openshift-ansible') }}/playbooks/byo/config.yml\"\n"}, {"commit_sha": "16f21c5c7c51314eea1d023a46bcaad8c889cd46", "sha": "a7f9bd3443c924e20ee75efac6f11b1b132fd1d9", "filename": "tasks/opensource/setup-suse.yml", "repository": "nginxinc/ansible-role-nginx", "decoded_content": "---\n- name: \"(Install: SUSE) Set Default SUSE NGINX Repository\"\n  set_fact:\n    default_repository: >-\n      https://nginx.org/packages/{{ (nginx_branch == 'mainline')\n      | ternary('mainline/', '') }}sles/{{ ansible_distribution_major_version }}\n\n- name: \"(Install: SUSE) Set SUSE NGINX Repository\"\n  set_fact:\n    repository: \"{{ nginx_repository | default(default_repository) }}\"\n\n- name: \"(Install: SUSE) Add NGINX Repository\"\n  zypper_repository:\n    name: \"nginx-{{ nginx_branch }}\"\n    repo: \"{{ repository }}\"\n"}, {"commit_sha": "157ca3c14314ff91b6450c98dd50f073d1427cee", "sha": "ddb591eb6fdd2b2fcd82c7d597a7658338fb100c", "filename": "playbooks/openshift/post-install.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- hosts: cluster_hosts\n  roles:\n    - role: sync-keys\n      key_url: \"{{ openshift_authorized_key_url | default('') }}\"\n\n- hosts: masters\n  roles:\n  - { role: create_users }\n\n- import_playbook: ../../galaxy/openshift-applier/playbooks/openshift-cluster-seed.yml\n  when:\n  - openshift_cluster_content is defined\n"}, {"commit_sha": "ec4de12ae75f7191a5f71aa775e0344679ea1477", "sha": "5fd7ead47a88000ab4e1f1e771b62abca1cbb4ae", "filename": "tasks/auth_initialization_ald.yml", "repository": "UnderGreen/ansible-role-mongodb", "decoded_content": "- name: create administrative user siteUserAdmin port=yes\n  mongodb_user:\n    database: admin\n    name: \"{{ item.name }}\"\n    password: \"{{ item.password }}\"\n    roles: \"{{ item.roles }}\"\n    login_host: 127.0.0.1\n    login_port: \"{{ ansible_local.mongodb.mongodb.mongodb_login_port }}\"\n  with_items:\n    - {\n      name: \"{{ mongodb_user_admin_name }}\",\n      password: \"{{ mongodb_user_admin_password }}\",\n      roles: \"userAdminAnyDatabase\"\n      }\n\n- name: create administrative user siteRootAdmin port=yes\n  mongodb_user:\n    database: admin\n    name: \"{{ item.name }}\"\n    password: \"{{ item.password }}\"\n    roles: \"{{ item.roles }}\"\n    login_host: 127.0.0.1\n    login_port: \"{{ ansible_local.mongodb.mongodb.mongodb_login_port }}\"\n    login_user: \"{{ mongodb_user_admin_name }}\"\n    login_password: \"{{ mongodb_user_admin_password }}\"\n  with_items:\n    - {\n      name: \"{{ mongodb_root_admin_name }}\",\n      password: \"{{ mongodb_root_admin_password }}\",\n      roles: \"root\"\n      }\n\n- name: create normal users\n  mongodb_user:\n    database: \"{{ item.database }}\"\n    name: \"{{ item.name }}\"\n    password: \"{{ item.password }}\"\n    roles: \"{{ item.roles }}\"\n    replica_set: \"{{ mongodb_conf_replSet }}\"\n    login_host: 127.0.0.1\n    login_port: \"{{ ansible_local.mongodb.mongodb.mongodb_login_port }}\"\n    login_user: \"{{ mongodb_user_admin_name }}\"\n    login_password: \"{{ mongodb_user_admin_password }}\"\n  with_items:\n    - \"{{ mongodb_users }}\"\n  when: mongodb_users is defined\n"}, {"commit_sha": "7736334bb143cf6ee56adc9db597785c09acf67d", "sha": "71a1e5ee02744c68e8e94eb54482a4f7065ebe1d", "filename": "roles/wp-cli/defaults/main.yml", "repository": "roots/trellis", "decoded_content": "wp_cli_version: 0.24.1\nwp_cli_bin_path: /usr/bin/wp\nwp_cli_phar_url: \"https://github.com/wp-cli/wp-cli/releases/download/v{{ wp_cli_version }}/wp-cli-{{ wp_cli_version }}.phar\"\nwp_cli_completion_url: \"https://raw.githubusercontent.com/wp-cli/wp-cli/v{{ wp_cli_version }}/utils/wp-completion.bash\"\nwp_cli_completion_path: /etc/bash_completion.d/wp-completion.bash\n"}, {"commit_sha": "b79a990b9cc80f1d419cebc46be91029fde17f76", "sha": "1d60273a0a9568f9c364273c0f2f0f9c96d53338", "filename": "tasks/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Set distribution facts\n  set_fact:\n    _docker_os_dist: \"{{ ansible_distribution }}\"\n    _docker_os_dist_release: \"{{ ansible_distribution_release }}\"\n    _docker_os_dist_major_version: \"{{ ansible_distribution_major_version }}\"\n    _docker_os_dist_check: yes\n  tags: [\"install\", \"configure\"]\n\n- name: Reinterpret distribution facts for Linux Mint 18\n  set_fact:\n    _docker_os_dist: \"Ubuntu\"\n    _docker_os_dist_release: \"xenial\"\n    _docker_os_dist_major_version: \"16\"\n  when:\n    _docker_os_dist == \"Linux Mint\" and\n    _docker_os_dist_major_version == \"18\"\n  tags: [\"install\", \"configure\"]\n\n- include_tasks: checks.yml\n  tags: [\"install\", \"configure\"]\n\n- include_tasks: setup-repository.yml\n  tags: [\"install\"]\n\n- include_tasks: remove-pre-docker-ce.yml\n  when: docker_remove_pre_ce | bool\n  tags: [\"install\"]\n\n- include_tasks: install-docker.yml\n  tags: [\"install\"]\n\n- include_tasks: setup-audit.yml\n  tags: [\"configure\"]\n\n- include_tasks: kernel-3-mount-fixes.yml\n  when: ansible_kernel | version_compare('4', '<')\n  tags: [\"configure\"]\n\n- include_tasks: configure-docker.yml\n  tags: [\"configure\"]\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "5e84fc63ccb802e45e96d213b7f023005f791e45", "filename": "meta/main.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\ngalaxy_info:\n  role_name: \"java\"\n  author: \"Lean Delivery team <team@lean-delivery.com>\"\n  description: \"Lean Delivery Java install\"\n  company: \"Epam Systems\"\n  license: \"Apache\"\n  min_ansible_version: \"2.7\"\n  issue_tracker_url: \"https://github.com/lean-delivery/ansible-role-java/issues\"\n  platforms:\n    - name: \"Ubuntu\"\n      versions:\n        - \"xenial\"\n        - \"bionic\"\n    - name: \"Debian\"\n      versions:\n        - \"stretch\"\n    - name: \"EL\"\n      versions:\n        - \"6\"\n        - \"7\"\n    - name: \"Amazon\"\n      versions:\n        - \"2017.12\"\n        - \"Candidate\"\n    - name: \"Windows\"\n      versions:\n        - \"2016\"\n        - \"2019\"\n\n  galaxy_tags:\n    - \"development\"\n    - \"system\"\n    - \"packaging\"\n    - \"java\"\n    - \"oracle\"\n    - \"jdk\"\n    - \"openjdk\"\n    - \"sapjvm\"\n    - \"windows\"\n\ndependencies: []\n"}, {"commit_sha": "c5ee48e16a5905db37ff878be57f5de16f769368", "sha": "05d3d68aba16906138e8460efb0095ec47b77fb6", "filename": "tasks/main.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: Check if OS is Debian-based (we do not support others)\n    debug: \"Check family\"\n    failed_when: ansible_os_family != \"Debian\"\n\n  - include: section_01.yml\n    tags: section01\n\n  - include: section_02.yml\n    tags: section02\n\n  - include: section_03.yml\n    tags: section03\n\n  - include: section_04.yml\n    tags: section04\n\n  - include: section_05.yml\n    tags: section05\n\n  - include: section_06.yml\n    tags: section06\n\n  - include: section_07.yml\n    tags: section07\n\n  - include: section_08.yml\n    tags: section08\n\n  - include: section_09.yml\n    tags: section09\n\n  - include: section_10.yml\n    tags: section10\n\n  - include: section_11.yml\n    tags: section11\n\n  - include: section_12.yml\n    tags: section12\n\n  - include: section_13.yml\n    tags: section13\n\n"}, {"commit_sha": "95a0ffb87f5a5ecbf178ee4a5b4f890acaba6cbe", "sha": "8d16346f0b8e9948b65bf34e38a532d416e5ceb1", "filename": "tasks/rpm_install.yml", "repository": "nusenu/ansible-relayor", "decoded_content": "---\n\n- name: Setup RPM specific variables (set_fact)\n  set_fact:\n    tor_user: toranon\n    tor_ConfDir: /etc/tor\n    tor_RunAsDaemon: 0\n    tor_DataDir: /var/lib/tor-instances\n  tags:\n   - reconfigure\n   - renewkey\n   - createdir\n\n- name: Ensure tor package is installed (dnf)\n  become: yes\n  dnf: name=tor,libselinux-python,libsemanage-python state=present\n  when: ansible_pkg_mgr == 'dnf'\n  notify: re-gather facts\n\n# re-gathering facts after installing libselinux-python on F23\n# is a workaround for https://github.com/ansible/ansible-modules-core/issues/2432\n- meta: flush_handlers\n\n- name: Ensure EPEL repo is installed (yum)\n  become: yes\n  yum: name=epel-release\n  when: ansible_pkg_mgr == 'yum'\n\n- name: Ensure tor package is installed (yum)\n  become: yes\n  yum: name=tor,libsemanage-python state=present\n  when: ansible_pkg_mgr == 'yum'\n\n- name: Ensure SELinux boolean (tor_can_network_relay) is set appropriately (Fedora)\n  become: yes\n  seboolean: name=tor_can_network_relay state=yes persistent=yes\n  when: ansible_selinux.status == 'enabled'\n\n- name: Ensure systemd drop-in folder is present\n  become: yes\n  file: path=/etc/systemd/system/tor@.service.d\n    state=directory\n    owner=root\n    mode=0755\n\n# this is needed for a small service file modification (allow it to write to /var/lib/tor-instances)\n# without replacing the maintainer's file, for details see\n# http://www.freedesktop.org/software/systemd/man/systemd.unit.html#id-1.11.3\n- name: Ensure service file drop-in is present\n  become: yes\n  copy: src=local.conf\n   dest=/etc/systemd/system/tor@.service.d/local.conf\n   owner=root\n   mode=640\n  notify: systemctl daemon-reload\n\n- meta: flush_handlers\n"}, {"commit_sha": "b79a990b9cc80f1d419cebc46be91029fde17f76", "sha": "57c615f17e2de879aa2b8570210d10f4c8b33e3c", "filename": "meta/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "galaxy_info:\n  author: Bjorn Oscarsson\n  description: \"Installs and configures Docker Community Edition (CE)\"\n  min_ansible_version: 2.4\n  license: MIT\n  platforms:\n  - name: Fedora\n    versions:\n      - 24\n      - 25\n      - 26\n\n  - name: EL\n    versions:\n      - 7\n\n  - name: Debian\n    versions:\n      - jessie\n      - stretch\n\n  - name: Ubuntu\n    versions:\n      - trusty\n      - xenial\n\n  galaxy_tags:\n    - docker\n    - ce\n    - system\n\ndependencies: []\n"}, {"commit_sha": "41231a43e2531086c3fb9f7499cbc44aacbe7792", "sha": "3d2f823ce63e949a0aea3d3873f0f8c3eeb5f4b5", "filename": "tasks/main-CentOS.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# tasks file for ansible-role-docker-ce\n\n- name: Ensure yum-utils is installed\n  package:\n    name: yum-utils\n    state: present\n  become: true\n\n- name: Add Docker CE repository\n  get_url:\n    url: https://download.docker.com/linux/centos/docker-ce.repo\n    dest: /etc/yum.repos.d/docker-ce.repo\n    mode: 0644\n  become: true\n  register: yum_repo\n\n- name: Determine Docker CE Edge repo status\n  shell: yum-config-manager docker-ce-edge | grep enabled\n  ignore_errors: yes\n  changed_when: false\n  register: cmd_docker_ce_edge_enabled\n\n- name: Set current Docker CE Edge repo status fact\n  set_fact:\n    fact_docker_ce_edge_enabled: \"{{ cmd_docker_ce_edge_enabled.stdout == 'enabled = True' }}\"\n\n- name: Enable/Disable Docker CE Edge Repository\n  shell: yum-config-manager --{{ (docker_enable_ce_edge == true) | ternary('enable','disable') }} docker-ce-edge\n  become: true\n  when: fact_docker_ce_edge_enabled != docker_enable_ce_edge\n\n- name: Update yum cache\n  shell: yum makecache fast\n  args:\n    warn: false  \n  become: true\n  when: yum_repo.changed\n\n- name: Stat /proc/sys/fs/may_detach_mounts\n  stat:\n    path: /proc/sys/fs/may_detach_mounts\n  register: may_detach_mounts\n\n- name: Ensure fs.may_detach_mounts is set to avoid 'Device or resource busy'\n  sysctl:\n    name: fs.may_detach_mounts\n    value: 1\n    sysctl_file: /etc/sysctl.d/99-docker.conf\n    reload: yes\n  become: true\n  when: ansible_kernel|version_compare('4', '<') and may_detach_mounts.stat.exists\n\n- include: main-Mountflags.yml\n  when: ansible_kernel|version_compare('4', '<')\n\n- include: main-Generic.yml\n"}, {"commit_sha": "da19a34b6fe5f0aa93dd4a66c624695ccd323ca8", "sha": "3f4508fee4d2e371978fda5f6a356ee87467deee", "filename": "tasks/nexus_install.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- name: Download nexus_package\n  get_url:\n    url: \"http://download.sonatype.com/nexus/3/{{ nexus_package }}\"\n    dest: \"{{ nexus_download_dir }}/{{ nexus_package }}\"\n    force: no\n  notify:\n    - nexus-service-stop\n\n- name: Ensure Nexus o/s group exists\n  group:\n    name: \"{{ nexus_os_group }}\"\n    state: present\n\n- name: Ensure Nexus o/s user exists\n  user:\n    name: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n    shell: \"/bin/bash\"\n    state: present\n\n- name: Ensure Nexus installation directory exists\n  file:\n    path: \"{{ nexus_installation_dir }}\"\n    state: \"directory\"\n\n- name: Unpack Nexus download\n  unarchive:\n    src: \"{{ nexus_download_dir }}/{{ nexus_package }}\"\n    dest: \"{{ nexus_installation_dir }}\"\n    creates: \"{{ nexus_installation_dir }}/nexus-{{ nexus_version }}\"\n    force: no\n    copy: false\n  notify:\n    - nexus-service-stop\n\n- meta: flush_handlers\n\n- name: Update symlink nexus-latest\n  file:\n    path: \"{{ nexus_installation_dir }}/nexus-latest\"\n    src: \"{{ nexus_installation_dir }}/nexus-{{ nexus_version }}\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n    state: link\n  register: nexus_latest_version\n\n- name: Delete unpacked data directory\n  file:\n    path: \"{{ nexus_installation_dir }}/nexus-latest/data\"\n    state: absent\n\n- name: Get path to default settings\n  set_fact:\n    nexus_default_settings_file: \"{{ nexus_installation_dir }}/nexus-latest/etc/org.sonatype.nexus.cfg\"\n  when: nexus_version is version_compare('3.1.0', '<')\n\n- name: Get path to default settings\n  set_fact:\n    nexus_default_settings_file: \"{{ nexus_installation_dir }}/nexus-latest/etc/nexus-default.properties\"\n  when: nexus_version is version_compare('3.1.0', '>=')\n\n- name: Get application settings directories\n  set_fact:\n    nexus_app_dir_settings_dirs:\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc\"\n  when: nexus_version is version_compare('3.1.0', '<')\n\n- name: Get application settings directories\n  set_fact:\n    nexus_app_dir_settings_dirs:\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/karaf\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/jetty\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/fabric\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/logback\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/scripts\"\n  when: nexus_version is version_compare('3.1.0', '>=')\n\n- name: Get rest API endpoint (v < 3.8.0)\n  set_fact:\n    nexus_rest_api_endpoint: \"service/siesta/rest/v1/script\"\n  when: nexus_version is version_compare('3.8.0', '<')\n\n- name: Get rest API endpoint (v >= 3.8.0)\n  set_fact:\n    nexus_rest_api_endpoint: \"service/rest/v1/script\"\n  when: nexus_version is version_compare('3.8.0', '>=')\n\n- name: Allow nexus to create first-time install configuration files in  {{ nexus_installation_dir }}/nexus-latest/etc\n  file:\n    path: \"{{ item }}\"\n    state: \"directory\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n    mode: \"0755\"\n    recurse: false\n  with_items: \"{{ nexus_app_dir_settings_dirs }}\"\n  when: nexus_latest_version.changed\n  register: chown_config_first_time\n  tags:\n    # hard to run as a handler for time being\n    - skip_ansible_lint\n\n- name: Create Nexus data directory\n  file:\n    path: \"{{ nexus_data_dir }}\"\n    state: \"directory\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n\n- name: Setup Nexus data directory\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Dkaraf.data=.*\"\n    line: \"-Dkaraf.data={{ nexus_data_dir }}\"\n\n- name: Setup JVM logfile directory\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-XX:LogFile=.*\"\n    line: \"-XX:LogFile={{ nexus_data_dir }}/log/jvm.log\"\n\n- name: Setup Nexus default timezone\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Duser.timezone=.*\"\n    line: \"-Duser.timezone={{ nexus_timezone }}\"\n\n- name: Create Nexus tmp/backup directory\n  file:\n    path: \"{{ item }}\"\n    state: \"directory\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n  with_items:\n    - \"{{ nexus_tmp_dir }}\"\n    - \"{{ nexus_backup_dir }}\"\n\n- name: Setup Nexus tmp directory\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Djava.io.tmpdir=.*\"\n    line: \"-Djava.io.tmpdir={{ nexus_tmp_dir }}\"\n\n- name: Set NEXUS_HOME for the service user\n  lineinfile:\n    dest: \"/home/{{ nexus_os_user }}/.bashrc\"\n    regexp: \"^export NEXUS_HOME=.*\"\n    line: \"export NEXUS_HOME={{ nexus_installation_dir }}/nexus-latest\"\n\n- name: Set nexus user\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.rc\"\n    regexp: \".*run_as_user=.*\"\n    line: \"run_as_user=\\\"{{ nexus_os_user }}\\\"\"\n\n- name: Set nexus port\n  lineinfile:\n    dest: \"{{ nexus_default_settings_file }}\"\n    regexp: \"^application-port=.*\"\n    line: \"application-port={{ nexus_default_port }}\"\n\n- name: Set nexus context path\n  lineinfile:\n    dest: \"{{ nexus_default_settings_file }}\"\n    regexp: \"^nexus-context-path=.*\"\n    line: \"nexus-context-path={{ nexus_default_context_path }}\"\n\n- name: Bind nexus service to 127.0.0.1 only\n  lineinfile:\n    dest: \"{{ nexus_default_settings_file }}\"\n    regexp: \"^application-host=.*\"\n    line: \"application-host=127.0.0.1\"\n  when: httpd_setup_enable\n\n- name: Create systemd service configuration\n  template:\n    src: \"nexus.service\"\n    dest: \"/etc/systemd/system\"\n  notify:\n    - systemd-reload\n\n- block:\n    - name: \"Deploy backup restore script\"\n      template:\n        src: \"nexus-blob-restore.sh.j2\"\n        dest: \"{{ nexus_script_dir }}/nexus-blob-restore.sh\"\n        mode: 0755\n    - name: \"Symlink backup restore script to /sbin\"\n      file:\n        src: \"{{ nexus_script_dir }}/nexus-blob-restore.sh\"\n        dest: \"/sbin/nexus-blob-restore.sh\"\n        state: link\n  when: nexus_backup_configure | bool\n\n- name: 'Check if data directory is empty (first-time install)'\n  command: \"ls {{ nexus_data_dir }}\"\n  register: nexus_data_dir_contents\n  check_mode: no\n  changed_when: false\n\n- name: Clean cache for upgrade process\n  file:\n    path: \"{{ nexus_data_dir }}/clean_cache\"\n    state: touch\n  when: nexus_latest_version.changed and nexus_data_dir_contents.stdout != \"\"\n  tags:\n    # hard to run as a handler for time being\n    - skip_ansible_lint\n\n- meta: flush_handlers\n\n- name: Enable nexus service and make sure it is started\n  systemd:\n    name: nexus.service\n    enabled: yes\n    state: started\n  notify:\n    - wait-for-nexus\n    - wait-for-nexus-port\n\n- meta: flush_handlers\n\n- name: Chown configuration files from {{ nexus_installation_dir }}/nexus-latest/etc back to root\n  file:\n    path: \"{{ nexus_installation_dir }}/nexus-latest/etc\"\n    owner: \"root\"\n    group: \"root\"\n    mode: a=rX,u+w\n    recurse: true\n  when: chown_config_first_time.changed\n  tags:\n    # hard to run as a handler for time being\n    - skip_ansible_lint\n\n- name: Prevent nexus to create any new configuration files in  {{ nexus_installation_dir }}/nexus-latest/etc\n  file:\n    path: \"{{ item }}\"\n    state: \"directory\"\n    owner: \"root\"\n    group: \"root\"\n    mode: \"0755\"\n    recurse: false\n  with_items: \"{{ nexus_app_dir_settings_dirs }}\"\n\n- name: First-time install admin password\n  set_fact:\n    current_nexus_admin_password: 'admin123'\n  when: nexus_data_dir_contents.stdout == \"\"\n\n- name: Subsequent re-provision admin password\n  set_fact:\n    current_nexus_admin_password: \"{{ nexus_admin_password }}\"\n  when: nexus_data_dir_contents.stdout != \"\"\n  no_log: true\n\n- name: Create directory to hold current groovy scripts for reference\n  file:\n    path: \"{{ nexus_data_dir }}/groovy-raw-scripts/current\"\n    state: directory\n    owner: root\n    group: root\n\n- name: Upload new scripts\n  synchronize:\n    archive: no\n    checksum: yes\n    recursive: yes\n    delete: yes\n    mode: push\n    use_ssh_args: yes\n    src: \"files/groovy/\"\n    dest: \"{{ nexus_data_dir }}/groovy-raw-scripts/new/\"\n\n- name: Sync new scripts to old and get differences\n  shell: 'rsync -ric {{ nexus_data_dir }}/groovy-raw-scripts/new/ {{ nexus_data_dir }}/groovy-raw-scripts/current/ | cut -d\" \" -f 2 | sed \"s/\\.groovy//g\"'\n  register: nexus_groovy_files_changed\n  check_mode: no\n  changed_when: false\n  # simple check on changed files kept on host\n  # skip ansible lint (we don't want to use synchronize module for this)\n  args:\n    warn: false\n\n- name: Declare new or changed groovy scripts in nexus\n  include: declare_script_each.yml\n  with_items: \"{{ nexus_groovy_files_changed.stdout_lines}}\"\n"}, {"commit_sha": "78b2001a2a68bb6526f50ae1e73f13e0dea1cbbd", "sha": "1b8fbe88430435dd9ce0ba47202fc2118e108584", "filename": "tasks/main-CentOS.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# tasks file for ansible-role-docker-ce\n\n- name: Ensure yum-utils is installed\n  package:\n    name: yum-utils\n    state: present\n  become: true\n\n- name: Add Docker CE repository\n  get_url:\n    url: https://download.docker.com/linux/centos/docker-ce.repo\n    dest: /etc/yum.repos.d/docker-ce.repo\n    mode: 0644\n  become: true\n  register: yum_repo\n\n- name: Determine Docker CE Edge repo status\n  shell: yum-config-manager docker-ce-edge | grep enabled\n  ignore_errors: yes\n  changed_when: false\n  register: cmd_docker_ce_edge_enabled\n\n- name: Set current Docker CE Edge repo status fact\n  set_fact:\n    fact_docker_ce_edge_enabled: \"{{ cmd_docker_ce_edge_enabled.stdout == 'enabled = True' }}\"\n\n- name: Enable/Disable Docker CE Edge Repository\n  shell: yum-config-manager --{{ (docker_enable_ce_edge == true) | ternary('enable','disable') }} docker-ce-edge\n  become: true\n  when: fact_docker_ce_edge_enabled != docker_enable_ce_edge\n\n- name: Update yum cache\n  shell: yum makecache fast\n  args:\n    warn: false  \n  become: true\n  when: yum_repo.changed\n\n- name: Ensure fs.may_detach_mounts is set to avoid 'Device or resource busy'\n  sysctl:\n    name: fs.may_detach_mounts\n    value: 1\n    sysctl_file: /etc/sysctl.d/99-docker.conf\n    reload: yes\n  become: true\n  when: ansible_kernel | version_compare('4', '<')\n\n- include: main-Mountflags.yml\n  when: ansible_kernel | version_compare('4', '<')\n\n- include: main-Generic.yml\n"}, {"commit_sha": "f85435227eb23c6e474103286c17d7406baeff47", "sha": "d8a225e51a9ca17f54084a7e551c373847fef863", "filename": "roles/mesos/tasks/slave.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# Tasks for Slave nodes\n\n- name: create mesos-slave work directory\n  when: mesos_install_mode == \"slave\"\n  file:\n    path: \"{{ mesos_slave_work_dir }}\"\n    state: directory\n    mode: 0755\n  sudo: yes\n  tags:\n    - mesos-slave\n\n- name: destroy old mesos-slave container\n  when: mesos_slave_rebuild_container\n  docker:\n    name: mesos-slave\n    image: \"{{ mesos_slave_image }}\"\n    state: absent\n  tags:\n    - mesos-slave\n\n- name: run mesos-slave container\n  when: mesos_install_mode == \"slave\"\n  docker:\n    name: mesos-slave\n    image: \"{{ mesos_slave_image }}\"\n    state: started\n    privileged: true\n    volumes:\n    - \"{{ mesos_slave_work_dir }}:{{ mesos_slave_work_dir }}\"\n    - \"/proc:/host/proc:ro\"\n    - \"/cgroup:/cgroup\"\n    - \"/sys:/sys\"\n    - \"/lib/libpthread.so.0:/lib/libpthread.so.0:ro\"\n    - \"/usr/bin/docker:/usr/bin/docker:ro\"\n    - \"/usr/lib/x86_64-linux-gnu/libapparmor.so.1.1.0:/usr/lib/x86_64-linux-gnu/libapparmor.so.1\"\n    - \"{{ mesos_docker_socket }}:/var/run/docker.sock\"\n    ports:\n    - \"{{ mesos_slave_port }}:{{ mesos_slave_port }}\"\n    net: \"host\"\n    env:\n      MESOS_MASTER: \"zk://{{ zookeeper_peers_nodes }}/mesos\"\n      MESOS_EXECUTOR_REGISTRATION_TIMEOUT: \"{{ mesos_executor_registration_timeout }}\"\n      MESOS_CONTAINERIZERS: \"{{ mesos_containerizers }}\"\n      MESOS_RESOURCES: \"{{ mesos_resources }}\"\n      MESOS_IP: \"{{ mesos_ip }}\"\n      MESOS_WORK_DIR: \"{{ mesos_slave_work_dir }}\"\n      MESOS_HOSTNAME: \"{{ mesos_hostname }}\"\n  tags:\n    - mesos-slave\n\n- name: upload mesos-slave template service\n  when: mesos_install_mode == \"slave\"\n  template:\n    src: mesos-slave.conf.j2\n    dest: /etc/init/mesos-slave.conf\n    mode: 0755\n  sudo: yes\n  tags:\n    - mesos-slave\n\n- name: ensure mesos-slave is running (and enable it at boot)\n  when: mesos_install_mode == \"slave\"\n  sudo: yes\n  service:\n    name: mesos-slave\n    state: started\n    enabled: yes\n  tags:\n    - mesos-slave\n\n- name: run prometheus mesos slave exporter container\n  when: mesos_install_mode == \"slave\" and prometheus_enabled|bool\n  docker:\n    name: mesos-exporter\n    image: \"{{ prometheus_mesos_exporter_image }}\"\n    command: \"-exporter.scrape-mode=slave -exporter.url=http://{{ mesos_hostname }}:{{ mesos_slave_port }}\"\n    state: started\n    restart_policy: always\n    ports:\n    - \"{{ prometheus_mesos_exporter_port }}:{{ prometheus_mesos_exporter_port }}\"\n  environment: proxy_env\n  tags:\n    - prometheus\n    - mesos_slave\n\n- name: Set mesos-exporter consul service definition\n  when: mesos_install_mode == \"slave\" and prometheus_enabled|bool\n  sudo: yes\n  template:\n    src: mesos-exporter-consul.j2\n    dest: \"{{ consul_dir }}/mesos-exporter.json\"\n  notify:\n    - restart consul\n  tags:\n    - prometheus\n    - mesos_slave\n"}, {"commit_sha": "d944da6c34c3e510a807079256632482b53f00d2", "sha": "c321bd17e98b7cb0ac9249a2ba1322954db1810b", "filename": "tasks/lvm-thinpool.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Ensure lvm2 is installed\n  become: true\n  package:\n    name: lvm2\n    state: present\n\n- name: Create LVM volume group\n  become: true\n  lvg:\n    pvs: '{{ pool.physical_volumes }}'\n    state: present\n    vg: '{{ pool.volume_group }}'\n  when: pool.physical_volumes|default(None)\n\n- name: Check if data volume exists\n  become: true\n  stat:\n    path: '/dev/mapper/{{ pool.volume_group }}-{{ pool.name }}'\n  ignore_errors: true\n  register: _volume\n\n- name: Create data volume\n  become: true\n  lvol:\n    lv: '{{ pool.name }}'\n    size: '{{ pool.data_size }}'\n    vg: '{{ pool.volume_group }}'\n  register: _datavolume_created\n  when: not _volume.stat.exists\n\n- name: Create meta data volume\n  become: true\n  lvol:\n    lv: '{{ pool.name }}meta'\n    size: '{{ pool.metadata_size }}'\n    vg: '{{ pool.volume_group }}'\n  when: _datavolume_created | changed\n\n- name: Convert data volume to thinpool\n  become: true\n  shell:\n    lvconvert\n        -y\n        --zero n\n        -c 512K\n        --thinpool \"{{ pool.volume_group }}/{{ pool.name }}\"\n        --poolmetadata \"{{ pool.volume_group }}/{{ pool.name }}meta\"\n  when: _datavolume_created | changed"}, {"commit_sha": "f85435227eb23c6e474103286c17d7406baeff47", "sha": "66a9cbcc21c0532b2bc4bc91c8316b1a50cb61ba", "filename": "roles/dnsmasq/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for dnsmasq\n- name: create dnsmasq config directory\n  file:\n    path: \"/etc/dnsmasq.d\"\n    state: directory\n    mode: 0755\n  sudo: yes\n  tags:\n    - dnsmasq\n\n- name: configure consul resolution dnsmasq\n  sudo: yes\n  template:\n    src: 10-consul.j2\n    dest: /etc/dnsmasq.d/10-consul\n    owner: root\n    group: root\n    mode: 0644\n  notify:\n    - restart dnsmasq\n  tags:\n    - dnsmasq\n\n- name: destroy old dnsmasq container\n  when: dnsmasq_rebuild_container\n  docker:\n    name: dnsmasq\n    image: \"{{ dnsmasq_image }}\"\n    state: absent\n  tags:\n    - dnsmasq\n\n# This should be using -cap-add=NET_ADMIN rather than privileged: true.\n# This will be supported in Ansible 2.0\n- name: run dnsmasq container\n  docker:\n    name: dnsmasq\n    image: \"{{ dnsmasq_image }}\"\n    state: started\n    net: \"host\"\n    privileged: true\n    volumes:\n    - \"{{ dnsmasq_config_folder }}/:{{ dnsmasq_config_folder }}/\"\n    ports:\n    - \"53:53/tcp\"\n    - \"53:53/udp\"\n    command: \"-r {{ dnsmasq_resolvconf_file }} --conf-dir={{ dnsmasq_config_folder }}\"\n\n- name: upload dnsmasq template service\n  template:\n    src: dnsmasq.conf.j2\n    dest: /etc/init/dnsmasq.conf\n    mode: 0755\n  sudo: yes\n  tags:\n    - dnsmasq\n\n- name: ensure dnsmasq is running (and enable it at boot)\n  service:\n    name: dnsmasq\n    state: started\n    enabled: yes\n  tags:\n    - dnsmasq\n"}, {"commit_sha": "9eb1a8fa8e281ef06687c1851f51fa0beec3e232", "sha": "3ca1b18faf03c323aea9ac1272b1f63cf33a734b", "filename": "tasks/section_13_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 13.1 Ensure Password Fields are Not Empty (Scored)\n    command: awk -F':' '($2 == \"\" ) { print $1 }' /etc/shadow\n    register: awk_empty_shadow\n    changed_when: False\n    failed_when: awk_empty_shadow.stdout != '' and not lock_shadow_accounts\n    tags:\n      - section13\n      - section13.1\n\n  - name: 13.1 Ensure Password Fields are Not Empty (locking accounts) (Scored)\n    command: passwd -l '{{ item }}'\n    with_items:\n        awk_empty_shadow.stdout_lines\n    when: lock_shadow_accounts\n    tags:\n      - section13\n      - section13.1\n\n  - name: 13.2 Verify No Legacy \"+\" Entries Exist in /etc/passwd File (Scored)\n    command: grep '^+:' /etc/passwd\n    register: plus_pass\n    failed_when: plus_pass.rc == 0\n    changed_when: plus_pass.rc == 0\n    tags:\n      - section13\n      - section13.2\n\n  - name: 13.3 Verify No Legacy \"+\" Entries Exist in /etc/shadow File (Scored)\n    command: grep '^+:' /etc/shadow\n    register: plus_shadow\n    failed_when: plus_shadow.rc == 0\n    changed_when: plus_shadow.rc == 0\n    tags:\n      - section13\n      - section13.3\n\n  - name: 13.4 Verify No Legacy \"+\" Entries Exist in /etc/group File (Scored)\n    command: grep '^+:' /etc/group\n    register: plus_group\n    failed_when: plus_group.rc == 0\n    changed_when: plus_group.rc == 0\n    tags:\n      - section13\n      - section13.4\n\n  - name: 13.5 Verify No UID 0 Accounts Exist Other Than root (Scored)\n    command: awk -F':' '($3 == 0) { print $1 }' /etc/passwd\n    register: uid_zero_root\n    changed_when: False\n    failed_when: uid_zero_root.stdout != 'root'\n    tags:\n      - section13\n      - section13.5\n\n  - name: 13.6.1 Ensure root PATH Integrity (empty value) (Scored)\n    shell: 'echo $PATH | grep ::'\n    register: path_colon\n    changed_when: False\n    failed_when: path_colon.rc == 0\n    tags:\n      - section13\n      - section13.6\n\n  - name: 13.6.2 Ensure root PATH Integrity (colon end) (Scored)\n    shell: 'echo $PATH | grep :$'\n    register: path_colon_end\n    changed_when: False\n    failed_when: path_colon_end.rc == 0\n    tags:\n      - section13\n      - section13.6\n\n  - name: 13.6.3 Ensure root PATH Integrity (dot in path) (Scored)\n    shell: \"echo $PATH | sed -e 's/::/:/' -e 's/:$//' -e 's/:/\\\\n/g'\"\n    register: dot_in_path\n    changed_when: False\n    failed_when: '\".\" in dot_in_path.stdout_lines'\n    tags:\n      - section13\n      - section13.6\n\n  - name: 13.6.4 Ensure root PATH Integrity (Scored)\n    file: >\n        path='{{ item }}'\n        state=directory\n        owner=root\n        mode='o-w,g-w'\n    with_items:\n        dot_in_path.stdout_lines\n    tags:\n      - section13\n      - section13.6\n\n  - name: 13.7.1 Check Permissions on User Home Directories (gather users) (Scored)\n    shell: /bin/egrep -v '(root|halt|sync|shutdown|false)' /etc/passwd | /usr/bin/awk -F':' '($7 != \"/usr/sbin/nologin\") { print $6 }'\n    register: home_users\n    changed_when: False\n    failed_when: False\n    tags:\n      - section13\n      - section13.7\n\n  - name: 13.7.2 Check Permissions on User Home Directories (Scored)\n    file: >\n        path='{{ item }}'\n        mode='g-w,o-rwx'\n        state=directory\n    with_items:\n        home_users.stdout_lines\n    when: modify_user_homes == True\n    tags:\n      - section13\n      - section13.7\n\n  - name: 13.8.1 Check User Dot File Permissions (gather dotfiles) (Scored)\n    shell: for pth in `/bin/egrep -v '(root|halt|sync|shutdown)' /etc/passwd | /usr/bin/awk -F':' '($7 != \"/usr/sbin/nologin\") { print $6 }'`; do ls -d -A -1 $pth/.* | egrep -v '[..]$'; done\n    changed_when: False\n    failed_when: False\n    always_run: True\n    register: home_dot_files\n    tags:\n      - section13\n      - section13.8\n\n  - name: 13.8.2 Check User Dot File Permissions (Scored)\n    file: >\n        path='{{ item }}'\n        mode='o-w,g-w'\n    with_items:\n        home_dot_files.stdout_lines\n    tags:\n      - section13\n      - section13.8\n\n  - name: 13.9 Check Permissions on User .netrc Files (Scored)\n    file: >\n        path='{{ item }}/.netrc'\n        mode='g-rwx,o-rwx'\n        recurse=yes\n        state=directory\n    with_items:\n        home_users.stdout_lines\n    tags:\n      - section13\n      - section13.9\n\n  - name: 13.10 Check for Presence of User .rhosts Files (Scored)\n    file: >\n        state=absent\n        path='{{ item }}/.rhosts'\n    with_items:\n        home_users.stdout_lines\n    tags:\n      - section13\n      - section13.10\n\n  - name: 13.11 Check Groups in /etc/passwd (preparation) (Scored)\n    command: cut -s -d':' -f4 /etc/passwd\n    register: groups_id_cut\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.11\n\n  - name: 13.11 Check Groups in /etc/passwd (Scored)\n    command: grep -q -P \"^.*?:[^:]*:{{ item }}:\" /etc/group\n    with_items:\n        groups_id_cut.stdout_lines\n    register: groups_present\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.11\n\n  - name: 13.12 Check That Users Are Assigned Valid Home Directories (Scored)\n    stat: path='{{ item }}'\n    with_items:\n        home_users.stdout_lines\n    register: rstat\n    failed_when: rstat is defined and rstat.stat.isdir == False\n    always_run: True\n    tags:\n      - section13\n      - section13.12\n\n  - name: 13.13 Check User Home Directory Ownership (Scored)\n    debug: msg=\"*** Hardcore ***\"\n    tags:\n      - section13\n      - section13.13\n\n  - name: 13.14 Check for Duplicate UIDs (Scored)\n    shell: cut -f3 -d':' /etc/passwd | sort | uniq -d\n    register: uids_list\n    failed_when: uids_list.stdout != ''\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.14\n\n  - name: 13.15 Check for Duplicate GIDs (Scored)\n    shell: cut -f3 -d':' /etc/group | sort | uniq -d\n    register: gids_list\n    failed_when: gids_list.stdout != ''\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.15\n\n  - name: 13.16 Check for Duplicate User Names (Scored)\n    shell: cut -f1 -d':' /etc/passwd | sort | uniq -d\n    register: uids_list\n    failed_when: uids_list.stdout != ''\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.16\n\n  - name: 13.17 Check for Duplicate Group Names (Scored)\n    shell: cut -f1 -d':' /etc/group | sort | uniq -d\n    register: uids_list\n    failed_when: uids_list.stdout != ''\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.17\n\n  - name: 13.18 Check for Presence of User .netrc Files (stat) (Scored)\n    stat: path='{{ item }}/.netrc'\n    with_items: '{{ home_users.stdout_lines }}'\n    register: netrc_files\n    tags:\n      - section13\n      - section13.18\n\n  - name: 13.18 Check for Presence of User .netrc Files (Scored)\n    debug: msg='Check if {{ item.stat.path}} is needed, and remove otherwise'\n    when: item is defined and item.stat.exists == True\n    with_items: '{{ netrc_files.results }}'\n    tags:\n      - section13\n      - section13.18\n\n  - name: 13.19 Check for Presence of User .forward Files (Scored)\n    file: >\n        path='{{ item }}/.forward'\n        state=absent\n    with_items:\n        home_users.stdout_lines\n    tags:\n      - section13\n      - section13.19\n\n  - name: 13.20.1 Ensure shadow group is empty (Scored)\n    shell: grep '^shadow' /etc/group | cut -f4 -d':'\n    register: shadow_group_empty\n    failed_when: shadow_group_empty.stdout != ''\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.20\n      - section13.20.1\n\n  - name: 13.20.2 Ensure shadow group is empty (preparation) (Scored)\n    shell: grep '^shadow' /etc/group | cut -f1 -d':'\n    register: shadow_group_id\n    failed_when: False\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.20\n      - section13.20.1\n\n  - name: 13.20.2 Ensure shadow group is empty (Scored)\n    shell: awk -F':' '($4 == \"{{ item }}\") { print }' /etc/passwd\n    register: awk_passwd_shadow\n    with_items:\n        shadow_group_id.stdout_lines\n    changed_when: False\n    failed_when: awk_passwd_shadow.stdout != ''\n    always_run: True\n    tags:\n      - section13\n      - section13.20\n      - section13.20.2\n"}, {"commit_sha": "8c4af8da901c68ce8c4bdd21c62e08cec5d3c23a", "sha": "af9c1c69700f43807fecab908888acb61b8a503e", "filename": "tasks/lvm-thinpool.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Ensure lvm2 is installed\n  become: true\n  package:\n    name: lvm2\n    state: present\n\n- name: Create LVM volume group\n  become: true\n  lvg:\n    pvs: '{{ pool.physical_volumes }}'\n    state: present\n    vg: '{{ pool.volume_group }}'\n  when: pool.physical_volumes|default(None)\n\n- name: Check if data volume exists\n  become: true\n  stat:\n    path: '/dev/mapper/{{ pool.volume_group }}-{{ pool.name }}'\n  ignore_errors: true\n  register: _volume\n\n- name: Create data volume\n  become: true\n  lvol:\n    lv: '{{ pool.name }}'\n    size: '{{ pool.data_size }}'\n    vg: '{{ pool.volume_group }}'\n  register: _datavolume_created\n  when: not _volume.stat.exists\n\n- name: Create meta data volume\n  become: true\n  lvol:\n    lv: '{{ pool.name }}meta'\n    size: '{{ pool.metadata_size }}'\n    vg: '{{ pool.volume_group }}'\n  when: _datavolume_created | changed\n\n- name: Convert data volume to thinpool\n  become: true\n  shell:\n    lvconvert\n        -y\n        --zero n\n        -c 512K\n        --thinpool \"{{ pool.volume_group }}/{{ pool.name }}\"\n        --poolmetadata \"{{ pool.volume_group }}/{{ pool.name }}meta\"\n  when: _datavolume_created | changed\n  tags:\n    - skip_ansible_lint"}, {"commit_sha": "903181fe699ba052fc8c94ccf16e531b40064f51", "sha": "7298a05b217bff1699236376ac9d7d5c85068a91", "filename": "tasks/section_13_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 13.1 Ensure Password Fields are Not Empty (Scored)\n    command: awk -F':' '($2 == \"\" ) { print $1 }' /etc/shadow\n    register: awk_empty_shadow\n    changed_when: False\n    failed_when: awk_empty_shadow.stdout != '' and lock_shadow_accounts == 'no'\n    tags:\n      - section13\n      - section13.1\n\n  - name: 13.1 Ensure Password Fields are Not Empty (locking accounts) (Scored)\n    command: passwd -l {{ item }}\n    with_items:\n        awk_empty_shadow.stdout_lines\n    when: lock_shadow_accounts\n    tags:\n      - section13\n      - section13.1\n\n  - name: 13.2 Verify No Legacy \"+\" Entries Exist in /etc/passwd File (Scored)\n    command: grep '^+:' /etc/passwd\n    register: plus_pass\n    failed_when: plus_pass.rc == 0\n    changed_when: plus_pass.rc == 0\n    tags:\n      - section13\n      - section13.2\n\n  - name: 13.3 Verify No Legacy \"+\" Entries Exist in /etc/shadow File (Scored)\n    command: grep '^+:' /etc/shadow\n    register: plus_shadow\n    failed_when: plus_shadow.rc == 0\n    changed_when: plus_shadow.rc == 0\n    tags:\n      - section13\n      - section13.3\n\n  - name: 13.4 Verify No Legacy \"+\" Entries Exist in /etc/group File (Scored)\n    command: grep '^+:' /etc/group\n    register: plus_group\n    failed_when: plus_group.rc == 0\n    changed_when: plus_group.rc == 0\n    tags:\n      - section13\n      - section13.4\n\n  - name: 13.5 Verify No UID 0 Accounts Exist Other Than root (Scored)\n    command: awk -F':' '($3 == 0) { print $1 }' /etc/passwd\n    register: uid_zero_root\n    changed_when: False\n    failed_when: uid_zero_root.stdout != 'root'\n    tags:\n      - section13\n      - section13.5\n\n  - name: 13.6.1 Ensure root PATH Integrity (empty value) (Scored)\n    shell: 'echo $PATH | grep ::'\n    register: path_colon\n    changed_when: False\n    failed_when: path_colon.rc == 0\n    tags:\n      - section13\n      - section13.6\n\n  - name: 13.6.2 Ensure root PATH Integrity (colon end) (Scored)\n    shell: 'echo $PATH | grep :$'\n    register: path_colon_end\n    changed_when: False\n    failed_when: path_colon_end.rc == 0\n    tags:\n      - section13\n      - section13.6\n\n  - name: 13.6.3 Ensure root PATH Integrity (dot in path) (Scored)\n    shell: \"echo $PATH | sed -e 's/::/:/' -e 's/:$//' -e 's/:/\\\\n/g'\"\n    register: dot_in_path\n    changed_when: False\n    failed_when: '\".\" in dot_in_path.stdout_lines'\n    tags:\n      - section13\n      - section13.6\n\n  - name: 13.6.4 Ensure root PATH Integrity (Scored)\n    file: >\n        path={{ item }}\n        state=directory\n        owner=root\n        mode='o-w,g-w'\n    with_items:\n        dot_in_path.stdout_lines\n    tags:\n      - section13\n      - section13.6\n\n  - name: 13.7.1 Check Permissions on User Home Directories (gather users) (Scored)\n    shell: /bin/egrep -v '(root|halt|sync|shutdown|false)' /etc/passwd | /usr/bin/awk -F':' '($7 != \"/usr/sbin/nologin\") { print $6 }'\n    register: home_users\n    changed_when: False\n    failed_when: False\n    tags:\n      - section13\n      - section13.7\n\n  - name: 13.7.2 Check Permissions on User Home Directories (Scored)\n    file: >\n        path={{ item }}\n        mode='g-w,o-rwx'\n        state=directory\n    with_items:\n        home_users.stdout_lines\n    when: modify_user_homes == True\n    tags:\n      - section13\n      - section13.7\n\n  - name: 13.8.1 Check User Dot File Permissions (gather dotfiles) (Scored)\n    shell: for pth in `/bin/egrep -v '(root|halt|sync|shutdown)' /etc/passwd | /usr/bin/awk -F':' '($7 != \"/usr/sbin/nologin\") { print $6 }'`; do ls -d -A -1 $pth/.* | egrep -v '[..]$'; done\n    changed_when: False\n    failed_when: False\n    always_run: True\n    register: home_dot_files\n    tags:\n      - section13\n      - section13.8\n\n  - name: 13.8.2 Check User Dot File Permissions (Scored)\n    file: >\n        path={{ item }}\n        mode='o-w,g-w'\n    with_items:\n        home_dot_files.stdout_lines\n    tags:\n      - section13\n      - section13.8\n\n  - name: 13.9 Check Permissions on User .netrc Files (Scored)\n    file: >\n        path={{ item }}/.netrc\n        mode='g-rwx,o-rwx'\n        recurse=yes\n        state=directory\n    with_items:\n        home_users.stdout_lines\n    tags:\n      - section13\n      - section13.9\n\n  - name: 13.10 Check for Presence of User .rhosts Files (Scored)\n    file: >\n        state=absent\n        path={{ item }}/.rhosts\n    with_items:\n        home_users.stdout_lines\n    tags:\n      - section13\n      - section13.10\n\n  - name: 13.11 Check Groups in /etc/passwd (preparation) (Scored)\n    command: cut -s -d':' -f4 /etc/passwd\n    register: groups_id_cut\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.11\n\n  - name: 13.11 Check Groups in /etc/passwd (Scored)\n    command: grep -q -P \"^.*?:[^:]*:{{ item }}:\" /etc/group\n    with_items:\n        groups_id_cut.stdout_lines\n    register: groups_present\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.11\n\n  - name: 13.12 Check That Users Are Assigned Valid Home Directories (Scored)\n    stat: path={{ item }}\n    with_items:\n        home_users.stdout_lines\n    register: rstat\n    failed_when: rstat is defined and rstat.stat.isdir == False\n    always_run: True\n    tags:\n      - section13\n      - section13.12\n\n  - name: 13.13 Check User Home Directory Ownership (Scored)\n    debug: msg=\"*** Hardcore ***\"\n    tags:\n      - section13\n      - section13.13\n\n  - name: 13.14 Check for Duplicate UIDs (Scored)\n    shell: cut -f3 -d':' /etc/passwd | sort | uniq -d\n    register: uids_list\n    failed_when: uids_list.stdout != ''\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.14\n\n  - name: 13.15 Check for Duplicate GIDs (Scored)\n    shell: cut -f3 -d':' /etc/group | sort | uniq -d\n    register: gids_list\n    failed_when: gids_list.stdout != ''\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.15\n\n  - name: 13.16 Check for Duplicate User Names (Scored)\n    shell: cut -f1 -d':' /etc/passwd | sort | uniq -d\n    register: uids_list\n    failed_when: uids_list.stdout != ''\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.16\n\n  - name: 13.17 Check for Duplicate Group Names (Scored)\n    shell: cut -f1 -d':' /etc/group | sort | uniq -d\n    register: uids_list\n    failed_when: uids_list.stdout != ''\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.17\n\n  - name: 13.18 Check for Presence of User .netrc Files (stat) (Scored)\n    stat: path='{{ item }}/.netrc'\n    with_items: home_users.stdout_lines\n    register: netrc_files\n    tags:\n      - section13\n      - section13.18\n\n  - name: 13.18 Check for Presence of User .netrc Files (Scored)\n    file: >\n        path='{{ item }}'\n        state=absent\n    when: item.stat.exists == True\n    with_items: netrc_files.results\n    tags:\n      - section13\n      - section13.18\n\n  - name: 13.19 Check for Presence of User .forward Files (Scored)\n    file: >\n        path='{{ item }}/.forward'\n        state=absent\n    with_items:\n        home_users.stdout_lines\n    tags:\n      - section13\n      - section13.19\n\n  - name: 13.20.1 Ensure shadow group is empty (Scored)\n    shell: grep '^shadow' /etc/group | cut -f4 -d':'\n    register: shadow_group_empty\n    failed_when: shadow_group_empty.stdout != ''\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.20\n      - section13.20.1\n\n  - name: 13.20.2 Ensure shadow group is empty (preparation) (Scored)\n    shell: grep '^shadow' /etc/group | cut -f1 -d':'\n    register: shadow_group_id\n    failed_when: False\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.20\n      - section13.20.1\n\n  - name: 13.20.2 Ensure shadow group is empty (Scored)\n    shell: awk -F':' '($4 == \"{{ item }}\") { print }' /etc/passwd\n    register: awk_passwd_shadow\n    with_items:\n        shadow_group_id.stdout_lines\n    changed_when: False\n    failed_when: awk_passwd_shadow.stdout != ''\n    always_run: True\n    tags:\n      - section13\n      - section13.20\n      - section13.20.2\n"}, {"commit_sha": "ad8509bbbbcef78d4a59feff8b18a9e84512dd2d", "sha": "0113d7672b4c6bb9f62fb98fca731e58fa5f9490", "filename": "roles/network/tasks/named.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install named packages\n  package: name={{ item }}\n           state=present\n  with_items:\n   - bind9\n   - bind9utils\n  when: is_debuntu\n  tags:\n    - download\n\n- name: Install named packages\n  package: name={{ item }}\n           state=present\n  with_items:\n   - bind\n   - bind-utils\n  when: not is_debuntu\n  tags:\n    - download\n\n# or we have to change the serial number in the config files.\n#- name: Stop named before copying files\n#  service: name={{ dns_service }} state=stopped\n#  when: not installing\n\n- name: Set folder permission\n  file: path={{ item }}\n        owner={{ dns_user }}\n        group=root\n        mode=0755\n        state=directory\n  with_items:\n    - /var/named-iiab\n    - /var/named-iiab/data\n    - /etc/sysconfig/olpc-scripts/domain_config.d\n\n- name: Configure named\n  template: src={{ item.src }}\n            dest={{ item.dest }}\n            owner={{ item.owner }}\n            group=root\n            mode={{ item.mode }}\n  with_items:\n    - { src: 'named/named-iiab.conf.j2', dest: '/etc/named-iiab.conf', owner: \"root\" , mode: '0644' }\n    - { src: 'named/named.j2', dest: '/etc/sysconfig/named', owner: \"root\" , mode: '0644' }\n    - { src: 'named/named', dest: '/etc/sysconfig/olpc-scripts/domain_config.d/named', owner: \"root\" , mode: '0644' }\n    - { src: 'named/localdomain.zone', dest: '/var/named-iiab/localdomain.zone',owner: \"{{ dns_user }}\" , mode: '0644' }\n    - { src: 'named/localhost.zone', dest: '/var/named-iiab/localhost.zone', owner: \"{{ dns_user }}\" , mode: '0644' }\n    - { src: 'named/named.broadcast', dest: '/var/named-iiab/named.broadcast', owner: \"{{ dns_user }}\" , mode: '0644'}\n    - { src: 'named/named.ip6.local', dest: '/var/named-iiab/named.ip6.local' , owner: \"{{ dns_user }}\" , mode: '0644'}\n    - { src: 'named/named.local', dest: '/var/named-iiab/named.local' , owner: \"{{ dns_user }}\" , mode: '0644'}\n    - { src: 'named/named.rfc1912.zones', dest: '/var/named-iiab/named.rfc1912.zones' , owner: \"{{ dns_user }}\" , mode: '0644'}\n    - { src: 'named/named.root', dest: '/var/named-iiab/named.root' , owner: \"{{ dns_user }}\" , mode: '0644'}\n    - { src: 'named/named.root.hints', dest: '/var/named-iiab/named.root.hints' , owner: \"{{ dns_user }}\" , mode: '0644'}\n    - { src: 'named/named.zero', dest: '/var/named-iiab/named.zero' , owner: \"{{ dns_user }}\" , mode: '0644'}\n    - { src: 'named/school.external.zone.db', dest: '/var/named-iiab/school.external.zone.db' , owner: \"{{ dns_user }}\" , mode: '0644'}\n    - { src: 'named/school.internal.zone.16.in-addr.db.j2', dest: '/var/named-iiab/school.internal.zone.16.in-addr.db' , owner: \"{{ dns_user }}\" , mode: '0644'}\n    - { src: 'named/school.internal.zone.32.in-addr.db.j2', dest: '/var/named-iiab/school.internal.zone.32.in-addr.db' , owner: \"{{ dns_user }}\" , mode: '0644'}\n    - { src: 'named/school.internal.zone.48.in-addr.db.j2', dest: '/var/named-iiab/school.internal.zone.48.in-addr.db' , owner: \"{{ dns_user }}\" , mode: '0644'}\n# the following two files are not writeable by named, but bind 9.4 cannot discover that fact correctly\n    - { src: 'named/school.internal.zone.db', dest: '/var/named-iiab/school.internal.zone.db' , owner: \"root\" , mode: '0644'}\n    - { src: 'named/school.local.zone.db', dest: '/var/named-iiab/school.local.zone.db' , owner: \"root\" , mode: '0644'}\n    - { src: 'named/school.internal.zone.in-addr.db.j2', dest: '/var/named-iiab/school.internal.zone.in-addr.db' , owner: \"{{ dns_user }}\" , mode: '0644'}\n    - { src: 'named/dummy', dest: '/var/named-iiab/data/dummy' , owner: \"{{ dns_user }}\" , mode: '0644'}\n    - { src: 'named/named.blackhole', dest: '/var/named-iiab/named.blackhole' , owner: \"{{ dns_user }}\" , mode: '0644'}\n\n- name: substitute our unit file which uses $OPTIONS from sysconfig\n  template: src=named/{{ dns_service }}.service\n            dest=/etc/systemd/system/{{ dns_service }}.service\n            mode=0644\n\n- name: The dns-jail redirect requires the named.blackhole,disabling recursion\n#        in named-iiab.conf, and the redirection of 404 error documents to /\n  template: src=named/dns-jail.conf dest=/etc/{{ apache_config_dir }}/\n  when: dns_jail_enabled\n\n- name: Separate enabling required for debian\n  file: src=/etc/{{ apache_config_dir }}/dns-jail.conf\n        path=/etc/{{ apache_service }}/sites-enabled/dns-jail.conf\n        state=link\n  when: is_debuntu and dns_jail_enabled\n\n- name: Separate enabling/disabling required for debian\n  file: src=/etc/{{ apache_config_dir }}/dns-jail.conf\n        path=/etc/{{ apache_service }}/sites-enabled/dns-jail.conf\n        state=absent\n  when: is_debuntu and not dns_jail_enabled\n\n- name: Separate enabling/disabling required for non debian\n  file: path=/etc/{{ apache_config_dir }}/dns-jail.conf\n        state=absent\n  when: not is_debuntu and not dns_jail_enabled\n"}, {"commit_sha": "1b857dd321d16eae8442dac8b317135668fa2be5", "sha": "dcaec52544a89fcdb12393ba13a645caf3541c47", "filename": "tasks/configure-non-systemd.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Combine Docker daemon environment variable configuration\n  set_fact:\n    docker_service_envs: \"{{ docker_service_envs | combine(_docker_service_opts) | combine(docker_daemon_envs) }}\"\n  vars:\n    _docker_service_opts:\n      DOCKER_OPTS: \"{{ docker_daemon_opts }}\"\n\n- name: Setup Docker environment file {{ docker_envs_dir[_docker_os_dist] }}/docker\n  template:\n    src: docker-envs.j2\n    dest: \"{{ docker_envs_dir[_docker_os_dist] }}/docker\"\n  become: yes\n  notify: restart docker\n  vars:\n    docker_envs: \"{{ docker_service_envs }}\""}, {"commit_sha": "157ca3c14314ff91b6450c98dd50f073d1427cee", "sha": "a7f5bb8be33196004dda0bba491ec5f3e9ec64e5", "filename": "playbooks/nagios/setup_nagios.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: \"Set up the nagios targets\"\n  hosts: nagios-targets\n  roles:\n  - role: ../../galaxy/infra-ansible/roles/nagios-target\n    \n- name: \"Set up nagios server\"\n  hosts: nagios-servers\n  roles:\n  - role: ../../galaxy/infra-ansible/roles/nagios-server\n\n"}, {"commit_sha": "8697b27085612cde54f2a9486f00e3d1b996a8f4", "sha": "27e9b447a06b40cd8027a0d8d8773ecb49d962f2", "filename": "tasks/install-pre5.yml", "repository": "geerlingguy/ansible-role-solr", "decoded_content": "---\n# Install Solr.\n- name: Check if Solr is already installed.\n  stat: \"path={{ solr_install_path }}/dist/{{ solr_filename }}.war\"\n  register: solr_war_file\n\n- name: Copy Solr into place.\n  command: \"cp -r {{ solr_workspace }}/{{ solr_filename }} {{ solr_install_path }}\"\n  when: not solr_war_file.stat.exists\n\n- name: Ensure Solr install files are owned by the solr_user.\n  file:\n    path: \"{{ solr_install_path }}\"\n    owner: \"{{ solr_user }}\"\n    group: \"{{ solr_user }}\"\n    recurse: yes\n  when: not solr_war_file.stat.exists\n\n# Set up solr_home.\n- name: Check if solr_home is already set up.\n  stat: \"path={{ solr_home }}/solr.xml\"\n  register: solr_example\n\n- name: Ensure solr_home directory exists.\n  file:\n    path: \"{{ solr_home }}\"\n    state: directory\n    owner: \"{{ solr_user }}\"\n    group: \"{{ solr_user }}\"\n    mode: 0755\n  when: not solr_example.stat.exists\n\n- name: Copy Solr example into solr_home.\n  shell: \"cp -r {{ solr_install_path }}/example/solr/* {{ solr_home }}\"\n  when: not solr_example.stat.exists\n\n- name: Fix the example solrconfig.xml file.\n  replace:\n    dest: \"{{ solr_home }}/collection1/conf/solrconfig.xml\"\n    regexp: ^.+solr\\.install\\.dir.+$\n    replace: \"\"\n  when: \"not solr_example.stat.exists and solr_version.split('.')[0] == '4'\"\n\n- name: Ensure Solr home files are owned by the solr_user.\n  file:\n    path: \"{{ solr_home }}\"\n    owner: \"{{ solr_user }}\"\n    group: \"{{ solr_user }}\"\n    recurse: yes\n  when: not solr_example.stat.exists\n\n# Set up Solr init script.\n- name: Ensure log file is created and has proper permissions.\n  file:\n    path: \"/var/log/solr.log\"\n    state: touch\n    owner: \"{{ solr_user }}\"\n    group: root\n    mode: 0664\n  changed_when: false\n\n- name: Copy solr init script into place.\n  template:\n    src: \"solr-init-{{ ansible_os_family }}-pre5.j2\"\n    dest: \"/etc/init.d/{{ solr_service_name }}\"\n    mode: 0755\n\n- name: Ensure daemon is installed (Debian).\n  apt: name=daemon state=installed\n  when: ansible_os_family == \"Debian\"\n\n- name: Copy solr systemd unit file into place (for systemd systems).\n  template:\n    src: solr.unit.j2\n    dest: /etc/systemd/system/solr.service\n    owner: root\n    group: root\n    mode: 0755\n  when: >\n    (ansible_distribution == 'Ubuntu' and ansible_distribution_version == '16.04') or\n    (ansible_distribution == 'Debian' and ansible_distribution_version|int >= 8) or\n    (ansible_distribution == 'CentOS' and ansible_distribution_version|int >= 7) or\n    (ansible_distribution == 'Fedora')\n"}, {"commit_sha": "5c1cde6cffcd979d0292e988da029c0ece96054c", "sha": "7a84ec83692f04b3f334d8d35bd4d0f17ca7650b", "filename": "tasks/variables-spatialite.yml", "repository": "openwisp/ansible-openwisp2", "decoded_content": "- name: Set spatialite_path for Fedora <= 27\n  set_fact:\n    openwisp2_spatialite_path: \"mod_spatialite\"\n  when: >\n    (ansible_distribution == 'Fedora'\n    and ansible_distribution_version is version_compare('27', 'le'))\n\n- name: Set spatialite_path (Ubuntu >= 18.04 or Debian >= 10 or Fedora >= 28)\n  set_fact:\n    openwisp2_spatialite_path: \"mod_spatialite.so\"\n  when: >\n    (ansible_distribution == 'Ubuntu' \n    and ansible_distribution_version is version_compare('18.04', 'ge')) \n    or (ansible_distribution == 'Debian' and \n    ansible_distribution_version is version_compare('10', 'ge'))\n    or (ansible_distribution == 'Fedora' and\n    ansible_distribution_version is version_compare('28', 'ge'))\n\n- name: Set spatialite_path (Ubuntu >= 16.04 or Debian >= 9)\n  set_fact:\n    openwisp2_spatialite_path: \"mod_spatialite\"\n  when: >\n    (ansible_distribution == 'Ubuntu'\n    and ansible_distribution_version is version_compare('16.04', 'ge'))\n    or (ansible_distribution == 'Debian'\n    and ansible_distribution_version is version_compare('9', 'ge'))\n"}, {"commit_sha": "a774d7f239841cdb705317557a11935ca87238ad", "sha": "8252deeeecfb6adaa52cf98b45f21e1d1928cd03", "filename": "tasks/section_08_level2.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: Check if the file auditd.conf exists\n    stat: >\n        path=/etc/audit/auditd.conf\n    register: auditd_file\n    tags:\n      - section8\n      - section8.1\n      - section8.1.1\n      - section8.1.1.1\n\n  - name: Create the audit directory if it does not exists\n    file: >\n        path=/etc/audit/\n        state=directory\n    when: not auditd_file.stat.exists\n    tags:\n      - section8\n      - section8.1\n      - section8.1.1\n      - section8.1.1.1\n\n  - name: 8.1.1-3 Configure Data Retention\n    lineinfile: >\n        dest=/etc/audit/auditd.conf\n        regexp=\"{{ item.rxp }}\"\n        line=\"{{ item.line }}\"\n        state=present\n        create=yes\n    with_items:\n      - { rxp: '^max_log_file ', line: 'max_log_file = {{ Max_Log_File }}' }\n      - { rxp: '^space_left_action', line: 'space_left_action = email' }\n      - { rxp: '^action_mail_acct', line: 'action_mail_acct = root' }\n      - { rxp: '^admin_space_left_action', line: 'admin_space_left_action = halt' }\n      - { rxp: '^max_log_file_action', line: 'max_log_file_action = keep_logs' }\n    notify: restart auditd\n    tags:\n      - section8\n      - section8.1\n      - section8.1.1\n      - section8.1.1.1\n      - section8.1.1.2\n      - section8.1.1.3\n\n  - name: 8.1.2 Install and Enable auditd Service (Scored)\n    apt: name=auditd state=present\n    tags:\n      - section8\n      - section8.1\n      - section8.1.2\n      - section8.1.2\n\n  - name: 8.1.3 Enable Auditing for Processes That Start Prior to auditd (Scored)\n    stat: path=/etc/default/grub\n    register: grubcfg_file\n    tags:\n      - section8\n      - section8.1\n      - section8.1.3\n\n  - name: 8.1.3 Enable Auditing for Processes That Start Prior to auditd (Scored)\n    file: >\n        path=/etc/default/grub\n        state=touch\n    when: not grubcfg_file.stat.exists\n    tags:\n      - section8\n      - section8.1\n      - section8.1.3\n\n  - name: 8.1.3 Enable Auditing for Processes That Start Prior to auditd (Scored)\n    lineinfile: >\n        dest=/etc/default/grub\n        line='GRUB_CMDLINE_LINUX=\"audit=1\"'\n    when: not grubcfg_file.stat.exists\n    tags:\n      - section8\n      - section8.1\n      - section8.1.3\n\n  - name: 8.1.4 Record Events That Modify Date and Time Information (Scored)\n    lineinfile: >\n      dest=/etc/audit/audit.rules\n      line='{{ item }}'\n      state=present\n      create=yes\n    with_items:\n      - '-a always,exit -F arch=b64 -S adjtimex -S settimeofday -k time-change'\n      - '-a always,exit -F arch=b32 -S adjtimex -S settimeofday -S stime -k time-change'\n      - '-a always,exit -F arch=b64 -S clock_settime -k time-change'\n      - '-a always,exit -F arch=b32 -S clock_settime -k time-change'\n      - '-w /etc/localtime -p wa -k time-change'\n    notify: restart auditd\n    when: ansible_userspace_bits == \"64\"\n    tags:\n      - section8\n      - section8.1\n      - section8.1.4\n\n  - name: 8.1.4 Record Events That Modify Date and Time Information (Scored)\n    lineinfile: >\n      dest=/etc/audit/audit.rules\n      line={{ item }}\n      state=present\n      create=yes\n    with_items:\n      - '-a always,exit -F arch=b32 -S adjtimex -S settimeofday -S stime -k time-change'\n      - '-a always,exit -F arch=b32 -S clock_settime -k time-change'\n      - '-w /etc/localtime -p wa -k time-change'\n    notify: restart auditd\n    when: ansible_userspace_bits == \"32\"\n    tags:\n      - section8\n      - section8.1\n      - section8.1.4\n\n  - name: 8.1.5,7,8,9,15,16,17 Record Events That Modify User/Group Information (Scored)\n    lineinfile: >\n      dest=/etc/audit/audit.rules\n      line='{{ item }}'\n      state=present\n      create=yes\n    with_items:\n      - '-w /etc/group -p wa -k identity'\n      - '-w /etc/passwd -p wa -k identity'\n      - '-w /etc/gshadow -p wa -k identity'\n      - '-w /etc/shadow -p wa -k identity'\n      - '-w /etc/security/opasswd -p wa -k identity'\n      - '-w /var/log/faillog -p wa -k logins'\n      - '-w /var/log/lastlog -p wa -k logins'\n      - '-w /var/log/tallylog -p wa -k logins'\n      - '-w /var/run/utmp -p wa -k session'\n      - '-w /var/log/wtmp -p wa -k session'\n      - '-w /var/log/btmp -p wa -k session'\n      - '-w /etc/selinux/ -p wa -k MAC-policy'\n      - '-w /etc/sudoers -p wa -k scope'\n      - '-w /var/log/sudo.log -p wa -k actions'\n      - '-w /sbin/insmod -p x -k modules'\n      - '-w /sbin/rmmod -px -k modules'\n      - '-w /sbin/modprobe -p x -k modules'\n    notify: restart auditd\n    tags:\n      - section8\n      - section8.1\n      - section8.1.5\n      - section8.1.7\n      - section8.1.8\n      - section8.1.9\n      - section8.1.15\n      - section8.1.16\n      - section8.1.17\n\n  - name: 8.1.6,10,11,13,14,17 Record Events That Modify the System's Network Environment (64b) (Scored)\n    lineinfile: >\n      dest=/etc/audit/audit.rules\n      line='{{ item }}'\n      state=present\n      create=yes\n    with_items:\n      - '-a exit,always -F arch=b64 -S sethostname -S setdomainname -k system-locale'\n      - '-a exit,always -F arch=b32 -S sethostname -S setdomainname -k system-locale'\n      - '-w /etc/issue -p wa -k system-locale'\n      - '-w /etc/issue.net -p wa -k system-locale'\n      - '-w /etc/hosts -p wa -k system-locale'\n      - '-w /etc/network -p wa -k system-locale'\n      - '-a always,exit -F arch=b64 -S chmod -S fchmod -S fchmodat -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S chmod -S fchmod -S fchmodat -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b64 -S chown -S fchown -S fchownat -S lchown -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S chown -S fchown -S fchownat -S lchown -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b64 -S setxattr -S lsetxattr -S fsetxattr -S removexattr -S lremovexattr -S fremovexattr -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S setxattr -S lsetxattr -S fsetxattr -S removexattr -S lremovexattr -S fremovexattr -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b64 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EACCES -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b32 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EACCES -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b64 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EPERM -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b32 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EPERM -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b64 -S mount -F auid>=500 -F auid!=4294967295 -k mounts'\n      - '-a always,exit -F arch=b32 -S mount -F auid>=500 -F auid!=4294967295 -k mounts'\n      - '-a always,exit -F arch=b64 -S unlink -S unlinkat -S rename -S renameat -F auid>=500 -F auid!=4294967295 -k delete'\n      - '-a always,exit -F arch=b32 -S unlink -S unlinkat -S rename -S renameat -F auid>=500 -F auid!=4294967295 -k delete'\n      - '-a always,exit -F arch=b64 -S init_module -S delete_module -k modules'\n    notify: restart auditd\n    when: ansible_userspace_bits == \"64\"\n    tags:\n      - section8\n      - section8.1\n      - section8.1.6\n      - section8.1.10\n      - section8.1.11\n      - section8.1.13\n      - section8.1.14\n      - section8.1.17\n\n  - name: 8.1.6,10,11,13,14,17 Record Events That Modify the System's Network Environment (32b) (Scored)\n    lineinfile: >\n      dest=/etc/audit/audit.rules\n      line='{{ item }}'\n      state=present\n      create=yes\n    with_items:\n      - '-a exit,always -F arch=b32 -S sethostname -S setdomainname -k system-locale'\n      - '-w /etc/issue -p wa -k system-locale'\n      - '-w /etc/issue.net -p wa -k system-locale'\n      - '-w /etc/hosts -p wa -k system-locale'\n      - '-w /etc/network -p wa -k system-locale'\n      - '-a always,exit -F arch=b32 -S chmod -S fchmod -S fchmodat -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S chown -S fchown -S fchownat -S lchown -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S setxattr -S lsetxattr -S fsetxattr -S removexattr -S lremovexattr -S fremovexattr -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EACCES -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b32 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EPERM -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b32 -S mount -F auid>=500 -F auid!=4294967295 -k mounts'\n      - '-a always,exit -F arch=b32 -S unlink -S unlinkat -S rename -S renameat -F auid>=500 -F auid!=4294967295 -k delete'\n      - '-a always,exit -F arch=b32 -S init_module -S delete_module -k modules'\n    notify: restart auditd\n    when: ansible_userspace_bits == \"32\"\n    tags:\n      - section8\n      - section8.1\n      - section8.1.6\n      - section8.1.10\n      - section8.1.11\n      - section8.1.13\n      - section8.1.14\n      - section8.1.17\n\n  - name: 8.1.12 Collect Use of Privileged Commands (Scored)\n    shell: find / -xdev \\( -perm -4000 -o -perm -2000 \\) -type f | awk '{print \"-a always,exit -F path=\" $1 \" -F perm=x -F auid>=500 -F auid!=4294967295 -k privileged\" }'\n    register: audit_lines_for_find\n    changed_when: False\n    tags:\n      - section8\n      - section8.1\n      - section8.1.12\n\n  - name: 8.1.12 Collect Use of Privileged Commands (infos) (Scored)\n    lineinfile: >\n        dest=/etc/audit/audit.rules\n        line={{ item }}\n        state=present\n        create=yes\n    with_items: audit_lines_for_find.stdout_lines\n    tags:\n      - section8\n      - section8.1\n      - section8.1.12\n\n  - name: 8.1.18 Make the Audit Configuration Immutable (Scored)\n    lineinfile: >\n        dest='/etc/audit/audit.rules'\n        line='-e 2'\n        insertafter=EOF\n        state=present\n        create=yes\n    tags:\n      - section8\n      - section8.1\n      - section8.1.18\n\n  - name: 8.3.1 Install AIDE (Scored)\n    apt: name=aide state=present\n    register: aide_installed\n    tags:\n      - section8\n      - section8.3\n      - section8.3.1\n\n  - name: 8.3.1 Install AIDE (init) (Scored)\n    command: aideinit\n    when: aide_installed.changed == True\n    tags:\n      - section8\n      - section8.3\n      - section8.3.1\n\n  - name: 8.3.1 Install AIDE (Scored)\n    stat: path=/var/lib/aide/aide.db.new\n    register: aide_db_path\n    when:\n      - aide_installed.changed == True\n    tags:\n      - section8\n      - section8.3\n      - section8.3.1\n\n  - name: 8.3.1 Install AIDE (copy db) (Scored)\n    command: mv /var/lib/aide/aide.db.new /var/lib/aide/aide.db\n    when:\n      - aide_installed.changed == True\n      - aide_db_path.stat.exists == True\n    tags:\n      - section8\n      - section8.3\n      - section8.3.1\n\n  - name: 8.3.2 Implement Periodic Execution of File Integrity (Scored)\n    cron: name=\"Check files integrity\" minute=\"0\" hour=\"5\" job=\"/usr/sbin/aide --check\"\n    tags:\n      - section8\n      - section8.3\n      - section8.3.2\n"}, {"commit_sha": "f4c9663fd9cba1b0bd8c3526cbc47c01bac36efd", "sha": "7b33bac707303b6e19f32cdde3bce60c84841f1b", "filename": "tasks/main.yml", "repository": "zzet/ansible-rbenv-role", "decoded_content": "---\n- name: include env vars\n  include_vars: \"{{ rbenv.env }}.yml\"\n\n- include: apt_build_depends.yml\n  when: ansible_pkg_mgr == 'apt'\n- include: yum_build_depends.yml\n  when: ansible_pkg_mgr == 'yum'\n# - include: pacman_build_depends.yml # Arch Linux\n#   when: ansible_pkg_mgr == 'pacman'\n- include: homebrew_build_depends.yml\n  when: ansible_os_family == 'Darwin'\n\n\n- name: checkout rbenv_repo for system\n  become: yes\n  git: >\n    repo={{ rbenv_repo }}\n    dest={{ rbenv_root }}\n    version={{ rbenv.version }}\n    accept_hostkey=yes\n    force=yes\n  when: rbenv.env == \"system\"\n  tags:\n    - rbenv\n\n- name: create plugins directory for system\n  become: yes\n  file: state=directory path={{ rbenv_root }}/plugins\n  when: rbenv.env == \"system\"\n  tags:\n    - rbenv\n\n- name: install plugins for system\n  become: yes\n  git: >\n    repo={{ item.repo }}\n    dest={{ rbenv_root }}/plugins/{{ item.name }}\n    version={{ item.version }}\n    accept_hostkey=yes\n    force=yes\n  with_items: \"{{ rbenv_plugins }}\"\n  when: rbenv.env == \"system\"\n  tags:\n    - rbenv\n\n- name: checkout rbenv_repo for selected users\n  git: >\n    repo={{ rbenv_repo }}\n    dest={{ rbenv_root }}\n    version={{ rbenv.version }}\n    accept_hostkey=yes\n    force=yes\n  with_items: \"{{ rbenv_users }}\"\n  become: yes\n  become_user: \"{{ item }}\"\n  when: rbenv.env != \"system\"\n  ignore_errors: yes\n  tags:\n    - rbenv\n\n- name: create plugins directory for selected users\n  file: state=directory path={{ rbenv_root }}/plugins\n  with_items: \"{{ rbenv_users }}\"\n  become: yes\n  become_user: \"{{ item }}\"\n  when: rbenv.env != \"system\"\n  ignore_errors: yes\n  tags:\n    - rbenv\n\n- name: install plugins for selected users\n  git: >\n    repo={{ item[1].repo }}\n    dest={{ rbenv_root }}/plugins/{{ item[1].name }}\n    version={{ item[1].version }}\n    accept_hostkey=yes\n    force=yes\n  with_nested:\n    - \"{{ rbenv_users }}\"\n    - \"{{ rbenv_plugins }}\"\n  become: yes\n  become_user: \"{{ item[0] }}\"\n  when: rbenv.env != \"system\"\n  ignore_errors: yes\n  tags:\n    - rbenv\n\n- name: add rbenv initialization to profile system-wide\n  template: src=rbenv.sh.j2 dest=/etc/profile.d/rbenv.sh owner=root group=root mode=0755\n  become: yes\n  when:\n    - ansible_os_family != 'OpenBSD'\n  tags:\n    - rbenv\n\n- name: set default-gems for select users\n  copy: src=default-gems dest={{ rbenv_root }}/default-gems\n  with_items: \"{{ rbenv_users }}\"\n  become: yes\n  become_user: \"{{ item }}\"\n  when:\n    - not \"system\" == \"{{ rbenv.env }}\"\n    - default_gems_file is not defined\n  ignore_errors: yes\n  tags:\n    - rbenv\n\n- name: set custom default-gems for select users\n  copy: src={{ default_gems_file }} dest={{ rbenv_root }}/default-gems\n  with_items: \"{{ rbenv_users }}\"\n  become: yes\n  become_user: \"{{ item }}\"\n  when:\n    - not \"system\" == \"{{ rbenv.env }}\"\n    - default_gems_file is defined\n  ignore_errors: yes\n  tags:\n    - rbenv\n\n- name: set gemrc for select users\n  copy: src=gemrc dest=~/.gemrc\n  with_items: \"{{ rbenv_users }}\"\n  become: yes\n  become_user: \"{{ item }}\"\n  when: rbenv.env != \"system\"\n  ignore_errors: yes\n  tags:\n    - rbenv\n\n- name: set vars for select users\n  copy: src=vars dest={{ rbenv_root }}/vars\n  with_items: \"{{ rbenv_users }}\"\n  become: yes\n  become_user: \"{{ item }}\"\n  when: rbenv.env != \"system\"\n  ignore_errors: yes\n  tags:\n    - rbenv\n\n- name: check ruby {{ rbenv.ruby_version }} installed for system\n  shell: $SHELL -lc \"rbenv versions | grep {{ rbenv.ruby_version }}\"\n  register: ruby_installed\n  changed_when: false\n  ignore_errors: yes\n  always_run: yes\n  when: rbenv.env == \"system\"\n  tags:\n    - rbenv\n\n- name: install ruby {{ rbenv.ruby_version }} for system\n  shell: bash -lc \"rbenv install {{ rbenv.ruby_version }}\"\n  become: yes\n  when:\n    - rbenv.env == \"system\"\n    - ruby_installed.rc != 0\n  tags:\n    - rbenv\n\n- name: check if current system ruby version is {{ rbenv.ruby_version }}\n  shell: $SHELL -lc \"rbenv version | cut -d ' ' -f 1 | grep -Fx '{{ rbenv.ruby_version }}'\"\n  register: ruby_selected\n  changed_when: false\n  ignore_errors: yes\n  always_run: yes\n  when: rbenv.env == \"system\"\n  tags:\n    - rbenv\n\n- name: set ruby {{ rbenv.ruby_version }} for system\n  become: yes\n  shell: bash -lc \"rbenv global {{ rbenv.ruby_version }} && rbenv rehash\"\n  when:\n    - rbenv.env == \"system\"\n    - ruby_selected.rc != 0\n  tags:\n    - rbenv\n\n- name: check ruby {{ rbenv.ruby_version }} installed for select users\n  shell: $SHELL -lc \"rbenv versions | grep {{ rbenv.ruby_version }}\"\n  become: yes\n  become_user: \"{{ item }}\"\n  with_items: \"{{ rbenv_users }}\"\n  when: rbenv.env != \"system\"\n  register: ruby_installed\n  changed_when: false\n  ignore_errors: yes\n  always_run: yes\n  tags:\n    - rbenv\n\n- name: install ruby {{ rbenv.ruby_version }} for select users\n  shell: $SHELL -lc \"rbenv install {{ rbenv.ruby_version }}\"\n  become: yes\n  become_user: \"{{ item[1] }}\"\n  with_together:\n    - \"{{ ruby_installed.results }}\"\n    - \"{{ rbenv_users }}\"\n  when:\n    - rbenv.env != \"system\"\n    - item[0].rc != 0\n  ignore_errors: yes\n  tags:\n    - rbenv\n\n- name: check if user ruby version is {{ rbenv.ruby_version }}\n  shell: $SHELL -lc \"rbenv version | cut -d ' ' -f 1 | grep -Fx '{{ rbenv.ruby_version }}'\"\n  become: yes\n  become_user: \"{{ item }}\"\n  with_items: \"{{ rbenv_users }}\"\n  when: rbenv.env != \"system\"\n  register: ruby_selected\n  changed_when: false\n  ignore_errors: yes\n  always_run: yes\n  tags:\n    - rbenv\n\n- name: set ruby {{ rbenv.ruby_version }} for select users\n  shell: $SHELL -lc \"rbenv global {{ rbenv.ruby_version }} && rbenv rehash\"\n  become: yes\n  become_user: \"{{ item[1] }}\"\n  with_together:\n    - \"{{ ruby_selected.results }}\"\n    - \"{{ rbenv_users }}\"\n  when:\n    - rbenv.env != \"system\"\n    - item[0].rc != 0\n  ignore_errors: yes\n  tags:\n    - rbenv\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "2880c831929bdf8b1b788ee2fc79faa464a938e1", "filename": "roles/config-lvm/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nlvm_fstype: \"xfs\"\ndefault_lv_size: \"100%VG\"\n"}, {"commit_sha": "59bde5ac149cca3058d0b8a8bd4b2d0e21e8c861", "sha": "3f1e9020aed99a681e97f29edda9cac313d192d5", "filename": "tasks/section_04_level2.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 4.5 Activate AppArmor (install) (Scored)\n    apt: >\n        name=apparmor\n        state=present\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Activate AppArmor (start) (Scored)\n    service: >\n        name=apparmor\n        state=started\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Activate AppArmor (Scored)\n    command: apparmor_status\n    register: aa_status_lines\n    failed_when: '\"0 profiles are loaded\" in aa_status_lines.stdout_lines or \"0 processes are in complain mode.\" not in aa_status_lines.stdout_lines or \"0 processes are unconfined but have a profile defined.\" not in aa_status_lines.stdout_lines'\n        # - '\"0 processes are unconfined but have a profile defined.\" not in aa_status_lines.stdout_lines'\n    changed_when: False\n    when: travis_env == False\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Activate AppArmor (enforce install) (Scored)\n    apt: >\n        name=apparmor-utils\n        state=present\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Activate AppArmor (enforce) (Scored)\n    #shell: 'aa-enforce /etc/apparmor.d/*'\n    shell: for profile in /etc/apparmor.d/*; do aa-enforce $profile; done\n    register: aaenforce_rc\n    failed_when: aaenforce_rc.rc == 1\n    changed_when: False\n    tags:\n      - section4\n      - section4.5\n"}, {"commit_sha": "5eb44163b7f6328c1f30168fa86326458d5dbaff", "sha": "13e409839c091e4ab7d2a74172c985c349fec696", "filename": "meta/main.yml", "repository": "ansiblebit/oracle-java", "decoded_content": "---\n# file: oracle-java/meta/main.yml\n#\n# meta file\n#\ngalaxy_info:\n  author: Pedro Salgado\n  description: Role to install Oracle Java.\n  company: ansiblebit.org\n  license: BSD\n  min_ansible_version: 1.9.3\n  platforms:\n    - name: CentOS\n      versions:\n        - all\n        - any\n        - 7\n        - 6\n    - name: Debian\n      versions:\n        - jessie\n        - wheezy\n    - name: RedHat\n      versions:\n        - all\n        - any\n        - 7\n        - 6\n    - name: Ubuntu\n      versions:\n        - vivid\n        - trusty\n        - precise\n  galaxy_tags:\n    - development\n    - java\n    - system\ndependencies:\n  - role: ansiblebit.launchpad-ppa-webupd8\n    when: (ansible_distribution | lower == 'debian') or (ansible_distribution | lower == 'ubuntu')\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "7c9b0c487d405d0846fcbfbe96f33a4be3fe5a2e", "filename": "tasks/selinux-RedHat.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n\n- name: Make sure we have the necessary yum packages available for selinux\n  yum:\n    name:\n      - libselinux-python{{ py_suffix }}\n      - libsemanage-python{{ py_suffix }}\n    state: present\n  vars:\n    py_suffix: '{{ \"3\" if ansible_python.version.major == 3 else \"\" }}'\n"}, {"commit_sha": "8c4af8da901c68ce8c4bdd21c62e08cec5d3c23a", "sha": "0e9045144bca23543912b9a3557fdd5f9c9ba165", "filename": "tasks/configure-systemd.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Combine all systemd service configuration options\n  set_fact:\n    _systemd_service_config: \"{{ docker_systemd_service_config_tweaks + docker_systemd_service_config }}\"\n\n- name: Ensure /etc/systemd/system/docker.service.d directory exists\n  become: true\n  file:\n    path: /etc/systemd/system/docker.service.d\n    state: directory\n    mode: 0755\n\n- name: Setup default Docker drop-in to enable use of environment file\n  become: true\n  template:\n    src: drop-ins/default.conf.j2\n    dest: /etc/systemd/system/docker.service.d/default.conf\n  notify: restart docker\n  vars:\n    systemd_envs_dir: \"{{ docker_envs_dir[_docker_os_dist] }}\"\n    systemd_service_conf: \"{{ _systemd_service_config }}\"\n\n- name: Combine Docker daemon environment variable configuration\n  set_fact:\n    docker_service_envs: \"{{ docker_service_envs | combine(_docker_service_opts) | combine(docker_daemon_envs) }}\"\n  vars:\n    _docker_service_opts:\n      DOCKER_OPTS: \"{{ docker_daemon_opts }}\"\n\n- name: Setup Docker environment file {{ docker_envs_dir[_docker_os_dist] }}/docker-envs\n  become: true\n  template:\n    src: docker-envs.j2\n    dest: \"{{ docker_envs_dir[_docker_os_dist] }}/docker-envs\"\n  notify: restart docker\n  vars:\n    docker_envs: \"{{ docker_service_envs }}\"\n"}]