[{"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "8bdb092f99157f54eac8ab3a7c5497a851c1398c", "filename": "roles/config-quay-enterprise/tasks/configure_systemd.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Configure systemd environment files\n  template:\n    src: \"{{ quay_name }}.j2\"\n    dest: \"{{ systemd_environmentfile_dir}}/{{ quay_name }}\"\n  notify: \"Restart quay service\"\n\n- name: Configure systemd unit files\n  template:\n    src: \"{{ quay_service }}.j2\"\n    dest: \"{{ systemd_service_dir}}/{{ quay_service }}\"\n  notify: \"Restart quay service\"\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "8a50e52bf1e859577842fb54f5319a9426fb27e1", "filename": "roles/marathon/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for marathon\n- name: restart marathon\n  sudo: yes\n  service:\n    name: marathon\n    state: restarted\n  sudo: yes\n  notify:\n    - wait for marathon to listen\n\n- name: wait for marathon to listen\n  command: /usr/local/bin/marathon-wait-for-listen.sh\n"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "233f879cc8838489e29b9fbeefea23ddd46f16ad", "filename": "tasks/security_policy_fetch/oracle-fallback.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Download security policy artifact from Oracle OTN\"\n  get_url:\n    url: \"{{ fallback_oracle_security_policy_artifacts[java_major_version|int] }}\"\n    dest: \"{{ download_path }}\"\n    mode: \"0755\"\n    headers: \"Cookie:gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; \\\n      oraclelicense=accept-securebackup-cookie; --no-check-certificate\"\n  until: \"'OK' in FILE_DOWNLOADED.msg\"\n  # I'm convinced that about half of what separates the successful entrepreneurs\n  # from the non-successful ones is pure perseverance. It is so hard.\n  # https://en.wikiquote.org/wiki/Steve_Jobs\n  retries: 15\n  delay: 5\n  delegate_to: \"localhost\"\n  connection: \"local\"\n  become: False\n  register: FILE_DOWNLOADED\n\n- name: \"Downloaded security policy artifact\"\n  set_fact:\n    security_policy_oracle_artifact: \"{{ FILE_DOWNLOADED.dest }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "d2fa58c068f762b41de91c56e529f4cc47a6bc04", "filename": "reference-architecture/gcp/ansible/playbooks/roles/master-http-proxy/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: install haproxy\n  package:\n    name: haproxy\n    state: present\n\n- name: configure haproxy\n  template:\n    src: haproxy.cfg.j2\n    dest: /etc/haproxy/haproxy.cfg\n  notify: restart haproxy\n\n- name: start and enable haproxy service\n  service:\n    name: haproxy\n    enabled: true\n    state: started\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "bef12c5742acb43265c852e1aea1830c3def4b05", "filename": "ops/playbooks/splunk_k8s.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- name: install Splunk Agents for Kubernetes\n  hosts: local\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - ./includes/internal_vars.yml\n\n  environment:\n    - \"{{ env }}\"\n\n  tasks:\n\n  - debug: msg=\"Entering splunk_k8s_meta.yml\"\n\n  - block:\n\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n\n    - name: source stack specific variables\n      include_vars:\n        file: ../templates/splunk/{{ monitoring_stack }}/vars.yml\n\n\n#\n# Retrieve and remember a Token for using the UCP API\n#\n    - name: Retrieve a token for the UCP API\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/auth/login\"\n        headers:\n          Content-Type: application/json\n        method: POST\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        body: '{\"username\":\"{{ ucp_username }}\",\"password\":\"{{ ucp_password }}\"}'\n        use_proxy: no\n      register: login\n      until: login.status == 200\n      retries: 20\n      delay: 5\n\n    - name: Remember the token\n      set_fact:\n        auth_token:  \"{{ login.json.auth_token }}\"\n\n    - include_tasks: includes/config_client.yml\n\n    - name: Copy K8S service account resource file\n      template:\n        src: ../templates/splunk/k8s_serviceaccount.yml.j2\n        dest: /tmp/k8s_serviceaccount.yml\n\n    - name: Deploy the splunk service account\n      shell: |\n        . env.sh\n        kubectl apply -f /tmp/k8s_serviceaccount.yml\n        rm /tmp/k8s_serviceaccount.yml\n      args:\n        chdir: ~/certs.{{ ucp_username }}\n        executable: /usr/bin/bash\n\n    - name: Grant the splunk service account with the role \"View Only\"\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/collectionGrants/system%3Aserviceaccount%3Adefault%3Asplunk/kubernetesnamespaces/viewonly?type=grantobject\"\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: PUT\n        status_code: 201\n        body_format: json\n        validate_certs: no\n        use_proxy: no\n      register: roles\n\n    - name: Copy K8S resource file (ta-k8s-meta and ta-k8s-logs)\n      template:\n        src: ../templates/splunk/k8s_splunkuf.yml.j2\n        dest: /tmp/k8s_splunkuf.yml\n\n\n    - name: Deploy the K8S universal forwarders\n      shell: |\n        . env.sh\n        kubectl apply -f /tmp/k8s_splunkuf.yml\n#        rm /tmp/k8s_splunkuf.yml\n      args:\n        chdir: ~/certs.{{ ucp_username }}\n        executable: /usr/bin/bash\n\n    when: monitoring_stack is defined\n\n  - debug: msg=\"No splunk integration wanted\"\n    when: monitoring_stack is not defined\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "7971bf70061294067f0415292e08cc3afe38616a", "filename": "reference-architecture/osp-cli/ch5.8.1.3_install_openshift-ansible-playbooks.sh", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#!/bin/sh\nsudo yum install -y openshift-ansible-playbooks\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "2056d71f632416bd886abb81df120fbf65423c3d", "filename": "reference-architecture/aws-ansible/playbooks/roles/non-atomic-docker-storage-setup/defaults/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ndocker_dev: \"/dev/xvdb\"\ndocker_vg: \"docker_vol\"\ndocker_data_size: \"95%VG\"\ndocker_dm_basesize: \"3G\"\ncontainer_root_lv_name: \"dockerlv\"\ncontainer_root_lv_mount_path: \"/var/lib/docker\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "53e586ecaa4c2a73249f70e78aa6e6a9c3955e6c", "filename": "playbooks/templates/fsf-client-config.j2", "repository": "rocknsm/rock", "decoded_content": "#!/usr/bin/env python\n#\n# Basic configuration attributes for scanner client.\n#\n\n# 'IP Address' is a list. It can contain one element, or more.\n# If you put multiple FSF servers in, the one your client chooses will\n# be done at random. A rudimentary way to distribute tasks.\nSERVER_CONFIG = { 'IP_ADDRESS' : ['localhost',],\n                  'PORT' : 5800 }\n\n# Full path to debug file if run with --suppress-report\nCLIENT_CONFIG = { 'LOG_FILE' : '{{ fsf_client_logfile }}' }\n"}, {"commit_sha": "e9fb46dc84b9c815a69f6de1347c9ece5db01cc8", "sha": "906324d4b3aa287478680d32743f78bf1eb9b4d2", "filename": "tasks/ivm.yml", "repository": "fubarhouse/ansible-role-nodejs", "decoded_content": "---\n\n- name: \"IVM | Ensure folder requirements are met\"\n  become: yes\n  become_user: root\n  file:\n    path: /usr/local/ivm\n    state: directory\n    mode: 0777\n    owner: root\n\n- name: \"IVM | Clone/Update\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  git:\n    repo: \"{{ ivm_repo }}\"\n    dest: \"{{ fubarhouse_npm.user_dir }}/.ivm\"\n    clone: yes\n    update: yes\n    force: yes\n    version: master\n    recursive: false\n  changed_when: false\n\n- name: \"IVM | Linking\"\n  become: yes\n  become_user: root\n  file:\n    src: \"{{ fubarhouse_npm.user_dir }}/.ivm/bin/ivm\"\n    dest: \"/usr/local/bin/ivm\"\n    state: link\n    force: yes\n  changed_when: false\n\n- name: \"IVM | Ensure shell profiles are configured\"\n  become: yes\n  become_user: \"root\"\n  lineinfile:\n    dest: \"{{ fubarhouse_npm.user_dir }}/{{ item.filename }}\"\n    regexp: 'export NVM_IOJS_ORG_MIRROR=https://iojs.org/dist'\n    line:  'export NVM_IOJS_ORG_MIRROR=https://iojs.org/dist;'\n    state: present\n  with_items:\n    - \"{{ fubarhouse_npm.shell_profiles }}\"\n  ignore_errors: yes"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "bfda1f949fd32a8892cfc436c99e2d2fc4e9217b", "filename": "reference-architecture/azure-ansible/3.6/ansibledeployocp/playbooks/test.yaml.example", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "# used to create a new host group. This host group is then used to wait for each instance to respond to the public IP SSH.\n---\n- hosts: all\n  tasks:\n    - name: Check communications\n      ping:\n    - name: wait for .updateok\n      wait_for: path=/root/.openshiftcomplete timeout=3600\n    - name: Copy kube config to root\n      command: cp -r -f /home/glennswest/.kube /root/.kube\n    - name: GetNodes\n      shell: oc get nodes\n      register: GetNodes\n    - debug: msg=\"{{ GetNodes.stdout_lines }}\"\n    - name: GetTemplates\n      shell: oc get templates -n openshift\n      register: GetTemplates\n    - debug: msg=\"{{ GetTemplates.stdout_lines }}\"\n    - name: NewProject\n      shell: oc new-project test\n      register: NewProject\n    - debug: msg=\"{{ NewProject.stdout_lines }}\"\n    - name: NewNodeJS\n      shell: oc new-app https://github.com/openshift/nodejs-ex.git\n      register: NewNodeJS\n    - debug: msg=\"{{ NewNodeJS.stdout_lines }}\"\n    - name: ExposeNodeJS\n      shell: oc expose svc/nodejs-ex\n      register: ExposeNodeJS\n    - debug: msg=\"{{ ExposeNodeJS.stdout_lines }}\"\n    - name: ListPods1\n      shell: oc get pods --all-namespaces=true\n      register: ListPods1\n    - debug: msg=\"{{ ListPods1.stdout_lines }}\"\n    - pause: \n        minutes: 2\n    - name: ListPods2\n      shell: oc get pods --all-namespaces=true\n      register: ListPods2\n    - debug: msg=\"{{ ListPods2.stdout_lines }}\"\n    - name: PVTest\n      shell: oc process -n openshift  nodejs-mongo-persistent |  oc create -f - \n      register: PVTest\n    - debug: msg=\"{ PVTest.stdout_lines }}\"\n    - pause:\n        minutes: 3\n    - name: ListPods3\n      shell: oc get pods --all-namespaces=true\n      register: ListPods3\n    - debug: msg=\"{{ ListPods3.stdout_lines }}\"\n    \n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "73faec7278b8b83d741a7c393c226a41dd4402a9", "filename": "roles/kalite/tasks/assessment.yml", "repository": "iiab/iiab", "decoded_content": "# This is for an OS other than Fedora 18\n\n# The stock install puts a small /library/ka-lite/content/assessmentitems.sqlite file, so use size instead of exists\n- name: See if assessment is already installed\n  stat: path=\"{{ kalite_root }}/content/assessment/khan/assessmentitems.sqlite\"\n  register: khan_assessment_installed\n\n- name: Run the assessment setup using kalite manage\n  command: \"{{ kalite_program }} manage unpack_assessment_zip {{ downloads_dir }}/khan_assessment.zip\"\n  environment:\n    KALITE_HOME: \"{{ kalite_root }}\"\n  async: 900\n  poll: 10\n  when: not khan_assessment_installed.stat.exists or khan_assessment_installed.stat.size < 20000\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "08d80dd66d1905ba83ef1c2517db835beda3a622", "filename": "examples/playbooks/remote_db.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n- hosts: database\n  remote_user: root\n  become: yes\n  roles:\n    - {role: ovirt-common}\n    - {role: ovirt-engine-remote-db}\n"}, {"commit_sha": "bf6e08dcb2440421477b6536ff6a8d11adc2be17", "sha": "86e6f45e6ad17439b35e06510487c2180946778a", "filename": "roles/docker/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for docker\n- name: remove docker override\n  file:\n    path: /etc/init/docker.override\n    state: absent\n  notify:\n    - restart docker\n  tags:\n    - docker\n\n- name: configure docker graph directory\n  sudo: yes\n  lineinfile:\n    dest: /etc/default/docker\n    state: present\n    regexp: ^DOCKER_OPTS=.*--graph.*\n    line: 'DOCKER_OPTS=\\\"$DOCKER_OPTS --graph={{ docker_graph_dir }}\\\"'\n  notify:\n    - restart docker\n\n- name: configure docker temporary directory\n  sudo: yes\n  lineinfile:\n    dest: /etc/default/docker\n    state: present\n    line: 'DOCKER_TMPDIR=\\\"{{ docker_tmp_dir }}\\\"'\n  notify:\n    - restart docker\n\n- name: ensure docker is running (and enable it at boot)\n  service:\n    name: docker\n    state: started\n    enabled: yes\n  tags:\n    - docker\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "c174a517d78f509a4f8511cf748e697920b8a39b", "filename": "reference-architecture/gcp/ansible/playbooks/openshift-install.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: create instance groups\n  hosts: localhost\n  roles:\n  - instance-groups\n\n- include: ../../../../playbooks/prerequisite.yaml\n- include: ../../../../../openshift-ansible/playbooks/byo/config.yml\n- include: openshift-post.yaml\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "68c998ce8d25b5c45f23ba03c934846d56b007bf", "filename": "playbooks/roles/kafka/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# tasks file for kafka"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7dcd74cb2f000c700f9c4ed11bf8c982cc93ff9a", "filename": "roles/config-httpd/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\ndefault_document_root: \"/var/www/html\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "ae1af07da53dc5c0d8dfbd9309d618010e2bba4e", "filename": "playbooks/README.md", "repository": "redhat-cop/casl-ansible", "decoded_content": "# The CASL Ansible playbooks\n\n## openshift-cluster-seed.yml (openshift-applier)\nThis playbook is mainly here to serve as the execution point for the [openshift-applier](https://github.com/redhat-cop/casl-ansible/tree/master/roles/openshift-applier). There are few important notes to make about how this is executed:\n\n1. Inventory\nTo better integrate with other tools leveraging this playbook, the `hosts` have been defined as `seed-hosts`. This means that the inventory needs to contain a valid group for `seed-hosts`, such as:\n\n```\n[seed-hosts]\nlocalhost\n```\n\n2. Local Execution\nIf the playbook is executed locally, i.e.: on your `localhost`, it's recommended to run with the local option to speed up the execution. This prevents the inventory from being copied to the remote host (even if the remote host is your localhost) and hence avoids the extra time it takes to iterate over the inventory content to copy the local files.\n\n```\n--connection=local\n```\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "83818f18b1c5787efafd20520bb855a348188c01", "filename": "roles/config-routes/tasks/route.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Configure route for the specific interface\"\n  template:\n    src: route.j2 \n    dest: /etc/sysconfig/network-scripts/route-{{ route.device }}\n  with_items:\n  - '{{ routes }}'\n  loop_control:\n    loop_var: route\n  when:\n  - routes is defined\n  - routes.device is defined\n  - routes.device|trim != \"\"\n  notify: 'Notify about Network reload'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "24a8b1a83da76400347ca2c17540fe1e7bece0ca", "filename": "roles/config-linux-desktop/config-xfce/tasks/xfce-Fedora.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: \"Install additional packages for XFCE\"\n  dnf:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n  - '@Xfce Desktop'\n\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "8cf0579f668dd87064a34c267113f22fead3c565", "filename": "playbooks/freebsd.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: FreeBSD / HardenedBSD | Install prerequisites\n  raw: sleep 10 && env ASSUME_ALWAYS_YES=YES sudo pkg install -y python27\n\n- name: FreeBSD / HardenedBSD | Configure defaults\n  raw: sudo ln -sf /usr/local/bin/python2.7 /usr/bin/python2.7\n\n- include: facts/FreeBSD.yml\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "966d219d038b426233eac0f00222e088b71bfb75", "filename": "roles/setup-slack/tasks/invite_users.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Get channels info\n  import_tasks: get_channel_info.yml\n\n- name: Convert channel name to its mapped id\n  set_fact:\n    channels_ids: \"{{ user.channels | map('extract', channel_mapping) | list }}\"\n\n- name: Invite user\n  uri:\n    url: \"https://slack.com/api/users.admin.invite?token={{ slack_token }}&channels={{ channels_ids|join(',') }}&email={{ user.email }}&first_name={{ user.first_name | urlencode }}&last_name={{ user.last_name | urlencode }}&set_active=true\"\n    method: GET\n    status_code: [200]"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "15db91ba53ca4b34bbef5df7ff1185243b8b97d9", "filename": "ops/playbooks/config_networking.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- hosts: vms\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n    - name: Change hostname with FQDN\n      hostname:\n        name: \"{{ inventory_hostname }}.{{ domain_name }}\"\n\n    - name: Update /etc/hosts\n      template:\n        src: ../templates/hosts.j2\n        dest: /etc/hosts\n        unsafe_writes: yes\n\n#    - name: Update DNS settings\n#      template:\n#        src: ../templates/resolv.conf.j2\n#        dest: /etc/resolv.conf\n#        unsafe_writes: yes\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "324b10d0689f671987560627d52f8fd6c37d5d96", "filename": "roles/config-repo-server/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nhosted_isos: []\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "8dc95d8ef270bc6bae521401a91243771303f50f", "filename": "roles/iiab-admin/tasks/admin-user.yml", "repository": "iiab/iiab", "decoded_content": "- name: Create iiab-admin user and password\n  user:\n    name: \"{{ iiab_admin_user }}\"\n    password: \"{{ iiab_admin_passw_hash }}\"\n    update_password: on_create\n    shell: /bin/bash\n\n- name: Create a wheel group\n  group:\n    name: wheel\n    state: present\n\n- name: Create a sudo group (redhat)\n  group:\n    name: sudo\n    state: present\n  when: is_redhat\n\n- name: Add user to wheel group\n  user:\n    name: \"{{ iiab_admin_user }}\"\n    groups: wheel,sudo\n\n- name: Create root .ssh\n  file:\n    path: /root/.ssh\n    owner: root\n    group: root\n    mode: 0700\n    state: directory\n\n- name: Install dummy root keys as placeholder\n  copy:\n    src: dummy_authorized_keys\n    dest: /root/.ssh/authorized_keys\n    owner: root\n    group: root\n    mode: 0600\n    force: no\n\n#        backup=yes\n\n- name: Edit the sudoers file -- first make it editable\n  file:\n    path: /etc/sudoers\n    mode: 0640\n\n- name: Have sudo log all commands it handles\n  lineinfile:\n    regexp: logfile\n    line: \"Defaults     logfile = /var/log/sudo.log\"\n    dest: /etc/sudoers\n    state: present\n\n- name: Lets wheel sudo without password\n  lineinfile:\n    line: \"%wheel ALL= NOPASSWD: ALL\"\n    dest: /etc/sudoers\n\n- name: Remove the line which requires tty\n  lineinfile:\n    regexp: requiretty\n    dest: /etc/sudoers\n    state: absent\n\n- name: End editing the sudoers file -- protect it again\n  file:\n    path: /etc/sudoers\n    mode: 0440\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "7cb37ef8651bf499da7ad527391ce2f464db7b0e", "filename": "roles/logstash/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n- name: Install packages\n  yum:\n    name:\n      - java-11-openjdk-headless\n      - logstash\n    state: present\n\n- name: Add sysconfig file\n  template:\n    src: logstash_sysconfig.j2\n    dest: /etc/sysconfig/logstash\n  notify: Restart logstash\n\n- name: Copy Logstash configs to /etc\n  copy:\n    remote_src: true\n    src: \"{{ rock_module_dir }}/ecs-configuration/logstash/conf.d/\"\n    dest: \"/etc/logstash/conf.d/\"\n    owner: \"{{ logstash_user }}\"\n    group: \"{{ logstash_group }}\"\n\n\n- name: Template input configs\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"/etc/logstash/conf.d/{{ item.dest }}\"\n    owner: \"{{ logstash_user }}\"\n    group: \"{{ logstash_group }}\"\n    mode: 0640\n  notify: Restart logstash\n  when: logstash_configs is defined\n  with_items: \"{{ logstash_configs }}\"\n\n- name: Template Elasticsearch output for Logstash\n  template:\n    src: \"{{ item }}.j2\"\n    dest: \"/etc/logstash/conf.d/{{ item }}.conf\"\n    owner: \"{{ logstash_user }}\"\n    group: \"{{ logstash_group }}\"\n    mode: 0640\n  notify: Restart logstash\n  loop:\n    - logstash-9999-output-elasticsearch.conf\n\n- name: Enable and start Logstash\n  service:\n    name: logstash\n    state: \"started\"\n    enabled: \"{{ 'True' if rock_services | selectattr('name', 'equalto', 'logstash') | map(attribute='enabled') | bool else 'False' }}\"\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "1c899585b0a887d149a183218fb2befdb99845f1", "filename": "tasks/Linux/system.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Perform install from artifacts\n  block:\n    - name: Install requirements\n      package:\n        name: '{{ java_package_requirements }}'\n        state: present\n      register: installed_packages\n      until: installed_packages is succeeded\n      when: transport != 'repositories'\n\n    - name: 'Perform {{ java_binary_type }} install'\n      include_tasks: '{{ install_task }}'\n      with_first_found:\n        - 'install/{{ java_distribution }}_{{ java_binary_type }}.yml'\n        - 'install/{{ java_binary_type }}.yml'\n        - 'install/{{ ansible_os_family }}.yml'\n      loop_control:\n        loop_var: install_task\n\n    - name: Find java_folder\n      find:\n        paths: '{{ java_path }}'\n        recurse: false\n        file_type: directory\n        patterns: '{{ java_folder }}'\n        use_regex: true\n      register: java_dir\n\n    - name: Set actual java directory\n      set_fact:\n        java_folder: \"{{ java_dir.files | map(attribute='path') | list | last | basename }}\"\n\n    - name: Put java profile\n      template:\n        src: java.sh.j2\n        dest: /etc/profile.d/java.sh\n        owner: root\n        group: root\n        mode: 0555\n\n    - name: Check for java binaries existence\n      stat:\n        path: '{{ java_path }}/{{ java_folder }}/bin/{{ binary }}'\n      register: java_binary_collection\n      loop:\n        - java\n        - javac\n        - jar\n        - keytool\n      loop_control:\n        loop_var: binary\n\n    - name: Update alternatives\n      alternatives:\n        name: '{{ java_item.binary }}'\n        path: '{{ java_path }}/{{ java_folder }}/bin/{{ java_item.binary }}'\n        link: '/usr/bin/{{ java_item.binary }}'\n        priority: 100\n      when: java_item.stat.exists | bool\n      loop: '{{ java_binary_collection.results }}'\n      loop_control:\n        loop_var: java_item\n  become: true\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "271c38f3cc09827c2fd5b3d7a5ab561b2e265a44", "filename": "roles/storage-cns/tasks/deprovision.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- name: Login As Super User\n  command: \"oc login -u {{ admin_user }} -p {{ admin_password }}\"\n  when: cluster==\"openshift\"\n        and admin_user is defined\n        and admin_password is defined\n\n- name: Render storage-cns deployment yaml\n  template:\n    src: storage-cns.yml\n    dest: /tmp/storage-cns.yml\n\n- name: Delete storage-cns Resources\n  command: kubectl delete -f /tmp/storage-cns.yml\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "452e1e4d8987fecce26437d0fe57f2c88c30d4bc", "filename": "playbooks/provisioning/openstack/custom_image_check.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Try to get image facts\n  os_image_facts:\n    image: \"{{ image }}\"\n  register: image_result\n- name: Check that custom image is available\n  assert:\n    that: \"image_result.ansible_facts.openstack_image\"\n    msg: \"Image {{ image }} is not available.\"\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "0002ab7869871aabdb1e5d8120e997ba6e556e04", "filename": "tasks/Linux/fetch/zulu-fallback.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: 'Fetch download page'\n  uri:\n    url: \"{{ zulu_api_page }}\\\n      /bundles/latest/\\\n      ?version={{ java_major_version }}\\\n      &ext=tar.gz&os=linux&\\\n      arch={{ (java_arch == 'x64') | ternary('x64', 'x86') }}\"\n    return_content: true\n    follow_redirects: all\n  register: root_page\n\n- name: Find release url\n  set_fact:\n    release_url: >-\n      {{ (root_page.content | from_json).url }}\n\n- name: Find checksum\n  set_fact:\n    checksum: >-\n      {{ (root_page.content | from_json).md5_hash }}\n\n- name: Exit if Zulu version is not found\n  fail:\n    msg: 'Zulu version {{ java_major_version }} not found'\n  when: release_url is not defined\n\n- name: 'Download artifact from {{ release_url }}'\n  get_url:\n    url: '{{ release_url }}'\n    dest: '{{ java_download_path }}'\n    checksum: 'md5:{{ checksum }}'\n  register: file_downloaded\n  retries: 20\n  delay: 5\n  until: file_downloaded is succeeded\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "304c72392dd950e4f49a2827b19ac245426d724c", "filename": "roles/authserver/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: check pip is installed\n  package: name=python-pip\n           state=present\n\n- name: Install xs-authserver from pypi\n  pip: name=xs-authserver\n  when: internet_available\n\n- name: install gunicorn\n  package: name=python-gunicorn\n           state=present\n\n- name: Configure xs-authserver\n  template: backup=yes\n            src={{ item.src }}\n            dest={{ item.dest }}\n            owner=root\n            group=root\n            mode={{ item.mode }}\n  with_items:\n    - src: xs-authserver.env.j2\n      dest: /etc/sysconfig/xs-authserver\n      mode: 0644\n    - src: xs-authserver.service.j2\n      dest: /etc/systemd/system/xs-authserver.service\n      mode: 0644\n\n- name: create database folder\n  file: state=directory\n        path=/var/lib/xs-authserver/\n        owner=root\n        group=root\n        mode=0644\n\n- name: init database\n  command: xs-authserverctl initdb\n  ignore_errors: yes\n  environment:\n    XS_AUTHSERVER_DATABASE: /var/lib/xs-authserver/data.db\n\n- name: Stop authserver service\n  service: name=xs-authserver\n           state=stopped\n           enabled=no\n  when: not authserver_enabled\n\n- name: Start xs-authserver service\n  service: name=xs-authserver\n           state=restarted\n           enabled=yes\n  when: authserver_enabled\n\n- name: add xs-authserver to service list\n  ini_file: dest='{{ service_filelist }}'\n            section=xs-authserver\n            option='{{ item.option }}'\n            value='{{ item.value }}'\n  with_items:\n    - option: name\n      value: XS-authserver\n    - option: description\n      value: '\"xs-authserver implements a seamless web authentication service\n             using XO laptop registration capabilities.  It is heavily inspired\n             by the Moodle OLPC-XS authentication plugin\"'\n    - option: port\n      value: 5000\n    - option: path\n      value: /\n"}, {"commit_sha": "c91b6076e3a957fb0a165131d0ff3b3b208ed419", "sha": "1fd1c3aefe697d2cfc2062e1261c7aa31b240d3d", "filename": "tasks/section_08_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n#  - name: 8.2 Configure rsyslog\n#  The rsyslog software is recommended as a replacement for the default syslogd daemon and\n#  provides improvements over syslogd, such as connection-oriented (i.e. TCP) transmission\n#  of logs, the option to log to database formats, and the encryption of log data en route to a\n#  central logging server.\n#  tags:\n#    - section8\n#    - section8.2\n\n\n  - name: 8.2.1 Install the rsyslog package (Scored)\n    apt: name=rsyslog state=present\n    tags:\n      - section8\n      - section8.2\n      - section8.2.1\n\n  - name: 8.2.2 Ensure the rsyslog Service is activated (Scored)\n    command: grep 'start on filesystem' /etc/rsyslog.conf\n    register: startonfilesystem\n    changed_when: False\n    failed_when: False\n    always_run: True\n    tags:\n      - section8\n      - section8.2\n      - section8.2.5\n\n  - name: 8.2.5.2 Configure rsyslog to Send Logs to a Remote Log Host (Scored)\n    lineinfile:\n       dest=/etc/rsyslog.conf\n       line='start on filesystem'\n       insertafter=EOF\n       state=present\n    when: startonfilesystem.rc == 1\n    tags:\n      - section8\n      - section8.2\n      - section8.2.5\n\n  - name: 8.2.3 Configure /etc/rsyslog.conf (Not Scored)\n    lineinfile:\n       dest=/etc/rsyslog.conf\n       line=\"{{ item }}\"\n       insertafter=EOF\n    with_items:\n      - '*.emerg :omusrmsg:*'\n      - 'mail.* -/var/log/mail'\n      - 'mail.info -/var/log/mail.info'\n      - 'mail.warning -/var/log/mail.warn'\n      - 'mail.err /var/log/mail.err'\n      - 'news.crit -/var/log/news/news.crit'\n      - 'news.err -/var/log/news/news.err'\n      - 'news.notice -/var/log/news/news.notice'\n      - '*.=warning;*.=err -/var/log/warn'\n      - '*.crit /var/log/warn'\n      - '*.*;mail.none;news.none -/var/log/messages'\n      - 'local0,local1.* -/var/log/localmessages'\n      - 'local2,local3.* -/var/log/localmessages'\n      - 'local4,local5.* -/var/log/localmessages'\n      - 'local6,local7.* -/var/log/localmessages'\n    changed_when: False\n    notify: restart rsyslog\n    tags:\n      - section8\n      - section8.2\n      - section8.2.3\n\n  - name: 8.2.4.1 Create and Set Permissions on rsyslog Log Files (Scored)\n    shell: awk '{ print $NF }' /etc/rsyslog.d/* /etc/rsyslog.conf | grep /var/log | sed 's/^-//' | sed 's/)$//' \n    register: result\n    changed_when: False\n    always_run: True\n    tags:\n      - section8\n      - section8.2\n      - section8.2.4\n\n  - name: 8.2.4.2 Create and Set Permissions on rsyslog Log Files (Scored)\n    shell: 'mkdir -p -- \"$(dirname -- {{ item }})\"; touch -- {{ item }}' \n    with_items: result.stdout_lines          \n    changed_when: False\n    tags:\n      - section8\n      - section8.2\n      - section8.2.4\n\n  - name: 8.2.4.3 Create and Set Permissions on rsyslog Log Files (Scored)\n    file: >\n        path='{{item}}' \n        owner=root \n        group=root \n        mode=\"og-rwx\" \n    with_items: result.stdout_lines\n    tags:\n      - section8\n      - section8.2\n      - section8.2.4\n\n  - name: 8.2.5.1 Configure rsyslog to Send Logs to a Remote Log Host (Scored)\n    command: grep \"^*.*[^I][^I]*@\" /etc/rsyslog.conf\n    register: remoteloghost \n    changed_when: False\n    failed_when: False\n    always_run: True\n    tags:\n      - section8\n      - section8.2\n      - section8.2.5\n\n  - name: 8.2.5.2 Configure rsyslog to Send Logs to a Remote Log Host (Scored)\n    lineinfile:\n       dest=/etc/rsyslog.conf\n       line=\"*.* @@{{Remote_Logs_Host_Address}}\"\n       insertafter=EOF\n       state=present\n    when: send_rsyslog_remote == True and remoteloghost.rc == 1\n    tags:\n      - section8\n      - section8.2\n      - section8.2.5\n\n  - name: 8.2.6.1 Accept Remote rsyslog Messages Only on Designated Log Hosts (Not Scored)\n    command: grep '^$ModLoad imtcp' /etc/rsyslog.conf\n    register: moadloadpresent\n    changed_when: False\n    failed_when: False\n    always_run: True\n    tags:\n      - section8\n      - section8.2\n      - section8.2.6\n\n  - name: 8.2.6.2 Accept Remote rsyslog Messages Only on Designated Log Hosts (Not Scored)\n    lineinfile: >\n        dest=/etc/rsyslog.conf\n        regexp='^#({{ item }})'\n        line='{{ item }}'\n        state=present\n    with_items:\n        - '$ModLoad imtcp'\n        - '$InputTCPServerRun 514'\n    when: send_rsyslog_remote == True and moadloadpresent.rc == 1\n    changed_when: False\n    notify: restart rsyslog\n    tags:\n      - section8\n      - section8.2\n      - section8.2.6\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "b044e772219734ae3782cd6e89d9e5fbb490b344", "filename": "roles/serverspec/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for serverspec\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "bbd6a050fb2af5bde4f5076d060ecb3e36e38c99", "filename": "playbooks/automation/check-patch.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "- import_playbook: \"{{ playbook_dir }}/../provider/{{ provider | default('noop') }}/config.yml\"\n- import_playbook: \"{{ playbook_dir }}/../utils/configure-std-ci-repos.yml\"\n  when: std_ci_yum_repos is defined\n- import_playbook: \"{{ playbook_dir }}/../cluster/{{ cluster | default('openshift') }}/config.yml\"\n- import_playbook: \"{{ playbook_dir }}/../cluster-login.yml\"\n- import_playbook: \"{{ playbook_dir }}/../kubevirt.yml\"\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "6a932559747bf4e3ff049cd3fc755521d8817aa3", "filename": "tasks/create_task_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_task\n    args: \"{{ item }}\""}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f697c88666ba168de632fa2c03042f6f85fd6fbf", "filename": "roles/scm/github.com/tests/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "github_org_name: a-github-org\ngithub_api_username: aGithubUser\ngithub_api_password: !vault |\n      $ANSIBLE_VAULT;1.1;AES256\n      38623834633062333931633933636435376166613935643238306138636136663537383634346263\n      38623834633062333931633933636435376166613935643238306138636136663537383634346263\n      38623834633062333931633933636435376166613935643238306138636136663537383634346263\n      38623834633062333931633933636435376166613935643238306138636136663537383634346263\n      3537\n\nteam:\n  name: The Avengers\n\nrepos:\n- repo_name: test-ci-cd\n  description: This is your first Repo\n  private_repo_bool: false\n  has_issues: true\n  has_projects: false\n  has_wiki: true\n  auto_init: false\n  deploy_key_location: \"{{ lookup('file', './files/test-1.pub') }}\"\n  deploy_key_read_only: yes\n  seed_repo_url: https://github.com/rht-labs/labs-ci-cd.git\n- repo_name: test-app\n  description: This is your first Repo\n  private_repo_bool: false\n  has_issues: true\n  has_projects: false\n  has_wiki: true\n  auto_init: false\n  deploy_key_location: \"{{ lookup('file', './files/test-2.pub') }}\"\n  deploy_key_read_only: yes\n  seed_repo_url: https://github.com/rht-labs/labs-ci-cd.git\n\nusers:\n- '\"aGithubUser\"'\n- '\"aGithubUser2\"'"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "d3d505f435591c7536f5d556a6453a869ebe0112", "filename": "roles/keepalived/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'reload sysctl'\n  command: sysctl -p\n\n\n- name: 'start and enable keepalived services'\n  service:\n    name: '{{ item }}'\n    enabled: yes\n    state: started\n  with_items:\n  - keepalived\n\n\n- name: 'restart keepalived'\n  service:\n    name: keepalived \n    state: restarted\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "3dab62df24c22476a7dfbab806489646edac64f6", "filename": "roles/static_inventory/tasks/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Remove any existing inventory\n  file:\n    path: \"{{ inventory_path }}/hosts\"\n    state: absent\n\n- name: Refresh the inventory\n  meta: refresh_inventory\n\n- name: Generate in-memory inventory\n  include: openstack.yml\n\n- name: Checkpoint in-memory data into a static inventory\n  include: checkpoint.yml\n\n- name: Generate SSH config for accessing hosts via bastion\n  include: sshconfig.yml\n  when: use_bastion|bool\n\n- name: Configure SSH tunneling to access UI\n  include: sshtun.yml\n  become: true\n  when:\n    - use_bastion|bool\n    - ui_ssh_tunnel|bool\n"}, {"commit_sha": "3c8d04f3e0875a9baf1f1282f6665b2e7d6871a8", "sha": "4071475d9dfb2ddd93827a69e93783022fa3478e", "filename": "tasks/autoupdate-Debian.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\n- name: Install unattended upgrades package.\n  package: name=unattended-upgrades state=present\n\n- name: Copy unattended-upgrades configuration files in place.\n  template:\n    src: \"../templates/{{ item }}.j2\"\n    dest: \"/etc/apt/apt.conf.d/{{ item }}\"\n    owner: root\n    group: root\n    mode: 0644\n  with_items:\n    - 10periodic\n    - 50unattended-upgrades\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "39746338da29e03d964671bf1e92d88892c67615", "filename": "roles/config-iscsi-client/tasks/iscsi.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: prereq.yml\n- import_tasks: iscsi-config.yml\n- import_tasks: multipath-config.yml\n- import_tasks: lvm-config.yml\n- import_tasks: lock-lvm.yml\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b54267d3e8ad6a53fc1b3732b6278ffb51c53575", "filename": "reference-architecture/gcp/ansible/playbooks/roles/ssh-config-tmp-instance/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: configure ssh for ocp temp instance\n  blockinfile:\n    dest: '{{ ssh_config_file }}'\n    create: true\n    mode: 0600\n    marker: '# {mark} OPENSHIFT ON GCP TEMP INSTANCE BLOCK'\n    state: present\n    block: |\n      Host {{ prefix }}-tmp-instance\n      HostName {{ hostvars[prefix + '-tmp-instance']['gce_public_ip'] }}\n      User cloud-user\n      IdentityFile ~/.ssh/google_compute_engine\n      UserKnownHostsFile ~/.ssh/google_compute_known_hosts\n      HostKeyAlias compute.{{ hostvars[prefix + '-tmp-instance']['gce_id'] }}\n      IdentitiesOnly yes\n      CheckHostIP no\n      StrictHostKeyChecking no\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "fcad0ef3e3ceaa5a05fc10c3ce0079ce9fb5af09", "filename": "reference-architecture/gcp/ansible/playbooks/roles/ssl-certificate/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: check if ssl certificate exists\n  command: gcloud --project {{ gcloud_project }} compute ssl-certificates describe {{ ssl_lb_cert_with_prefix }}\n  register: ssl_cert_exists\n  changed_when: false\n  ignore_errors: true\n\n- block:\n  - block:\n    - name: stat key file\n      stat:\n        path: '{{ master_https_key_file }}'\n      register: https_key_file\n\n    - name: stat cert file\n      stat:\n        path: '{{ master_https_cert_file }}'\n      register: https_cert_file\n\n    - name: check key file\n      assert:\n        that:\n        - 'https_key_file.stat.exists'\n        - 'https_key_file.stat.readable'\n        msg: Master HTTPS key file must exist and it must be readable\n\n    - name: check cert file\n      assert:\n        that:\n        - 'https_cert_file.stat.exists'\n        - 'https_cert_file.stat.readable'\n        msg: Master HTTPS certificate file must exist and it must be readable\n\n    - name: set certificate files facts\n      set_fact:\n        ssl_key: '{{ master_https_key_file }}'\n        ssl_cert: '{{ master_https_cert_file }}'\n        ssl_selfsigned: false\n    when:\n    - master_https_key_file is defined\n    - master_https_key_file is not none\n    - master_https_key_file | trim != ''\n    - master_https_cert_file is defined\n    - master_https_cert_file is not none\n    - master_https_cert_file | trim != ''\n\n  - block:\n    - name: set certificate files facts\n      set_fact:\n        ssl_key: /tmp/ocp-ssl.key\n        ssl_cert: /tmp/ocp-ssl.crt\n        ssl_selfsigned: true\n\n    - name: create self signed certificate\n      command: openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -subj '/C=US/L=Raleigh/O={{ public_hosted_zone }}/CN={{ openshift_master_cluster_public_hostname }}' -keyout '{{ ssl_key }}' -out '{{ ssl_cert }}' creates='{{ ssl_cert }}'\n    when: master_https_key_file is not defined or master_https_key_file is none or master_https_key_file | trim == '' or\n          master_https_cert_file is not defined or master_https_cert_file is none or master_https_cert_file | trim == ''\n\n  - name: create ssl certificate\n    command: gcloud --project {{ gcloud_project }} compute ssl-certificates create {{ ssl_lb_cert_with_prefix }} --private-key \"{{ ssl_key }}\" --certificate \"{{ ssl_cert }}\"\n\n  - name: delete self-signed certificate\n    file:\n      path: '{{ item }}'\n      state: absent\n    with_items:\n    - '{{ ssl_key }}'\n    - '{{ ssl_cert }}'\n    when: ssl_selfsigned\n  when: ssl_cert_exists | failed\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b42ca83afd87284f865ad420b6bc9380a52df46a", "filename": "playbooks/libvirt/openshift-cluster/tasks/configure_libvirt_network.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Create the libvirt network for OpenShift\n  virt_net:\n    name: '{{ libvirt_network }}'\n    state: '{{ item }}'\n    autostart: 'yes'\n    xml: \"{{ lookup('template', 'network.xml') }}\"\n    uri: '{{ libvirt_uri }}'\n  with_items:\n    - present\n    - active\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "518cfc927c881ed6fe264dddc79938065f37eb25", "filename": "roles/rachel/tasks/rachel_enabled.yml", "repository": "iiab/iiab", "decoded_content": "- name: Copy RACHEL httpd conf file\n  template: src=rachel.conf.j2\n            dest=/etc/{{ apache_config_dir }}/rachel.conf\n\n- name: enable Rachel\n  file: path=/etc/apache2/sites-enabled/rachel.conf\n        src=/etc/apache2/sites-available/rachel.conf\n        state=link\n  when: rachel_enabled and is_debuntu\n\n- name: Remove RACHEL conf file if we are disabled\n  file: path=/etc/apache2/sites-enabled/rachel.conf\n        state=absent\n  when: not rachel_enabled and is_debuntu\n\n# This probably doesn't work, but we can't get search to work either\n- name: Create link to rachel mysql db from mysql data dir\n  file: src={{ rachel_mysqldb_path }}\n        dest=/var/lib/mysql/sphider_plus\n        owner=root\n        group=admin\n        state=link\n\n- name: Set mysql password\n  lineinfile: regexp=mysql_password1\n              line=\"$mysql_password1 = '{{ mysql_root_password }}';\"\n              state=present\n              dest={{ rachel_content_path }}/rsphider/settings/database.php\n\n- name: Restart mysqld service\n  service: name={{ mysql_service }}\n           state=restarted\n\n- name: Restart apache2 service\n  service: name={{ apache_service }}\n           state=restarted\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "765e03fdc2e7b223ebce309fe49d00f81c238858", "filename": "playbooks/gce/openshift-cluster/add_nodes.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Launch instance(s)\n  hosts: localhost\n  connection: local\n  become: no\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  vars:\n    oo_extend_env: True\n  tasks:\n  - fail:\n      msg: Deployment type not supported for gce provider yet\n    when: deployment_type == 'enterprise'\n\n  - include: ../../common/openshift-cluster/tasks/set_node_launch_facts.yml\n    vars:\n      type: \"compute\"\n      count: \"{{ num_nodes }}\"\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ node_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"{{ sub_host_type }}\"\n      gce_machine_type: \"{{ lookup('env', 'gce_machine_node_type') | default(lookup('env', 'gce_machine_type'), true) }}\"\n      gce_machine_image: \"{{ lookup('env', 'gce_machine_node_image') | default(lookup('env', 'gce_machine_image'), true) }}\"\n\n  - include: ../../common/openshift-cluster/tasks/set_node_launch_facts.yml\n    vars:\n      type: \"infra\"\n      count: \"{{ num_infra }}\"\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ node_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"{{ sub_host_type }}\"\n      gce_machine_type: \"{{ lookup('env', 'gce_machine_node_type') | default(lookup('env', 'gce_machine_type'), true) }}\"\n      gce_machine_image: \"{{ lookup('env', 'gce_machine_node_image') | default(lookup('env', 'gce_machine_image'), true) }}\"\n\n- include: scaleup.yml\n- include: list.yml\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "35228714e39a4ee12a113b8de9affbcf9ce53096", "filename": "playbooks/manage-jira/README.md", "repository": "redhat-cop/infra-ansible", "decoded_content": "## Jira Project Playbook\nThis playbook is used to automate the creation of project on Jira.\n\n### Example\nPlease refer to the [roles](../../roles/manage-jira/README.md) directory for information regarding the variables required to run this playbook.\n\n### Running the playbook\n`$ ansible-playbook -i inventory playbook.yaml`\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "6c56cc8e4dd2492e5ad72cc50d70cce508ee2989", "filename": "roles/nfs-server/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: prep.yml\n- import_tasks: lvm.yml\n- import_tasks: shares.yml\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "219decd7ce4300d1e39da582857bf842e79e971a", "filename": "roles/config-satellite/tasks/install.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install Satellite with default parameters\"\n  command: > \n    satellite-installer \n      --scenario satellite\n      --foreman-initial-organization \"{{ satellite_organization }}\"\n      --foreman-initial-location \"{{ satellite_location }}\"\n      --foreman-admin-username \"{{ satellite_username }}\"\n      --foreman-admin-password \"{{ satellite_password }}\"\n      --foreman-proxy-dns-managed=false\n      --foreman-proxy-dhcp-managed=false\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "3475ec7643459081ab97439f493f04e6602e2c12", "filename": "roles/0-init/tasks/iiab_ini.yml", "repository": "iiab/iiab", "decoded_content": "# workaround for fact that auto create does not work on ini_file\n- name: Create /etc/iiab/iiab.ini (iiab_config_file)\n  file:\n    dest: \"{{ iiab_config_file }}\"\n    state: touch\n\n- name: Add location section to config file\n  ini_file:\n    dest: \"{{ iiab_config_file }}\"\n    section: location\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: iiab_base\n      value: \"{{ iiab_base }}\"\n    - option: iiab_dir\n      value: \"{{ iiab_dir }}\"\n\n- name: Add version section\n  ini_file:\n    dest: \"{{ iiab_config_file }}\"\n    section: version\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: distribution\n      value: \"{{ ansible_distribution }}\"\n    - option: arch\n      value: \"{{ ansible_architecture }}\"\n    - option: iiab_base_ver\n      value: \"{{ iiab_base_ver }}\"\n    - option: iiab_branch\n      value: \"{{ ansible_local.local_facts.iiab_branch }}\"\n    - option: iiab_commit\n      value: \"{{ ansible_local.local_facts.iiab_commit }}\"\n    - option: install_date\n      value: \"{{ ansible_date_time.iso8601 }}\"\n    - option: install_xo\n      value: \"{{ xo_model }}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "1a84d3164fff643d986f7bc1e23f57e9cffda829", "filename": "roles/wordpress/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "wordpress_download_base_url: https://wordpress.org\nwordpress_src: latest.tar.gz\n\nwp_db_name: iiab_wp\nwp_db_user: iiab_wp\nwp_db_user_password: changeme\n\nwordpress_install: True\nwordpress_enabled: True\n\nwp_install_path: \"{{ content_base }}\"\n#wp_install_path: /library\n\nwp_abs_path: \"{{ wp_install_path }}/wordpress\"\n#wp_abs_path: /library/wordpress\n\nwp_url: /wordpress\nwp_full_url: \"http://{{ iiab_hostname }}{{ wp_url }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8028187b830c6ddbada41867715bf45515bd0852", "filename": "playbooks/update-dns-zones.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Update DNS zones'\n  hosts: dns-zones-manage-host\n  roles:\n  - role: dns/manage-dns-zones\n  tags:\n  - update_dns_zones\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "c081b7be4e5dd7fe132323bb4b5968b7e7a3a806", "filename": "archive/roles/cicd/tasks/maven.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n  \n- name: Install Maven\n  yum: \n    name: maven\n    enablerepo: rhel-7-server-optional-rpms \n    state: present\n  tags: maven\n  \n- name: Copy Maven Settings File\n  copy: \n    src: maven/settings.xml\n    dest: /usr/share/maven/conf/\n  tags: maven"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "84494e2fc065457c5079e8b3e9d063adbd896e72", "filename": "roles/ovirt-collect-logs/vars/engine.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_collect_logs_tar_optional_params: \"--exclude '*/jboss_runtime/*' --exclude '*/branding/*'\"\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "f4a95a26768286b68d04358c0471137842051c7d", "filename": "roles/ovirt-collect-logs/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n- name: Include correct variables based on system\n  include_vars: \"{{ ovirt_collect_logs_from_system }}.yml\"\n\n- name: Ensure required archive tools\n  yum:\n    name: \"{{ item }}\"\n    state: \"present\"\n  with_items:\n    - gzip\n    - tar\n\n# Prepare place to store data\n\n- name: Clean temporary directory and previous archive\n  file:\n    path: \"{{ item }}\"\n    state: \"absent\"\n  with_items:\n    - \"{{ ovirt_collect_logs_tmp_dir }}\"\n    - \"{{ ovirt_collect_logs_archive }}\"\n\n- name: Create temporary directory\n  file:\n    path: \"{{ ovirt_collect_logs_tmp_dir }}\"\n    state: \"directory\"\n    mode: \"0777\"\n\n# Collect common stuff\n- name: Dump system information using shell commands\n  shell: \"{{ item.value }} &> {{ ovirt_collect_logs_tmp_dir }}/{{ item.key }}.txt\"\n  with_dict: \"{{ ovirt_collect_logs_shell_commands }}\"\n  ignore_errors: true\n  tags:\n    - skip_ansible_lint # check for shell module is not working correctly in Lint\n\n- name: Check if /var/log/messages exists else we have to use journalctl\n  stat: path=/var/log/messages\n  register: message_file\n\n- name: Copy journalctl output if /var/log/messages is missing\n  shell: journalctl > \"/var/log/messages\"\n  when: not message_file.stat.exists\n\n- name: Link common logs\n  file:\n    src: \"{{ item.src }}\"\n    dest: \"{{ ovirt_collect_logs_tmp_dir }}/{{ item.dest }}\"\n    state: link\n  with_items:\n    - { src: \"/var/log/messages\", dest: \"messages\" }\n\n# Collect system specific stuff\n\n- include: \"{{ ovirt_collect_logs_from_system }}.yml\"\n\n# Fetch stuff to local system\n- name: Archive logs\n  shell: \"tar {{ ovirt_collect_logs_tar_optional_params}}\n            -hczf {{ ovirt_collect_logs_archive }}\n            {{ ovirt_collect_logs_tmp_dir }}\"\n  ignore_errors: true\n  tags:\n    - skip_ansible_lint # achive module has insufficient functionality\n\n- name: Fetch logs\n  fetch:\n    src: \"{{ ovirt_collect_logs_archive }}\"\n    dest: \"logs/{{ ovirt_collect_logs_from_system }}-{{ ansible_hostname }}/ovirt-engine-logs.tar.gz\"\n    flat: yes\n\n- name: Remove /var/log/messages if it was not there before\n  file:\n    path: /var/log/messages\n    state: absent\n  when: not message_file.stat.exists\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "dd399244f11b02f8ef064b5c3d93e18b2aaa9501", "filename": "roles/config-nagios-target/tasks/nrpe_nfs.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Copy in additional Nagios service plugin\n  copy: \n    src: plugins/check_service.sh\n    dest: /usr/lib64/nagios/plugins/check_service.sh\n    owner: root\n    group: root\n    mode: 0755\n\n- name: Copy nrpe.d NFS configuration files\n  copy: \n    src: nrpe.d/check_nfs.cfg\n    dest: /etc/nrpe.d/check_nfs.cfg\n    owner: root\n    group: root\n    mode: 0644\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "faa397002b6aa9be0f7e436e65d2cd014abf44ca", "filename": "roles/dns/config-dns-server/tasks/named/named.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Setup DNS Logging configuration\n  template:\n    src: named/logging.j2\n    dest: /etc/named/named.conf.logging\n    owner: named\n    group: named\n    mode: 0660\n  notify: restart named\n\n- name: Setup Controls configuration\n  template:\n    src: named/controls.j2\n    dest: /etc/named/named.conf.controls\n    owner: named\n    group: named\n    mode: 0660\n  notify: restart named\n\n- name: Configure named options\n  vars:\n    named_config: \"{{ dns_data.named_global_config }}\"\n  template:\n    src: named/options.j2\n    dest: /etc/named/named.conf.options\n    owner: named\n    group: named\n  notify: restart named\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "86713e1b3144daeb147da2a72e62fb199f3fec89", "filename": "playbooks/osp/inventory/hosts", "repository": "redhat-cop/infra-ansible", "decoded_content": "\n\n[osp_instances]\n\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "aa6cc1ddd9b003edfebc427bf8d9152b678e0dd9", "filename": "reference-architecture/gcp/ansible/playbooks/roles/pre-flight-validation/tasks/check-package.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: check if package {{ item }} is installed\n  command: rpm -q --whatprovides {{ item }}\n  register: package_result\n  ignore_errors: true\n  changed_when: false\n\n- name: assert that package {{ item }} exists\n  assert:\n    that:\n    - package_result | succeeded\n    msg: Package '{{ item }}' is required. Please install it with your package manager, e.g. 'sudo yum install {{ item }}'\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "ad73831b7b217f6660cefeb57ebe603c5c2a8712", "filename": "roles/ipaclient/defaults/main.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "---\n# defaults file for ipaclient\n\nipaclient_force_join: no\nipaclient_mkhomedir: no\nipaclient_kinit_attempts: 5\nipaclient_use_otp: no\nipaclient_allow_repair: no\nipaclient_on_master: no\nipaclient_no_ntp: no\nipaclient_no_dns_lookup: no\nipaclient_ssh_trust_dns: no\nipaclient_no_ssh: no\nipaclient_no_sshd: no\nipaclient_no_sudo: no\nipaclient_no_dns_sshfp: no\nipaclient_force: no\nipaclient_force_ntpd: no\nipaclient_no_nisdomain: no\nipaclient_configure_firefox: no\nipaclient_all_ip_addresses: no\nipassd_fixed_primary: no\nipassd_permit: no\nipassd_enable_dns_updates: no\nipassd_no_krb5_offline_passwords: no\nipassd_preserve_sssd: no\nipaclient_request_cert: no\n\n### packages ###\nipaclient_install_packages: yes\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "05a58db731e4fcffd4dfeb19481d3fe3d615f3b9", "filename": "playbooks/libvirt/openshift-cluster/cluster_hosts.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ng_all_hosts: \"{{ groups['tag_clusterid-' ~ cluster_id] | default([])\n                 | intersect(groups['tag_environment-' ~ cluster_env] | default([])) }}\"\n\ng_etcd_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type-etcd'] | default([])) }}\"\n\ng_lb_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type-lb'] | default([])) }}\"\n\ng_nfs_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type-nfs'] | default([])) }}\"\n\ng_glusterfs_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type-glusterfs'] | default([])) }}\"\n\ng_master_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type-master'] | default([])) }}\"\n\ng_new_master_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type-new-master'] | default([])) }}\"\n\ng_node_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type-node'] | default([])) }}\"\n\ng_new_node_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type-new-node'] | default([])) }}\"\n\ng_infra_hosts: \"{{ g_node_hosts | intersect(groups['tag_sub-host-type-infra'] | default([])) }}\"\n\ng_compute_hosts: \"{{ g_node_hosts | intersect(groups['tag_sub-host-type-compute'] | default([])) }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6770b0e44802521cf82e526dba56a59c5841c5e9", "filename": "reference-architecture/vmware-ansible/playbooks/roles/keepalived_haproxy/defaults/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\nkeepalived_priority_start: 100\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "443005a226af965dcc8370c7c5046393c6d30efe", "filename": "reference-architecture/gcp/ansible/playbooks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- include: gold-image.yaml\n- include: core-infra.yaml\n- include: openshift-install.yaml\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9e2b1cff0e54cc4113996e264f6dd48caae845c5", "filename": "roles/dns/manage-dns-zones/tasks/named/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: determine-action.yml\n\n- block:\n    - import_tasks: prereq.yml\n    - import_tasks: process-views.yml\n    - import_tasks: keys.yml\n    - import_tasks: print_keys.yml\n  when:\n    - named_processing|bool == True\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "2a2244803166ab37a584231a7d6ee97fbf9acb12", "filename": "playbooks/generate-defaults.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- hosts: localhost\n  become: true\n  vars:\n    - config_src: rock_config.yml.j2\n  tags:\n    - common\n    - local\n  tasks:\n    - name: Create config directory\n      file:\n        state: directory\n        owner: root\n        group: root\n        mode: 0755\n        path: \"{{ rock_conf_dir }}\"\n\n    - name: Render template\n      template:\n        backup: true\n        src: \"{{ config_src }}\"\n        dest: \"{{ rock_config }}\"\n        owner: root\n        group: root\n        mode: 0644\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "b7a47c05a1326bd469535173a64555370d45878b", "filename": "roles/owncloud/tasks/owncloud_enabled.yml", "repository": "iiab/iiab", "decoded_content": "# This chould go in computed_network.yml, but here for now\n\n- name: Compute owncloud listen ip addr for owncloud.conf\n  set_fact:\n     owncloud_required_ip: \"{{ ansible_default_ipv4.network }}/{{ ansible_default_ipv4.netmask }}\"\n  when: ansible_default_ipv4.network is defined\n\n- name: Enable owncloud by copying template to httpd config\n  template: src=owncloud.conf.j2\n            dest=/etc/{{ apache_config_dir }}/owncloud.conf\n            owner=root\n            group=root\n            mode=0644\n\n- name: Enable owncloud\n  file: path=/etc/apache2/sites-enabled/owncloud.conf\n        src=/etc/apache2/sites-available/owncloud.conf\n        state=link\n  when: owncloud_enabled and is_debuntu\n\n- name: Disable owncloud\n  file: path=/etc/apache2/sites-enabled/owncloud.conf\n        state=absent\n  when: not owncloud_enabled and is_debuntu\n\n- name: Restart apache, so it picks up the new aliases\n  service: name={{ apache_service }} state=restarted\n\n- name: Run owncloud initial install wizard\n  shell: curl http://{{ iiab_hostname }}{{ owncloud_url }}/index.php\n\n- name: Remove Rewrite URL\n  lineinfile: regexp='overwrite.cli.url'\n              state=absent\n              dest=\"{{ owncloud_prefix }}/owncloud/config/config.php\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "8bd24a8cfc4da50067f18f0bd3d6d4355b68e86b", "filename": "playbooks/libvirt/openshift-cluster/service.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# TODO: need to figure out a plan for setting hostname, currently the default\n# is localhost, so no hostname value (or public_hostname) value is getting\n# assigned\n\n- name: Call same systemctl command for openshift on all instance(s)\n  hosts: localhost\n  become: no\n  connection: local\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  tasks:\n  - fail: msg=\"cluster_id is required to be injected in this playbook\"\n    when: cluster_id is not defined\n\n  - name: Evaluate g_service_masters\n    add_host:\n      name: \"{{ item }}\"\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n      groups: g_service_masters\n    with_items: \"{{ g_master_hosts | default([]) }}\"\n\n  - name: Evaluate g_service_nodes\n    add_host:\n      name: \"{{ item }}\"\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n      groups: g_service_nodes\n    with_items: \"{{ g_node_hosts | default([]) }}\"\n\n- include: ../../common/openshift-node/service.yml\n- include: ../../common/openshift-master/service.yml\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "a6a8bce40aba9a7496e16f2e207cfa885513a70f", "filename": "playbooks/roles/docket/README.md", "repository": "rocknsm/rock", "decoded_content": "rocknsm.docket\n=========\n\nThis role installs and configures Docket\n\nRequirements\n------------\n\npyOpenSSL\n\nRole Variables\n--------------\n\nA description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.\n\nDependencies\n------------\n\nA list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.\n\nExample Playbook\n----------------\n\nIncluding an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:\n\n    - hosts: servers\n      roles:\n         - { role: username.rolename, x: 42 }\n\nLicense\n-------\n\nBSD\n\nAuthor Information\n------------------\n\nAn optional section for the role authors to include contact information, or a website (HTML is not allowed).\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "7498487cfe03ce4f91084428d9459fcaeadd60a4", "filename": "roles/kibana/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n- name: Restart kibana\n  service:\n    name: kibana\n    state: restarted\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c916439a2c614bb7b4679386a29f9a05dca32833", "filename": "roles/config-nagios-target/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# Ensure all prerequisites are met\n- import_tasks: prerequisites.yml\n\n# Setup and prepare NRPE (Nagios Remote Plugin Executor)\n- import_tasks: nrpe.yml\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "eeb7ab25900f0b6f58c1cd3152151adb331fe2a7", "filename": "roles/user-management/manage-local-user-password/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- include_tasks: 'password.yml'\n  when: \n  - user_name is defined\n  - user_name|trim != \"\"\n  - clear_text_password is defined\n  - clear_text_password|trim != \"\"\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "020d827f3e571d3ecf4fd67a22cce6f4e11fdca1", "filename": "reference-architecture/azure-ansible/3.6/ansibledeployocp/playbooks/roles/prepare/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Add EPEL repository\n  yum_repository:\n    name: epel\n    description: EPEL YUM repo\n    mirrorlist: https://mirrors.fedoraproject.org/mirrorlist?repo=epel-7&arch=$basearch\n    enabled: no\n    gpgcheck: no\n  tags: epel\n\n- name: Install required packages\n  yum:\n    name: \"{{ item }}\"\n    state: latest\n    disablerepo: \"epel\"\n  with_items: \"{{ packages }}\"\n  tags: packages\n\n- name: Install EPEL required packages\n  yum:\n    name: \"{{ item }}\"\n    state: latest\n    enablerepo: \"epel\"\n  with_items: \"{{ epelpackages }}\"\n  tags: epelpackages\n\n- name: Install pip required packages\n  pip:\n    name: \"{{ item }}\"\n  with_items: \"{{ pippackages }}\"\n  tags: pip\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "858fad61f52a094350c75b9ff8b66a8fe6262f11", "filename": "roles/docker/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for docker\n- name: Restart docker\n  shell: restart docker\n  sudo: yes\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "862a78d321f69a659a7446966a7782711f803880", "filename": "roles/config-quay-enterprise/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# Base Configurations\nquay_name: quay\nquay_service: \"{{ quay_name }}.service\"\nquay_server_hostname:\nquay_database_type: postgresql\n\n#Systemd\nsystemd_service_dir: /usr/lib/systemd/system\nsystemd_environmentfile_dir: /etc/sysconfig\n\n# Quay\nquay_image: quay.io/coreos/quay:v2.9.2\nquay_config_dir: /var/lib/quay/config\nquay_container_config_dir: /conf/stack\nquay_storage_dir: /var/lib/quay/storage\nquay_container_storage_dir: /datastorage\n\n# External Databases\npostgresql_db_uri: \"postgresql://{{ quay_database_username }}:{{ quay_database_password }}@{{ quay_database_host }}:{{ quay_database_port | default('5432') }}/{{ quay_database_name }}\"\nmysql_db_uri: \"mysql+pymysql://{{ quay_database_username }}:{{ quay_database_password }}@{{ quay_database_host }}:{{ quay_database_port | default('3306') }}/{{ quay_database_name }}\"\n\n# Container Credentials\ncontainer_credentials_file: /root/.docker/config.json\ncontainer_credentials_file_content: {}\nquay_registry_server: quay.io\nquay_registry_auth:\nquay_registry_email:\n\n# Port Configurations\nquay_host_http_port: 80\nquay_container_http_port: 80\nquay_host_https_port: 443\nquay_container_https_port: 443\n\n# SSL\nquay_ssl_enable: True\nquay_ssl_key_file: \"\"\nquay_ssl_cert_file: \"\"\nquay_ssl_generate_city: Raleigh\nquay_ssl_generate_state: NC\nquay_ssl_generate_country: US\nquay_ssl_generate_organization: Red Hat\nquay_ssl_generate_organizational_unit: CoP\nquay_ssl_generate_days_validity: 365\nquay_ssl_local_tmp_dir: \"/tmp\"\nquay_ssl_delete_generated_cert: True\n\n# Clair\nquay_clair_enable: False\nquay_clair_endpoint: \"\"\n\n# Superuser Configuration\nquay_superuser_username: \"\"\nquay_superuser_password: \"\"\nquay_superuser_email: \"\""}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "04eab30065254275bece22cf99f0c7708e423a9b", "filename": "roles/dnsmasq/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for dnsmasq\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "002953d6de58465657ee785cd5a3fe3239ba4802", "filename": "roles/marathon/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for marathon\nmarathon_port: 8080\nconsul_dir: /etc/consul.d\nmarathon_local_address: \"{{ansible_eth0.ipv4.address}}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "313924dccce2c424ed41b8d028cec34dd777f0ca", "filename": "roles/osp/admin-nova-service/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Disable nova compute services on selected hosts\"\n  shell: >\n    source {{ admin_keystonerc_file }};\n    openstack compute service set --disable {{ item }} nova-compute\n  with_items:\n  - \"{{ ansible_play_hosts }}\"\n  when:\n  - hostvars[item].nova_service is defined\n  - hostvars[item].nova_service == 'disabled'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ac77f536c8c3ae0b64070d9f560d8ac680fc1569", "filename": "roles/install-mongodb/tests/install_mongodb.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Install mongodb\n  hosts: dbserver\n  become: yes\n  vars:\n    mongodb_ver: 3.4\n    os_family: redhat\n    os_ver: 7\n  roles:\n    - install-mongodb"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "e0061bb773502f5c58d2d2afdc1459e06fbde8f3", "filename": "reference-architecture/aws-ansible/playbooks/roles/openshift-versions/library/openshift_facts.py", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "/usr/share/ansible/openshift-ansible/roles/openshift_facts/library/openshift_facts.py"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "a8566f14f8bc3d7dc24c5ef16a8efc83b4f8bc68", "filename": "roles/consul/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for consul\n- name: remove consul override\n  command: /bin/rm -f /etc/init/consul.override\n\n- name: start consul\n  service: name=consul state=started\n\n- name: configure consul\n  sudo: yes\n  template: src=consul.json.j2 dest=/etc/consul.d/consul.json owner=root group=root mode=0644\n  notify:\n    - Restart consul\n  tags:\n    - consul\n\n- name: configure atlas for consul\n  sudo: yes\n  template: src=atlas.json.j2 dest=/etc/consul.d/atlas.json owner=root group=root mode=0644\n  when: consul_atlas_join\n  notify:\n    - Restart consul\n  tags:\n    - consul\n\n- name: remove consul-join override\n  command: /bin/rm -f /etc/init/consul-join.override\n  when: consul_join is defined\n\n- name: configure consul-join\n  sudo: yes\n  template: src=consul-join.j2 dest=/etc/service/consul-join owner=root group=root mode=0644\n  notify:\n    - Restart consul\n  when: consul_join is defined\n  tags:\n    - consul\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "49dc1027f4ca5045b721dbb2673877394bc405bf", "filename": "dev/playbooks/config_subscription.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: vms\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Clean Any Subscription Data\n      shell:\n        cmd: subscription-manager clean\n\n    - name: Register now\n      shell:\n        cmd: subscription-manager register --org=\"{{rhn_orgid}}\" --activationkey=\"{{rhn_key}}\" \n        #cmd: subscription-manager register --username=\"{{redhat_user}}\" --password=\"{{redhat_pass}}\" --auto-attach\n      register: res\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "cc302da343b528868f35a6f316ab161fdbba7436", "filename": "roles/manage-confluence-space/tasks/download_attachment.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Download attachment from source\n  get_url:\n    url: '{{ confluence.space.url }}/wiki/{{ attachment_data._links.download }}'\n    dest: '{{ attachment_tempdir.path }}/{{ attachment_data.title }}'\n    force: yes\n    force_basic_auth: yes\n    url_username: '{{ confluence.source.username }}'\n    url_password: '{{ confluence.source.password }}'\n  no_log: true\n  delegate_to: 127.0.0.1\n\n- name: Upload attachment to destination\n  command: 'curl -u {{ confluence.destination.username }}:{{ confluence.destination.password }} -X POST -H \"X-Atlassian-Token: no-check\" -F \"file=@{{ attachment_tempdir.path }}/{{ attachment_data.title }}\" {{ confluence.destination.url }}/wiki/rest/api/content/{{ confluence_content_ids.value.id }}/child/attachment'\n  delegate_to: 127.0.0.1\n"}, {"commit_sha": "bf6e08dcb2440421477b6536ff6a8d11adc2be17", "sha": "4b9f189fd78e1d72f4b8ff7f06e4b6cd40637f65", "filename": "roles/mesos/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for mesos\nmesos_zk_port: 2181\nmesos_zookeeper_group: zookeeper_servers\nmesos_master_port: 5050\nconsul_dir: /etc/consul.d\nmesos_executor_registration_timeout: 10mins\nmesos_cluster_name: \"Cluster01\"\nmesos_containerizers: \"docker,mesos\"\nmesos_resources: \"ports(*):[31000-32000]\"\nmesos_slave_work_dir: \"/tmp/mesos\"\nmesos_ip: \"{{ ansible_default_ipv4.address }}\"\nmesos_hostname: \"{{ inventory_hostname }}\"\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "0c411a69d34de1e274b279b9c47dfc9041916a61", "filename": "tasks/Win32NT/install/sapjvm_tarball.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Check that the java_folder exists\n  win_stat:\n    path: '{{ java_path }}\\{{ java_folder }}/bin'\n  register: java_folder_bin\n\n- name: Install java from tarball\n  block:\n    - name: Mkdir for java installation\n      win_file:\n        path: '{{ java_path }}\\{{ java_folder }}'\n        state: directory\n\n    - name: Create temporary directory\n      win_tempfile:\n        state: directory\n      register: temp_dir_path\n\n    - name: Unarchive to temporary directory\n      win_unzip:\n        src: '{{ java_artifact }}'\n        dest: '{{ temp_dir_path }}'\n\n    - name: Find java_folder in temp\n      win_find:\n        paths: '{{ temp_dir_path }}'\n        recurse: false\n        file_type: directory\n      register: java_temp_folder\n\n    - name: Copy from temporary directory\n      win_copy:\n        src: '{{ java_temp_folder.files | map(attribute=\"path\") | list | last }}\\'\n        dest: '{{ java_path }}\\{{ java_folder }}'\n        remote_src: true\n\n    - name: Check choco\n      win_chocolatey:\n        name: chocolatey\n        state: present\n\n    # https://help.sap.com/viewer/65de2977205c403bbc107264b8eccf4b/Cloud/en-US/76137f42711e1014839a8273b0e91070.html\n    - name: 'Install vcredist package prior to using SAP JVM'\n      win_chocolatey:\n        name: vcredist2013\n      register: choco_install\n      retries: 15\n      delay: 5\n      until: choco_install is succeeded\n  when: not java_folder_bin.stat.exists\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "999c0e3f3996a15df7a11916acc41f825406e45b", "filename": "ops/playbooks/includes/storage_driver_devicemapper.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n        \n    - name: Check for partitions on disk\n      parted:\n        state: info\n        device: \"{{ disk2 }}\"\n        number: 1\n      register: DiskInfo\n\n    - set_fact:\n        partPresent: \"{{ DiskInfo.partitions[0] is defined }}\"\n\n    - name: Create partition on second disk\n      parted:\n        label: gpt\n        part_type: primary\n        device: \"{{ disk2 }}\"\n        flags: [ lvm ]\n        state: present\n        number: 1\n        part_start: 0%\n        part_end: 100%\n      when: partPresent == false\n\n    - name: Create Docker VG\n      lvg:\n        vg: docker\n        pvs: \"{{ disk2_part }}\"\n      when: partPresent == false\n\n    - name: Create thinpool LV\n      lvol:\n        lv: thinpool\n        opts: --wipesignatures y\n        vg: docker\n        size: 95%VG\n      when: partPresent == false\n\n    - name: Create thinpoolmeta LV\n      lvol:\n        lv: thinpoolmeta\n        opts: --wipesignatures y\n        vg: docker\n        size: 1%VG\n      when: partPresent == false\n\n    - name: Convert LVs to thinpool and storage for metadata\n      command: lvconvert -y --zero n -c 512K  --thinpool docker/thinpool --poolmetadata docker/thinpoolmeta\n      when: partPresent == false\n\n    - name: Config thinpool profile\n      copy: src=../files/docker-thinpool.profile dest=/etc/lvm/profile/docker-thinpool.profile\n\n    - name: Apply the LVM profile\n      command: lvchange --metadataprofile docker-thinpool docker/thinpool\n#      ignore_errors: yes\n\n    - name: Enable monitoring for LVs\n      command: lvs -o+seg_monitor\n\n    - name: Create /etc/docker directory\n      file:\n        path: /etc/docker\n        state: directory\n\n    - name: Config Docker daemon\n      template: src=../templates/daemon.devicemapper.json.j2 dest=/etc/docker/daemon.json\n\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "878877b0776c44f55fc4e458f70840f31da5bb01", "filename": "ops/playbooks/roles/hpe.openports/tests/inventory", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "localhost\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "5156789e7248ebc49603201f3464b39ae1b7af01", "filename": "playbooks/libvirt/openshift-cluster/vars.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ndefault_pool_path: \"{{ lookup('env','HOME') }}/libvirt-storage-pool-openshift-ansible\"\nlibvirt_storage_pool_path: \"{{ lookup('oo_option', 'libvirt_storage_pool_path') | default(default_pool_path, True) }}\"\nlibvirt_storage_pool: \"{{ lookup('oo_option', 'libvirt_storage_pool') | default('openshift-ansible', True) }}\"\nlibvirt_network: \"{{ lookup('oo_option', 'libvirt_network') | default('openshift-ansible', True) }}\"\nlibvirt_instance_memory_mib: \"{{ lookup('oo_option', 'libvirt_instance_memory_mib') | default(1024, True) }}\"\nlibvirt_instance_vcpu: \"{{ lookup('oo_option', 'libvirt_instance_vcpu') | default(2, True) }}\"\nlibvirt_uri: \"{{ lookup('oo_option', 'libvirt_uri') | default('qemu:///system', True) }}\"\ndebug_level: 2\n\n# Automatic download of the qcow2 image for RHEL cannot be done directly from the RedHat portal because it requires authentication.\n# The default value of image_url for enterprise and openshift-enterprise deployment types below won't work.\ndeployment_rhel7_ent_base:\n  image:\n    url: \"{{ lookup('oo_option', 'image_url') |\n             default('https://access.cdn.redhat.com//content/origin/files/sha256/25/25f880767ec6bf71beb532e17f1c45231640bbfdfbbb1dffb79d2c1b328388e0/rhel-guest-image-7.2-20151102.0.x86_64.qcow2', True) }}\"\n    name: \"{{ lookup('oo_option', 'image_name') |\n              default('rhel-guest-image-7.2-20151102.0.x86_64.qcow2', True) }}\"\n    sha256: \"{{ lookup('oo_option', 'image_sha256') |\n                default('25f880767ec6bf71beb532e17f1c45231640bbfdfbbb1dffb79d2c1b328388e0', True) }}\"\n    compression: \"\"\n  ssh_user: openshift\n  become: yes\n\ndeployment_vars:\n  origin:\n    image:\n      url: \"{{ lookup('oo_option', 'image_url') |\n               default('http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud-1602.qcow2.xz', True) }}\"\n      compression: \"{{ lookup('oo_option', 'image_compression') |\n                       default('xz', True) }}\"\n      name: \"{{ lookup('oo_option', 'image_name') |\n                default('CentOS-7-x86_64-GenericCloud.qcow2', True) }}\"\n      sha256: \"{{ lookup('oo_option', 'image_sha256') |\n                  default('dd0f5e610e7c5ffacaca35ed7a78a19142a588f4543da77b61c1fb0d74400471', True) }}\"\n    ssh_user: openshift\n    become: yes\n  enterprise: \"{{ deployment_rhel7_ent_base }}\"\n  openshift-enterprise: \"{{ deployment_rhel7_ent_base }}\"\n  atomic-enterprise: \"{{ deployment_rhel7_ent_base }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "38a4d726b1471a5f2adb0bad4650155b02587d52", "filename": "reference-architecture/rhv-ansible/playbooks/openshift-validate.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: yes\n  become: no\n  pre_tasks:\n  - name: set fact\n    set_fact:\n      openshift_master_cluster_public_hostname: \"{{ openshift_master_cluster_public_hostname }}\"\n  - name: set fact\n    set_fact:\n      openshift_master_cluster_hostname: \"{{ openshift_master_cluster_hostname }}\"\n  - name: set fact\n    set_fact:\n      console_port: \"{{ console_port}}\"\n  - name: set fact\n    set_fact:\n      wildcard_zone: \"{{app_dns_prefix}}.{{public_hosted_zone}}\"\n  roles:\n  # Group systems\n  - instance-groups\n  - validate-public\n\n- hosts: masters\n  gather_facts: no\n  roles:\n  - validate-masters\n\n- hosts: masters\n  gather_facts: yes\n  roles:\n  - validate-etcd\n\n- hosts: single_master\n  gather_facts: yes\n  become: yes\n  roles:\n  - validate-app\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "0161a1f6a0b2dc55e9f4dde0581323e4bad886e5", "filename": "roles/update-instances/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Gather facts\n  openshift_facts:\n    role: common\n\n- block:\n  - name: Clear yum cache\n    command: \"yum clean all\"\n    ignore_errors: true\n\n  - name: Update rpms\n    package:\n      name: \"*\"\n      state: latest\n\n  when: not openshift.common.is_atomic | bool\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "fe13dc17990e2b961d9bd5593bf8ee7e7842d6fb", "filename": "reference-architecture/vmware-ansible/playbooks/roles/etcd-storage/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Create openshift volume group\n  lvg: vg=etcd_vg pvs=/dev/sdd\n\n- name: Create lvm volumes\n  lvol: vg=etcd_vg lv=etcd_lv size=95%FREE state=present shrink=no\n\n- name: Create local partition on lvm lv\n  filesystem:\n    fstype: xfs\n    dev: /dev/etcd_vg/etcd_lv\n\n- name: Make mounts owned by nfsnobody\n  file: path=/var/lib/etcd state=directory mode=0755\n\n- name: Mount the partition\n  mount:\n    name: /var/lib/etcd\n    src: /dev/etcd_vg/etcd_lv\n    fstype: xfs\n    state: present\n\n- name: Remount new partition\n  command: \"mount -a\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "d4a2cdaa6072534e23054883745ef596a5bf7f42", "filename": "roles/config-postgresql/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Restart PostgreSQL Service\n  systemd:\n    name: \"{{ postgresql_service }}\"\n    enabled: yes\n    state: restarted\n    daemon_reload: yes\n\n- name: restart firewalld\n  service:\n    name: firewalld\n    state: restarted\n\n- name: restart iptables\n  service:\n    name: iptables\n    state: restarted\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "362176b1712b84767d3cd2bbed53901106fc14fb", "filename": "reference-architecture/vmware-ansible/playbooks/haproxy.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - create-vm-haproxy\n    - instance-groups\n\n- name: Deploy ha proxy server\n  hosts: haproxy_group\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - rhsm\n    - vmware-guest-setup\n  ignore_errors: yes\n\n- name: Configure ha proxy server\n  hosts: haproxy_group\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - haproxy-server\n\n- name: Configure ha proxy server files\n  hosts: haproxy_group, master, infra\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - haproxy-server-config\n\n- name: Configure ha proxy server files\n  hosts: haproxy_group\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - keepalived_haproxy\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "61fd7b40f571337ec2bc88611abdb2e2c3a589a4", "filename": "roles/config-packages/tests/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nlist_of_packages_to_install:\n - 'vim'\n - 'git'\n - 'ansible'\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "0801a477f0408cc45fa12bd9b7a5a105d637828c", "filename": "roles/network/tasks/avahi.yml", "repository": "iiab/iiab", "decoded_content": "- name: Create a user for avahi\n  user: name=avahi\n        createhome=no\n        shell=/bin/false\n  when: is_debuntu\n\n- name: Install avahi announce config files\n  template: src=avahi/schoolserver.service\n            dest=/etc/avahi/services/schoolserver.service\n            owner=avahi\n            group=avahi\n            mode=0640\n  when: 'gui_wan == True'\n\n- name: Find a clean copy of ssh.service\n  shell: \"ls /usr/share/doc/ |grep avahi | head -n1\"\n  register: avahi_ver\n  ignore_errors: True\n  changed_when: false\n\n- name: Grab a clean copy of ssh.service\n  copy: src='/usr/share/doc/{{ avahi_ver.stdout }}/ssh.service'\n        dest='/etc/avahi/services/'\n  when: avahi_ver.stdout != \"\" and not is_debuntu\n\n- name: Grab a clean copy of ssh.service\n  copy: src='/usr/share/doc/avahi-daemon/examples/ssh.service'\n        dest='/etc/avahi/services/'\n  when: is_debuntu\n\n- name: Set ssh port for avahi\n  lineinfile: dest=/etc/avahi/services/ssh.service\n              regexp='</port>$'\n              line='    <port>{{ ssh_port }}</port>'\n              state=present\n              backrefs=yes\n\n- name: Enable avahi service\n  service: name=avahi-daemon\n           enabled=yes\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "183ec5f827027c56cc713113ddc9b97945f140c5", "filename": "playbooks/templates/rock_config.yml.j2", "repository": "rocknsm/rock", "decoded_content": "---\n# These are all the current variables that could affect\n# the configuration of ROCKNSM. Take care when modifying\n# these. The defaults should be used unless you really\n# know what you are doing!\n\n# interfaces that should be configured for sensor applications\nrock_monifs:\n  {% for item in rock_monifs %}\n  - {{ item }}\n  {% endfor %}\n\n# Secifies the hostname of the sensor\nrock_hostname: {{ rock_hostname }}\n# the FQDN\nrock_fqdn: {{ rock_fqdn }}\n# the number of CPUs that bro will use\nbro_cpu: {{ bro_cpu }}\n# name of elasticsearch cluster\nes_cluster_name: {{ es_cluster_name }}\n# name of node in elasticsearch cluster\nes_node_name: {{ es_node_name }}\n# how much memory to use for elasticsearch\nes_mem: {{ es_mem }}\n# (optional) personal configured key for pulled pork to pull latest sigs from snort.org\npulled_pork_oinkcode: {{ pulled_pork_oinkcode }}\n\n########## Offline/Enterprise Network Options ##############\n\n# configure if this system may reach out to the internet\n# (configured repos below) during configuration\nrock_online_install: {{ rock_online_install }}\n# (online) enable RockNSM testing repos\nrock_enable_testing: {{ rock_enable_testing }}\n# (online) the URL for the EPEL repo mirror\nepel_baseurl: {{ epel_baseurl }}\n# (online) the URL for the EPEL GPG key\nepel_gpgurl: {{ epel_gpgurl }}\n# (online) the URL for the elastic repo mirror\nelastic_baseurl: {{ elastic_baseurl }}\n# (online) the URL for the elastic GPG key\nelastic_gpgurl: {{ elastic_gpgurl }}\n# (online) the URL for the rocknsm repo mirror\nrocknsm_baseurl: {{ rocknsm_baseurl }}\n# (online) the URL for the rocknsm GPG key\nrocknsm_gpgurl: {{ rocknsm_gpgurl }}\n\n# (offline) the filesytem path for a local repo if doing an \"offline\" install\nrocknsm_local_baseurl: {{ rocknsm_local_baseurl }}\n\n# the git repo from which to checkout rocknsm customization scripts for bro\nbro_rockscripts_repo: {{ bro_rockscripts_repo }}\n\n# the git repo from which pulled pork should be installed\npulled_pork_repo: {{ pulled_pork_repo }}\n\n#### Retention Configuration ####\nelastic_close_interval: {{ elastic_close_interval }}\nelastic_delete_interval: {{ elastic_delete_interval }}\nkafka_retention: {{ kafka_retention }}\nsuricata_retention: {{ suricata_retention }}\nbro_log_retention: {{ bro_log_retention }}\nbro_stats_retention: {{ bro_stats_retention }}\n\n### Advanced Feature Selection ######\n# Don't flip these unless you know what you're doing\nwith_stenographer: {{ with_stenographer }}\nwith_docket: {{ with_docket }}\nwith_bro: {{ with_bro }}\nwith_suricata: {{ with_suricata }}\nwith_snort: {{ with_snort }}\nwith_pulledpork: {{ with_pulledpork }}\nwith_logstash: {{ with_logstash }}\nwith_elasticsearch: {{ with_elasticsearch }}\nwith_kibana: {{ with_kibana }}\nwith_zookeeper: {{ with_zookeeper }}\nwith_kafka: {{ with_kafka }}\nwith_nginx: {{ with_nginx }}\nwith_lighttpd: {{ with_lighttpd }}\nwith_fsf: {{ with_fsf }}\n\n# Specify if a service is enabled on startup\nenable_stenographer: {{ enable_stenographer }}\nenable_docket: {{ enable_docket }}\nenable_bro: {{ enable_bro }}\nenable_suricata: {{ enable_suricata }}\nenable_snort: {{ enable_snort }}\nenable_pulledpork: {{ enable_pulledpork }}\nenable_logstash: {{ enable_logstash }}\nenable_elasticsearch: {{ enable_elasticsearch }}\nenable_kibana: {{ enable_kibana }}\nenable_zookeeper: {{ enable_zookeeper }}\nenable_kafka: {{ enable_kafka }}\nenable_nginx: {{ enable_nginx }}\nenable_lighttpd: {{ enable_lighttpd }}\nenable_fsf: {{ enable_fsf }}\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "dbd459fef5e3c16b0b04835c89ec7fa924cdef56", "filename": "roles/vpn/tasks/ubuntu.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- set_fact:\n    strongswan_additional_plugins: []\n\n- name: Ubuntu | Install StrongSwan\n  apt: name=strongswan state=latest update_cache=yes install_recommends=yes\n\n- name: Ubuntu | Enforcing ipsec with apparmor\n  shell: aa-enforce \"{{ item }}\"\n  when: apparmor_enabled is defined and apparmor_enabled == true\n  with_items:\n    - /usr/lib/ipsec/charon\n    - /usr/lib/ipsec/lookip\n    - /usr/lib/ipsec/stroke\n  notify:\n    - restart apparmor\n  tags: ['apparmor']\n\n- name: Ubuntu | Enable services\n  service: name={{ item }} enabled=yes\n  with_items:\n    - apparmor\n    - strongswan\n    - netfilter-persistent\n\n- name: Ubuntu | Ensure that the strongswan service directory exist\n  file: path=/etc/systemd/system/strongswan.service.d/ state=directory mode=0755  owner=root group=root\n\n- name: Ubuntu | Setup the cgroup limitations for the ipsec daemon\n  template: src=100-CustomLimitations.conf.j2 dest=/etc/systemd/system/strongswan.service.d/100-CustomLimitations.conf\n  notify:\n    - daemon-reload\n    - restart strongswan\n\n- include: iptables.yml\n  tags: iptables\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "ded2f5e0ed74371b3817386ed9f913667c668b76", "filename": "roles/ipaclient/vars/Ubuntu.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# vars/Ubuntu.yml\nipaclient_packages: [ \"freeipa-client\" ]\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "e564c918a5c92d5e331ae535fcc5817c36a64ff2", "filename": "meta/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "galaxy_info:\n  role_name: docker_ce\n  author: Bjorn Oscarsson\n  company: none\n  description: \"Installs and configures Docker Community Edition (CE)\"\n  min_ansible_version: 2.6\n  license: MIT\n  platforms:\n  - name: Fedora\n    versions:\n      - 25\n      - 26\n      - 27\n      - 28\n      - 29\n      - 30\n\n  - name: EL\n    versions:\n      - 7\n\n  - name: Debian\n    versions:\n      - wheezy\n      - jessie\n      - stretch\n      - buster\n\n  - name: Ubuntu\n    versions:\n      - trusty\n      - artful\n      - xenial\n      - bionic\n      - cosmic\n\n  galaxy_tags:\n    - docker\n    - containers\n    - virtualization\n    - compose\n    - orchestration\n    - system\n\ndependencies: []\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "fc5bf254a268d2e1f000c4173b9a95a70650e39d", "filename": "roles/osp/packstack-install/tasks/sync-keys.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Fetch the private SSH key from the first host'\n  fetch:\n    src: \"~/.ssh/id_rsa\"\n    dest: \"/tmp/id_rsa\"\n    flat: yes\n  run_once: true\n  delegate_to: \"{{ ansible_play_hosts | first }}\"\n\n- name: 'Fetch the public SSH key from the first host'\n  fetch:\n    src: \"~/.ssh/id_rsa.pub\"\n    dest: \"/tmp/id_rsa.pub\"\n    flat: yes\n  run_once: true\n  delegate_to: \"{{ ansible_play_hosts | first }}\"\n\n- name: 'Ensure hosts have the SSH private key loaded'\n  copy:\n    src: \"/tmp/id_rsa\"\n    dest: \"~/.ssh/id_rsa\"\n    force: yes\n    mode: 0600\n\n- name: 'Ensure hosts have the SSH public key loaded'\n  authorized_key:\n    user: root\n    state: present\n    key: \"{{ lookup('file', '/tmp/id_rsa.pub') }}\"\n\n- name: \"Clean up files\"\n  file:\n    path: \"{{ item }}\"\n    state: absent\n  with_items:\n  - '/tmp/id_rsa'\n  - '/tmp/id_rsa.pub'\n  run_once: true\n  delegate_to: \"localhost\"\n"}, {"commit_sha": "6d10af54bdbf8e81c3d90a70ffea87b4d2c20eb2", "sha": "8e740bea359aed922e64142595117196fe5fea97", "filename": "handlers/main.yml", "repository": "Oefenweb/ansible-wordpress", "decoded_content": "# handlers file for wordpress\n---\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "dc49787c39fbcd359e1178f1dfeff4ef73c73c3d", "filename": "roles/seed-git-server/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Create OpenShift Git Repository Content Home\n  file:\n    path: \"{{ item }}\"\n    state: directory\n    owner: \"{{ git_user }}\"\n    group: \"{{ git_user }}\"\n  with_items:\n    - \"{{ git_repo_home }}\"\n    - \"{{ openshift_git_repo_home }}\"\n\n- name: Clone OpenShift Examples\n  git:\n    repo: \"{{ item }}\"\n    bare: yes\n    dest: \"{{ openshift_git_repo_home }}/{{ item | basename }}\"\n  with_items:\n    - \"{{ openshift_example_repos }}\"\n  become: yes\n  become_user: \"{{ git_user }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c093d4b36b49b877f4ec06f77930e79923f88b7e", "filename": "playbooks/identity.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Configure IdM/IPA'\n  hosts: idm-clients\n  roles:\n  - role: config-ipa-client\n  tags: \n  - configure_idm_client\n"}, {"commit_sha": "4a9aaf0951e383c57077cf651b93e78eeea1b5ac", "sha": "e2f1c9e1c44e49944dbac9a08952a1504029db57", "filename": "handlers/main.yml", "repository": "geerlingguy/ansible-role-solr", "decoded_content": "---\n- name: restart solr\n  service:\n    name: \"{{ solr_service_name }}\"\n    state: restarted\n    sleep: 5\n  when: solr_restart_handler_enabled\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "de3985cd332c4586884445d7575a713bb2c43f84", "filename": "ops/playbooks/includes/monitoring_splunk.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n\n  - block:\n\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n\n    - name: source stack specific variables\n      include_vars:\n        file: ../templates/monitoring/{{ monitoring_stack }}/vars.yml\n\n#\n# section for the logger VM which is used to collect syslog from the ESX inrastructure and the UCP syslogs\n#\n    - block:\n\n      - name: Open  ports  in the firewall\n        firewalld:\n          port: \"{{ item }}\"\n          immediate: true\n          permanent: true\n          state: enabled\n        with_items: \"{{ splunk_architecture_syslog_ports }}\"\n\n      when: inventory_hostname in groups.logger\n\n#\n# Section for Non Docker hosts\n#\n    - block:\n\n      - name: Copy Universal forwarder Pkg \n        copy:\n          src: \"../files/{{ splunk_architecture_universal_forwarder_package }}\"\n          dest: /root/scripts/monitoring/\n\n      - name: Copy script file for non docker hosts\n        template:\n          src: ../templates/monitoring/{{ monitoring_stack }}/nondocker.sh.j2\n          dest: /root/scripts/monitoring/nondocker.sh\n\n      - file:\n          path: /root/scripts/monitoring/nondocker.sh\n          mode: 0744\n\n      - name: Install and Start Universal Forwarder\n        shell: /root/scripts/monitoring/nondocker.sh\n        args:\n          chdir: /root/scripts/monitoring\n      \n      when: inventory_hostname not in groups.docker\n      \n# end of section of non docker hosts\n\n#\n# Deploy a stack for Docker hosts\n#\n    - block:\n\n      - name: Create script directory\n        file:\n          path: /root/scripts/monitoring\n          state: directory\n\n      - name: Copy script file for non docker hosts\n        template:\n          src: ../templates/monitoring/{{ monitoring_stack }}/splunk.yml.j2\n          dest: /root/scripts/monitoring/splunk.yml\n\n      - name: Deploy Splunk Stack\n        command: docker stack deploy --compose-file splunk.yml {{ monitoring_stack | default ('splunk') }}\n        args:\n          chdir: /root/scripts/monitoring\n\n      when: inventory_hostname == ucp_instance\n\n# end of section for docker hosts\n\n    when: monitoring_stack is defined\n\n\n  - debug: msg=\"No splunk integration wanted\"\n    when: monitoring_stack is not defined\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "27454c70eff39eaceb36a739c343afea21da7d3f", "filename": "roles/rhsm/tests/group_vars/test-sat6.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nrhsm_server_hostname: \"sat6.example.com\"\nrhsm_org_id: \"my_org\"\nrhsm_activationkey: \"my_activation_key\"\nrhsm_pool: \"^my_pool_name$\"\n\nrhsm_repos:\n- \"rhel-7-server-rpms\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "56c8e8e9bc10553680d92e218ea9510024722ad5", "filename": "roles/0-init/tasks/hostname.yml", "repository": "iiab/iiab", "decoded_content": "- name: Is ubuntu-18 server\n  stat:\n    path: /etc/cloud/cloud.cfg\n  register: U18_server\n\n- name: Edit cloud.cfg yaml\n  lineinfile:\n    dest: /etc/cloud/cloud.cfg\n    regexp: '^preserve_hostname*'\n    line: 'preserve_hostname: true'\n    state: present\n  when: U18_server is defined and U18_server.stat.exists\n\n- name: Turn the crank for systemd (debuntu)\n  shell: hostnamectl set-hostname \"{{ iiab_hostname }}.{{ iiab_domain }}\"\n  when: is_debuntu\n\n- name: Configure /etc/sysconfig/network (redhat)\n  template:\n    src: roles/network/templates/network/sysconfig.network.j2\n    dest: /etc/sysconfig/network\n    owner: root\n    group: root\n    mode: 0644\n  when: is_redhat\n\n- name: Configure short hostname in /etc/hosts\n  lineinfile:\n    dest: /etc/hosts\n    regexp: '^127\\.0\\.0\\.1'\n    line: '127.0.0.1            localhost.localdomain   localhost  box {{ iiab_hostname }}'\n    owner: root\n    group: root\n    mode: 0644\n\n#- name: Re-configuring httpd - not initial install\n#  include_tasks: roles/httpd/tasks/main.yml\n#  when: iiab_stage|int > 3\n\n#- name: Re-configuring rest of networking - not initial install\n#  include_tasks: roles/network/tasks/main.yml\n#  when: iiab_stage|int > 4\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "1c98c5ac42f9b6fe55390ddc15b126ba6514ac58", "filename": "roles/cloud-lightsail/tasks/prompts.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- pause:\n    prompt: |\n      Enter your aws_access_key (http://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html)\n      Note: Make sure to use an IAM user with an acceptable policy attached (see https://github.com/trailofbits/algo/blob/master/docs/deploy-from-ansible.md)\n    echo: false\n  register: _aws_access_key\n  when:\n     - aws_access_key is undefined\n     - lookup('env','AWS_ACCESS_KEY_ID')|length <= 0\n\n- pause:\n    prompt: |\n      Enter your aws_secret_key (http://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html)\n    echo: false\n  register: _aws_secret_key\n  when:\n    - aws_secret_key is undefined\n    - lookup('env','AWS_SECRET_ACCESS_KEY')|length <= 0\n\n- set_fact:\n    access_key: \"{{ aws_access_key | default(_aws_access_key.user_input|default(None)) | default(lookup('env','AWS_ACCESS_KEY_ID'), true) }}\"\n    secret_key: \"{{ aws_secret_key | default(_aws_secret_key.user_input|default(None)) | default(lookup('env','AWS_SECRET_ACCESS_KEY'), true) }}\"\n\n- block:\n  - name: Get regions\n    lightsail_region_facts:\n      aws_access_key: \"{{ access_key }}\"\n      aws_secret_key: \"{{ secret_key }}\"\n      region: us-east-1\n    register: _lightsail_regions\n\n  - name: Set facts about the regions\n    set_fact:\n      lightsail_regions: \"{{ _lightsail_regions.results.regions | sort(attribute='name') }}\"\n\n  - name: Set the default region\n    set_fact:\n      default_region: >-\n        {% for r in lightsail_regions %}\n        {%- if r['name'] == \"us-east-1\" %}{{ loop.index }}{% endif %}\n        {%- endfor %}\n\n  - pause:\n      prompt: |\n        What region should the server be located in?\n        (https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/)\n          {% for r in lightsail_regions %}\n          {{ (loop.index|string + '.').ljust(3) }} {{ r['name'].ljust(20) }} {{ r['displayName'] }}\n          {% endfor %}\n\n        Enter the number of your desired region\n        [{{ default_region }}]\n    register: _algo_region\n  when: region is undefined\n\n- set_fact:\n    algo_region: >-\n      {% if region is defined %}{{ region }}\n      {%- elif _algo_region.user_input is defined and _algo_region.user_input != \"\" %}{{ lightsail_regions[_algo_region.user_input | int -1 ]['name'] }}\n      {%- else %}{{ lightsail_regions[default_region | int - 1]['name'] }}{% endif %}\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "1ae126b05cc4d83aea92e952adaa330655f3ec3c", "filename": "playbooks/roles/kafka/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# handlers file for kafka"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "f7a951bcf055acec6fea41f1cc9a1c2532e458d0", "filename": "roles/ipareplica/vars/RedHat-7.3.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# defaults file for ipareplica\n# vars/RedHat-7.3.yml\nipareplica_packages: [ \"ipa-server\", \"libselinux-python\" ]\nipareplica_packages_dns: [ \"ipa-server-dns\" ]\nipareplica_packages_adtrust: [ \"ipa-server-trust-ad\" ]"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8d5e6e25518f52cb44d805a3189e3dfd1811c9c5", "filename": "roles/osp/admin-network/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: localhost\n  roles:\n  - osp-admin-network\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5c738fcb7d730e1f1744448dfee085451d75b068", "filename": "roles/config-linux-desktop/config-mate/tasks/mate-Fedora.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: \"Install additional packages for MATE\"\n  dnf:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n  - '@MATE Desktop'\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "52f4663c928cdf2e819f494e2f9f54ad5a26863c", "filename": "roles/config-mysql/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "- name: Restart MySQL Service\n  systemd:\n    name: \"{{ mysql_name }}\"\n    enabled: yes\n    state: restarted\n    daemon_reload: yes\n\n- name: restart firewalld\n  service:\n    name: firewalld\n    state: restarted\n\n- name: restart iptables\n  service:\n    name: iptables\n    state: restarted"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "ffceb7bbadf524ff4d44851d5aafb906f1f9e558", "filename": "roles/ovirt-guest-agent/meta/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\ngalaxy_info:\n  author: \"Katerina Koukiou\"\n  description: \"Installs guest agents on VMs\"\n  company: \"Red Hat\"\n  license: \"GPLv3\"\n  min_ansible_version: 1.9\n  platforms:\n  - name: EL\n    versions:\n    - all\n  galaxy_tags:\n    - installer\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "bbb64f4d0ab7c2eae6befa33ab72ff158f100775", "filename": "reference-architecture/vmware-ansible/playbooks/roles/cloud-provider-setup/vars/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\nvsphere_conf_dir: /etc/vsphere\nvsphere_conf: \"{{vsphere_conf_dir }}/vsphere.conf\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b741aa3dbce62c5259099ec357a14dfd1ac7e2ff", "filename": "playbooks/provision-satellite-server/roles", "repository": "redhat-cop/infra-ansible", "decoded_content": "../../roles"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "8083bcef1356240b6a99969c0d7396005204fe15", "filename": "tasks/setup-audit.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Ensure auditd is installed\n  become: true\n  package:\n    name: auditd\n    state: present\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when:\n    - docker_enable_audit | bool\n    - docker_network_access | bool\n    - _docker_os_dist == \"Ubuntu\" or _docker_os_dist == \"Debian\"\n\n- name: Copy Docker audit rules\n  become: yes\n  copy:\n    src: files/etc/audit/rules.d/docker.rules\n    dest: /etc/audit/rules.d/docker.rules\n  notify: restart auditd\n  when: docker_enable_audit | bool\n\n- name: Ensure Docker audit rules are removed\n  become: yes\n  file:\n    path: /etc/audit/rules.d/docker.rules\n    state: absent\n  notify: restart auditd\n  when: not docker_enable_audit | bool\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "5618651a2af2a73c1d0362e43495e8bb77d2c584", "filename": "archive/roles/openshift-install/tasks/main.yaml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n  - name: \"Creating Inventory\"\n    template:\n      dest: \"{{rhc_ose_inv_dest | default('/tmp') }}/inventory_{{ hostvars['localhost'].env_id}}\"\n      src: \"{{ role_path }}/templates/inventory_template.j2\"\n      force: yes\n#    - include: /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml\n#     when: ose_install | default(true) | bool\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e024cb7d4510eb63e892c87767215250a1e8d6ae", "filename": "playbooks/infra-hosts.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Subscribe the hosts to RHSM'\n  hosts: infra_hosts\n  vars:\n    rhsm_username: \"{{ hostvars['localhost'].rhsm_username|default(omit) }}\"\n    rhsm_password: \"{{ hostvars['localhost'].rhsm_password|default(omit) }}\"\n  roles:\n  - role: rhsm\n  tags:\n  - configure_rhsm\n\n- name: 'Configure networking on the infrastructure hosts'\n  hosts: infra_hosts\n  roles:\n  - role: config-bonding\n  - role: config-vlans\n  - role: config-routes\n  tags: \n  - configure_infra_hosts_networking\n  \n- name: 'Make sure the host is running the latest'\n  hosts: infra_hosts\n  roles:\n  - role: update-host\n  tags: \n  - update_host\n\n- name: 'Setup iSCSI and MultiPathing - if applicable'\n  hosts: infra_hosts\n  roles:\n  - role: config-iscsi-client\n  tags:\n  - configure_iscsi_client\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "984107acd026980961691858642c18cf0df3bac4", "filename": "roles/rhsm-subscription/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- block:\n    - name: Allow rhsm a longer timeout to help out with subscription-manager\n      lineinfile:\n        dest: /etc/rhsm/rhsm.conf\n        line: 'server_timeout=600'\n        insertafter: '^proxy_password ='\n\n    - name: Check for sat config file\n      stat: path=/etc/rhsm/rhsm.conf.kat-backup\n      register: sat_cfg\n\n    - name: Remove satellite configuration if using RH CDN\n      command: \"mv -f /etc/rhsm/rhsm.conf.kat-backup /etc/rhsm/rhsm.conf\"\n      when: rhsm_user is defined and rhsm_user and sat_cfg.stat.exists == True\n      ignore_errors: yes\n\n    - name: Remove satellite SSL if using RH CDN\n      command: \"rpm -e $(rpm -qa katello-ca-consumer*)\"\n      when: rhsm_user is defined and rhsm_user and sat_cfg.stat.exists == True\n      ignore_errors: yes\n\n    - name: Is the host already registered?\n      command: \"subscription-manager version\"\n      register: subscribed\n      changed_when: no\n      ignore_errors: yes\n\n    - name: Install Katello RPM if set\n      yum:\n        name: \"{{ rhsm_katello_url }}\"\n        state: present\n      when: rhsm_katello_url is defined and rhsm_katello_url\n\n    - name: Register host via Activation key\n      redhat_subscription:\n        activationkey: \"{{ rhsm_activation_key }}\"\n        org_id: \"{{ rhsm_org_id }}\"\n        state: present\n        pool: \"{{ rhsm_pool }}\"\n      when: rhsm_activation_key is defined and rhsm_activation_key\n      register: register_key_result\n      ignore_errors: yes\n\n    - name: Register host\n      redhat_subscription:\n        username: \"{{ rhsm_user }}\"\n        password: \"{{ rhsm_password }}\"\n        server_hostname: \"{{ rhsm_server }}\"\n        state: present\n        pool: \"{{ rhsm_pool }}\"\n      when: \"('not registered' in subscribed.stdout or 'Current' not in subscribed.stdout) and rhsm_user is defined and rhsm_user\"\n\n    - name: Check if subscription is attached\n      command: subscription-manager list --consumed --pool-only --matches=\"{{ rhsm_pool }}\"\n      register: subscription_attached\n      when: rhsm_pool is defined and rhsm_pool\n      changed_when: no\n\n    - block:\n        - name: Get pool id\n          shell: subscription-manager list --available --pool-only --matches=\"{{ rhsm_pool }}\" | head -n 1\n          register: pool_id\n          changed_when: no\n\n        - name: Fail if no pool ID is returned\n          fail:\n            msg: No subscription matching \"{{ rhsm_pool }}\" found\n          when: pool_id.stdout == \"\"\n\n        - name: Attach subscription\n          command: subscription-manager attach --pool=\"{{ pool_id.stdout }}\"\n\n      #when: rhsm_pool is defined and rhsm_pool and not skip_packages is defined\n      when:\n        - rhsm_pool is defined\n        - subscription_attached.stdout == ''\n\n    - name: Install katello-agent RPM if we are using satellite\n      yum:\n        name: katello-agent\n        state: present\n      when: rhsm_katello_url is defined and rhsm_katello_url\n\n  when: ansible_distribution == \"RedHat\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "eaa61b71c2d948208bb05e10165bac1a3904ef2c", "filename": "roles/ejabberd/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install ejabberd packages\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n   - ejabberd\n  tags:\n    - download\n\n#- name: Configure ejabberd\n#  template:\n#    backup: yes\n#    src: \"{{ item.src }}\"\n#    dest: \"{{ item.dest }}\"\n#    owner: root\n#    group: root\n#    mode: \"{{ item.mode }}\"\n#  with_items:\n#    - { src: 'ejabberd-iiab.cfg.j2', dest: '/etc/ejabberd/ejabberd-iiab.cfg' , mode: '0644' }\n#    - { src: 'ejabberdctl.cfg.j2', dest: '/etc/ejabberd/ejabberdctl-iiab.cfg', mode: '0644' }\n#    - { src: 'ejabberd-iiab', dest: '/etc/sysconfig/ejabberd-iiab', mode: '0755' }\n#    #- { src: 'ejabberd-domain-config', dest: '/etc/sysconfig/olpc-scripts/domain_config.d/ejabberd', mode: '0755'}\n#    #- { src: 'ejabberd', dest: '/etc/sysconfig/olpc-scripts/domain_config.d/ejabberd' , mode: '0755' }\n#    - { src: 'ejabberd-iiab.service.j2', dest: '/etc/systemd/system/ejabberd-iiab.service', mode: '0644' }\n#    - { src: 'iiab-ejabberd-srg', dest: '/usr/bin/iiab-ejabberd-srg' , mode: '0755' }\n#    #- { src: '10-ejabberdmoodle', dest: '/etc/sudoers.d/10-ejabberdmoodle', mode: '0440' }\n#    - { src: 'ejabberd.tmpfiles', dest: '/etc/tmpfiles.d/ejabberd.conf', mode: '0640' }\n#  register: ejabberd_config\n\n#- name: Stop and disable OS provided systemd ejabberd service\n#  service:\n#    name: ejabberd\n#    state: stopped\n#    enabled: no\n\n#- name: Put the startup script in place - debian\n#  template:\n#    src: ejabberd-iiab.init\n#    dest: /etc/init.d/ejabberd-iiab\n#    mode: 0755\n#  when: is_debuntu\n\n#- name: Put the startup script in place - non debian\n#  template:\n#    src: ejabberd-iiab.init\n#    dest: /usr/libexec/ejabberd-iiab\n#    mode: 0755\n#  when: not is_debuntu\n\n#- name: Remove ejabberd_domain if domain changes\n#  file:\n#    path: /etc/sysconfig/ejabberd_domain_name\n#    state: absent\n#  when: ejabberd_config.changed\n\n#- name: Enable ejabberd service\n#  file:\n#    src: /etc/systemd/system/ejabberd-iiab.service\n#    dest: /etc/systemd/system/multi-user.target.wants/ejabberd-iiab.service\n#    owner: root\n#    group: root\n#    state: link\n\n- name: Stop ejabberd service\n  service:\n    name: ejabberd\n    #name: ejabberd-iiab\n    state: stopped\n    enabled: no\n  when: not ejabberd_enabled\n\n- name: Start ejabberd service\n  service:\n    name: ejabberd\n    #name: ejabberd-iiab\n    state: restarted\n    enabled: yes\n  when: ejabberd_enabled\n  #when: ejabberd_config.changed and ejabberd_enabled\n\n#- name: Wait for ejabberd service start\n#  wait_for:\n#    port: 5280\n#    delay: 15\n#    state: started\n#    timeout: 300\n#  when: ejabberd_config.changed and ejabberd_enabled\n\n# ejabberd-iiab.init has the logic for the below, needs to be done once \n# and only if the group does not exist based on presence of\n# /var/lib/ejabberd online_src_created\n\n#- name: Create online group\n#  shell: ejabberdctl srg_create Online \"{{ iiab_hostname }}\" Online \"Online_Users\" Online\n#  when: ejabberd_config.changed\n\n#- name: Add all users to online group\n#  shell: ejabberdctl srg_user_add '@online@' \"{{ iiab_hostname }}\" Online \"schoolserver\"\n#  when: ejabberd_config.changed\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "e56f8806ec60812da512503a1f50f5ee0d7815f1", "filename": "roles/network/tasks/rpi_debian.yml", "repository": "iiab/iiab", "decoded_content": "# rpi_debian.yml\n# Start out making simplifying assumptions\n#   1. we are dealing with a rpi3\n#   2. Gui inputs define the config -- auto config is more difficult\n#      a. gui_desired_network_role\n#      b. hostapd_enabled\n#      c. gui_static_wan_ip\n#   3. In appliance mode: wan is either wired dhcp/static or wlan0 and hostapd off\n#   4. In lan_controller: wan is off, eth0 and wlan0 under br0\n#   5. In gateway: user gateway is wan, and wlan0 under br0 if not acting as\n#      the gateway\n\n- name: Supply resolvconf.conf\n  template:\n    dest: /etc/resolvconf.conf\n    src: network/resolvconf.j2\n\n- name: Supply dhcpcd.conf\n  template:\n    dest: /etc/dhcpcd.conf\n    src: network/dhcpcd.conf.j2\n\n- name: New raspbian requires country code -- check for it\n  shell: grep country /etc/wpa_supplicant/wpa_supplicant.conf\n  register: country_code\n  ignore_errors: True\n\n- name: Put a country code if it does not exist\n  lineinfile: \n    dest: /etc/wpa_supplicant/wpa_supplicant.conf\n    regexp: \"^country.*\"\n    line: country={{ host_country_code }}\n  when: country_code is defined and country_code.stdout == \"\"\n\n- name: Enable the wifi with rfkill\n  shell: rfkill unblock 0\n  ignore_errors: True\n\n- name: Copy the bridge script for RPi\n  template:\n    dest: /etc/network/interfaces.d/iiab\n    src: network/iiab.j2\n  when: iiab_lan_iface == \"br0\"\n\n- name: Stopping services\n  include_tasks: down-debian.yml\n\n- name: Reload systemd\n  systemd:\n    daemon_reload: yes\n\n# now pick up denyinterfaces\n- name: Restart dhcpcd\n  service:\n    name: dhcpcd\n    state: restarted\n\n- name: Restart the networking service if appropriate\n  service:\n    name: networking\n    enabled: yes\n    state: restarted\n  when: not nobridge is defined and not no_net_restart\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "6aecba1fdf92a536f853fa3c93b07a6401c4f7b6", "filename": "tasks/section_01_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 1.1.1 Install Updates, Patches and Additional Security Software (Not Scored)\n    apt: update_cache=yes\n    tags:\n      - section1\n      - section1.1\n      - section1.1.1\n\n  - name: 1.1.2 Install Updates, Patches and Additional Security Software (Not Scored)\n    apt: upgrade=yes\n    when: apt_upgrade == True\n    tags:\n      - section1\n      - section1.1\n      - section1.1.2\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "e28a6fe7ab706577fb760c8d60789ea23b75deea", "filename": "tasks/section_04_level2.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: Check if the grub config file exists (Not Scored)\n    stat: path=/etc/default/grub\n    register: grub_cfg_file\n\n  - name: Determines if apparmor is set in grub config (Not Scored)\n    command: \"grep 'GRUB_CMDLINE_LINUX' /etc/default/grub\"\n    register: grub_apparmor\n    changed_when: False\n    when: grub_cfg_file.stat.exists\n\n  - name: Check if the extlinux config file exists (Not Scored)\n    stat: path=/extlinux.conf\n    register: extlinux_cfg_file\n\n  - name: Determines if apparmor is set in extlinux (Not Scored)\n    command: \"grep 'apparmor' /extlinux.conf\"\n    register: extlinux_apparmor\n    changed_when: False\n    when: extlinux_cfg_file.stat.exists\n\n  - name: Determines if apparmor is already set in boot config (Not Scored)\n    command: \"grep CONFIG_DEFAULT_SECURITY_APPARMOR /boot/config-{{ ansible_kernel }}\"\n    register: boot_apparmor\n    changed_when: False\n\n  - name: 4.5 Activate AppArmor (install) (Scored)\n    apt: >\n        name=apparmor\n        state=present\n    when: use_apparmor == True\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Activate AppArmor (Kernel LSM - grub) (Scored)\n    lineinfile: >\n        dest='/etc/default/grub'\n        regexp='^GRUB_CMDLINE_LINUX=\"\"'\n        line='GRUB_CMDLINE_LINUX=apparmor=\"1 security=apparmor\"'\n        state=present\n    when: \"(use_apparmor == True) and grub_cfg_file.stat.exists and ('apparmor' not in grub_apparmor['stdout']) and ('not set' in boot_apparmor['stdout'])\"\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Activate AppArmor (Kernel LSM - extlinux) (Scored)\n    lineinfile: >\n        dest='/extlinux.conf'\n        regexp=\"^append initrd=\"\n        line=\"append initrd={{ ansible_cmdline['initrd'] }} root={{ ansible_cmdline['root'] }} console=tty0 console={{ ansible_cmdline['console'] }} apparmor=1 security=apparmor ro quiet\"\n    when: \"(use_apparmor == True) and extlinux_cfg_file.stat.exists and ('apparmor' not in extlinux_apparmor['stdout']) and ('not set' in boot_cfg_apparmor['stdout'])\"\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Activate AppArmor (start) (Scored)\n    service: >\n        name=apparmor\n        state=started\n    when: use_apparmor == True\n    register: apparmor_status\n    ignore_errors: True\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Determine if Apparmor started without error (Not Scored)\n    fail: msg=\"Apparmor can not be started. This is normal behavior if you run the playbook for the first time.\\nPlease reboot the machine and run it again to proceed with the rest of the playbook.\"\n    when: apparmor_status.failed is defined\n\n  - name: 4.5 Fix rsyslog /run/utmp permissions (Not Scored)\n    lineinfile: >\n        dest=\"/etc/apparmor.d/usr.sbin.rsyslogd\"\n        line=\"  /run/utmp rk,\"\n        insertbefore=\"  /var/spool/rsyslog/ r,\"\n        state=present\n    when: use_apparmor == True\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Activate AppArmor (fix profiles) (Scored)\n    service: >\n        name=rsyslog\n        state=restarted\n    changed_when: False\n    when: use_apparmor == True\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Activate AppArmor (Scored)\n    command: apparmor_status\n    register: aa_status_lines\n    failed_when: '\"0 profiles are loaded\" in aa_status_lines.stdout_lines or \"0 processes are in complain mode.\" not in aa_status_lines.stdout_lines or \"0 processes are unconfined but have a profile defined.\" not in aa_status_lines.stdout_lines'\n        # - '\"0 processes are unconfined but have a profile defined.\" not in aa_status_lines.stdout_lines'\n    changed_when: False\n    when: use_apparmor == True\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Activate AppArmor (enforce install) (Scored)\n    apt: >\n        name=apparmor-utils\n        state=present\n    when: use_apparmor == True\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Activate AppArmor (enforce) (Scored)\n    #shell: 'aa-enforce /etc/apparmor.d/*'\n    shell: for profile in /etc/apparmor.d/*; do aa-enforce $profile; done\n    register: aaenforce_rc\n    failed_when: aaenforce_rc.rc == 1\n    changed_when: False\n    when: use_apparmor == True\n    tags:\n      - section4\n      - section4.5\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "504c9eda0e7f87818b42b950f0c4fb2c42899cbc", "filename": "roles/ovirt-guest-agent/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "- name: Install latest {{ ovirt_guest_agent_pkg_prefix }}-guest-agent\n  yum:\n    name: \"{{ item }}\"\n    state: latest\n  with_items:\n    - \"{{ ovirt_guest_agent_pkg_prefix }}-guest-agent\"\n  notify: enable and start guest-agent service\n  tags:\n    - skip_ansible_lint\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "333ba515c1faa9366630bf0cae4046e08e98854e", "filename": "playbooks/dns/vars/inventory", "repository": "redhat-cop/casl-ansible", "decoded_content": "\n[dns]\ndns.example.com ansible_user=fedora ansible_become=yes\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "9e6f6f1dc7d6a8d451d4f8e05dbe905b9fe88033", "filename": "playbooks/roles/stenographer/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# handlers file for stenographer\n\n- name: start stenographer service\n  service:\n    name: stenographer\n    state: \"{{ 'started' if enable_stenographer else 'stopped' }}\"\n\n- name: start stenographer per interface\n  service:\n    name: \"stenographer@{{ item }}\"\n    state: \"{{ 'started' if enable_stenographer else 'stopped' }}\"\n  with_items: \"{{ stenographer_monitor_interfaces }}\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "c9a20b5766e71757d892964497c89ce731ac16e6", "filename": "playbooks/roles/zookeeper/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# tasks file for zookeeper\n- import_tasks: \"{{ method }}.yml\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "27b3d09dba2d92b04a433b289fb3feb4a03b596f", "filename": "reference-architecture/vmware-ansible/playbooks/roles/heketi-ocp-clean/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Switch to default project\n  command: oc project default\n\n- name: Check to see if heketi secret is already created\n  command: \"oc get secrets\"\n  register: oc_secrets\n\n- name: Check to see if storage class is already created\n  command: \"oc get storageclass\"\n  register: storage_class\n\n- name: Remove storage class from OCP\n  command: \"oc delete storageclass crs-gluster\"\n  when: \"'crs-gluster' in storage_class.stdout\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "3fad2b357a82c8eb35e4275a854946cb9ebf6222", "filename": "roles/mysql/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "    - name: Install MySQL (debuntu)\n      package:\n        name: \"{{ item }}\"\n        state: present\n      with_items:\n        - mariadb-server\n        - mariadb-client\n        - python-mysqldb\n        - php{{ php_version }}\n        - php{{ php_version }}-mysql\n        - php-pear\n        - php{{ php_version }}-gd\n        - php{{ php_version }}-imap\n        - php{{ php_version }}-ldap\n        - php{{ php_version }}-odbc\n#        - php{{ php_version }}-xml\n        - php{{ php_version }}-xmlrpc\n      when: is_debuntu\n      tags:\n        - download\n\n    - name: php-xml (ubuntu or debian-9)\n      package:\n        name: \"php{{ php_version }}-xml\"\n        state: present\n      when: is_ubuntu or is_debian_9\n\n    - name: php-xml (debian-8)\n      package:\n        name: \"php-xml-parser\"\n        state: present\n      when: is_debian_8\n\n    - name: Install MySQL (OS's other than debuntu)\n      package:\n        name: \"{{ item }}\"\n        state: present\n      with_items:\n        - MySQL-python\n        - mysql\n        - php\n        - php-mysql\n        - php-pear\n        - php-gd\n        - php-imap\n        - php-ldap\n        - php-odbc\n        - php-xml\n        - php-xmlrpc\n      when: not is_debuntu\n      tags:\n        - download\n\n    - include_tasks: centos.yml\n      when: ansible_distribution == \"CentOS\"\n      tags:\n        - download\n\n    - include_tasks: fedora.yml\n      when: ansible_distribution == \"Fedora\"\n      tags:\n        - download\n\n# Name of mysql service varies by OS so softcoded in 1-prep\n    - name: Start the MySQL service\n      service:\n        name: \"{{ mysql_service }}\"\n        state: started\n\n    - name: Enable the MySQL service\n      service:\n        name: \"{{ mysql_service }}\"\n        enabled: yes\n      when: mysql_enabled\n\n# 'localhost' needs to be the last item for idempotency, see\n# http://ansible.cc/docs/modules.html#mysql-user\n# unfortunately it still doesn't work\n    - name: Update MySQL root password for localhost root accounts\n      mysql_user:\n        name: root\n        host: \"{{ item }}\"\n        password: \"{{ mysql_root_password }}\"\n        priv: \"*.*:ALL,GRANT\"\n      with_items:\n        - localhost\n      when: mysql_enabled\n\n    - name: Copy .my.cnf file with root password credentials\n      template:\n        src: my.cnf.j2\n        dest: /root/.my.cnf\n        owner: root\n        mode: 0600\n      when: mysql_enabled\n\n    - name: Update MySQL root password for all remaining root accounts\n      mysql_user:\n        name: root\n        host: \"{{ item }}\"\n        password: \"{{ mysql_root_password }}\"\n        priv: \"*.*:ALL,GRANT\"\n      with_items:\n#        - \"{{ iiab_hostname }}.{{ iiab_domain }}\"\n        - 127.0.0.1\n        - ::1\n      when: mysql_enabled\n\n    - name: Delete anonymous MySQL server user for {{ ansible_hostname }}\n      mysql_user:\n        user: \"\"\n        host: \"{{ ansible_hostname }}\"\n        state: absent\n      when: mysql_enabled\n\n    - name: Delete anonymous MySQL server user for localhost\n      mysql_user:\n        user: \"\"\n        state: absent\n      when: mysql_enabled\n\n    - name: Remove the MySQL test database\n      mysql_db:\n        db: test\n        state: absent\n      when: mysql_enabled\n\n# we had to start mysql in order to configure it, now turn if off if not enabled\n    - name: Provisionally Disable the MySQL service\n      service:\n        name: \"{{ mysql_service }}\"\n        enabled: no\n        state: stopped\n      when: not mysql_enabled\n\n    - name: Add 'mysql' to list of services at /etc/iiab/iiab.ini\n      ini_file:\n        dest: \"{{ service_filelist }}\"\n        section: mysql\n        option: \"{{ item.option }}\"\n        value: \"{{ item.value }}\"\n      with_items:\n        - option: name\n          value: MySQL\n        - option: description\n          value: '\"MySQL is a widely used free and open source (GPLv2) database, offered by most web hosting services, on a diversity of platforms.\"'\n        - option: enabled\n          value: \"{{ mysql_enabled }}\"\n"}, {"commit_sha": "e9fb46dc84b9c815a69f6de1347c9ece5db01cc8", "sha": "cc28372c21fc35712df3fafeef6c43f984313be4", "filename": "tasks/main.yml", "repository": "fubarhouse/ansible-role-nodejs", "decoded_content": "---\n# Main tasks file for fubarhouse.nodejs\n\n- name: \"Define user variable for ssh use\"\n  set_fact:\n    fubarhouse_user: \"{{ ansible_ssh_user }}\"\n  when: ansible_ssh_user is defined and fubarhouse_user is undefined\n\n- name: \"Define user variable for non-ssh use\"\n  set_fact:\n    fubarhouse_user: \"{{ ansible_user_id }}\"\n  when: ansible_ssh_user is not defined and fubarhouse_user is undefined\n\n- name: \"Define OS-specific variables\"\n  include_vars: \"config-{{ ansible_os_family }}.yml\"\n  when: fubarhouse_npm is not defined\n\n- include: nvm.yml\n  when: install_nvm\n\n- include: ivm.yml\n  when:\n    - install_ivm == true\n    - '\"{{ ansible_os_family }}\" != \"Darwin\"'\n\n- include: nodejs.yml\n  when: install_nodejs == true\n\n- include: iojs.yml\n  when:\n    - install_iojs == true\n    - '\"{{ ansible_os_family }}\" != \"Darwin\"'\n\n- include: npm.yml\n  when: install_npm == true"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ecf50727f5cb0fbc82ba1e5590eaebf3ae65a795", "filename": "roles/dhcp/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: prereq.yml\n\n# build the config file locally\n- import_tasks: dhcpconfig.yml\n  delegate_to: localhost\n  run_once: true\n\n# install the packages and copy the file from the local system\n- import_tasks: dhcp.yml\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "0beb5221b779361b35dbb0925b17fa6876e5ec7c", "filename": "tasks/section_01.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - include: section_01_level1.yml\n    tags:\n      - section01\n      - level1\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "a1425982b5051bf11aa13d1a976210f4110bf4b4", "filename": "roles/kiwix/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "# Which kiwix-tools to download from http://download.iiab.io/packages/\n# As obtained from http://download.kiwix.org/release/kiwix-tools/ or http://download.kiwix.org/nightly/\n\nkiwix_version_armhf: \"kiwix-tools_linux-armhf-0.6.0\"\nkiwix_version_linux64: \"kiwix-tools_linux-x86_64-0.6.0\"\nkiwix_version_i686: \"kiwix-tools_linux-i586-0.6.0\"\n# kiwix_src_file_i686: \"kiwix-linux-i686.tar.bz2\"\n# v0.9 for i686 published May 2014 (\"use it to test legacy ZIM content\")\n# v0.10 for i686 published Oct 2016 (\"experimental\") REPLACED IN EARLY 2018, thx to Matthieu Gautier:\n# https://github.com/kiwix/kiwix-build/issues/94\n# https://github.com/kiwix/kiwix-tools/issues/170\n\nkiwix_src_file_armhf: \"{{ kiwix_version_armhf }}.tar.gz\"\nkiwix_src_file_linux64: \"{{ kiwix_version_linux64 }}.tar.gz\"\nkiwix_src_file_i686: \"{{ kiwix_version_i686 }}.tar.gz\"\n\nkiwix_port: 3000\n# Used for Kiwix proxy http://box/kiwix/\nkiwix_url: /kiwix/\nkiwix_path: \"{{ iiab_base }}/kiwix\"\n\n# /library/zims contains 3 important things:\n# - library.xml\n# - content directory for all *.zim's\n# - index directory for legacy *.zim.idx's\niiab_zim_path: \"{{ content_base }}/zims\"\nkiwix_library_xml: \"{{ iiab_zim_path }}/library.xml\"\n\n# Installation Variables\nkiwix_install: True\nkiwix_enabled: True\n# MOVE FILE /opt/iiab/kiwix/bin/kiwix-serve TO FORCE A REINSTALL OF kiwix-tools\nkiwix_force_install: False\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8b0a9102e6d5a7f40d4ac33c828159582412c4dc", "filename": "roles/osp/packstack-post/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'restart mariadb'\n  service:\n    name: 'mariadb'\n    state: restarted\n\n- name: 'restart rabbitmq-server'\n  service:\n    name: 'rabbitmq-server'\n    state: restarted\n\n- name: 'restart openstack-nova-compute'\n  service:\n    name: 'openstack-nova-compute'\n    state: restarted\n\n- name: 'restart libvirtd'\n  service:\n    name: 'libvirtd'\n    state: restarted\n\n- name: 'restart iptables'\n  service:\n    name: 'iptables'\n    state: restarted\n\n- name: 'restart keystone'\n  service:\n    name: 'httpd'\n    state: restarted\n\n- name: 'restart openstack-cinder-api'\n  service:\n    name: 'openstack-cinder-api'\n    state: restarted\n\n- name: 'restart openstack-cinder-backup'\n  service:\n    name: 'openstack-cinder-backup'\n    state: restarted\n\n- name: 'restart openstack-cinder-scheduler'\n  service:\n    name: 'openstack-cinder-scheduler'\n    state: restarted\n\n- name: 'restart openstack-cinder-volume'\n  service:\n    name: 'openstack-cinder-volume'\n    state: restarted\n\n- name: \"remove temporary keystone rc file\"\n  file:\n    path: \"{{ keystonerc_file.path }}\"\n    state: absent\n\n- name: \"remove local temporary keystone rc file\"\n  file:\n    path: \"{{ keystonerc_file.path }}\"\n    state: absent\n  delegate_to: localhost\n\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "65e15a5e16950d1c9a5abdec4049d4b0b01701e6", "filename": "roles/storage-cns/templates/storage-cns.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\nkind: StorageClass\napiVersion: storage.k8s.io/v1beta1\nmetadata:\n  name: kubevirt\nprovisioner: kubernetes.io/glusterfs\nreclaimPolicy: Delete\nparameters:\n  # Enable this when CNS supports cloning\n  # smartclone: \"true\"\n  resturl: http://heketi-storage-app-storage.router.default.svc.cluster.local\n  restuser: admin\n  secretName: heketi-storage-admin-secret\n  secretNamespace: app-storage\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b4d65a74cf2c9ebe690a833e33c5e20de2a79cf5", "filename": "roles/ansible/tower/manage-credential-types/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- block: # when ansible_tower.credential_types is defined\n\n  - name: \"Set default values\"\n    set_fact:\n      processed_credential_types: []\n      existing_credential_types_output: []\n\n  # Utilize the `rest_get` library routine to ensure REST pagination is handled\n  - name: \"Get the existing credential types\"\n    rest_get:\n      host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n      rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      rest_password: \"{{ ansible_tower.admin_password }}\"\n      api_uri: \"/api/v2/credential_types/\"\n    register: existing_credential_types_output\n\n  - name: \"Process the inventory credential types\"\n    include_tasks: process-credential-type.yml\n    with_items:\n    - \"{{ ansible_tower.credential_types }}\"\n    loop_control:\n      loop_var: credential_type\n\n  when:\n  - ansible_tower.credential_types is defined\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "1869e6a23fc7cd94ea74cc97442c2bafe219d6c9", "filename": "roles/dns_encryption/defaults/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\nalgo_local_dns: false\nlisten_port: \"{% if algo_local_dns %}5353{% else %}53{% endif %}\"\n# the version used if the latest unavailable (in case of Github API rate limited)\ndnscrypt_proxy_version: 2.0.10\napparmor_enabled: true\ndns_encryption: true\nipv6_support: false\ndnscrypt_servers:\n  ipv4:\n    - cloudflare\n  ipv6:\n    - cloudflare-ipv6\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "569e00da274d8380649122e54a22cf14ebead937", "filename": "playbooks/libvirt/openshift-cluster/config.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# TODO: need to figure out a plan for setting hostname, currently the default\n# is localhost, so no hostname value (or public_hostname) value is getting\n# assigned\n\n- include: ../../common/openshift-cluster/std_include.yml\n\n- hosts: localhost\n  gather_facts: no\n  tasks:\n  - include_vars: vars.yml\n  - include_vars: cluster_hosts.yml\n  - add_host:\n      name: \"{{ item }}\"\n      groups: l_oo_all_hosts\n    with_items: \"{{ g_all_hosts | default([]) }}\"\n\n- hosts: l_oo_all_hosts\n  gather_facts: no\n  tasks:\n  - include_vars: vars.yml\n  - include_vars: cluster_hosts.yml\n\n- include: ../../common/openshift-cluster/config.yml\n  vars:\n    g_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n    g_sudo: \"{{ deployment_vars[deployment_type].become }}\"\n    g_nodeonmaster: true\n    openshift_cluster_id: \"{{ cluster_id }}\"\n    openshift_debug_level: \"{{ debug_level }}\"\n    openshift_deployment_type: \"{{ deployment_type }}\"\n    openshift_hosted_registry_selector: 'type=infra'\n    openshift_hosted_router_selector: 'type=infra'\n    openshift_master_cluster_method: 'native'\n    openshift_use_openshift_sdn: \"{{ lookup('oo_option', 'use_openshift_sdn') }}\"\n    os_sdn_network_plugin_name: \"{{ lookup('oo_option', 'sdn_network_plugin_name') }}\"\n    openshift_use_flannel: \"{{ lookup('oo_option', 'use_flannel') }}\"\n    openshift_use_calico: \"{{ lookup('oo_option', 'use_calico') }}\"\n    openshift_use_fluentd: \"{{ lookup('oo_option', 'use_fluentd') }}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "43566b8fbff6c61ecd27ca095bb320e5cceed7fa", "filename": "roles/calibre/tasks/debs.yml", "repository": "iiab/iiab", "decoded_content": "# roles/calibre/tasks/main.yml requires calibre_via_debs (to be True) before calling this script.\n\n# MOVED UP TO roles/calibre/tasks/main.yml\n#- name: Start by installing OS's Calibre package\n#  package:\n#    name: \"{{ item }}\"\n#    state: latest\n#  with_items:\n#    - calibre\n#    - calibre-bin\n#  when: internet_available\n\n# April/May 2018: Raspbian .deb's for the latest Calibre now appear\n# (http://raspbian.raspberrypi.org/raspbian/pool/main/c/calibre/)\n# within about 10 days of Calibre's quasi-monthly releases\n# (https://calibre-ebook.com/whats-new).\n\n# If you want the latest Calibre, run the appropriate below script, standalone.\n# HOWEVER: it's strongly suggested you wait for apt (blessed by your OS!)\n\n#- name: Install packages that Raspbian .deb's had installed for Calibre 3.23 (rpi)\n#  #command: scripts/calibre-install-latest-rpi.sh  # FAILS with Calibre 3.24+ (\"calibre : Depends: python-pyqt5 (>= 5.10.1+dfsg-2) but 5.10.1+dfsg-1+rpi1 is to be installed\") since June 2018.\n#  command: scripts/calibre-install-packages.sh     # BORROWED package list from /var/log/apt/history.log (that resulted from 2018-05-22 install of Calibre 3.23 using calibre-install-latest-rpi.sh).\n#  when: is_rpi and internet_available\n\n#- name: Upgrade to latest Calibre using Debian's own .deb's from testing (rpi)\n#  command: scripts/calibre-install-latest.sh       # NECESSARY since Calibre 3.24 (BEWARE installing libc6 will prevent boot in RPi Zero W, i.e. if calibre-install-packages.sh isn't run above!)\n#  when: is_rpi and internet_available\n\n- name: Upgrade to latest Calibre using .deb's from testing (rpi)\n  command: scripts/calibre-install-latest-rpi-plus.sh    # HOPE IT WORKS FOR Calibre 3.27.1+ starting 2018-07-22 -- PLEASE TEST IF BOOTABLE IN Zero W?\n  #command: scripts/calibre-install-latest-rpi.sh    # WORKED FOR Calibre 3.26.x (Calibre 3.24.x & 3.25 required above prereq calibre-install-packages.sh then Debian's own calibre-install-latest.sh to be bootable in Zero W)\n  when: is_rpi and internet_available\n\n- name: Upgrade to Calibre testing .deb's - target Ubuntu 16.04 (not rpi and not ubuntu_18)\n  command: scripts/calibre-install-latest.sh\n  when: not is_rpi and not is_ubuntu_18 and internet_available\n\n- name: Upgrade to Calibre unstable .deb's IF calibre_unstable_debs\n  command: scripts/calibre-install-unstable.sh\n  when: calibre_unstable_debs and internet_available\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "57f820564fb18eb9817d3254640a29b9b67a5d85", "filename": "roles/phpmyadmin/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "phpmyadmin_install: False\nphpmyadmin_enabled: False\nphpmyadmin_name: \"phpMyAdmin-4.8.2-all-languages\"\nphpmyadmin_name_zip: \"{{ phpmyadmin_name }}.zip\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7a81c01718a09b873d5515993f85a7f9b09e443d", "filename": "roles/config-quay-enterprise/tasks/configure_systemd.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Configure systemd environment files\n  template:\n    src: \"quay.j2\"\n    dest: \"{{ systemd_environmentfile_dir}}/{{ quay_name }}\"\n  notify: \"Restart quay service\"\n\n- name: Configure systemd unit files\n  template:\n    src: \"quay.service.j2\"\n    dest: \"{{ systemd_service_dir}}/{{ quay_service }}\"\n  notify: \"Restart quay service\"\n"}, {"commit_sha": "01c4359d8ad17ba10149ac898663e598069b9055", "sha": "47be08cf1561963cbd52f10c1962726f8268f3e1", "filename": "tasks/ssh.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\n- name: Update SSH configuration to be more secure.\n  lineinfile:\n    dest: \"{{ security_ssh_config_path }}\"\n    regexp: \"{{ item.regexp }}\"\n    line: \"{{ item.line }}\"\n    state: present\n  with_items:\n    - regexp: \"^PasswordAuthentication\"\n      line: \"PasswordAuthentication {{ security_ssh_password_authentication }}\"\n    - regexp: \"^PermitRootLogin\"\n      line: \"PermitRootLogin {{ security_ssh_permit_root_login }}\"\n    - regexp: \"^Port\"\n      line: \"Port {{ security_ssh_port }}\"\n  notify: restart ssh\n\n- name: Add configured user accounts to passwordless sudoers.\n  lineinfile:\n    dest: /etc/sudoers\n    regexp: '^{{ item }}'\n    line: '{{ item }} ALL=(ALL) NOPASSWD: ALL'\n    state: present\n    validate: 'visudo -cf %s'\n  with_items: security_sudoers_passwordless\n  when: security_sudoers_passwordless | length > 0\n\n- name: Add configured user accounts to passworded sudoers.\n  lineinfile:\n    dest: /etc/sudoers\n    regexp: '^{{ item }}'\n    line: '{{ item }} ALL=(ALL) ALL'\n    state: present\n    validate: 'visudo -cf %s'\n  with_items: security_sudoers_passworded\n  when: security_sudoers_passworded | length > 0\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "a89171f8bc2dffd3dc218b141b6044f4249beec9", "filename": "roles/calibre/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "calibre_port: 8080\n\n# http://box:8080 & http://box:8080/mobile WORK BUT THESE OTHER URL'S ARE A MESS (BOOKS RARELY DISPLAY)\ncalibre_web_path: calibre\n# In addition to: http://box:8080 http://box/books box/libros box/livres box/livros box/liv\n\ncalibre_dbpath: \"{{ content_base }}/calibre\"\n# i.e. /library/calibre (holds metadata.db + book directories + our users.sqlite)\n\ncalibre_userdb: \"{{ calibre_dbpath }}/users.sqlite\"\n# i.e. /library/calibre/users.sqlite since github.com/iiab/iiab/issues/830\n# Teachers add/remove/convert books & edit metadata using: Admin/changeme\n# Stub/Student acnts to browse: box/box, h/h, ht/ht, m/m, mx/mx, p/p, pe/pe\n# Edit accounts/permissions using:\n#   calibre-server --manage-users --userdb /library/calibre/users.sqlite\n\ncalibre_sample_book: \"Metamorphosis-jackson.epub\"\n# Must be downloadable from http://download.iiab.io/packages\n\ncalibre_src_url: \"https://raw.githubusercontent.com/kovidgoyal/calibre/master/setup/linux-installer.py\"\n\n# USE TO TEST debs.yml (RASPBIAN APPROACH!) ON DEBIAN 9.X: (now handled by calibre_via_debs in /opt/iiab/iiab/vars/*)\n#calibre_debs_on_debian: True\n# Enable unstable .deb's, not just testing .deb's: (moved to /etc/iiab/local_vars.yml & /opt/iiab/iiab/vars/default_vars.yml)\n#calibre_unstable_debs: False\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "1523e5ce1b993181c989b4aef6830e200ad068f1", "filename": "roles/network/tasks/computed_services.yml", "repository": "iiab/iiab", "decoded_content": "- name: No LAN configured - 'Appliance' mode\n  set_fact:\n    dansguardian_enabled: False\n    squid_enabled: False\n    wondershaper_enabled: False\n    iiab_network_mode: \"Appliance\"\n  when: iiab_lan_iface == \"none\" or user_lan_iface == \"none\"\n\n- name: LAN configured - 'LanController' mode\n  set_fact:\n    dansguardian_enabled: False\n    squid_enabled: False\n    wondershaper_enabled: False\n    iiab_network_mode: \"LanController\"\n  when: iiab_lan_iface != \"none\" and iiab_wan_iface == \"none\"\n\n- name: LAN configured - 'Gateway' mode\n  set_fact:\n    iiab_network_mode: \"Gateway\"\n  when: iiab_lan_iface != \"none\" and iiab_wan_iface != \"none\"\n\n- name: No LAN configured - non-dnsmasq\n  set_fact:\n    named_enabled: True\n    dhcpd_enabled: False\n    dhcp_service2: \"dhcpd\"\n  when: not dnsmasq_enabled and iiab_network_mode == \"Appliance\"\n\n- name: LAN configured - non-dnsmasq\n  set_fact:\n    named_enabled: True\n    dhcpd_enabled: True\n    dhcp_service2: \"dhcpd\"\n  when: not dnsmasq_enabled and iiab_network_mode != \"Appliance\"\n\n- name: LAN configured - dnsmasq\n  set_fact:\n    named_enabled: False\n    dhcpd_enabled: False\n    dhcp_service2: \"dnsmasq\"\n  when: dnsmasq_install and dnsmasq_enabled and iiab_network_mode != \"Appliance\"\n\n- name: Add location section to config file\n  ini_file: dest='{{ iiab_config_file }}'\n            section=network\n            option='{{ item.option }}'\n            value='{{ item.value }}'\n  with_items:\n  - option: 'iiab_network_mode_applied'\n    value: '{{ iiab_network_mode }}'\n  - option: 'dhcp_service2'\n    value: '{{ dhcp_service2 }}'\n  - option: 'dnsmasq_enabled'\n    value: '{{ dnsmasq_enabled }}'\n  - option: 'no_net_restart'\n    value: '{{ no_net_restart }}'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "85f00aec5c656093ed2af7e6230242e9ee3db0a7", "filename": "roles/config-nagios-target/tasks/firewall.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Check if firewalld is enabled - if so, use it\n  command: systemctl status firewalld\n  register: firewalld_status\n  failed_when: false\n  changed_when: false\n\n- name: Open port in firewalld (if enabled)\n  when: firewalld_status.rc == 0\n  firewalld:\n    port: \"{{firewall_port}}/{{firewall_protocol}}\"\n    permanent: true\n    state: enabled\n  notify:\n  - restart firewalld\n\n- name: Ensure iptables is correctly configured \n  when: firewalld_status.rc != 0\n  lineinfile:\n    insertafter: \"^-A INPUT .* --dport 22 .* ACCEPT\"\n    state: present\n    dest: /etc/sysconfig/iptables\n    regexp: \"^-A INPUT .* --dport {{firewall_port}} .* ACCEPT\"\n    line: \"-A INPUT -p {{firewall_protocol}} -m state --state NEW -m {{firewall_protocol}} --dport {{firewall_port}} -j ACCEPT\"\n  notify:\n  - restart iptables\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "6b3c08e0ac966599daff1e549be5f1faf4afc15a", "filename": "roles/common/tasks/configure-time.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# timedatectl.yml - configure ntp\n- name: Install chrony\n  yum:\n    name: chrony\n    state: installed\n\n- name: Enable and start chrony\n  service:\n    name: chronyd\n    enabled: true\n    state: started\n\n- name: Set system timezone\n  command: /usr/bin/timedatectl set-timezone UTC\n  when: ansible_date_time.tz != \"UTC\"\n\n- name: Check if RTC set to UTC\n  shell: |\n    set -o pipefail\n    timedatectl | awk '/RTC in local/ { print $5 }'\n  changed_when: false\n  register: chrony_local_utc\n\n- name: Set system hardware clock to UTC\n  command: |\n    set -o pipefail\n    /usr/bin/timedatectl set-local-rtc no\n  when: chrony_local_utc == 'yes'\n\n- name: Check if NTP is enabled\n  shell: |\n    set -o pipefail\n    timedatectl | awk '/NTP enabled/ { print $3 }'\n  changed_when: false\n  register: chrony_ntp_enabled\n\n- name: Set NTP enabled\n  command: /usr/bin/timedatectl set-ntp yes\n  when: chrony_ntp_enabled == 'no'\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "5a0ed15911f8d394dae2bed81a7ed24eac77c4d2", "filename": "reference-architecture/vmware-ansible/playbooks/prerequisite.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: cluster_hosts\n  gather_facts: yes\n  become: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - instance-groups\n  - rhsm\n\n- hosts: cluster_hosts\n  gather_facts: no\n  vars_files:\n  - vars/main.yaml\n  become: yes\n  roles:\n  - prerequisites\n\n- hosts: master\n  gather_facts: yes\n  vars_files:\n  - vars/main.yaml\n  become: yes\n  roles:\n  - master-prerequisites\n  - etcd-storage\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b7601aa4820e1f4563fef598027357ae0146e044", "filename": "roles/openshift-prep/tasks/prerequisites.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: \"Cleaning yum repositories\"\n  command: \"yum clean all\"\n\n- name: \"Install required packages\"\n  yum:\n    name: \"{{ item }}\"\n    state: latest\n  with_items: \"{{ required_packages }}\"\n  when: manage_packages|bool\n\n- name: \"Install debug packages (optional)\"\n  yum:\n    name: \"{{ item }}\"\n    state: latest\n  with_items: \"{{ debug_packages }}\"\n  when: install_debug_packages|bool\n\n- name: \"Update all packages (this can take a very long time)\"\n  yum:\n    name: '*'\n    state: latest\n  when: manage_packages|bool\n\n- name: \"Verify hostname\"\n  shell: hostnamectl status | awk \"/Static hostname/\"'{ print $3 }'\n  register: hostname_fqdn\n\n- name: \"Set hostname if required\"\n  hostname:\n    name: \"{{ ansible_fqdn }}\"\n  when: hostname_fqdn.stdout != ansible_fqdn\n\n- name: \"Verify SELinux is enforcing\"\n  fail:\n    msg: \"SELinux is required for OpenShift and has been detected as '{{ ansible_selinux.config_mode }}'\"\n  when: ansible_selinux.config_mode != \"enforcing\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "c5954a72a34c621da88157e5be293c3872f9343c", "filename": "roles/network/tasks/down-debian.yml", "repository": "iiab/iiab", "decoded_content": "# down-debian.yml\n\n#- name: Supply resolvconf.conf\n#  template:\n#     dest: /etc/resolvconf.conf\n#    src: network/resolvconf.j2\n\n- name: BIND may be affected\n  service:\n    name: \"{{ dns_service }}\"\n    state: stopped\n  when: named_install and dnsmasq_enabled\n\n# dhcpd_server release the interface\n- name: dhcpd_server may be affected - stopping dhcpd\n  service:\n    name: dhcpd\n    state: stopped\n  when: dhcpd_install\n\n- name: dhcpd_server may be affected - stopping dnsmasq\n  service:\n    name: dnsmasq\n    state: stopped\n  when: dnsmasq_install\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "cc8c8411b7c41671987a060089dfe2006ee0b4af", "filename": "reference-architecture/gcp/ansible/playbooks/gold-image.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: check if gold image exists\n  hosts: localhost\n  tasks:\n  - name: check for gold image\n    command: gcloud --project {{ gcloud_project }} compute images describe {{ gold_image }}\n    register: gold_image_exists\n    changed_when: false\n    ignore_errors: true\n\n- include: gold-image-include.yaml\n  when: hostvars['localhost']['gold_image_exists'] | failed\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "b3d7010f389e91cf7d2d03f3e5ed4d268f4c09e6", "filename": "playbooks/templates/stenographer-config.j2", "repository": "rocknsm/rock", "decoded_content": "{\n  \"Threads\": [\n    {\n      \"PacketsDirectory\": \"/data/stenographer/{{ item.1 }}/thread0/packets\"\n    , \"IndexDirectory\": \"/data/stenographer/{{ item.1 }}/thread0/index\"\n    , \"MaxDirectoryFiles\": 30000\n    , \"DiskFreePercentage\": 10\n    }\n  ]\n  , \"StenotypePath\": \"/usr/bin/stenotype\"\n  , \"Interface\": \"{{ item.1 }}\"\n  , \"Port\": {{ 1234 + item.0 }}\n  , \"Host\": \"127.0.0.1\"\n  , \"Flags\": [\"-v\"]\n  , \"CertPath\": \"/etc/stenographer/certs\"\n}\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "29b4ae27ba47c3bfce99a80aa63bb9aafd14a879", "filename": "roles/common/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: Restart sshd\n  service:\n    name: sshd\n    state: restarted\n...\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "00eb0495f73f92553c722f8b35f3bd057259c0b9", "filename": "tasks/section_06.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - include: section_06_level1.yml\n    tags:\n      - section06\n      - level1\n\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "23c9e705b16f18260e173be2cbc0f77f5f6f0fc9", "filename": "ops/playbooks/monitoring.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- name: Monitoring \n  hosts: vms\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - ./includes/internal_vars.yml\n\n  environment:\n    - \"{{ env }}\"\n\n  tasks:\n\n  - block:\n\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n\n    - name: source stack specific variables\n      include_vars:\n        file: ../templates/monitoring/{{ monitoring_stack }}/vars.yml\n\n#\n# section for the logger VM which is used to collect syslog from the ESX infrastructure and the UCP syslogs\n#\n    - block:\n\n      - name: Open  ports  in the firewall\n        firewalld:\n          port: \"{{ item }}\"\n          immediate: true\n          permanent: true\n          state: enabled\n        with_items: \"{{ splunk_architecture_syslog_ports }}\"\n\n      when: inventory_hostname in groups.logger\n\n#\n# Section for all Linux hosts\n#\n    - block:\n\n      - name: Copy Universal forwarder for Linux Pkg\n        copy:\n          src: \"../files/splunk/linux/{{ splunk_architecture_universal_forwarder_package }}.rpm\"\n          dest: /root/scripts/monitoring/\n\n      - name: Copy Splunk Universal Forwarder for Linux start script\n        template:\n          src: ../templates/monitoring/{{ monitoring_stack }}/start_uf_linux.sh.j2\n          dest: /root/scripts/monitoring/start_uf_linux.sh\n        notify: RestartLinuxSplunkUF\n      - file:\n          path: /root/scripts/monitoring/start_uf_linux.sh\n          mode: 0744\n        notify: RestartLinuxSplunkUF\n\n      - name: Install Universal Forwarder for Linux\n        yum:\n          name:  /root/scripts/monitoring//{{ splunk_architecture_universal_forwarder_package }}.rpm\n          state: present\n\n      - name: Copy Splunk Universal Forwarder for Linux configuration files\n        copy:\n          src: \"../files/splunk/linux/SPLUNK_HOME/\"\n          dest: \"/opt/splunkforwarder/\"\n        notify: RestartLinuxSplunkUF\n\n      - name: Copy Splunk Universal Forwarder for Linux Technical Add-ons on Docker hosts\n        copy:\n          src: \"../files/splunk/linux/DOCKER_TAS/\"\n          dest: \"/opt/splunkforwarder/\"\n          mode: preserve\n        notify: RestartLinuxSplunkUF\n        when: inventory_hostname in groups.docker\n\n      - local_action: stat path=\"../templates/monitoring/{{ monitoring_stack }}/outputs.conf.j2\"\n        register: res\n\n      - name: Debug\n        debug: var=res.stat.exists\n        when: _debug is defined\n\n      - name: Copy output.conf from template folder if present\n        template:\n           src: \"../templates/monitoring/{{ monitoring_stack }}/outputs.conf.j2\"\n           dest: \"/opt/splunkforwarder/etc/system/local/outputs.conf\"\n        notify: RestartLinuxSplunkUF\n        when: res.stat.exists == true\n\n      when: inventory_hostname in groups.vms\n      \n# end of section of non docker hosts\n\n    when: monitoring_stack is defined\n\n\n  - debug: msg=\"No splunk integration wanted\"\n    when: monitoring_stack is not defined\n\n  handlers:\n  - name: RestartLinuxSplunkUF\n    shell: /root/scripts/monitoring/start_uf_linux.sh\n    args:\n      chdir: /root/scripts/monitoring\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "6b9e77d1a634a613efbfcf5161d9158f431a9824", "filename": "roles/user-management/manage-user-passwd/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: generate-passwords.yml\n- import_tasks: idm-set-passwd.yml\n\n- name: \"New password list\"\n  debug:\n    var: user_passwords\n    verbosity: 2\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "baf58d2a689c986c0e889df561c512db695b93ae", "filename": "roles/validate-public/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Validate the public address\n  uri:\n    url: \"https://{{ hostvars['localhost']['openshift_master_cluster_public_hostname'] }}:{{ hostvars['localhost']['console_port'] }}/healthz/ready\"\n    validate_certs: False\n    status_code: 200\n    method: GET\n"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "848158354d153b5a9e66b7de9578517ac3730d13", "filename": "tasks/security_policy_apply/Linux.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Unzip patch file\"\n  unarchive:\n    src: \"{{ security_policy_oracle_artifact }}\"\n    dest: \"{{ java_path }}{{ java_package }}1.{{ java_major_version }}.0_\\\n      {{ java_minor_version }}/jre/lib/security\"\n    owner: \"root\"\n    group: \"root\"\n    mode: \"0755\"\n  become: True\n  when:\n    - (java_major_version|int == 8 and java_minor_version|int < 151) or (java_major_version|int < 8)\n\n- name: \"Apply patch file\"\n  copy:\n    src: \"{{ java_path }}{{ java_package }}1.{{ java_major_version }}.0_\\\n      {{ java_minor_version }}/jre/lib/security/\\\n      {{ security_patch_folders[java_major_version|int] }}/{{ item }}\"\n    dest: \"{{ java_path }}{{ java_package }}1.{{ java_major_version }}.0_\\\n    {{ java_minor_version }}/jre/lib/security/\"\n    remote_src: True\n    directory_mode: True\n  loop:\n    - \"local_policy.jar\"\n    - \"US_export_policy.jar\"\n    - \"README.txt\"\n  become: True\n  when:\n    - (java_major_version|int == 8 and java_minor_version|int < 151) or (java_major_version|int < 8)\n\n- name: \"Apply setting\"\n  replace:\n    path: \"{{ java_path }}{{ java_package }}1.{{ java_major_version }}.0_\\\n      {{ java_minor_version }}/jre/lib/security/java.security\"\n    regexp: \"#crypto.policy=unlimited\"\n    replace: \"crypto.policy=unlimited\"\n  become: True\n  when:\n    - java_major_version|int == 8\n    - java_minor_version|int >= 151\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "2b8aada65b7e7cbd7cda78e37fff2b53d824052f", "filename": "playbooks/provision-nfs-server/nfs-server.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Create NFS server'\n  hosts: nfs-server\n  roles:\n  - role: nfs-server\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "1b9624d62e2a3678762bff5ba535d6fd40d2ded2", "filename": "roles/osp/admin-network/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: \"manage-networks.yml\"\n\n- import_tasks: \"manage-subnets.yml\"\n\n- import_tasks: \"manage-routers.yml\"\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "3bec06b2c6da5cae0939500582b4f8734f178f9c", "filename": "roles/cloud-openstack/defaults/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\nopenstack_venv: \"{{ playbook_dir }}/configs/.venvs/openstack\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "6f0a5cb413ca7e912c68dde2801845aa7519a00f", "filename": "playbooks/update-dhcp-config.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Update dhcpd config'\n  hosts: dhcp-servers\n  roles:\n  - role: dhcp\n  tags: \n  - update_dhcp_config\n\n\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "a95db66ed45aede09dff05d8c4dd690466d8597c", "filename": "roles/kubernetes-master/vars/main.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\nipalloc_range: \"172.30.0.0/16\"\n\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "a407191638843da2b4c6a2f94cdc6b1e2548502d", "filename": "roles/zookeeper/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for zookeeper\n- name: Restart zookeeper\n  shell: restart zookeeper\n  sudo: yes\n\n- name: Start zookeeper\n  shell: start zookeeper\n  sudo: yes\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "3d94eaedf779dbb399d5628fbc994e736433b72f", "filename": "tasks/create_repo_rubygems_group_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_rubygems_group\n    args: \"{{ _nexus_repos_rubygems_defaults|combine(item) }}\""}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "849169c71aa714dc6dc717e061b702a4b38752b9", "filename": "roles/config-bonding/tasks/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "8c28c7ebbe725c2ccf726a39d2e9263fd58b2106", "filename": "tasks/Win32NT/fetch/local.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Copy artifact to destination\n  win_copy:\n    src: '{{ transport_local }}'\n    dest: '{{ java_download_path }}\\'\n  register: file_downloaded\n  retries: 5\n  delay: 2\n  until: file_downloaded is succeeded\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "97e5377b1b9254eaec0c8e2777fc921eb5c4f2f3", "filename": "reference-architecture/gcp/ansible/playbooks/roles/deployment-create/defaults/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ndeployment_name_with_prefix: '{{ prefix }}-{{ deployment_name }}'\ndeployment_config_template: '{{ playbook_dir }}/../../deployment-manager/{{ deployment_name }}-config.yaml.j2'\ndeployment_config: '{{ playbook_dir }}/../../deployment-manager/{{ deployment_name_with_prefix }}-config.yaml'\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "be2eeced2c0c83126e93410a2c4c409b982c310e", "filename": "roles/cloud-ec2/tasks/venv.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Clean up the environment\n  file:\n    dest: \"{{ ec2_venv }}\"\n    state: absent\n  when: clean_environment\n\n- name: Install requirements\n  pip:\n    name:\n      - boto>=2.5\n      - boto3\n    state: latest\n    virtualenv: \"{{ ec2_venv }}\"\n    virtualenv_python: python2.7\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "505f7b3a83ef9a0657f82ab953277f836d949aa4", "filename": "playbooks/openstack/openshift-cluster/cluster_hosts.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ng_all_hosts: \"{{ groups['meta-clusterid_' ~ cluster_id] | default([])\n                 | intersect(groups['meta-environment_' ~ cluster_env] | default([])) }}\"\n\ng_etcd_hosts: \"{{ g_all_hosts | intersect(groups['meta-host-type_etcd'] | default([])) }}\"\n\ng_lb_hosts: \"{{ g_all_hosts | intersect(groups['meta-host-type_lb'] | default([])) }}\"\n\ng_nfs_hosts: \"{{ g_all_hosts | intersect(groups['meta-host-type_nfs'] | default([])) }}\"\n\ng_glusterfs_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type-glusterfs'] | default([])) }}\"\n\ng_master_hosts: \"{{ g_all_hosts | intersect(groups['meta-host-type_master'] | default([])) }}\"\n\ng_new_master_hosts: \"{{ g_all_hosts | intersect(groups['meta-host-type_new_master'] | default([])) }}\"\n\ng_node_hosts: \"{{ g_all_hosts | intersect(groups['meta-host-type_node'] | default([])) }}\"\n\ng_new_node_hosts: \"{{ g_all_hosts | intersect(groups['meta-host-type_new_node'] | default([])) }}\"\n\ng_infra_hosts: \"{{ g_node_hosts | intersect(groups['meta-sub-host-type_infra'] | default([])) }}\"\n\ng_compute_hosts: \"{{ g_node_hosts | intersect(groups['meta-sub-host-type_compute'] | default([])) }}\"\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "72414a20ac256a9266c895d25c3e978560e429fa", "filename": "roles/ovirt-engine-db-dump/defaults/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_engine_etc_conf_path: \"/etc/ovirt-engine/engine.conf.d/\"\novirt_engine_db_dump_dwh: False\novirt_engine_db_dump_start_services: True\n\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "2dbe18f347709fd84a11f6fbe712fe0a35fea906", "filename": "roles/serverspec/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for serverspec\n\n- name: upload serverspecs\n  synchronize:\n    src: ../../../tests\n    dest: \"{{ serverspec_tests_path }}\"\n    recursive: yes\n    delete: yes\n  when: serverspec_run_tests and serverspec_install_bundler and serverspec_upload_folder\n  tags:\n    - serverspec\n\n- name: upload marathon runtime serverspecs\n  template:\n    src: marathon_runtime_spec.rb.j2\n    dest: \"{{ serverspec_tests_path }}/tests/spec/marathon/marathon_runtime_spec.rb\"\n    mode: 0755\n  sudo: yes\n  when: serverspec_run_tests and serverspec_upload_folder\n  tags:\n    - serverspec\n\n- name: install bundler\n  command: gem install bundler --no-ri --no-rdoc\n  args:\n    creates: /usr/local/bin/bundler\n  when: serverspec_run_tests and serverspec_install_bundler\n  tags:\n    - serverspec\n\n- name: install bundle files\n  command: bundle install --path vendor\n  args:\n    chdir: \"{{ serverspec_tests_path }}/tests\"\n    creates: \"{{ serverspec_tests_path }}/tests/vendor\"\n  when: serverspec_run_tests\n  tags:\n    - serverspec\n\n- name: run serverspec tests\n  sudo: yes\n  command: \"bundle exec rake serverspec:{{ test_role }}\"\n  args:\n    chdir: \"{{ serverspec_tests_path }}/tests\"\n  when: test_role is defined and serverspec_run_tests\n  tags:\n    - serverspec\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "3a8b624ea05aea186008b3d2519e754f1c495dcd", "filename": "tasks/bug-tweaks/bug-centos7-resource-busy.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Stat /proc/sys/fs/may_detach_mounts (CentOS/RedHat)\n  stat:\n    path: /proc/sys/fs/may_detach_mounts\n  register: _may_detach_mounts\n  check_mode: no\n\n- name: Ensure fs.may_detach_mounts is set to avoid 'Device or resource busy' (CentOS/RedHat)\n  become: true\n  sysctl:\n    name: fs.may_detach_mounts\n    value: \"1\"\n    sysctl_file: /etc/sysctl.d/99-docker.conf\n    reload: yes\n  when:\n    - docker_enable_mount_flag_fix | bool\n    - _may_detach_mounts.stat.exists\n\n- name: Stat /etc/sysctl.d/99-docker.conf (CentOS/RedHat)\n  stat:\n    path: /etc/sysctl.d/99-docker.conf\n  register: _sysctl_docker\n  check_mode: no\n  when:\n    - not docker_enable_mount_flag_fix | bool\n\n- name: Unset fs.may_detach_mounts (CentOS/RedHat)\n  become: true\n  sysctl:\n    name: fs.may_detach_mounts\n    value: \"0\"\n    sysctl_file: /etc/sysctl.d/99-docker.conf\n    reload: yes\n  when:\n    - not docker_enable_mount_flag_fix | bool\n    - _sysctl_docker.stat.exists\n\n# Keep for compatibility reasons of this role. Now everything is in the same file.\n- name: Remove systemd drop-in for Docker Mount Flags slave configuration (CentOS/RedHat)\n  become: true\n  file:\n    path: /etc/systemd/system/docker.service.d/mountflags-slave.conf\n    state: absent\n  notify: restart docker\n\n- name: Set MountFlags option to \"slave\" to prevent \"device busy\" errors on CentOS/RedHat 7.3 kernels (CentOS/RedHat)\n  set_fact:\n    docker_systemd_service_config_tweaks: \"{{ docker_systemd_service_config_tweaks + \\\n      _systemd_service_config_tweaks }}\"\n  vars:\n    _systemd_service_config_tweaks:\n      - 'MountFlags=slave'\n  when:\n    - docker_enable_mount_flag_fix | bool\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "a108896db65b4102a351626d1e6e05696c7f4948", "filename": "roles/kubernetes-master/tasks/main.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- name: install cockpit packages\n  package:\n    name: \"{{ item }}\"\n    state: present\n    update_cache: yes\n  with_items:\n    - cockpit\n    - cockpit-kubernetes\n\n- name: enable and start cockpit service\n  service:\n    name: \"cockpit.socket\"\n    state: started\n    enabled: yes\n\n- name: copy kubernetes deployment script\n  template:\n    src: deploy_kubernetes.j2\n    dest: /root/deploy_kubernetes.sh\n    mode: 0755\n\n- name: deploy kubernetes\n  shell: /root/deploy_kubernetes.sh\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f69bccfcbb02ddebe59d8cb0629bea382a13d05a", "filename": "roles/nfs-server/tasks/configure_lvm.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Set VG name\"\n  set_fact:\n    nfs_vg_name: \"{{ nfs_vg_name | default(default_nfs_vg_name) }}\"\n\n- name: \"Set LV name\"\n  set_fact:\n    nfs_lv_name: \"{{ nfs_lv_name | default(default_nfs_lv_name) }}\"\n\n- name: \"Set NFS share basename\"\n  set_fact:\n    nfs_share_basedir: \"{{ nfs_share_basedir | default(default_nfs_share_basedir) }}\"\n\n- name: \"Setup and create PV & VG\"\n  lvg:\n    vg: \"{{ nfs_vg_name }}\"\n    pvs: \"{{ nfs_storage_device }}\"\n    force: yes\n\n- name: \"Setup LV\"\n  lvol: \n    vg: \"{{ nfs_vg_name }}\"\n    lv: \"{{ nfs_lv_name }}\"\n    force: yes\n    size: \"100%VG\"\n\n- name: \"Create file system on share\"\n  filesystem:\n    fstype: xfs\n    dev: \"/dev/mapper/{{ nfs_vg_name }}-{{ nfs_lv_name }}\"\n\n- name: \"Ensure the base dir for NFS shares exists\" \n  file:\n    path: \"{{ nfs_share_basedir }}\"\n    state: directory\n\n- name: \"Mount NFS base dir\"\n  mount:\n    src: \"/dev/mapper/{{ nfs_vg_name }}-{{ nfs_lv_name }}\" \n    path: \"{{ nfs_share_basedir }}\"\n    fstype: xfs \n    state: mounted\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "5ef170e52c654d4261481dfb24f27ff0e8aa67f7", "filename": "roles/network/tasks/dansguardian.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install DansGuardian packages\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - dansguardian\n  tags:\n    - download\n\n- name: Copy DansGuardian config file (Fedora)\n  template:\n    src: roles/network/templates/squid/dansguardian.conf.j2\n    dest: /etc/dansguardian/dansguardian.conf\n    owner: dansguardian\n    group: dansguardian\n    mode: 0640\n  when: ansible_distribution == \"Fedora\"\n\n- name: Copy DansGuardian config file (debuntu)\n  template:\n    src: roles/network/templates/squid/dansguardian.conf.debian.j2\n    dest: /etc/dansguardian/dansguardian.conf\n    owner: dansguardian\n    group: dansguardian\n    mode: 0640\n  when: is_debuntu\n\n- name: Copy DansGuardian config file (CentOS)\n  template:\n    src: roles/network/templates/squid/dansguardian.conf.centos.j2\n    dest: /etc/dansguardian/dansguardian.conf\n    owner: dansguardian\n    group: vscan\n    mode: 0640\n  when: ansible_distribution == \"CentOS\"\n\n- name: Create 'dansguardian' log directory (OS's other than CentOS)\n  file:\n    path: /var/log/dansguardian\n    owner: dansguardian\n    group: dansguardian\n    mode: 0750\n    state: directory\n  when: ansible_distribution != \"CentOS\"\n\n- name: Create DansGuardian log directory (CentOS)\n  file:\n    path: /var/log/dansguardian\n    owner: dansguardian\n    group: vscan\n    mode: 0750\n    state: directory\n  when: ansible_distribution == \"CentOS\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "cd89c0c71032762a649b9c7498f3e8819d0e0642", "filename": "roles/mediawiki/tasks/install.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install packages required by MediaWiki\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - \"php{{ php_version }}-intl\"\n    - \"php{{ php_version }}-mbstring\"\n  tags:\n    - download\n\n- name: Download MediaWiki software, per roles/mediawiki/defaults/main.yml\n  get_url:\n    url: \"{{ mediawiki_download_base_url }}/{{ mediawiki_src }}\"\n    dest: \"{{ downloads_dir }}\"\n    timeout: \"{{ download_timeout }}\"\n#   force: yes\n#   backup: yes\n  register: mediawiki_download_output\n  when: internet_available\n\n- name: Unpack download to permanent location\n  unarchive:\n    src: \"{{ downloads_dir }}/{{ mediawiki_src }}\"\n    dest: \"{{ mediawiki_install_path }}\"\n    owner: root\n    group: \"{{ apache_user }}\"\n    mode: 0755\n    keep_newer: yes\n\n- name: MySQL database needs to be running if we are trying to create a new db\n  service:\n    state: started\n    name: \"{{ mysql_service }}\"\n\n- name: Create MySQL mediawiki database\n  mysql_db:\n    name: \"{{ mediawiki_db_name }}\"\n    state: present\n\n- name: Create MySQL mediawiki database user\n  mysql_user:\n    name: \"{{ mediawiki_db_user }}\"\n    password: \"{{ mediawiki_db_user_password }}\"\n    priv: \"{{ mediawiki_db_name }}.*:ALL,GRANT\"\n    state: present\n\n- name: Configure MediaWiki\n  shell: >\n    php '{{ mediawiki_abs_path }}/maintenance/install.php'\n    --dbname={{ mediawiki_db_name }}\n    --dbserver=\"localhost\"\n    --installdbuser={{ mediawiki_db_user }}\n    --installdbpass={{ mediawiki_db_user_password }}\n    --dbuser={{ mediawiki_db_user }}\n    --dbpass={{ mediawiki_db_user_password }}\n    --scriptpath=/mediawiki\n    --lang=en\n    --pass={{ mediawiki_admin_user_password }}\n    \"{{ mediawiki_site_name }}\"\n    \"{{ mediawiki_admin_user }}\"\n  args:\n    chdir: \"{{ mediawiki_abs_path }}\"\n    creates: \"{{ mediawiki_abs_path }}/LocalSettings.php\"\n\n- name: Copy mediawiki httpd conf file\n  template:\n    src: mediawiki.conf.j2\n    dest: \"/etc/{{ apache_config_dir }}/mediawiki.conf\"\n\n- name: Enable httpd conf file if we are disabled (debuntu)\n  file:\n    src: /etc/apache2/sites-available/mediawiki.conf\n    dest: /etc/apache2/sites-enabled/mediawiki.conf\n    state: link\n  when: mediawiki_enabled and is_debuntu\n\n- name: Remove httpd conf file if we are disabled (OS's other than debuntu)\n  file:\n    path: /etc/apache2/sites-enabled/mediawiki.conf\n    state: absent\n  when: not mediawiki_enabled and is_debuntu\n\n- name: Restart Apache, so it picks up the new aliases\n  service:\n    name: \"{{ apache_service }}\"\n    state: restarted\n\n- name: Add 'mediawiki' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: mediawiki\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: mediawiki\n    - option: description\n      value: '\"mediawiki is a blog and web site management application.\"'\n    - option: mediawiki_src\n      value: \"{{ mediawiki_src }}\"\n    - option: mediawiki_abs_path\n      value: \"{{ mediawiki_abs_path }}\"\n    - option: mediawiki_db_name\n      value: \"{{ mediawiki_db_name }}\"\n    - option: mediawiki_db_user\n      value: \"{{ mediawiki_db_user }}\"\n    - option: mediawiki_url\n      value: \"{{ mediawiki_url }}\"\n    - option: mediawiki_full_url\n      value: \"{{ mediawiki_full_url }}\"\n    - option: mediawiki_enabled\n      value: \"{{ mediawiki_enabled }}\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "a7d0d9595b11ba900e1a768f04efcb6d9f0cc6b1", "filename": "ops/playbooks/clean_all.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- hosts: all_vms\n  name: Delete ALl VMs\n  gather_facts: false\n  connection: local\n  user: remote\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Power off VMs\n      vmware_guest:\n        hostname: \"{{ vcenter_hostname }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        validate_certs: \"{{ vcenter_validate_certs }}\"\n        esxi_hostname: \"{{ esxi_host }}\"\n        datacenter: \"{{ datacenter }}\"\n        folder: \"{{ datacenter }}/vm{{ folder_name }}\"\n        name: \"{{ inventory_hostname }}\"\n        state: poweredoff\n      vars:\n        ansible_connection: local\n      failed_when: false\n\n    - name: Delete VMs\n      vmware_guest:\n        hostname: \"{{ vcenter_hostname }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        validate_certs: \"{{ vcenter_validate_certs }}\"\n        esxi_hostname: \"{{ esxi_host }}\"\n        datacenter: \"{{ datacenter }}\"\n        folder: \"{{ datacenter }}/vm{{ folder_name }}\"\n        name: \"{{ inventory_hostname }}\"\n        disk: \"{{ disks_specs }}\"\n        state: absent\n      vars:\n        ansible_connection: local\n\n#############################################################################xxx\n# \n# play 2: SimpliVity Specific, delete Dummy VMs and backup policies\n#\n################################################################################\n\n- hosts: localhost\n  gather_facts: false\n  name: Cleanup SimpliVity Specific\n  connection: local\n  user: remote\n  become: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  environment: \"{{ env }}\"\n\n  tasks:\n  - block:\n    - name: Build the list of Dummy VM names\n      set_fact: dummy_vms=\"{{ dummy_vms | default([]) + [ prefix+'-in-dockvols-'+item  ] }}\"\n      vars:\n        prefix: \"{{ dummy_vm_prefix }}\"\n      with_items:\n        - \"{{ datastores }}\"\n\n    - name: Power off Dummy VMs\n      vmware_guest:\n        hostname: \"{{ vcenter_hostname }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        validate_certs: \"{{ vcenter_validate_certs }}\"\n        datacenter: \"{{ datacenter }}\"\n        folder: \"{{ datacenter }}/vm{{ folder_name }}\"\n        name: \"{{ item }}\"\n        state: poweredoff\n      vars:\n        ansible_connection: local\n      with_items: \"{{ dummy_vms }}\"\n      failed_when: false\n\n    - name: Delete  Dummy VMs\n      vmware_guest:\n        hostname: \"{{ vcenter_hostname }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        validate_certs: \"{{ vcenter_validate_certs }}\"\n        datacenter: \"{{ datacenter }}\"\n        folder: \"{{ datacenter }}/vm{{ folder_name }}\"\n        name: \"{{ item }}\"\n        state: absent\n      vars:\n        ansible_connection: local\n      with_items: \"{{ dummy_vms }}\"\n    when: svt_cleanup is defined and svt_cleanup\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "a2f553f4ccc387ac91e9af534f4c87b20a995aff", "filename": "playbooks/provisioning/openstack/README.md", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "# OpenStack Provisioning\n\nThis directory contains [Ansible][ansible] playbooks and roles to create\nOpenStack resources (servers, networking, volumes, security groups,\netc.). The result is an environment ready for OpenShift installation\nvia [openshift-ansible].\n\nWe provide everything necessary to be able to install OpenShift on\nOpenStack (including the DNS and load balancer servers when\nnecessary). In addition, we work on providing integration with the\nOpenStack-native services (storage, lbaas, baremetal as a service,\ndns, etc.).\n\n\n## OpenStack Requirements\n\nBefore you start the installation, you need to have an OpenStack\nenvironment to connect to. You can use a public cloud or an OpenStack\nwithin your organisation. It is also possible to\nuse [Devstack][devstack] or [TripleO][tripleo]. In the case of\nTripleO, we will be running on top of the **overcloud**.\n\nThe OpenStack release must be Newton (for Red Hat OpenStack this is\nversion 10) or newer. It must also satisfy these requirements:\n\n* Heat (Orchestration) must be available\n* The deployment image (CentOS 7 or RHEL 7) must be loaded\n* The deployment flavor must be available to your user\n  - `m1.medium` / 4GB RAM + 40GB disk should be enough for testing\n  - look at\n    the [Minimum Hardware Requirements page][hardware-requirements]\n    for production\n* The keypair for SSH must be available in openstack\n* `keystonerc` file that lets you talk to the openstack services\n   * NOTE: only Keystone V2 is currently supported\n\nOptional:\n* External Neutron network with a floating IP address pool\n\n\n## Installation\n\nThere are four main parts to the installation:\n\n1. [Preparing Ansible and dependencies](#1-preparing-ansible-and-dependencies)\n2. [Configuring the desired OpenStack environment and OpenShift cluster](#2-configuring-the-openstack-environment-and-openshift-cluster)\n3. [Creating the OpenStack resources (VMs, networking, etc.)](#3-creating-the-openstack-resources-vms-networking-etc)\n4. [Installing OpenShift](#4-installing-openshift)\n\nThis guide is going to install [OpenShift Origin][origin]\nwith [CentOS 7][centos7] images with minimal customisation.\n\nWe will create the VMs for running OpenShift, in a new Neutron\nnetwork, assign Floating IP addresses and configure DNS.\n\nThe OpenShift cluster will have a single Master node that will run\n`etcd`, a single Infra node and two App nodes.\n\nYou can look at\nthe [Advanced Configuration page][advanced-configuration] for\nadditional options.\n\n\n\n### 1. Preparing Ansible and dependencies\n\nFirst, you need to select where to run [Ansible][ansible] from (the\n*Ansible host*). This can be the computer you read this guide on or an\nOpenStack VM you'll create specifically for this purpose.\n\nWe will use\na\n[Docker image that has all the dependencies installed][control-host-image] to\nmake things easier. If you don't want to use Docker, take a look at\nthe [Ansible host dependencies][ansible-dependencies] and make sure\nthey're installed.\n\nYour *Ansible host* needs to have the following:\n\n1. Docker\n2. `keystonerc` file with your OpenStack credentials\n3. SSH private key for logging in to your OpenShift nodes\n\nAssuming your private key is `~/.ssh/id_rsa` and `keystonerc` in your\ncurrent directory:\n\n```bash\n$ sudo docker run -it -v ~/.ssh:/mnt/.ssh:Z \\\n     -v $PWD/keystonerc:/root/.config/openstack/keystonerc.sh:Z \\\n     redhatcop/control-host-openstack bash\n```\n\nThis will create the container, add your SSH key and source your\n`keystonerc`. It should be set up for the installation.\n\nYou can verify that everything is in order:\n\n\n```bash\n$ less .ssh/id_rsa\n$ ansible --version\n$ openstack image list\n```\n\n\n### 2. Configuring the OpenStack Environment and OpenShift Cluster\n\nThe configuration is all done in an Ansible inventory directory. We\nwill clone the [openshift-ansible-contrib][contrib] repository and set\nthings up for a minimal installation.\n\n\n```\n$ git clone https://github.com/openshift/openshift-ansible-contrib\n$ cp -r openshift-ansible-contrib/playbooks/provisioning/openstack/sample-inventory/ inventory\n```\n\nIf you're testing multiple configurations, you can have multiple\ninventories and switch between them.\n\n#### OpenStack Configuration\n\nThe OpenStack configuration is in `inventory/group_vars/all.yml`.\n\nOpen the file and plug in the image, flavor and network configuration\ncorresponding to your OpenStack installation.\n\n```bash\n$ vi inventory/group_vars/all.yml\n```\n\n1. Set the `openstack_ssh_public_key` to your OpenStack keypair name.\n   - See `openstack keypair list` to find the keypairs registered with\n   OpenShift.\n   - This must correspond to your private SSH key in `~/.ssh/id_rsa`\n2. Set the `openstack_external_network_name` to the floating IP\n   network of your openstack.\n   - See `openstack network list` for the list of networks.\n   - It's often called `public`, `external` or `ext-net`.\n3. Set the `openstack_default_image_name` to the image you want your\n   OpenShift VMs to run.\n   - See `openstack image list` for the list of available images.\n4. Set the `openstack_default_flavor` to the flavor you want your\n   OpenShift VMs to use.\n   - See `openstack flavor list` for the list of available flavors.\n\n**NOTE**: In most OpenStack environments, you will also need to\nconfigure the forwarders for the DNS server we create. This depends on\nyour environment.\n\nLaunch a VM in your OpenStack and look at its `/etc/resolv.conf` and\nput the IP addresses into `public_dns_nameservers` in\n`inventory/group_vars/all.yml`.\n\n\n#### OpenShift configuration\n\nThe OpenShift configuration is in `inventory/group_vars/OSEv3.yml`.\n\nThe default options will mostly work, but unless you used the large\nflavors for a production-ready environment, openshift-ansible's\nhardware check will fail.\n\nLet's disable those checks by putting this in\n`inventory/group_vars/OSEv3.yml`:\n\n```yaml\nopenshift_disable_check: disk_availability,memory_availability\n```\n\n**NOTE**: The default authentication method will allow **any username\nand password** in! If you're running this in a public place, you need\nto set up access control.\n\nFeel free to look at\nthe [Sample OpenShift Inventory][sample-openshift-inventory] and\nthe [advanced configuration][advanced-configuration].\n\n\n### 3. Creating the OpenStack resources (VMs, networking, etc.)\n\nWe will install the DNS server roles using ansible galaxy and then run\nthe openstack provisioning playbook. The `ansible.cfg` file we provide\nhas useful defaults -- copy it to the directory you're going to run\nAnsible from.\n\n```bash\n$ ansible-galaxy install -r openshift-ansible-contrib/playbooks/provisioning/openstack/galaxy-requirements.yaml -p openshift-ansible-contrib/roles\n$ cp openshift-ansible-contrib/playbooks/provisioning/openstack/ansible.cfg ansible.cfg\n```\n(you will only need to do this once)\n\nThen run the provisioning playbook -- this will create the OpenStack\nresources:\n\n```bash\n$ ansible-playbook -i inventory openshift-ansible-contrib/playbooks/provisioning/openstack/provision.yaml\n```\n\nIf you're using multiple inventories, make sure you pass the path to\nthe right one to `-i`.\n\n\n### 4. Installing OpenShift\n\nWe will use the `openshift-ansible` project to install openshift on\ntop of the OpenStack nodes we have prepared:\n\n```bash\n$ git clone https://github.com/openshift/openshift-ansible\n$ ansible-playbook -i inventory openshift-ansible/playbooks/byo/config.yml\n```\n\n\n### Next Steps\n\nAnd that's it! You should have a small but functional OpenShift\ncluster now.\n\nTake a look at [how to access the cluster][accessing-openshift]\nand [how to remove it][uninstall-openshift] as well as the more\nadvanced configuration:\n\n* [Accessing the OpenShift cluster][accessing-openshift]\n* [Removing the OpenShift cluster][uninstall-openshift]\n* Set Up Authentication (TODO)\n* [Multiple Masters with a load balancer][loadbalancer]\n* [External Dns][external-dns]\n* Multiple Clusters (TODO)\n* [Cinder Registry][cinder-registry]\n* [Bastion Node][bastion]\n\n\n[ansible]: https://www.ansible.com/\n[openshift-ansible]: https://github.com/openshift/openshift-ansible\n[devstack]: https://docs.openstack.org/devstack/\n[tripleo]: http://tripleo.org/\n[ansible-dependencies]: ./advanced-configuration.md#dependencies-for-localhost-ansible-controladmin-node\n[contrib]: https://github.com/openshift/openshift-ansible-contrib\n[control-host-image]: https://hub.docker.com/r/redhatcop/control-host-openstack/\n[hardware-requirements]: https://docs.openshift.org/latest/install_config/install/prerequisites.html#hardware\n[origin]: https://www.openshift.org/\n[centos7]: https://www.centos.org/\n[sample-openshift-inventory]: https://github.com/openshift/openshift-ansible/blob/master/inventory/byo/hosts.example\n[advanced-configuration]: ./advanced-configuration.md\n[accessing-openshift]: ./advanced-configuration.md#accessing-the-openshift-cluster\n[uninstall-openshift]: ./advanced-configuration.md#removing-the-openshift-cluster\n[loadbalancer]: ./advanced-configuration.md#multi-master-configuration\n[external-dns]: ./advanced-configuration.md#dns-configuration-variables\n[cinder-registry]: ./advanced-configuration.md#creating-and-using-a-cinder-volume-for-the-openshift-registry\n[bastion]: ./advanced-configuration.md#configure-static-inventory-and-access-via-a-bastion-node\n\n\n\n## License\n\nLike the rest of the openshift-ansible-contrib repository, the code\nhere is licensed under Apache 2.\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "5e484e75f5f6307f5c6c414ce58bacd0ee129e31", "filename": "roles/openshift-prep/tasks/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# Starting Point for OpenShift Installation and Configuration\n- include: prerequisites.yml\n  tags: [prerequisites]\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "bfe929ca47e7dbec73b7aaac658686bb48dc8228", "filename": "roles/vpn/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- block:\n    - name: Include WireGuard role\n      include_role:\n        name: wireguard\n      tags: wireguard\n      when: wireguard_enabled and ansible_distribution == 'Ubuntu'\n\n    - include_tasks: ubuntu.yml\n      when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'\n\n    - name: Ensure that the strongswan user exist\n      user:\n        name: strongswan\n        group: nogroup\n        shell: \"{{ strongswan_shell }}\"\n        home: \"{{ strongswan_home }}\"\n        state: present\n\n    - name: Install strongSwan\n      package: name=strongswan state=present\n\n    - import_tasks: ipsec_configuration.yml\n    - import_tasks: openssl.yml\n      tags: update-users\n    - import_tasks: distribute_keys.yml\n    - import_tasks: client_configs.yml\n      delegate_to: localhost\n      become: no\n      tags: update-users\n\n    - name: strongSwan started\n      service:\n        name: strongswan\n        state: started\n        enabled: true\n\n    - meta: flush_handlers\n  rescue:\n    - debug: var=fail_hint\n      tags: always\n    - fail:\n      tags: always\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7b66997eb5742476edb49fb346295b9586250ca8", "filename": "roles/config-clair/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# Base Configurations\nclair_name: clair\nclair_service: \"{{ clair_name }}.service\"\nclair_address: \"\"\n\n#Systemd\nsystemd_service_dir: /usr/lib/systemd/system\nsystemd_environmentfile_dir: /etc/sysconfig\n\n# Clair\nclair_image: quay.io/coreos/clair-jwt:v2.0.4\nclair_config_dir: /var/lib/clair/config\nclair_container_config_dir: /config\nclair_ssl_trust_configure: False\nclair_ssl_trust_src_file: /tmp/clair-ssl-trust.crt\nclair_ssl_trust_host_file: \"{{ clair_config_dir }}/ca.crt\"\nclair_ssl_trust_container_file: /usr/local/share/ca-certificates/ca.crt\n\n# Quay\nquay_enterprise_address: \"\"\n\n# PostgreSQL\n# External Databases\npostgresql_ssl_enabled: False\npostgresql_username: \"clair\"\npostgresql_password: \"clair\"\npostgresql_database: \"clair\"\npostgresql_host: \"\"\npostgresql_port: \"5432\"\npostgresql_db_uri: \"postgresql://{{ postgresql_username }}:{{ postgresql_password }}@{{ postgresql_host if postgresql_host is defined and postgresql_host|trim != '' else hostvars[inventory_hostname]['ansible_eth0']['ipv4']['address'] }}:{{ postgresql_port | default('5432') }}/{{ postgresql_database | default('clair') }}{{ '?sslmode=disable' if not postgresql_ssl_enabled }}\"\n\n# Ports\nclair_host_proxy_port: 6060\nclair_container_proxy_port: 6060\nclair_host_api_port: 6061\nclair_container_api_port: 6061\n\n# SSL\n#clair_ssl_enable: True\n#clair_ssl_key_file: \"\"\n#clair_ssl_cert_file: \"\"\n#clair_ssl_generate_city: Raleigh\n#clair_ssl_generate_state: NC\n#clair_ssl_generate_country: US\n#clair_ssl_generate_organization: Red Hat\n#clair_ssl_generate_organizational_unit: CoP\n#clair_ssl_generate_days_validity: 365"}, {"commit_sha": "6d10af54bdbf8e81c3d90a70ffea87b4d2c20eb2", "sha": "9a0395f4b32444006e3d95ed99737895f1d77d10", "filename": "tasks/options.yml", "repository": "Oefenweb/ansible-wordpress", "decoded_content": "# tasks file for wordpress, options\n---\n- name: add options\n  command: \"wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} '{{ item.1.name }}' '{{ item.1.value }}'\"\n  register: check_installation_options\n  failed_when: False\n  changed_when: \"'Added' in check_installation_options.stdout\"\n  with_subelements:\n    - wordpress_installs\n    - options\n  when: item.1.command == 'add'\n  tags: [configuration, wordpress, wordpress-options]\n\n- name: update options\n  command: \"wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} '{{ item.1.name }}' '{{ item.1.value }}'\"\n  register: check_installation_options\n  changed_when: \"'unchanged' not in check_installation_options.stdout\"\n  with_subelements:\n    - wordpress_installs\n    - options\n  when: item.1.command == 'update'\n  tags: [configuration, wordpress, wordpress-options]\n\n- name: delete options\n  command: \"wp-cli --allow-root --no-color --path='{{ item.0.path }}' option {{ item.1.command }} '{{ item.1.name }}'\"\n  register: check_installation_options\n  failed_when: False\n  changed_when: \"'Could not delete' not in check_installation_options.stderr\"\n  with_subelements:\n    - wordpress_installs\n    - options\n  when: item.1.command == 'delete'\n  tags: [configuration, wordpress, wordpress-options]\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "77dd9d7f9d3a73f8ffbb0cea7fbb773ba39fd269", "filename": "tasks/create_repo_maven_group_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_maven_group\n    args: \"{{ _nexus_repos_maven_defaults|combine(item) }}\""}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "6c35c739af5cea9321ffc6741ec0fcd6c2985a1e", "filename": "roles/config-pxe/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: pxe-server\n  roles:\n  - config-pxe \n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "e146ecf900253cf8bf60f25ac402ce9a2d9b4b2e", "filename": "roles/usb-lib/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Add a content directory for links to be located\n  file:\n    dest: \"{{ doc_root }}/local_content\"\n    state: directory\n    owner: \"{{ apache_user }}\"\n    group: \"{{ iiab_admin_user }}\"\n    mode: 0775\n\n- name: Copy mount file to usbmount when enabled\n  template:\n    src: mount.d/70-usb-library\n    dest: /etc/usbmount/mount.d/\n    owner: root\n    group: root\n    mode: 0751\n  when: usb_lib_enabled\n\n- name: Install udev to systemd link -> usbmount\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    mode: \"{{ item.mode }}\"\n  with_items:\n    - { src: 'usbmount@.service.j2' , dest: '/etc/systemd/system/usbmount@.service', mode: '0644' }\n    - { src: 'usbmount.rules.j2' , dest: '/etc/udev/rules.d/usbmount.rules', mode: '0644' }\n    - { src: 'iiab-usb-lib-show-all-on' , dest: '/usr/bin/', mode: '0755' }\n    - { src: 'iiab-usb-lib-show-all-off' , dest: '/usr/bin/', mode: '0755' }\n\n- name: Enable exFAT and NTFS\n  lineinfile: \n    regexp: '^FILESYSTEMS.*'\n    line: 'FILESYSTEMS=\"vfat ext2 ext3 ext4 hfsplus exfat fuseblk ntfs\"'\n    dest: /etc/usbmount/usbmount.conf\n\n- name: Copy umount file to usbmount when enabled\n  template:\n    src: umount.d/70-usb-library\n    dest: /etc/usbmount/umount.d\n    owner: root\n    group: root\n    mode: 0751\n  when: usb_lib_enabled\n\n- name: Remove mount file to usbmount when not enabled\n  file:\n    path: /etc/usbmount/mount.d/70-usb-library\n    state: absent\n  when: not usb_lib_enabled\n\n- name: Remove umount file to usbmount when not enabled\n  file:\n    path: /etc/usbmount/umount.d/70-usb-library\n    state: absent\n  when: not usb_lib_enabled\n\n- name: Put variable in iiab.env that enables display of content at root of USB\n  lineinfile:\n    dest: /etc/iiab/iiab.env\n    regexp: \"^IIAB_USB_LIB_SHOW_ALL.*\"\n    line: \"IIAB_USB_LIB_SHOW_ALL={{ iiab_usb_lib_show_all }}\"\n\n- name: Add Apache config for content directory\n  template:\n    src: content_dir.conf\n    dest: \"/etc/{{ apache_config_dir }}\"\n  when: usb_lib_enabled\n\n- name: Create the link to enable (debuntu)\n  file:\n    src: \"/etc/{{ apache_config_dir }}/content_dir.conf\"\n    dest: /etc/apache2/sites-enabled/content_dir.conf\n    state: link\n  when: is_debuntu\n\n- name: Remove the link that enables (debuntu)\n  file:\n    src: \"/etc/{{ apache_config_dir }}/content_dir.conf\"\n    dest: /etc/apache2/sites-enabled/content_dir.conf\n    state: absent\n  when: is_debuntu and not usb_lib_enabled\n\n- name: Remove Apache config for content directory\n  file:\n    name: \"/etc/{{ apache_config_dir }}/content_dir.conf\"\n    state: absent\n  when: not usb_lib_enabled\n\n- name: Add usb-lib to service list\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: usb-lib\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: usb-lib\n    - option: description\n      value: '\"usb-lib automounts Teacher Content on USB drives to /library/www/html/local_content, so students can browse it almost immediately at http://box/usb\"'\n    - option: enabled\n      value: \"{{ usb_lib_enabled }}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "06ac8b391c160c9c73fa7c4a00fe8dcd911ef96a", "filename": "roles/network/tasks/static.yml", "repository": "iiab/iiab", "decoded_content": "# supply an ifcfg if no gateway detected but wan_ip is set\n# set user_wan_iface: <device> and use wan_* for static info\n- name: Supply WAN interface file\n  template: src=network/ifcfg-WAN.j2\n            dest=/etc/sysconfig/network-scripts/ifcfg-WAN\n\n- include_tasks: NM.yml\n  when: 'ansible_distribution_version <= \"20\" and wan_ip != \"dhcp\"'\n\n- name: Re-read network config files\n  shell: 'nmcli con reload'\n  ignore_errors: yes\n  when: 'ansible_distribution_version >= \"21\" and wan_ip != \"dhcp\"'\n\n- name: Use upstream nameserver until named is installed\n  lineinfile: dest=/etc/resolv.conf\n              line='nameserver {{ wan_nameserver }}'\n              create=yes\n              state=present\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e5c3a0d5a8087bc89c1a58f9c0d35d7a01c19a67", "filename": "playbooks/manage-confluence-space/playbook.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "--- \n- hosts: confluence\n  roles: \n    - manage-confluence-space\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "aed7576318b2a6e8e76e0fe6c7a729602c147dc4", "filename": "roles/security/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: Install tools\n  apt: name=\"{{ item }}\" state=latest\n  with_items:\n    - unattended-upgrades\n\n- name: Configure unattended-upgrades\n  template: src=50unattended-upgrades.j2 dest=/etc/apt/apt.conf.d/50unattended-upgrades owner=root group=root mode=0644\n\n- name: Periodic upgrades configured\n  template: src=10periodic.j2 dest=/etc/apt/apt.conf.d/10periodic owner=root group=root mode=0644\n\n- name: Find directories for minimizing access\n  stat:\n    path: \"{{ item }}\"\n  register: minimize_access_directories\n  with_items:\n    - '/usr/local/sbin'\n    - '/usr/local/bin'\n    - '/usr/sbin'\n    - '/usr/bin'\n    - '/sbin'\n    - '/bin'\n\n- name: Minimize access\n  file: path='{{ item.stat.path }}' mode='go-w' recurse=yes\n  when: item.stat.isdir\n  with_items: \"{{ minimize_access_directories.results }}\"\n  no_log: True\n\n- name: Change shadow ownership to root and mode to 0600\n  file: dest='/etc/shadow' owner=root group=root mode=0600\n\n- name: change su-binary to only be accessible to user and group root\n  file: dest='/bin/su' owner=root group=root mode=0750\n\n- name: Collect Use of privileged commands\n  shell: >\n    /usr/bin/find {/usr/local/sbin,/usr/local/bin,/sbin,/bin,/usr/sbin,/usr/bin} -xdev \\( -perm -4000 -o -perm -2000 \\) -type f | awk '{print \"-a always,exit -F path=\" $1 \" -F perm=x -F auid>=500 -F auid!=4294967295 -k privileged\" }'\n  args:\n    executable: /bin/bash\n  register: privileged_programs\n\n# Core dumps\n\n- name: Restrict core dumps (with PAM)\n  lineinfile: dest=/etc/security/limits.conf line=\"* hard core 0\" state=present\n\n- name: Restrict core dumps (with sysctl)\n  sysctl: name=fs.suid_dumpable value=0 ignoreerrors=yes sysctl_set=yes reload=yes state=present\n\n# Kernel fixes\n\n- name: Disable Source Routed Packet Acceptance\n  sysctl: name=\"{{item}}\" value=0 ignoreerrors=yes sysctl_set=yes reload=yes state=present\n  with_items:\n    - net.ipv4.conf.all.accept_source_route\n    - net.ipv4.conf.default.accept_source_route\n  notify:\n    - flush routing cache\n\n- name: Disable ICMP Redirect Acceptance\n  sysctl: name=\"{{item}}\" value=0 ignoreerrors=yes sysctl_set=yes reload=yes state=present\n  with_items:\n    - net.ipv4.conf.all.accept_redirects\n    - net.ipv4.conf.default.accept_redirects\n\n- name: Disable Secure ICMP Redirect Acceptance\n  sysctl: name=\"{{item}}\" value=0 ignoreerrors=yes sysctl_set=yes reload=yes state=present\n  with_items:\n    - net.ipv4.conf.all.secure_redirects\n    - net.ipv4.conf.default.secure_redirects\n  notify:\n    - flush routing cache\n\n- name: Enable Bad Error Message Protection\n  sysctl: name=net.ipv4.icmp_ignore_bogus_error_responses value=1 ignoreerrors=yes sysctl_set=yes reload=yes state=present\n  notify:\n    - flush routing cache\n\n- name: Enable RFC-recommended Source Route Validation\n  sysctl: name=\"{{item}}\" value=1 ignoreerrors=yes sysctl_set=yes reload=yes state=present\n  with_items:\n    - net.ipv4.conf.all.rp_filter\n    - net.ipv4.conf.default.rp_filter\n  notify:\n    - flush routing cache\n\n- name: Do not send ICMP redirects (we are not a router)\n  sysctl: name=net.ipv4.conf.all.send_redirects value=0\n\n- name: SSH config\n  template: src=sshd_config.j2 dest=/etc/ssh/sshd_config owner=root group=root mode=0644\n  notify:\n    - restart ssh\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "660d5769adefb8ac3fd2a4d4dbbc7619579d5a8e", "filename": "roles/fsf/meta/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\ngalaxy_info:\n  author: RockNSM Contributors\n  description: A role to deploy FSF to a RockNSM sensor\n  company: RockNSM Foundation\n\n  license: Apache\n\n  min_ansible_version: 2.7\n\n  platforms:\n    - name: CentOS\n      versions:\n        - 7\n    - name: RedHat\n      versions:\n        - 7\n\n  galaxy_tags:\n    - fsf\n    - rocknsm\n    - fileanalysis\n    - yara\n\ndependencies:\n  - name: Install and configure filebeat\n    role: filebeat\n    vars:\n      filebeat_configs:\n        - { src: 'fb-fsf.yml.j2', dest: 'fsf.yml' }\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "750a4ade9e9a4da8efebea2188e57ff48dbfa3e1", "filename": "reference-architecture/aws-ansible/playbooks/roles/cloudformation-infra/files/user_data_bastion.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#cloud-config\nusers:\n- default\n\nsystem_info:\n  default_user:\n    name: ec2-user\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4eec57a1bbc67f56887025e00196babb6b0d73c3", "filename": "roles/config-iscsi-client/tests/group_vars/iscsi.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\niscsi_target: \"192.168.1.21\"\niscsi_brand: \"NETAPP\"\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "f2804ca8f6f9da4400e1adddd9bc990a00d4d47f", "filename": "roles/cloud-digitalocean/tasks/prompts.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- pause:\n    prompt: |\n      Enter your API token. The token must have read and write permissions (https://cloud.digitalocean.com/settings/api/tokens):\n    echo: false\n  register: _do_token\n  when:\n    - do_token is undefined\n    - lookup('env','DO_API_TOKEN')|length <= 0\n\n- name: Set the token as a fact\n  set_fact:\n    algo_do_token: \"{{ do_token | default(_do_token.user_input|default(None)) | default(lookup('env','DO_API_TOKEN'), true) }}\"\n\n- name: Get regions\n  uri:\n    url: https://api.digitalocean.com/v2/regions\n    method: GET\n    status_code: 200\n    headers:\n      Content-Type: \"application/json\"\n      Authorization: \"Bearer {{ algo_do_token }}\"\n  register: _do_regions\n\n- name: Set facts about thre regions\n  set_fact:\n    do_regions: \"{{ _do_regions.json.regions | sort(attribute='slug') }}\"\n\n- name: Set default region\n  set_fact:\n    default_region: >-\n      {% for r in do_regions %}\n      {%- if r['slug'] == \"nyc3\" %}{{ loop.index }}{% endif %}\n      {%- endfor %}\n\n- pause:\n    prompt: |\n      What region should the server be located in?\n        {% for r in do_regions %}\n        {{ loop.index }}. {{ r['slug'] }}     {{ r['name'] }}\n        {% endfor %}\n\n      Enter the number of your desired region\n      [{{ default_region }}]\n  register: _algo_region\n  when: region is undefined\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "61969e2158d8c71461fe697b541a372d40777ec7", "filename": "roles/notifications/md-to-html/tasks/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n\n- name: \"Install additional packages\"\n  package:\n    name: \"{{ item }}\"\n    state: \"{{ prereq_state | default('installed') }}\"\n  with_items:\n  - pandoc\n  \n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "e2c4f86a17fb179aeedacffca4999ca072eb0f66", "filename": "roles/cloud-openstack/tasks/venv.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Clean up the environment\n  file:\n    dest: \"{{ openstack_venv }}\"\n    state: absent\n  when: clean_environment\n\n- name: Install requirements\n  pip:\n    name: shade\n    state: latest\n    virtualenv: \"{{ openstack_venv }}\"\n    virtualenv_python: python2.7\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ca3a57a92b6e2c92f1b027d30b5c93b1b4e57bd8", "filename": "roles/ansible/tower/manage-inventories/tasks/process-group.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Load up the inventory (group)\"\n  uri:\n    url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/groups/\"\n    user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n    password: \"{{ ansible_tower.admin_password }}\"\n    force_basic_auth: yes\n    method: POST\n    body: \"{{ lookup('template', 'group.j2') }}\"\n    body_format: 'json'\n    headers:\n      Content-Type: \"application/json\"\n      Accept: \"application/json\"\n    validate_certs: no\n    status_code: 200,201,400\n\n# Utilize the `rest_get` library routine to ensure REST pagination is handled\n- name: \"Get the updated list of existing groups\"\n  rest_get:\n    host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n    rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n    rest_password: \"{{ ansible_tower.admin_password }}\"\n    api_uri: \"/api/v2/groups/\"\n  register: existing_groups_output\n\n- name: \"Get the group id based on the group name\"\n  set_fact:\n    group_id: \"{{ item.id }}\"\n  when:\n  - item.name|trim == group.name|trim\n  with_items:\n  - \"{{ existing_groups_output.rest_output }}\"\n\n- name: \"Process the inventory group members\"\n  include_tasks: process-group-member.yml\n  with_items:\n  - \"{{ group.hosts }}\"\n  loop_control:\n    loop_var: group_member\n\n- name: \"Clear/Update facts\"\n  set_fact:\n    group_id: ''\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "bfd26caf7ab11c290c6671bcab11bc5bd4a1b046", "filename": "handlers/main.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: restart auditd\n    service: name=auditd state=restarted\n    changed_when: False\n    ignore_errors: True\n\n  - name: restart rsyslog\n    service: name=rsyslog state=restarted\n    changed_when: False\n    ignore_errors: True\n\n  - name: restart ssh\n    service: name=ssh state=restarted\n    changed_when: False\n    ignore_errors: True\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ffdd498988f8a911f2f9ad08cb1a164cbdba9897", "filename": "roles/user-management/manage-idm-users/tests/create_idm.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n# This test covers the full feature set provided by the role\n\n- name: Create Test Identities\n  hosts: ipa\n\n  vars_files:\n    - vars/idm.json\n\n  vars:\n    ipa_admin_user: admin\n    ipa_admin_password: test123\n    ipa_host: idm.example.com \n\n  roles:\n    - user-management/manage-idm-users\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "6dfd2e624fa57953a8005ce3e59853de01559de8", "filename": "roles/0-init/tasks/tz.yml", "repository": "iiab/iiab", "decoded_content": "- name: Check if the TZ is not already set via /etc/localtime - Can Fail\n  shell: readlink /etc/localtime | awk -F \"zoneinfo/\" '{print $2}'\n  register: TZ_set\n  ignore_errors: True\n\n- name: Set local and iiab TZ to UTC if /etc/localtime is not set\n  set_fact:\n    local_tz: \"UTC\"\n    iiab_TZ: \"UTC\"\n  when: TZ_set.stdout == \"\"\n\n- name: Override ansible on timezone if TZ set\n  set_fact:\n    local_tz: \"{{ TZ_set.stdout }}\"\n  when: TZ_set.stdout != \"\"\n\n- name: Using iiab TZ for local TZ\n  set_fact:\n    local_tz: \"{{ iiab_TZ }}\"\n  when: iiab_TZ is defined and iiab_TZ != \"\" and iiab_TZ != \"TZ_set.stdout\"\n\n- name: Set default Timezone from iiab TZ (debuntu)\n  shell: timedatectl set-timezone {{ iiab_TZ }}\n  when: is_debuntu and iiab_TZ is defined and iiab_TZ != \"\" and iiab_TZ != \"TZ_set.stdout\"\n\n- name: Set default Timezone from iiab TZ (redhat)\n  file:\n    path: /etc/localtime\n    src: \"/usr/share/zoneinfo/{{ iiab_TZ }}\"\n    force: yes\n    state: link\n  when: is_redhat and iiab_TZ is defined and iiab_TZ != \"\" and iiab_TZ != \"TZ_set.stdout\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8818ce01993d876a026d94d0c6f10ed183caeac7", "filename": "roles/config-routes/tasks/prereq-Fedora.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install additional packages needed to process tasks\"\n  package:\n    name: \"{{ item }}\"\n    state: latest\n  with_items:\n  - libselinux-python\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4a7334a8a8ca21a84006ec856cf39729b6a7164e", "filename": "roles/dns/manage-dns-zones/tasks/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Fail when required items are not defined\"\n  fail:\n    msg: \"view name and zones must be defined for each dictionary record\"\n  when:\n    - (item.name is not defined) or (item.zones is not defined)\n  with_items:\n    - \"{{ dns_data.views | default({}) }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e5e888fe7fdcb48946d8041bc37743cffa5a4766", "filename": "roles/user-management/manage-local-user-ssh-authkeys/test/playbook.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Add a test user\"\n  hosts: all\n  tasks:\n  - user:\n      name: \"{{ user_name }}\"\n      comment: \"Test User\"\n    when:\n      - user_name != \"root\"\n\n# Test the role to update the access keys\n- name: \"Update {{ user_name }} access\"\n  hosts: all\n  roles:\n    - role: manage-local-user-ssh-authkeys\n\n# Test the SSH Key access by running a remote command on machine\n#\n\n- name: \"Testing authorized ssh keyfile\" \n  hosts: all\n  tasks:\n  - name: \"Test authorized key for {{ user_name }} on {{ ansible_host }}\"\n    raw: \"ssh -v -i id_rsa_user1 {{ user_name }}@{{ ansible_host }} /bin/true\"\n    delegate_to: localhost\n    register: result\n    become: False\n    changed_when: False\n    failed_when:\n      result.rc != 0\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b7c01a063b1ede7235f696f964fa16f16d057740", "filename": "reference-architecture/ansible-tower-integration/tower_config_aws/tower_unconfig_aws/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n\n- name: Remove workflow-ocp-aws-install\n  command: tower-cli workflow delete --name=\"workflow-ocp-aws-install\"\n\n- name: Remove aws-openshift-install job template\n  tower_job_template:\n    name: aws-openshift-install\n    job_type: run\n    project: openshift-ansible-contrib\n    playbook: \"reference-architecture/aws-ansible/playbooks/openshift-install.yaml\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove infrastructure job template\n  tower_job_template:\n    name: aws-infrastructure\n    job_type: run\n    project: openshift-ansible-contrib\n    playbook: \"reference-architecture/aws-ansible/playbooks/infrastructure.yaml\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove create-httpd-file job template\n  tower_job_template:\n    name: create_httpd_file\n    job_type: run\n    project: openshift-ansible-contrib\n    playbook: \"reference-architecture/ansible-tower-integration/create_httpd_file/create_httpd_file.yaml\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove insights job template\n  tower_job_template:\n    name: redhat-access-insights-client\n    job_type: run\n    project: ansible-redhat-access-insights-client\n    playbook: \"redhat-access-insights-client.yaml\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower group\n  tower_group:\n    name: aws\n    inventory: \"aws-inventory\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower inventory\n  tower_inventory:\n    name: \"aws-inventory\"\n    organization: \"Default\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower credential for machine\n  tower_credential:\n    name: aws-privkey\n    kind: ssh\n    organization: \"Default\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower credential for aws\n  tower_credential:\n    name: ec2\n    state: absent\n    kind: aws\n    organization: \"Default\"\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower project\n  tower_project:\n    name: \"openshift-ansible-contrib\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower project\n  tower_project:\n    name: \"ansible-redhat-access-insights-client\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower organization\n  tower_organization:\n    name: \"Default\"\n    description: \"Set to Default since the trial license only allows one organization. You can change it if you have deep pockets\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "4f0d8e0f37cd043e20b70523826bafaaaf0ed09e", "filename": "roles/2-common/tasks/prep.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install iiab-extra repos\n  template:\n    backup: no\n    dest: /etc/yum.repos.d/iiab-extra.repo\n    src: iiab-extra.repo\n    owner: root\n    group: root\n    mode: 0666\n\n- name: Install iiab-testing repos\n  template:\n    backup: no\n    dest: /etc/yum.repos.d/iiab-testing.repo\n    src: iiab-testing.repo\n    owner: root\n    group: root\n    mode: 0666\n\n- name: Get the createrepo program\n  package:\n    name: createrepo\n    state: present\n\n- name: Install local repo file\n  template:\n    dest: /etc/yum.repos.d/iiab-local.repo\n    src: local.repo\n    owner: root\n    group: root\n    mode: 0644\n\n- name: Create local repo\n  shell: createrepo {{ yum_packages_dir }}\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "0b03d4ef5793c04c5f9db504cbcee078426f05b4", "filename": "roles/dns/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: restart named\n  service: name=named state=restarted\n\n- name: reload named\n  service: name=named state=reloaded\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "2f48d68ea8d1e98a21111562713798fd9f7f7313", "filename": "roles/user-management/list-users-by-group/tasks/generate-list-of-users.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Populate list of users\"\n  set_fact:\n    list_of_users: \"{{ list_of_users | default([]) }} + [ {{ item }} ]\"\n  when:\n  - item.user_name in user_group.members\n  with_items:\n  - \"{{ users }}\"\n\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "c59679ce103577080ab576a1feb785cbd184d67b", "filename": "roles/notifications/send-email/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n# This test covers the full feature set provided by the role\n\n- name: Test email/send role\n  hosts: localhost\n\n  roles:\n    - email/send\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "1783f7e327932977f3ff67b6bd65793bf3106115", "filename": "roles/prerequisites/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Gather facts\n  openshift_facts:\n    role: common\n\n- block:\n  - name: Clear yum cache\n    command: \"yum clean all\"\n    ignore_errors: true\n\n  - name: Install the required rpms\n    package:\n      name: \"{{ item }}\"\n      state: latest\n    with_items: \"{{ openshift_required_packages }}\"\n\n  - name: Start NetworkManager and network\n    service:\n      name: \"{{ item }}\"\n      state: restarted\n      enabled: true\n    with_items:\n    - NetworkManager\n    - network\n\n  - name: Determine if firewalld is installed\n    rpm_q:\n      name: \"firewalld\"\n      state: present\n    register: firewalld_installed\n    failed_when: false\n\n  - name: Stop firewalld\n    service:\n      name: firewalld\n      state: stopped\n      enabled: false\n    when:\n    - \"{{ firewalld_installed.installed_versions | default([]) | length > 0 }}\"\n\n  - name: Start iptables\n    service:\n      name: iptables\n      state: started\n      enabled: true\n\n  - name: Start docker\n    service:\n      name: docker\n      state: started\n      enabled: true\n\n  when: not openshift.common.is_atomic | bool\n\n# Fail as early as possible if Atomic and old version of Docker\n- block:\n  - name: Determine Atomic Host Docker Version\n    shell: 'CURLY=\"{\"; docker version --format \"$CURLY{json .Server.Version}}\"'\n    register: l_atomic_docker_version\n\n  - assert:\n      msg: Installation on Atomic Host requires Docker 1.12 or later. Attempting to patch.\n      that:\n      - l_atomic_docker_version.stdout | replace('\"', '') | version_compare('1.12','>=')\n\n  rescue:\n  - name: Patching Atomic instances\n    shell: atomic host upgrade\n    register: patched\n\n  - name: Reboot when patched\n    shell: sleep 5 && shutdown -r now \"Reboot due to Atomic Patching\"\n    async: 1\n    poll: 0\n    ignore_errors: true\n    when: patched.changed\n\n  - name: Wait for hosts to be back\n    pause:\n      seconds: 60\n    delegate_to: 127.0.0.1\n    when: patched.changed\n\n  when: openshift.common.is_atomic | bool\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "75bf5d70ed409662d0ec0bfab1a0eb1a241c7778", "filename": "roles/scm/add-webhooks-github/tests/inventory/host_vars/localhost.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\nansible_connection: local\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "7388a5980dc9f9fce219c33aed3a808b2bd0e91b", "filename": "reference-architecture/aws-ansible/playbooks/openshift-minor-upgrade.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  connection: local\n  gather_facts: yes\n  become: no\n  vars_files:\n  - vars/main.yaml\n  roles:\n  # Group systems\n  - cfn-outputs\n  - instance-groups\n\n- include: minor-update.yaml\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "d8e4c92f0294b88b8abb4db37cb52d132e834e7e", "filename": "tasks/Linux/security_policy.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: 'Fetch oracle security policy with {{ java_unlimited_policy_transport }} transport'\n  include_tasks: '{{ transport_driver }}'\n  with_first_found:\n    - 'fetch/security-fetch/security-fetch-{{ java_unlimited_policy_transport }}.yml'\n    - 'fetch/unknown-transport.yml'\n  loop_control:\n    loop_var: transport_driver\n  when:\n    - java_unlimited_policy_enabled\n    - java_distribution == 'oracle_java'\n    - java_full_version is version('8.151', '<')\n\n- name: Become block\n  block:\n    - name: Unzip patch file\n      unarchive:\n        src: '{{ security_policy_java_artifact }}'\n        dest: '{{ java_path }}/{{ java_folder }}/jre/lib/security'\n        remote_src: true\n        owner: root\n        group: root\n        mode: 0755\n\n    - name: Apply patch file\n      copy:\n        src: \"{{ java_path }}/{{ java_folder }}/jre/lib/security/\\\n          {{ security_patch_folders[java_major_version|int] }}/{{ policy_item }}\"\n        dest: '{{ java_path }}/{{ java_folder }}/jre/lib/security/'\n        remote_src: true\n        directory_mode: true\n        owner: root\n        group: root\n        mode: 0644\n      loop:\n        - local_policy.jar\n        - US_export_policy.jar\n        - README.txt\n      loop_control:\n        loop_var: policy_item\n  when: java_full_version is version('8.151', '<')\n  become: true\n\n- name: Apply setting\n  replace:\n    path: '{{ java_path }}/{{ java_folder }}/jre/lib/security/java.security'\n    regexp: '#crypto.policy=unlimited'\n    replace: 'crypto.policy=unlimited'\n  when: java_major_version | int < 9\n  become: true\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "677254353595f1c4ed5d837d598eb8138a8c4e0b", "filename": "roles/config-docker/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install, configure and enable Docker\"\n  import_tasks: docker.yml\n  when: \n  - docker_install|default(False)\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e4a41dd1ea63fba264e3b5379b26b9f4ff38a41e", "filename": "roles/manage-jira/tasks/prepare_vars.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Configure Jira Variables\n  set_fact:\n    jira_url: \"{{ atlassian.jira.url | default(atlassian.url) }}\"\n    jira_username: \"{{ atlassian.jira.username | default(atlassian.username) }}\"\n    jira_password: \"{{ atlassian.jira.password | default(atlassian.password) }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "54752e5c08fa93ab74e5be024783cbc3bd97aa8a", "filename": "roles/manage-jira/tasks/create_project.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Create Jira Project\n  uri:\n    url: \"{{ jira_url }}/rest/api/2/project\"\n    method: POST\n    user: \"{{ jira_username }}\"\n    password: \"{{ jira_password }}\"\n    return_content: yes\n    force_basic_auth: yes\n    body_format: json\n    header:\n      - Accept: 'application/json'\n      - Content-Type: 'application/json'\n    body: \"{ 'key': '{{ atlassian.jira.project.key }}',\n             'name': '{{ atlassian.jira.project.name }}',\n             'projectTypeKey': '{{ atlassian.jira.project.type_key | default('software')}}',\n             'projectTemplateKey': '{{ atlassian.jira.project.template_key | default('com.pyxis.greenhopper.jira:gh-simplified-scrum')}}',\n             'description': '{{ atlassian.jira.project.description }}',\n             'lead': '{{ atlassian.jira.lead }}',\n             'assigneeType': 'PROJECT_LEAD',\n             'avatarId': 10200,\n             'permissionScheme': '{{ atlassian.jira.permission_scheme.id | default(PermissionScheme) }}',\n             'notificationScheme': 10000, \n             'categoryId': '{{ CategoryID }}' \n           }\"\n    status_code: 201         \n  register: result\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "a5d21237cbde47d54117b745b80b9d1b057ab168", "filename": "playbooks/roles/sensor-common/tasks/deploy.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# deploy.yml - Common tasks for ROCK\n- import_tasks: prechecks.yml\n- import_tasks: configure.yml\n- import_tasks: configure-time.yml\n- import_tasks: configure-pipelining.yml\n- import_tasks: install.yml\n\n...\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "ffc888763c1e4a0ecdf7fdf71ee2f0ec64ca519c", "filename": "roles/dns_adblocking/tasks/ubuntu.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Ubuntu | Dnsmasq profile for apparmor configured\n  template:\n    src: usr.sbin.dnsmasq.j2\n    dest: /etc/apparmor.d/usr.sbin.dnsmasq\n    owner: root\n    group: root\n    mode: 0600\n  when: apparmor_enabled|default(false)|bool == true\n  notify:\n    - restart dnsmasq\n\n- name: Ubuntu | Enforce the dnsmasq AppArmor policy\n  shell: aa-enforce usr.sbin.dnsmasq\n  when: apparmor_enabled|default(false)|bool == true\n  tags: ['apparmor']\n\n- name: Ubuntu | Ensure that the dnsmasq service directory exist\n  file:\n    path: /etc/systemd/system/dnsmasq.service.d/\n    state: directory\n    mode: 0755\n    owner: root\n    group: root\n\n- name: Ubuntu | Setup the cgroup limitations for the ipsec daemon\n  template:\n    src: 100-CustomLimitations.conf.j2\n    dest: /etc/systemd/system/dnsmasq.service.d/100-CustomLimitations.conf\n  notify:\n    - daemon-reload\n    - restart dnsmasq\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "194980124c01b15c410348f2d1bf82c4c71db2f4", "filename": "dev/playbooks/config_networking.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: vms\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n  \n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Change hostname\n      local_action:\n        module: vmware_vm_shell\n        hostname: \"{{ vcenter_hostname }}\"\n        username: \"{{ vcenter_username }}\"\n        password:  \"{{ vcenter_password }}\"\n        validate_certs: \"{{ vcenter_validate_certs }}\" \n        vm_id: \"{{ inventory_hostname }}\"\n        vm_username: \"{{ vm_username }}\"\n        vm_password: \"{{ vm_password }}\"\n        vm_shell: /bin/echo\n        vm_shell_args: \"{{ inventory_hostname }}.{{ domain_name }} > /etc/hostname\"\n\n    - name: Update hostname\n      local_action:\n        module: vmware_vm_shell\n        hostname: \"{{ vcenter_hostname }}\"\n        username: \"{{ vcenter_username }}\"\n        password:  \"{{ vcenter_password }}\"\n        validate_certs: \"{{ vcenter_validate_certs }}\" \n        vm_id: \"{{ inventory_hostname }}\"\n        vm_username: \"{{ vm_username }}\"\n        vm_password: \"{{ vm_password }}\"\n        vm_shell: /usr/bin/hostnamectl\n        vm_shell_args: \"set-hostname {{ inventory_hostname }}\"\n\n    - name: Add new connection\n      local_action:\n        module: vmware_vm_shell\n        hostname: \"{{ vcenter_hostname }}\"\n        username: \"{{ vcenter_username }}\"\n        password:  \"{{ vcenter_password }}\"\n        validate_certs: \"{{ vcenter_validate_certs }}\" \n        vm_id: \"{{ inventory_hostname }}\"\n        vm_username: \"{{ vm_username }}\"\n        vm_password: \"{{ vm_password }}\"\n        vm_shell: /usr/bin/nmcli\n        vm_shell_args: \"con add con-name {{ nic_name }} ifname {{ nic_name }} type ethernet ip4 {{ ip_addr }} gw4 {{ gateway }}\"\n\n    - name: Bring connection up\n      local_action:\n        module: vmware_vm_shell\n        hostname: \"{{ vcenter_hostname }}\"\n        username: \"{{ vcenter_username }}\"\n        password:  \"{{ vcenter_password }}\"\n        validate_certs: \"{{ vcenter_validate_certs }}\" \n        vm_id: \"{{ inventory_hostname }}\"\n        vm_username: \"{{ vm_username }}\"\n        vm_password: \"{{ vm_password }}\"\n        vm_shell: /usr/bin/nmcli\n        vm_shell_args: \"con up {{ nic_name }}\"\n\n    - name: Enable connection autoconnect\n      local_action:\n        module: vmware_vm_shell\n        hostname: \"{{ vcenter_hostname }}\"\n        username: \"{{ vcenter_username }}\"\n        password:  \"{{ vcenter_password }}\"\n        validate_certs: \"{{ vcenter_validate_certs }}\" \n        vm_id: \"{{ inventory_hostname }}\"\n        vm_username: \"{{ vm_username }}\"\n        vm_password: \"{{ vm_password }}\"\n        vm_shell: /usr/bin/nmcli\n        vm_shell_args: \"con mod {{ nic_name }} connection.autoconnect yes\"\n\n    - name: Update /etc/hosts\n      template:\n        src: ../templates/hosts.j2\n        dest: /etc/hosts\n        unsafe_writes: yes\n\n    - name: Update DNS settings\n      template:\n        src: ../templates/resolv.conf.j2\n        dest: /etc/resolv.conf\n        unsafe_writes: yes\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "c8e609069b7df676fce73442c0064ef5d954e81a", "filename": "tasks/httpd_reverse_proxy_config.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n\n- name: Copy {{ httpd_package_name }} vhost\n  template:\n    src: \"nexus-vhost.conf\"\n    dest: \"{{ httpd_config_dir }}\"\n  notify:\n    - httpd-service-reload\n    - wait-for-httpd\n\n- name: Copy SSL certificate file\n  copy:\n    src: \"{{ httpd_ssl_certificate_file }}\"\n    dest: \"{{ certificate_file_dest }}\"\n    mode: 0600\n  when: httpd_copy_ssl_files\n  notify:\n    - httpd-service-reload\n    - wait-for-httpd\n\n- name: Copy SSL certificate key file\n  copy:\n    src: \"{{ httpd_ssl_certificate_key_file }}\"\n    dest: \"{{ certificate_key_dest }}\"\n    mode: 0600\n  when: httpd_copy_ssl_files\n  notify:\n    - httpd-service-reload\n    - wait-for-httpd\n\n- name: Setsebool httpd_can_network_connect\n  seboolean:\n    name: httpd_can_network_connect\n    persistent: yes\n    state: yes\n  when: ansible_selinux.status is defined and ansible_selinux.status == \"enabled\"\n\n- meta: flush_handlers\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "72237997416054b6ceb4dcba9ceeb992ab8e7f71", "filename": "ops/playbooks/roles/hpe.openports/meta/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "galaxy_info:\n  author: your name\n  description: your description\n  company: your company (optional)\n\n  # If the issue tracker for your role is not on github, uncomment the\n  # next line and provide a value\n  # issue_tracker_url: http://example.com/issue/tracker\n\n  # Some suggested licenses:\n  # - BSD (default)\n  # - MIT\n  # - GPLv2\n  # - GPLv3\n  # - Apache\n  # - CC-BY\n  license: license (GPLv2, CC-BY, etc)\n\n  min_ansible_version: 1.2\n\n  # If this a Container Enabled role, provide the minimum Ansible Container version.\n  # min_ansible_container_version:\n\n  # Optionally specify the branch Galaxy will use when accessing the GitHub\n  # repo for this role. During role install, if no tags are available,\n  # Galaxy will use this branch. During import Galaxy will access files on\n  # this branch. If Travis integration is configured, only notifications for this\n  # branch will be accepted. Otherwise, in all cases, the repo's default branch\n  # (usually master) will be used.\n  #github_branch:\n\n  #\n  # platforms is a list of platforms, and each platform has a name and a list of versions.\n  #\n  # platforms:\n  # - name: Fedora\n  #   versions:\n  #   - all\n  #   - 25\n  # - name: SomePlatform\n  #   versions:\n  #   - all\n  #   - 1.0\n  #   - 7\n  #   - 99.99\n\n  galaxy_tags: []\n    # List tags for your role here, one per line. A tag is a keyword that describes\n    # and categorizes the role. Users find roles by searching for tags. Be sure to\n    # remove the '[]' above, if you add tags to this list.\n    #\n    # NOTE: A tag is limited to a single word comprised of alphanumeric characters.\n    #       Maximum 20 tags per role.\n\ndependencies: []\n  # List your role dependencies here, one per line. Be sure to remove the '[]' above,\n  # if you add dependencies to this list."}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "5296050789e8283020af0548bef391988f46d335", "filename": "roles/marathon/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for marathon\n- name: upload marathon template service\n  template:\n    src: marathon.conf.j2\n    dest: /etc/init/marathon.conf\n    mode: 0755\n  notify:\n    - restart marathon\n  sudo: yes\n  tags:\n    - marathon\n\n- name: set marathon hostname\n  sudo: yes\n  copy:\n    content: \"{{ marathon_hostname }}\"\n    dest: /etc/marathon/conf/hostname\n    mode: 0644\n  notify:\n    - restart marathon\n  tags:\n    - marathon\n\n- name: create marathon artifact store directory\n  file:\n    path: \"{{ marathon_artifact_store_dir }}\"\n    state: directory\n    mode: 0755\n  when: marathon_artifact_store_dir is defined\n  sudo: yes\n  notify:\n    - restart marathon\n  tags:\n    - marathon\n\n- name: set marathon artifact store\n  sudo: yes\n  copy:\n    content: \"{{ marathon_artifact_store }}\"\n    dest: /etc/marathon/conf/artifact_store\n    mode: 0644\n  notify:\n    - restart marathon\n  tags:\n    - marathon\n\n- name: remove marathon override\n  sudo: yes\n  file:\n    path: /etc/init/marathon.override\n    state: absent\n  notify:\n    - restart marathon\n  tags:\n    - marathon\n\n- name: install wait script\n  sudo: yes\n  template:\n    src: marathon-wait-for-listen.sh.j2\n    dest: /usr/local/bin/marathon-wait-for-listen.sh\n    mode: 0755\n  notify:\n    - restart marathon\n  tags:\n    - marathon\n\n- name: ensure marathon is running (and enable it at boot)\n  sudo: yes\n  service:\n    name: marathon\n    state: started\n    enabled: yes\n  notify:\n    - wait for marathon to listen\n  tags:\n    - marathon\n\n- meta: flush_handlers\n\n# This is here to workaround an issue where marathon does not receive an\n# acknowledgement correctly from Mesos.\n- name: force restart marathon\n  sudo: yes\n  service:\n    name: marathon\n    state: restarted\n  notify:\n    - wait for marathon to listen\n  tags:\n    - marathon\n\n- meta: flush_handlers\n\n- name: set marathon consul service definition\n  sudo: yes\n  template:\n    src: marathon-consul.j2\n    dest: \"{{ consul_dir }}/marathon.json\"\n  notify:\n    - restart consul\n  tags:\n    - marathon\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e68d3cfbc1ae0b5c5f4ec21fabbaa43fb4895e52", "filename": "roles/ansible/tower/config-ansible-tower-ldap/tasks/ldap.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- block: # become: True\n\n  - name: \"Upload Cert CA to Ansible Tower (if applicable)\"\n    copy:\n      src: \"{{ ansible_tower.ldap.ca_cert }}\"\n      dest: \"/etc/pki/ca-trust/source/anchors/{{ ansible_tower.ldap.ca_cert | basename }}\"\n    when:\n    - ansible_tower.ldap.ca_cert is defined\n    - ansible_tower.ldap.ca_cert|trim != ''\n    notify:\n    - restart-tower\n    register: ca_uploaded\n\n  - name: \"Update CA trust if a new CA was added\"\n    command: update-ca-trust\n    when:\n    - ca_uploaded is defined\n    - ca_uploaded.changed\n\n  - name: \"Update Ansible Tower LDAP config\"\n    uri:\n      url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v1/settings/ldap/\"\n      user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      password: \"{{ ansible_tower.admin_password }}\"\n      force_basic_auth: yes\n      method: PUT\n      body: \"{{ lookup('template', 'ldap.j2') }}\"\n      body_format: 'json'\n      headers:\n        Content-Type: \"application/json\"\n        Accept: \"application/json\"\n      validate_certs: no\n    notify:\n    - restart-tower\n\n  - name: \"Force LDAP Sync\"\n    uri:\n      url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/\"\n      user: \"{{ ansible_tower.ldap.bind_dn.split(',') | first | regex_replace('uid=') }}\"\n      password: \"{{ ansible_tower.ldap.bind_password }}\"\n      force_basic_auth: yes\n      method: GET\n      validate_certs: no\n    register: status_output\n    until: status_output.status == 200\n    retries: 6\n    delay: 5\n\n  become: True\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "5deca8d7c5c9dc0563b4d6fe2f91792b8a5d5d85", "filename": "reference-architecture/aws-ansible/playbooks/roles/terminate-all/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Force remove s3 bucket\n  s3_bucket:\n    name: \"{{ s3_bucket_name }}\"\n    state: absent\n    force: yes\n\n- name: Remove extra App nodes Cloudformation\n  cloudformation:\n    stack_name: \"{{ item }}\"\n    state: absent\n    template: roles/cloudformation-infra/files/add-node.json\n    region: \"{{ region }}\"\n  with_items:\n  - \"{{ extra_app_nodes }}\"\n  when: extra_app_nodes is defined\n\n- name: Remove extra Infra nodes Cloudformation\n  cloudformation:\n    stack_name: \"{{ item }}\"\n    state: absent\n    template: roles/cloudformation-infra/files/add-infra-node.json\n    region: \"{{ region }}\"\n  with_items:\n  - \"{{ extra_infra_nodes }}\"\n  when: extra_infra_nodes is defined\n\n- name: Remove Gluster Cloudformation\n  cloudformation:\n    stack_name: \"{{ item }}\"\n    state: absent\n    template: roles/cloudformation-infra/files/add-cns-storage.json\n    region: \"{{ region }}\"\n  with_items:\n  - \"{{ gluster_nodes }}\"\n  when: gluster_nodes is defined\n\n- name: Remove Cloudformation\n  cloudformation:\n    stack_name: \"{{ stack_name }}\"\n    state: absent\n    template: \"roles/cloudformation-infra/files/{{ stack_name }}-greenfield.json\"\n    region: \"{{ region }}\"\n  ignore_errors: true\n\n- name: lookup up vols\n  ec2_vol_facts:\n    region: \"{{ region }}\"\n    filters:\n      status: available\n  register: vols\n\n- name: cleanup\n  ec2_vol:\n    id: \"{{ item.id }}\"\n    state: absent\n    region: \"{{ region }}\"\n  with_items:\n  - \"{{ vols.volumes }}\"\n  when: ci\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "17bb2db8fdc67163cc8d77e97b82d3ce8acf0e8f", "filename": "reference-architecture/aws-ansible/playbooks/roles/cloudformation-infra/files/user_data_gluster.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#cloud-config\ncloud_config_modules:\n\nusers:\n- default\n\nsystem_info:\n  default_user:\n    name: ec2-user\n\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "00c1dc103f02a0d488737fdbd8f938654ed0861c", "filename": "roles/cloud-scaleway/defaults/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\nscaleway_regions:\n  - alias: par1\n  - alias: ams1\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "f0e6703fcecfe3b5616dc20f71c18389e617846b", "filename": "roles/handlers/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "- name: restart consul\n  service:\n    name: consul\n    state: restarted\n  sudo: yes\n  notify:\n    - wait for consul to listen\n\n- name: wait for consul to listen\n  wait_for:\n    port: 8500\n\n- name: restart docker\n  service:\n   name: docker\n   state: restarted\n  sudo: yes\n"}, {"commit_sha": "4a9aaf0951e383c57077cf651b93e78eeea1b5ac", "sha": "8c899a30c36d076c5295997531ec4340f3a0842d", "filename": "tasks/user.yml", "repository": "geerlingguy/ansible-role-solr", "decoded_content": "---\n- name: Ensure solr_user group exists.\n  group: \"name={{ solr_user }} state=present\"\n\n- name: Ensure solr_user exists.\n  user:\n    name: \"{{ solr_user }}\"\n    state: present\n    group: \"{{ solr_user }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "66de7cf25fafd0ba3781adf31c808827fcb8821a", "filename": "playbooks/install.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_playbook: \"prep.yml\"\n\n- import_playbook: \"infra-hosts.yml\"\n\n- import_playbook: \"infra-virt-hosts.yml\"\n\n- import_playbook: \"vm.yml\"\n\n- import_playbook: \"services.yml\"\n\n- import_playbook: \"lb-vms.yml\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "0119c1baf57d8e2aea443c24ae99241ddad9bf7a", "filename": "playbooks/roles/zookeeper/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# handlers file for zookeeper"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7dd9390b8dff26d8d16dee350b623e084865c64b", "filename": "roles/config-nagios-server/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Setup hostgroup directories\n  file:\n    path: /etc/nagios/objects/{{hostvars[item]['hostgroup_name']}}\n    state: directory\n    mode: 0755\n  with_items: \"{{groups['nagios-targets']}}\"\n\n- name: Setup common hostgroup configuration\n  template:\n    src: common.cfg.j2 \n    dest: /etc/nagios/objects/{{hostvars[item]['hostgroup_name']}}/common.cfg\n    owner: root\n    group: root\n    mode: 0644 \n  with_items: \"{{groups['nagios-targets']}}\"\n\n- name: Setup the hosts in the hostgroup configuration\n  template:\n    src: host.cfg.j2 \n    dest: /etc/nagios/objects/{{hostvars[item]['hostgroup_name']}}/{{hostvars[item]['inventory_hostname_short']}}.cfg\n    owner: root\n    group: root\n    mode: 0644 \n  with_items: \"{{groups['nagios-targets']}}\"\n\n- name: Add common hostgroup configuration to master nagios configuration file\n  lineinfile:\n    dest: /etc/nagios/nagios.cfg\n    regexp: \"^cfg_file=/etc/nagios/objects/{{hostvars[item]['hostgroup_name']}}/common.cfg\"\n    line: \"cfg_file=/etc/nagios/objects/{{hostvars[item]['hostgroup_name']}}/common.cfg\"\n  with_items: \"{{groups['nagios-targets']}}\"\n\n- name: Add host configuration to master nagios configuration file\n  lineinfile:\n    dest: /etc/nagios/nagios.cfg\n    regexp: \"^cfg_file=/etc/nagios/objects/{{hostvars[item]['hostgroup_name']}}/{{hostvars[item]['inventory_hostname_short']}}.cfg\"\n    line: \"cfg_file=/etc/nagios/objects/{{hostvars[item]['hostgroup_name']}}/{{hostvars[item]['inventory_hostname_short']}}.cfg\"\n  with_items: \"{{groups['nagios-targets']}}\"\n\n- name: Restart nagios\n  service:\n    name: nagios\n    state: restarted\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "920d631173cc5ec7efceca35c68d4a99d678f001", "filename": "roles/ansible/prep-for-ansible/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Force gather facts - an error is normal\"\n  setup:\n  check_mode: no\n  ignore_errors: True\n  register: facts\n\n- name: \"Install python2 and dnf stuff to allow for Ansible operation\"\n  raw: dnf -y install python-dnf libselinux-python\n  when:\n  - facts is failed\n\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "54cedab0bf1cb2259df94ebd3f96c56c9a7c1d41", "filename": "roles/user-management/list-users-by-group/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: manage-users-host\n  gather_facts: no\n  roles:\n  - role: list-users-by-group \n  tasks:\n  - debug:\n      msg: \"{{ list_of_users }}\"\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "16eb4adc4c062e3a89c32a60a0424497cfa9b46d", "filename": "examples/playbooks/install_engine.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# example playbook for installing engines\n# use with examples/inventory/install_engine.inv\n- hosts: engine\n  remote_user: root\n  roles:\n    - {role: ovirt-common}\n    - {role: ovirt-engine-install-packages}\n    - {role: ovirt-engine-setup}\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "6cb1d571f22a3cb9c566575222731dbf06bc1363", "filename": "tasks/call_script.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- name: Calling Groovy script {{ script_name }}\n  uri:\n    url: \"http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}service/siesta/rest/v1/script/{{ script_name }}/run\"\n    user: 'admin'\n    password: \"{{ current_nexus_admin_password }}\"\n    headers:\n      Content-Type: \"text/plain\"\n    method: POST\n    status_code: 200,204\n    force_basic_auth: yes\n    body: \"{{ args | to_json }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8466ea772fb927884158891d7be351a8f8f754e6", "filename": "roles/update-host/tasks/reboot-host.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Reboot the host\"\n  shell: sleep 5 && shutdown -r now \"Ansible Reboot of host\"\n  async: 1\n  poll: 0\n  ignore_errors: true\n  when:\n    - host_updated.changed or force_host_reboot\n  become: True\n\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "ef71a470ae1fc9dde29c23d7cc33f57b64f837e5", "filename": "roles/proxy/meta/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\ndependencies:\n  - { role: common, tags: common }\n  - { role: vpn,  tags: vpn }\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "810f1047096a36b85225334faae2008b25e634ff", "filename": "reference-architecture/azure-ansible/3.6/ansibledeployocp/playbooks/roles/azure-delete/defaults/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\nlocation: \"westus\"\n"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "cf2b6bd942998992f5263c9f80cf317794b392be", "filename": "tasks/security_policy_fetch/web.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Download security policy artifact from web\"\n  get_url:\n    url: \"{{ java_unlimited_policy_transport_web }}\"\n    dest: \"{{ download_path }}\"\n  until: \"'OK' in FILE_DOWNLOADED.msg\"\n  retries: 3\n  delay: 2\n  register: FILE_DOWNLOADED\n  delegate_to: \"localhost\"\n  connection: \"local\"\n\n- name: \"Downloaded security policy artifact\"\n  set_fact:\n    security_policy_oracle_artifact: \"{{ FILE_DOWNLOADED.dest }}\"\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "b26bee3e27eb747306f047e2ae9afe483d734d61", "filename": "roles/docket/vars/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# vars file for rocknsm.docket\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "9ff5911d5dd7432a954f8711ba3a81e2e3cf4290", "filename": "ops/playbooks/scale_dtr.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- name: Scale DTR\n  hosts: dtr\n  serial: 1\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - ../group_vars/backups\n    - includes/internal_vars.yml\n\n  vars:\n    http_proxy_switch:  \"{% if  env.http_proxy is defined %} --http-proxy {{ env.http_proxy }} {% endif %}\"\n    https_proxy_switch:  \"{% if  env.https_proxy is defined %} --https-proxy {{ env.https_proxy }} {% endif %}\"\n    no_proxy_switch:  \"{% if  env.no_proxy is defined %} --no-proxy '{{ env.no_proxy }}' {% endif %}\"\n\n  environment:\n    - UCP_USERNAME: \"{{ ucp_username }}\"\n    - UCP_PASSWORD: \"{{ ucp_password }}\"\n    - UCP_CA: \"{{ ucp_ca_cert | default('') }}\"\n\n  pre_tasks:\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n\n  roles:\n    - role: worker\n      ARG_UCP_IP:        \"{{ ucp_instance }}.{{domain_name}}\"\n      ARG_UCP_USER:      \"{{ ucp_username }}\"\n      ARG_UCP_PASSWORD:  \"{{ ucp_password }}\"\n      ARG_ADVERTIZE_IP:  \"{{ ucp_instance }}.{{ domain_name }}:2377\"\n      worker_role_ports: \"{{ internal_dtr_ports }}\"\n      worker_join_delay: 180\n\n    - role: get-dtr-replica\n      ARG_UCP_IP:        \"{{ ucp_instance }}.{{domain_name}}\"\n      ARG_UCP_USER:      \"{{ ucp_username }}\"\n      ARG_UCP_PASSWORD:  \"{{ ucp_password }}\"\n\n  tasks:\n\n#\n# Load Certificates\n#\n    - include_tasks: includes/load_certificates.yml\n\n#\n# There is probably a better way of doing this\n#\n    - name: \"Poll DTR on {{ inventory_hostname }}\" \n      uri:\n        url: https://{{ inventory_hostname }}.{{ domain_name }}/login\n        validate_certs: no\n        return_content: yes\n        status_code: 200,111,-1\n      register: webpage\n\n    - name: See if there is a DTR replica running on this node\n      set_fact:\n        dtr_is_there: \"{% if 'Docker Trusted Registry' in webpage.content %}true{% else %}false{% endif %}\"\n\n#\n# existing_dtr_replica_id is returned by the role get-dtr-replica\n#\n\n    - name: Join DTR nodes\n      command: docker run --env UCP_USERNAME --env UCP_PASSWORD {{ switch_env_ucp_ca }}  docker/dtr:{{ dtr_version }} join \n        --ucp-node {{ inventory_hostname }}.{{ domain_name }} \n        --ucp-url https://{{ ucp_instance }}.{{ domain_name }} \n        {{ switch_ucp_insecure }} --existing-replica-id {{ existing_dtr_replica_id }}\n      vars:\n        switch_ucp_insecure:         \"{% if ucp_ca_cert is defined %}{% else %}--ucp-insecure-tls{% endif %}\"\n        switch_env_ucp_ca:           \"{% if ucp_ca_cert is defined %}--env UCP_CA{% endif %}\"\n \n      register: task_result\n#      until: task_result.rc == 0\n#      retries: 1\n#      delay: 60\n      when: not dtr_is_there and existing_dtr_replica_id != 0\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8d2a00cdd0d36add7125041b4ba982e5e4a2b30c", "filename": "roles/scm/add-webhooks-github/tests/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\napi_token: 1234567890abcdefghijklmnopqrstuvwxyz9876\n\nowner: aGithubUser\n\nrepo: aRepo\n\n# To see all possible events https://developer.github.com/v3/activity/events/types/\nwebhooks:\n  - url: \"https://website1.com/\"\n    events: \n      - push\n    is_active: true\n  - url: \"https://website2.com/\"\n    events:\n      - push\n      - delete\n    is_active: true\n\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "c81cf5b7c4fb0a0db916e51cd9f9a67ba57f66ac", "filename": "playbooks/roles/sensor-common/tasks/prechecks.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n...\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "1568045abb2c35552562235c46ca0dee8d4091bb", "filename": "tasks/not-supported.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Warn on unsupported platform\n  fail:\n    msg: |\n      This role does not support '{{ ansible_os_family }}' platform.\n        Please contact support@lean-delivery.com\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "1f3dce4a0eb3a7d8ed1c30bb0fb275157f708939", "filename": "roles/config-ipa-client/tasks/prereq-CentOS.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install additional packages for IPA/IdM\"\n  package:\n    name: \"{{ item }}\"\n    state: latest\n  with_items:\n  - ipa-client\n  - libsss_sudo\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6d4d5403ac72d5159ccab62fa7dc82c60fde00e6", "filename": "reference-architecture/ansible-tower-integration/tower_unconfig_aws/tower_unconfig_aws/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n\n- name: Remove workflow-ocp-aws-install\n  command: tower-cli workflow delete --name=\"workflow-ocp-aws-install\"\n\n- name: Remove aws-openshift-cfme-install job template\n  tower_job_template:\n    name: aws-openshift-cfme-install\n    job_type: run\n    project: openshift-ansible-contrib\n    playbook: \"cfme_ose_install.yaml\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove aws-openshift-cfme-ocp-provider job template\n  tower_job_template:\n    name: aws-openshift-cfme-ocp-provider\n    job_type: run\n    project: openshift-ansible-contrib\n    playbook: \"cfme_ose_aws_provision.yaml\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove aws-openshift-install job template\n  tower_job_template:\n    name: aws-openshift-install\n    job_type: run\n    project: openshift-ansible-contrib\n    playbook: \"reference-architecture/aws-ansible/playbooks/openshift-install.yaml\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove infrastructure job template\n  tower_job_template:\n    name: aws-infrastructure\n    job_type: run\n    project: openshift-ansible-contrib\n    playbook: \"reference-architecture/aws-ansible/playbooks/infrastructure.yaml\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove create-httpd-file job template\n  tower_job_template:\n    name: create_httpd_file\n    job_type: run\n    project: openshift-ansible-contrib\n    playbook: \"reference-architecture/ansible-tower-integration/create_httpd_file/create_httpd_file.yaml\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove insights job template\n  tower_job_template:\n    name: redhat-access-insights-client\n    job_type: run\n    project: ansible-redhat-access-insights-client\n    playbook: \"redhat-access-insights-client.yaml\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower group\n  tower_group:\n    name: aws\n    inventory: \"aws-inventory\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower inventory\n  tower_inventory:\n    name: \"aws-inventory\"\n    organization: \"Default\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower credential for machine\n  tower_credential:\n    name: aws-privkey\n    kind: ssh\n    organization: \"Default\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower credential for aws\n  tower_credential:\n    name: ec2\n    state: absent\n    kind: aws\n    organization: \"Default\"\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower project\n  tower_project:\n    name: \"openshift-ansible-contrib\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower project\n  tower_project:\n    name: \"ansible-redhat-access-insights-client\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower organization\n  tower_organization:\n    name: \"Default\"\n    description: \"Set to Default since the trial license only allows one organization. You can change it if you have deep pockets\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "092959088f237d1e384c980a17fccc1a995ab1f0", "filename": "roles/dns/manage-dns-zones/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: prereq.yml\n- import_tasks: named/main.yml\n- import_tasks: route53/main.yml\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "73cf244d29872c80bfc99595c7433dd6080e88ed", "filename": "roles/ansible/tower/manage-credentials/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: tower\n  roles:\n  - role: ansible/tower/manage-credentials\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "ddb435834a1e09440fea388bf54b1ead4ad93fc2", "filename": "tasks/section_09_level1_03.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 9.3.1 Set SSH Protocol to 2 (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^Protocol'\n        state=present\n        line='Protocol 2'\n    notify: restart ssh\n    tags:\n      - section9\n      - section9.3\n      - section9.3.1\n\n  - name: 9.3.2 Set LogLevel to INFO (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^LogLevel'\n        state=present\n        line='LogLevel INFO'\n    notify: restart ssh\n    tags:\n      - section9\n      - section9.3\n      - section9.3.2\n\n  - name: 9.3.3 Set Permissions on /etc/ssh/sshd_config (Scored)\n    file: path='/etc/ssh/sshd_config' owner=root group=root mode=600\n    notify: restart ssh\n    tags:\n      - section9\n      - section9.3\n      - section9.3.3\n\n#Regroups sections 9.3.4 9.3.7 9.3.8 9.3.9 9.3.10\n  - name: 9.3.{4,7,8,9,10} Disable some SSH options (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^{{ item }}'\n        line='{{ item }} no'\n        state=present\n    with_items:\n      - X11Forwarding\n      - HostbasedAuthentication\n      - PermitRootLogin\n      - PermitEmptyPasswords\n      - PermitUserEnvironment\n    notify: restart ssh\n    tags:\n      - section9\n      - section9.3\n      - section9.3.4\n      - section9.3.7\n      - section9.3.8\n      - section9.3.9\n      - section9.3.10\n\n  - name: 9.3.5 Set SSH MaxAuthTries to 4 or Less (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^MaxAuthTries'\n        line='MaxAuthTries 4'\n        state=present\n    notify: restart ssh\n    tags:\n      - section9\n      - section9.3\n      - section9.3.5\n\n  - name: 9.3.6 Set SSH IgnoreRhosts to Yes (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^IgnoreRhosts'\n        line='IgnoreRhosts yes'\n        state=present\n    notify: restart ssh\n    tags:\n      - section9\n      - section9.3\n      - section9.3.6\n\n  - name: 9.3.11 Use Only Approved Cipher in Counter Mode (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^Ciphers'\n        line='Ciphers aes128-ctr,aes192-ctr,aes256-ctr'\n        state=present\n    notify: restart ssh\n    tags:\n      - section9\n      - section9.3\n      - section9.3.11\n\n  - name: 9.3.12.1 Set Idle Timeout Interval for User Login (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^ClientAliveInterval'\n        line='ClientAliveInterval 300'\n        state=present\n    tags:\n      - section9\n      - section9.3\n      - section9.3.12\n      - section9.3.12.1\n\n  - name: 9.3.12.2 Set Idle Timeout Interval for User Login (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^ClientAliveCountMax'\n        line='ClientAliveCountMax 0'\n        state=present\n    notify: restart ssh\n    tags:\n      - section9\n      - section9.3\n      - section9.3.12\n      - section9.3.12.2\n\n  - name: 9.3.13.1 Limit Access via SSH (Scored)\n    shell: grep AllowUsers /etc/ssh/sshd_config\n    register: allow_rc\n    failed_when: allow_rc.rc == 1\n    changed_when: False\n    ignore_errors: True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.13\n      - section9.3.13.1\n\n  - name: 9.3.13.2 Limit Access via SSH (Scored)\n    shell: grep AllowGroups /etc/ssh/sshd_config\n    register: allow_rc\n    failed_when: allow_rc.rc == 1\n    changed_when: False\n    ignore_errors: True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.13\n      - section9.3.13.2\n\n  - name: 9.3.13.3 Limit Access via SSH (Scored)\n    shell: grep DenyUsers /etc/ssh/sshd_config\n    register: allow_rc\n    failed_when: allow_rc.rc == 1\n    changed_when: False\n    ignore_errors: True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.13\n      - section9.3.13.3\n\n  - name: 9.3.13.4 Limit Access via SSH (Scored)\n    shell: grep DenyGroups /etc/ssh/sshd_config\n    register: allow_rc\n    failed_when: allow_rc.rc == 1\n    changed_when: False\n    ignore_errors: True\n    notify: restart ssh\n    tags:\n      - section9\n      - section9.3\n      - section9.3.13\n      - section9.3.13.4\n\n  - name: 9.3.14 Set SSH Banner (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^Banner'\n        line='Banner /etc/issue.net'\n        state=present\n    notify: restart ssh\n    tags:\n      - section9\n      - section9.3\n      - section9.3.14\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4876d36f339f7b9e9ce28ce23c8e9e02282c9a35", "filename": "roles/load-balancers/manage-haproxy/tasks/activate-config.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# Use a 'block' to ensure \"become: True\" for all tasks requiring elevated privileges\n- block:\n\n  - name: 'Copy the HAproxy config file to a temp location for validity checking'\n    copy:\n      src: '{{ haproxy_temp_file }}'\n      dest: '{{ temp_new_file }}'\n\n  - name: 'Check the validity'\n    command: 'haproxy -c -f {{ temp_new_file }}'\n    notify: 'remove tmp new file'\n\n  - name: 'Copy and activate the HAproxy config file'\n    copy:\n      src: '{{ haproxy_temp_file }}'\n      dest: '/etc/haproxy/haproxy.cfg'\n      backup: 'yes'\n    notify: 'reload haproxy'\n\n  - name: 'Open Firewall for LB use (TCP only)'\n    firewalld:\n      port: \"{{ fe.lb_host_port }}/tcp\"\n      permanent: yes\n      state: enabled\n      immediate: yes\n    loop_control:\n      loop_var: fe\n    loop: \"{{ lb_config.frontends|flatten(levels=1) }}\"\n\n  - name: 'Tweak SELinux for LB use (TCP only)'\n    seport:\n      ports: \"{{ fe.lb_host_port }}\"\n      proto: tcp\n      setype: http_port_t\n      state: present\n    loop_control:\n      loop_var: fe\n    loop: \"{{ lb_config.frontends|flatten(levels=1) }}\"\n\n  # End of outer block for \"become: True\"\n  become: True\n\n- name: 'Clean-up the temp file'\n  file:\n    path: \"{{ haproxy_temp_file }}\"\n    state: absent\n  when:\n  - (clean_up_temp|default('yes'))|lower == 'yes'\n  delegate_to: localhost\n  run_once: True\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "609df85d8dde0b7d0369ec22a0f1686b7d359517", "filename": "tasks/create_task_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include_tasks: call_script.yml\n  vars:\n    script_name: create_task\n    args: \"{{ item }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "bfa63f3860b50fafa2a9a261d622e5400c38540b", "filename": "playbooks/prerequisite.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: nodes\n  gather_facts: yes\n  become: yes\n  roles:\n  - rhsm-timeout\n  - role: atomic-update\n    when: openshift.common.is_atomic\n\n- hosts: nodes\n  gather_facts: no\n  become: yes\n  serial: 1\n  roles:\n  - role: rhsm-subscription\n    when: deployment_type in [\"enterprise\", \"atomic-enterprise\", \"openshift-enterprise\"] and\n          ansible_distribution == \"RedHat\" and ( rhsm_user is defined or rhsm_activation_key is defined)\n\n- hosts: nodes\n  gather_facts: no\n  become: yes\n  roles:\n  - role: rhsm-repos\n    when: deployment_type in [\"enterprise\", \"atomic-enterprise\", \"openshift-enterprise\"] and\n          ansible_distribution == \"RedHat\" and ( rhsm_user is defined or rhsm_activation_key is defined)\n  - prerequisites\n\n- hosts: masters\n  gather_facts: yes\n  become: yes\n  roles:\n  - master-prerequisites\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ff9b80777ddae1882e8485fc68abd8be698b6005", "filename": "roles/config-routes/tests/inventory/group_vars/infra_hosts.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nroutes:\n- device: eth0\n  entries:\n  - address: 192.168.10.0\n    netmask: 255.255.255.0\n    gateway: 192.168.1.1 \n  - address: 192.168.11.0\n    netmask: 255.255.255.0\n    gateway: 192.168.1.1\n\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "f9415e33ab491ddec5468f1dab30f1a44c5d2b97", "filename": "tasks/create_repo_rubygems_proxy_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_rubygems_proxy\n    args: \"{{ _nexus_repos_rubygems_defaults|combine(item) }}\""}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "cc7c21ec0f83e9aeadbb6ab03634edbfd8d9a51c", "filename": "roles/vpn/tasks/ipsec_configuration.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Setup the config files from our templates\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: \"{{ item.owner }}\"\n    group: \"{{ item.group }}\"\n    mode: \"{{ item.mode }}\"\n  with_items:\n    - src: strongswan.conf.j2\n      dest: \"{{ config_prefix|default('/') }}etc/strongswan.conf\"\n      owner: root\n      group: \"{{ root_group|default('root') }}\"\n      mode: \"0644\"\n    - src: ipsec.conf.j2\n      dest: \"{{ config_prefix|default('/') }}etc/ipsec.conf\"\n      owner: root\n      group: \"{{ root_group|default('root') }}\"\n      mode: \"0644\"\n    - src: ipsec.secrets.j2\n      dest: \"{{ config_prefix|default('/') }}etc/ipsec.secrets\"\n      owner: strongswan\n      group: \"{{ root_group|default('root') }}\"\n      mode: \"0600\"\n  notify:\n    - restart strongswan\n\n- name: Get loaded plugins\n  shell: >\n    find {{ config_prefix|default('/') }}etc/strongswan.d/charon/ -type f -name '*.conf' -exec basename {} \\; | cut -f1 -d.\n  register: strongswan_plugins\n\n- name: Disable unneeded plugins\n  lineinfile:\n    dest: \"{{ config_prefix|default('/') }}etc/strongswan.d/charon/{{ item }}.conf\"\n    regexp: '.*load.*'\n    line: 'load = no'\n    state: present\n  notify:\n    - restart strongswan\n  when: item not in strongswan_enabled_plugins and item not in strongswan_additional_plugins\n  with_items: \"{{ strongswan_plugins.stdout_lines }}\"\n\n- name: Ensure that required plugins are enabled\n  lineinfile: dest=\"{{ config_prefix|default('/') }}etc/strongswan.d/charon/{{ item }}.conf\" regexp='.*load.*' line='load = yes' state=present\n  notify:\n    - restart strongswan\n  when: item in strongswan_enabled_plugins or item in strongswan_additional_plugins\n  with_items: \"{{ strongswan_plugins.stdout_lines }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "daef6ccbe576a3ab061bd8dbe7049c591c3effbb", "filename": "roles/config-selinux/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Test SELinux config\"\n  hosts: all\n  roles:\n    - role: config-selinux \n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "13c754c1e2b07f548139d990d4129a96216bf822", "filename": "playbooks/gce/openshift-cluster/vars.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ndebug_level: 2\n\ndeployment_rhel7_ent_base:\n  image: \"{{ lookup('oo_option', 'image_name') | default('rhel-7', True) }}\"\n  machine_type: \"{{ lookup('oo_option', 'machine_type') | default('n1-standard-1', True) }}\"\n  ssh_user: \"{{ lookup('env', 'gce_ssh_user') |  default(ansible_ssh_user, true) }}\"\n  become: yes\n\ndeployment_vars:\n  origin:\n    image: \"{{ lookup('oo_option', 'image_name') | default('centos-7', True) }}\"\n    machine_type: \"{{ lookup('oo_option', 'machine_type') | default('n1-standard-1', True) }}\"\n    ssh_user: \"{{ lookup('env', 'gce_ssh_user') |  default(ansible_ssh_user, true) }}\"\n    become: yes\n  enterprise: \"{{ deployment_rhel7_ent_base }}\"\n  openshift-enterprise: \"{{ deployment_rhel7_ent_base }}\"\n  atomic-enterprise: \"{{ deployment_rhel7_ent_base }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "da139afd09e089a44cc802e8fde45951e9586bcb", "filename": "playbooks/libvirt/openshift-cluster/templates/storage-pool.xml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "<pool type='dir'>\n  <name>{{ libvirt_storage_pool }}</name>\n  <target>\n    <path>{{ libvirt_storage_pool_path }}</path>\n  </target>\n</pool>\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "865b55273be72c130a7738db01cfde7df305f36b", "filename": "tasks/declare_script_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- name: Removing (potential) previously declared Groovy script {{ item }}\n  uri:\n    url: \"{{ nexus_api_scheme }}://{{ nexus_api_hostname }}:{{ nexus_api_port }}\\\n      {{ nexus_api_context_path }}{{ nexus_rest_api_endpoint }}/{{ item }}\"\n    user: 'admin'\n    password: \"{{ current_nexus_admin_password }}\"\n    method: DELETE\n    force_basic_auth: yes\n    status_code: 204,404\n    validate_certs: \"{{ nexus_api_validate_certs }}\"\n\n- name: Declaring Groovy script {{ item }}\n  uri:\n    url: \"{{ nexus_api_scheme }}://{{ nexus_api_hostname }}:{{ nexus_api_port }}\\\n      {{ nexus_api_context_path }}{{ nexus_rest_api_endpoint }}\"\n    user: 'admin'\n    password: \"{{ current_nexus_admin_password }}\"\n    body_format: json\n    method: POST\n    force_basic_auth: yes\n    status_code: 204\n    validate_certs: \"{{ nexus_api_validate_certs }}\"\n    body:\n      name: \"{{ item }}\"\n      type: 'groovy'\n      content: \"{{ lookup('file', 'groovy/' + item + '.groovy') }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "47afb2f1d82eaa890942e0ebddf7bed51f7609ca", "filename": "roles/notifications/md-to-html/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: \"prereq.yml\"\n  when:\n    - install_prereq|default(False)\n \n- import_tasks: \"convert_md_to_html.yml\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a978d0a66004287f574390169097e4149f944d35", "filename": "roles/virt-install/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\ndefault_http_dir: \"/var/www/html\"\ndefault_http_host: \"192.168.122.1\"\n\ndefault_connect: \"qemu:///system\"\ndefault_virt_type: \"kvm\"\ndefault_name: \"tmpvm-{{ ansible_date_time.epoch }}\"\ndefault_title: \"tmpvm-title\"\ndefault_description: \"tmpvm-description\"\ndefault_memory: \"1024\"\ndefault_vcpus: \"2\"\ndefault_disk_size: \"10\"\ndefault_disk_pool: \"default\"\ndefault_os_variant: \"rhel7.3\"\ndefault_iso: \"/tmp/rhel-server-7.3-x86_64-dvd.iso\"\ndefault_ksfile: \"/tmp/my.ks\"\ndefault_authorized_keys: \"/tmp/authorized_keys\"\ndefault_network_hostif: \"eth0\"\ndefault_network_model: \"virtio\"\ndefault_network_type: \"direct\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "9699f50e9f9a7bb1819d83a62a5816f005c870c8", "filename": "reference-architecture/vmware-ansible/playbooks/heketi-ocp.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: master\n  gather_facts: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - instance-groups\n\n- hosts: single_master\n  gather_facts: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - instance-groups\n  - heketi-ocp\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "d135e55ac4b34eb29734856b23bb94f8d33f06fb", "filename": "roles/marathon/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for marathon\nmarathon_port: 8080\nconsul_dir: /etc/consul.d\nmarathon_hostname: \"{{ ansible_ssh_host }}\"\nmarathon_artifact_store: \"file:///etc/marathon/store\"\nmarathon_artifact_store_dir: /etc/marathon/store\nmarathon_libprocess_ip: \"{{ ansible_ssh_host }}\""}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "2e2b6162988896cf6efba50dc6c6dd78446b3350", "filename": "playbooks/cluster/openshift/vars/3.7.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "openshift_image_tag: v3.7.0\nopenshift_service_catalog_image_version: \"{{ openshift_image_tag }}\"\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "fc952474bc1af96cdccac3ed5809475efc24c255", "filename": "tasks/install/mariadb/upstream.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n- name: APT_KEY | Install MariaDB key\n  apt_key:\n    keyserver: \"{{ mariadb_key_server }}\"\n    id: \"{{ item }}\"\n  with_items: \"{{ mariadb_key_ids }}\"\n\n- name: TEMPLATE | Deploy APT pinning (prevent upgrades from Debian)\n  template:\n    src: etc/apt/preferences.d/95-mariadb.j2\n    dest: /etc/apt/preferences.d/95-mariadb\n\n- name: APT_REPOSITORY | Add MariaDB repository\n  apt_repository:\n    repo: 'deb {{ mariadb_repository }} {{ ansible_distribution_release }} main'\n\n- name: APT_REPOSITORY | Add MariaDB (src) repository\n  apt_repository:\n    repo: 'deb-src {{ mariadb_repository }} {{ ansible_distribution_release }} main'\n  when: mariadb_upstream_apt_src\n\n- name: INCLUDE | Normal Install\n  include: default.yml\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "488ea2d178f4ba4a97137121daab19f766623ef7", "filename": "roles/cloud-digitalocean/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- block:\n    - name: Build python virtual environment\n      import_tasks: venv.yml\n\n    - block:\n      - name: Include prompts\n        import_tasks: prompts.yml\n\n      - name: Set additional facts\n        set_fact:\n          algo_do_region: >-\n            {% if region is defined %}{{ region }}\n            {%- elif _algo_region.user_input is defined and _algo_region.user_input != \"\" %}{{ do_regions[_algo_region.user_input | int -1 ]['slug'] }}\n            {%- else %}{{ do_regions[default_region | int - 1]['slug'] }}{% endif %}\n          public_key: \"{{ lookup('file', '{{ SSH_keys.public }}') }}\"\n\n      - block:\n          - name: \"Delete the existing Algo SSH keys\"\n            digital_ocean:\n              state: absent\n              command: ssh\n              api_token: \"{{ algo_do_token }}\"\n              name: \"{{ SSH_keys.comment }}\"\n            register: ssh_keys\n            until: ssh_keys.changed != true\n            retries: 10\n            delay: 1\n\n        rescue:\n          - name: Collect the fail error\n            digital_ocean:\n              state: absent\n              command: ssh\n              api_token: \"{{ algo_do_token }}\"\n              name: \"{{ SSH_keys.comment }}\"\n            register: ssh_keys\n            ignore_errors: yes\n\n          - debug: var=ssh_keys\n\n          - fail:\n              msg: \"Please, ensure that your API token is not read-only.\"\n\n      - name: \"Upload the SSH key\"\n        digital_ocean:\n          state: present\n          command: ssh\n          ssh_pub_key: \"{{ public_key }}\"\n          api_token: \"{{ algo_do_token }}\"\n          name: \"{{ SSH_keys.comment }}\"\n        register: do_ssh_key\n\n      - name: \"Creating a droplet...\"\n        digital_ocean:\n          state: present\n          command: droplet\n          name: \"{{ algo_server_name }}\"\n          region_id: \"{{ algo_do_region }}\"\n          size_id: \"{{ cloud_providers.digitalocean.size }}\"\n          image_id: \"{{ cloud_providers.digitalocean.image }}\"\n          ssh_key_ids: \"{{ do_ssh_key.ssh_key.id }}\"\n          unique_name: yes\n          api_token: \"{{ algo_do_token }}\"\n          ipv6: yes\n        register: do\n\n      - set_fact:\n          cloud_instance_ip: \"{{ do.droplet.ip_address }}\"\n          ansible_ssh_user: root\n\n      - name: Tag the droplet\n        digital_ocean_tag:\n          name: \"Environment:Algo\"\n          resource_id: \"{{ do.droplet.id }}\"\n          api_token: \"{{ algo_do_token }}\"\n          state: present\n\n      - block:\n          - name: \"Delete the new Algo SSH key\"\n            digital_ocean:\n              state: absent\n              command: ssh\n              api_token: \"{{ algo_do_token }}\"\n              name: \"{{ SSH_keys.comment }}\"\n            register: ssh_keys\n            until: ssh_keys.changed != true\n            retries: 10\n            delay: 1\n\n        rescue:\n          - name: Collect the fail error\n            digital_ocean:\n              state: absent\n              command: ssh\n              api_token: \"{{ algo_do_token }}\"\n              name: \"{{ SSH_keys.comment }}\"\n            register: ssh_keys\n            ignore_errors: yes\n\n          - debug: var=ssh_keys\n\n          - fail:\n              msg: \"Please, ensure that your API token is not read-only.\"\n      environment:\n        PYTHONPATH: \"{{ digitalocean_venv }}/lib/python2.7/site-packages/\"\n  rescue:\n    - debug: var=fail_hint\n      tags: always\n    - fail:\n      tags: always\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "e7971a7d44f77f47958ed2b6683426152013144e", "filename": "reference-architecture/aws-ansible/playbooks/minor-update.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- include: /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/upgrades/{{ openshift_vers | default('v3_6') }}/upgrade.yml\n  vars:\n    debug_level: 2\n    openshift_debug_level: \"{{ debug_level }}\"\n    openshift_master_cluster_method: native\n    openshift_node_debug_level: \"{{ node_debug_level | default(debug_level, true) }}\"\n    openshift_master_debug_level: \"{{ master_debug_level | default(debug_level, true) }}\"\n    openshift_master_access_token_max_seconds: 2419200\n    openshift_master_api_port: \"{{ console_port }}\"\n    openshift_master_console_port: \"{{ console_port }}\"\n    osm_cluster_network_cidr: 172.16.0.0/16\n    openshift_cloudprovider_kind: aws\n    openshift_master_cluster_hostname: \"internal-openshift-master.{{ public_hosted_zone }}\"\n    openshift_master_cluster_public_hostname: \"openshift-master.{{ public_hosted_zone }}\"\n    osm_default_node_selector: \"role=app\"\n    deployment_type: openshift-enterprise\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "56ef3f1ff5ed9e52e39a988f4be0dba4a14ad3dc", "filename": "roles/config-libvirt/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: libvirt.yml\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "01177eb87af23951286b19a0661e9b61c4076da7", "filename": "archive/roles/openstack-create/defaults/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\ndefault_security_groups:\n  - name: default\n    rules: []\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "22977e625ec79375887fab3000f0a5bb62c4797d", "filename": "reference-architecture/vmware-ansible/playbooks/roles/nfs-server/files/etc-sysconfig-nfs", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "# {{ ansible_managed }}\n#\n#LOCKDARG=\nLOCKD_TCPPORT=32803\nLOCKD_UDPPORT=32769\n#\nRPCNFSDARGS=\"\"\n#RPCNFSDCOUNT=16\n#NFSD_V4_GRACE=90\n#NFSD_V4_LEASE=90\n#\nRPCMOUNTDOPTS=\"\"\nMOUNTD_PORT=892\n#\nSTATDARG=\"\"\nSTATD_PORT=662\n#STATD_OUTGOING_PORT=2020\n#STATD_HA_CALLOUT=\"/usr/local/bin/foo\"\n#\nSMNOTIFYARGS=\"\"\nRPCIDMAPDARGS=\"\"\nRPCGSSDARGS=\"\"\nGSS_USE_PROXY=\"yes\"\nRPCSVCGSSDARGS=\"\"\nBLKMAPDARGS=\"\"\n"}, {"commit_sha": "b2591b9333f6e7e70f6b9d99e55356b30d7e173c", "sha": "5e4a4257d7118ad4ddea41a0227dc9ed646b3a44", "filename": "tasks/users.yml", "repository": "inkatze/wildfly", "decoded_content": "---\n# tasks file for wildfly\n\n# The user will always be overwritten every time a user and password is given.\n- name: Create management user\n  command: >\n    {{ wildfly_dir }}/bin/add-user.sh\n    {{ wildfly_management_user }} {{ wildfly_management_password }}\n  become_user: '{{ wildfly_user }}'\n  when: wildfly_management_user is defined and\n        wildfly_management_password is defined\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "7de2c790ae28e7122495e3277cde460192feb7d6", "filename": "roles/samba/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "---\n  smbuser : smbuser\n  smbpassword : $6$51iiab$VXzRJK88k9k8SXd5sjs37bEbVQ9x4ob1ng7A5PSGyoVXTKQrhu.89BRuXZAgn8a2DPqZkKDcCpqCZVOs.cieT/ \n#  python -c 'import crypt; print crypt.crypt(\"smbpw\", \"$6$51iiab\")'\n  shared_dir : /library/public\n  samba_enabled : false\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "aaee519c31c71d03695d329cbff0e1222742fce5", "filename": "roles/user-management/manage-local-user-ssh-authkeys/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: 'authorizedkeys.yml'\n  when:\n  - user_name is defined\n  - key_url is defined\n  - key_url|trim != \"\"\n\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "7a95aabebf233bc27aab9a28e97f5d5cfff59909", "filename": "roles/kalite/tasks/enable.yml", "repository": "iiab/iiab", "decoded_content": "# By the time we get here we should have ka-lite of some version\n# And the systemd unit files should be defined\n\n- name: Enable 'kalite-serve' service\n  service:\n    name: kalite-serve\n    enabled: yes\n    state: started\n\n- name: Disable 'kalite-serve' service\n  service:\n    name: kalite-serve\n    enabled: no\n    state: stopped\n  when: not kalite_enabled\n\n# Since Fedora 18 we don't have a separate unit file for kalite-cron\n\n- name: Disable kalite cron server F18\n  service:\n    name: kalite-cron\n    enabled: no\n    state: stopped\n  when: not kalite_cron_enabled and is_F18\n\n- name: Enable kalite cron server F18\n  service:\n    name: kalite-cron\n    enabled: yes\n    state: started\n  when: kalite_cron_enabled and is_F18\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "098c031a3dac9fb15e86d0b9208242cb14de5eeb", "filename": "roles/marathon/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for marathon\n- name: Set Marathon hostname\n  copy:\n    content: \"{{marathon_local_address}}\"\n    dest: /etc/marathon/conf/hostname\n    mode: 0644\n  sudo: yes\n\n- name: remove marathon override\n  command: /bin/rm -f /etc/init/marathon.override\n\n- name: ensure marathon is running (and enable it at boot)\n  service: name=marathon state=started enabled=yes\n\n- name: Set Marathon consul service definition\n  sudo: yes\n  template:\n    src: marathon-consul.j2\n    dest: \"{{ consul_dir }}/marathon.json\"\n  notify:\n    - Restart consul\n"}, {"commit_sha": "b11c4477d973b0cc87a296f6b028eaf9abab4686", "sha": "74964eb09d06e31552a1b618cbd0a6e38dea15a6", "filename": "tasks/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# tasks file for ansible-role-docker-ce\n\n- include: main-CentOS.yml\n  when: ansible_distribution == \"CentOS\" and ansible_distribution_major_version > '6'\n\n- include: main-Fedora.yml\n  when: ansible_distribution == \"Fedora\" and ansible_distribution_major_version > '23'\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "99829a438f5bf0ecfecb70b5da0929d94ec18f3b", "filename": "roles/ovirt-engine-rename/handlers/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# delete the temp file which was created for grep comparison\n- name: delete engine-rename engine rename logs\n  file:\n    path: '/tmp/engine-rename-logs-current.grep'\n    state: 'absent'\n\n- name: delete engine-rename expected log files\n  file:\n    path: '/tmp/check-engine-name-expected-result.grep'\n    state: 'absent'\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "3275ffd0fe633444edff87e36dfc09e5e631ab7c", "filename": "roles/fsf/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n- name: Install packages\n  yum:\n    name:\n      - fsf\n    state: present\n\n- name: Create FSF data directory\n  file:\n    path: \"{{ fsf_data_dir }}\"\n    mode: 0755\n    owner: \"{{ fsf_user }}\"\n    group: \"{{ fsf_group }}\"\n    state: directory\n    setype: var_log_t\n\n- name: Create FSF archive directory\n  file:\n    path: \"{{ fsf_archive_dir }}\"\n    mode: 0755\n    owner: \"{{ fsf_user }}\"\n    group: \"{{ fsf_group }}\"\n    state: directory\n\n- name: Configure logrotate for FSF logs\n  template:\n    src: templates/logrotate-fsf.j2\n    dest: /etc/logrotate.d/fsf\n    mode: 0644\n    owner: root\n    group: root\n\n- name: Configure fsf-server\n  template:\n    src: templates/fsf-server-config.j2\n    dest: /opt/fsf/fsf-server/conf/config.py\n    owner: \"{{ fsf_user }}\"\n    group: \"{{ fsf_group }}\"\n    mode: 0644\n\n- name: Configure fsf-client\n  template:\n    src: templates/fsf-client-config.j2\n    dest: /opt/fsf/fsf-client/conf/config.py\n    owner: \"{{ fsf_user }}\"\n    group: \"{{ fsf_group }}\"\n    mode: 0644\n\n- name: Discover facts about data mount\n  set_fact:\n    rock_mounts:\n      mount: \"{{ item.mount }}\"\n      device: \"{{ item.device }}\"\n      size_total: \"{{ item.size_total }}\"\n  loop:\n    \"{{ ansible_mounts }}\"\n  when: (default_mount is defined and item.mount == default_mount and rock_mounts is not defined)\n\n- name: Determining if quotas are enabled\n  shell: grep \"{{ default_mount }}\" /etc/fstab | awk /prjquota/\n  register: prjquota\n  changed_when: false\n\n- name: Create fsf quota project id\n  getent:\n    database: group\n    split: ':'\n    key: fsf\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Map fsf quota project id to name\n  lineinfile:\n    create: true\n    state: present\n    insertafter: EOF\n    path: /etc/projid\n    line: \"fsf:{{ getent_group.fsf[1] }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Define fsf quota project directories\n  lineinfile:\n    create: true\n    state: present\n    insertafter: EOF\n    path: /etc/projects\n    line: \"{{ getent_group.fsf[1] }}:{{ fsf_data_dir }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: set fsf weight\n  set_fact:\n    fsf_weight: \"{{ rock_services | selectattr('name', 'equalto', 'fsf') | map(attribute='quota_weight') | first }}\"\n  when: fsf_quota is not defined and (prjquota.stdout|length>0)\n\n- name: set fsf quota if not user defined\n  set_fact:\n    fsf_quota: \"{{ rock_mounts.size_total | int / xfs_quota_weight | int * fsf_weight | int }}\"\n  when: fsf_quota is not defined and (prjquota.stdout|length>0)\n\n- name: set fsf project quota\n  xfs_quota:\n    type: project\n    name: fsf\n    bhard: \"{{ fsf_quota }}\"\n    state: present\n    mountpoint: \"{{ rock_mounts.mount }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Enable and start FSF\n  service:\n    name: fsf\n    state: \"{{ 'started' if rock_services | selectattr('name', 'equalto', 'fsf') | map(attribute='enabled') | bool else 'stopped' }}\"\n    enabled: \"{{ rock_services | selectattr('name', 'equalto', 'fsf') | map(attribute='enabled') | bool }}\"\n\n- name: Apply Logstash role\n  include_role:\n    name: logstash\n    apply:\n      delegate_to: \"{{ host }}\"\n      vars:\n        logstash_configs:\n          - { src: 'ls-input-fsf.j2', dest: '100-input-fsf.conf' }\n  loop:\n    \"{{ groups['logstash'] }}\"\n  loop_control:\n    loop_var: host\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "d04a6079aa0e912ecadf52af72d6c8281601eafb", "filename": "roles/cdi/templates/cdi-resourcequota.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: protect-{{ cdi_namespace }}\nspec:\n  hard:\n    {{ cdi_storageclass }}.storageclass.storage.k8s.io/requests.storage: \"500Gi\"\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "10c2940e5970dee2082e86d97e17a7517f4e1488", "filename": "roles/iiab-admin/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "---\n# must keep roles/0-once/defaults/main.yml sync'd\n# The values here are defaults.\n\niiab_admin_user: iiab-admin\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "5f9ffe66e17c9791f02825e4be8c5dec92990e55", "filename": "tasks/Win32NT/fetch/sapjvm-fallback.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: 'Fetch root page {{ sapjvm_root_page }}'\n  win_uri:\n    url: '{{ sapjvm_root_page }}'\n    return_content: true\n  register: root_page\n\n- name: Find release url\n  set_fact:\n    release_url: >-\n      {{ root_page['content']\n        | regex_findall('(additional/sapjvm-'\n          + java_major_version|string\n          + '[\\d.]+-windows-x64.zip)')\n      }}\n\n- name: Download sapjvm artifact\n  win_get_url:\n    url: '{{ sapjvm_root_page }}/{{ release_url[0] }}'\n    headers:\n      Cookie: eula_3_1_agreed=tools.hana.ondemand.com/developer-license-3_1.txt\n    dest: '{{ java_download_path }}'\n    force: false\n  register: file_downloaded\n  retries: 20\n  delay: 5\n  until: file_downloaded is succeeded\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "5267c18fe24ac817a99ce5b425dbac4c02846d86", "filename": "roles/iiab-admin/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- include_tasks: admin-user.yml\n  tags:\n    - base\n  when: admin_install\n\n- include_tasks: access.yml\n  tags:\n    - base\n\n- name: Add 'iiab-admin' to list at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: iiab-admin\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: iiab-admin\n    - option: description\n      value: '\"Admin User\"'\n    - option: iiab_admin_user\n      value: \"{{ iiab_admin_user }}\"\n\n- name: Set up to issue warning if iiab-admin password is still default\n  template:\n    src: profile_ssh_warn.sh\n    dest: /etc/profile.d/\n\n- name: Is this LXDE?\n  stat:\n    path: /home/pi/.config/lxsession\n  register: lx\n\n- name: Do the same if running on Raspbian\n  template:\n    src: lxde_ssh_warn.sh\n    dest: /home/pi/.config/lxsession/LXDE-pi/\n  when: lx.stat.isdir is defined and lx.stat.isdir and is_rpi and is_debuntu\n\n- name: Put an autostart line to check for default password in LXDE (raspbian)\n  lineinfile:\n    line: \"@/home/pi/.config/lxsession/LXDE-pi/lxde_ssh_warn.sh\"\n    dest: /home/pi/.config/lxsession/LXDE-pi/autostart\n  when: lx.stat.isdir is defined and lx.stat.isdir and is_rpi and is_debuntu\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "0f8e384c7daca1a8982b13782b3965f60dc64684", "filename": "tasks/nexus-restore.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- name: \"Run restoration script\"\n  shell: \"nexus-blob-restore.sh {{ nexus_restore_point }} 2>&1 | tee -a {{ nexus_restore_log }}\"\n  tags:\n    # This is only run when a restore point is defined\n    # shut-off ansible-lint error on this one: this is the desired way of doing it\n    - skip_ansible_lint\n\n  notify:\n    - nexus-service-restart\n    - wait-for-nexus\n    - wait-for-nexus-port\n\n- meta: flush_handlers\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "41f6ad1b37b42bbdae8371cae869308c0cf38c61", "filename": "roles/mesos/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for mesos\n- name: Start mesos master\n  shell: start mesos-master\n  sudo: yes\n\n- name: Start mesos slave\n  shell: start mesos-slave\n  sudo: yes\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4490c038e9463955409ccfd645aba97d8c145902", "filename": "roles/dns/manage-dns-zones/tasks/route53/determine-action.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Determine if Route53 processing is required\n  set_fact:\n    route53_processing: True\n  when:\n    - item.1.route53 is defined\n  with_subelements:\n    - \"{{ dns_data.views | default({}) }}\"\n    - zones\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "8cdc2e39b717d4a239532d9070c86d4ec63a1e4e", "filename": "reference-architecture/rhv-ansible/playbooks/openshift-install.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: yes\n  roles:\n    - instance-groups\n  tags:\n    - always\n\n- include: ../../../playbooks/prerequisite.yaml\n  tags:\n    - pre\n    - rhsm\n\n- hosts: nodes\n  roles:\n    - role: docker-storage-setup\n      docker_dev: '/dev/vdb'\n  tags:\n    - pre\n    - storage\n\n- hosts: schedulable_nodes\n  roles:\n    - role: openshift-volume-quota\n      local_volumes_device: '/dev/vdc'\n  tags:\n    - pre\n    - storage\n\n- name: call openshift includes for installer\n  include: /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml\n  tags:\n    - byo\n  vars:\n    debug_level: 2\n    openshift_debug_level: \"{{ debug_level }}\"\n    wildcard_zone: \"{{app_dns_prefix}}.{{public_hosted_zone}}\"\n    osm_cluster_network_cidr: 172.16.0.0/16\n    osm_use_cockpit: false\n    osm_default_node_selector: \"role=app\"\n    osm_default_subdomain: \"{{ wildcard_zone }}\"\n    openshift_hosted_registry_replicas: 1\n    openshift_hosted_registry_storage_access_modes: ['ReadWriteMany']\n    openshift_hosted_registry_storage_volume_size: 30Gi\n    openshift_hosted_router_replicas: 2\n    openshift_master_access_token_max_seconds: 2419200\n    openshift_master_api_port: \"{{ console_port }}\"\n    openshift_master_cluster_method: native\n    openshift_master_cluster_hostname: \"{{ load_balancer_hostname }}\"\n    openshift_master_cluster_public_hostname: \"{{ load_balancer_hostname }}\"\n    openshift_master_console_port: \"{{ console_port }}\"\n    openshift_master_debug_level: \"{{ master_debug_level | default(debug_level, true) }}\"\n    openshift_master_default_subdomain: \"{{osm_default_subdomain}}\"\n    openshift_master_logging_public_url: \"https://kibana.{{ osm_default_subdomain }}\"\n    openshift_master_metrics_public_url: \"https://metrics.{{ osm_default_subdomain }}/hawkular/metrics\"\n    openshift_node_debug_level: \"{{ node_debug_level | default(debug_level, true) }}\"\n    openshift_node_kubelet_args:\n      node-labels:\n        - \"role={{ openshift_node_labels.role }}\"\n    openshift_registry_selector: \"role=infra\"\n    openshift_router_selector: \"role=infra\"\n    # Load balancer config\n    openshift_loadbalancer_additional_frontends:\n      - name: apps-http\n        option: tcplog\n        binds:\n          - \"*:80\"\n        default_backend: apps-http\n      - name: apps-https\n        option: tcplog\n        binds:\n          - \"*:443\"\n        default_backend: apps-http\n    openshift_loadbalancer_additional_backends:\n      - name: apps-http\n        balance: source\n        servers:\n          - name: infra0\n            address: \"{{ hostvars[groups['tag_openshift_infra'].0]['ansible_host'] }}:80\"\n            opts: check\n          - name: infra1\n            address: \"{{ hostvars[groups['tag_openshift_infra'].1]['ansible_host'] }}:80\"\n            opts: check\n      - name: apps-https\n        balance: source\n        servers:\n          - name: infra0\n            address: \"{{ hostvars[groups['tag_openshift_infra'].0]['ansible_host'] }}:443\"\n            opts: check\n          - name: infra1\n            address: \"{{ hostvars[groups['tag_openshift_infra'].1]['ansible_host'] }}:443\"\n            opts: check\n...\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "0db73efca087cbb62839ca672f89cb27c9fc8094", "filename": "roles/docker/meta/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\ngalaxy_info:\n  author: Graham Taylor\n  description:\n  company: Capgemini\n  # Some suggested licenses:\n  # - BSD (default)\n  # - MIT\n  # - GPLv2\n  # - GPLv3\n  # - Apache\n  # - CC-BY\n  license: license (MIT)\n  min_ansible_version: 1.2\n  #\n  # Below are all platforms currently available. Just uncomment\n  # the ones that apply to your role. If you don't see your\n  # platform on this list, let us know and we'll get it added!\n  #\n  platforms:\n  #- name: EL\n  #  versions:\n  #  - all\n  #  - 5\n  #  - 6\n  #  - 7\n  #- name: GenericUNIX\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Fedora\n  #  versions:\n  #  - all\n  #  - 16\n  #  - 17\n  #  - 18\n  #  - 19\n  #  - 20\n  #- name: SmartOS\n  #  versions:\n  #  - all\n  #  - any\n  #- name: opensuse\n  #  versions:\n  #  - all\n  #  - 12.1\n  #  - 12.2\n  #  - 12.3\n  #  - 13.1\n  #  - 13.2\n  #- name: Amazon\n  #  versions:\n  #  - all\n  #  - 2013.03\n  #  - 2013.09\n  #- name: GenericBSD\n  #  versions:\n  #  - all\n  #  - any\n  #- name: FreeBSD\n  #  versions:\n  #  - all\n  #  - 8.0\n  #  - 8.1\n  #  - 8.2\n  #  - 8.3\n  #  - 8.4\n  #  - 9.0\n  #  - 9.1\n  #  - 9.1\n  #  - 9.2\n  - name: Ubuntu\n    versions:\n  #  - all\n  #  - lucid\n  #  - maverick\n  #  - natty\n  #  - oneiric\n  #  - precise\n  #  - quantal\n  #  - raring\n  #  - saucy\n     - trusty\n  #- name: SLES\n  #  versions:\n  #  - all\n  #  - 10SP3\n  #  - 10SP4\n  #  - 11\n  #  - 11SP1\n  #  - 11SP2\n  #  - 11SP3\n  #- name: GenericLinux\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Debian\n  #  versions:\n  #  - all\n  #  - etch\n  #  - lenny\n  #  - squeeze\n  #  - wheezy\n  #\n  # Below are all categories currently available. Just as with\n  # the platforms above, uncomment those that apply to your role.\n  #\n  categories:\n  - cloud\n  #- cloud:ec2\n  #- cloud:gce\n  #- cloud:rax\n  #- clustering\n  #- database\n  #- database:nosql\n  #- database:sql\n  #- development\n  #- monitoring\n  #- networking\n  #- packaging\n  - system\n  #- web\ndependencies: []\n  # List your role dependencies here, one per line. Only\n  # dependencies available via galaxy should be listed here.\n  # Be sure to remove the '[]' above if you add dependencies\n  # to this list.\n"}, {"commit_sha": "218cdc58f9fe9d7ece7d43e5f100fe9631fde5cc", "sha": "aa53ca1456a4f52868c7f3bbd9c66e82c32774d4", "filename": "tasks/main.yml", "repository": "fubarhouse/ansible-role-golang", "decoded_content": "---\n# main tasks file for fubarhouse-golang\n\n- name: \"Include tasks gathering system information\"\n  include: setup.yml\n\n- name: \"Include tasks to clean installation\"\n  include: cleanup.yml\n  when: go_install_clean == true\n\n- name: \"Include tasks for installation\"\n  include: install.yml\n  when:\n    - current_go_version is undefined or \"go version go{{ go_version }} {{ GOOS }}/{{ GOARCH }}\" not in current_go_version.stdout\n    - install_go_from_source == false\n\n# - name: \"Include tasks for installation from source\"\n#   include: install-source.yml\n#   when: install_go_from_source == true\n\n- name: \"Include tasks for Go Get\"\n  include: go-get.yml\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "08511e6381e9fc1c295475f77761723fab76494a", "filename": "roles/dns/manage-dns-records/tasks/nsupdate/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- include_tasks: nsupdate-server.yml\n  with_subelements:\n    - \"{{ dns_data.views }}\"\n    - zones\n  loop_control:\n    loop_var: dns\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8bd0d05d7af9ef982757781c4f4d3f2bf228194c", "filename": "roles/dns/manage-dns-zones/tasks/named/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Use a unique temporary directory to store the config files pre-assemble'\n  tempfile:\n    state: directory\n    prefix: dns-zone\n  register: dns_zone_tempdir\n  notify:\n  - 'cleanup temp'\n\n- name: 'Store away the temporary configuration files directory name'\n  set_fact:\n    dns_zone_temp_config_dir: '{{ dns_zone_tempdir.path }}'\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b2a0408f2668467dd412bc54d72dd0f1336e2655", "filename": "reference-architecture/aws-ansible/playbooks/roles/openshift-versions/tasks/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Set Origin fact\n  set_fact:\n    openshift_release: \"{{ origin_release }}\"\n  when: deployment_type == \"origin\"\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "dbbb3f7a5e57e63d583d9fa84786a5a9adec6bf0", "filename": "roles/ovirt-engine-remote-db/defaults/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_engine_remote_db_port: 5432\novirt_engine_remote_db_listen_address: '*'\novirt_engine_db_name: 'engine'\novirt_engine_db_user: 'engine'\n\novirt_engine_remote_db: False\novirt_engine_dwh_remote_db: False\n\novirt_engine_dwh_db_name: 'ovirt_engine_history'\novirt_engine_dwh_db_user: 'ovirt_engine_history'\n"}, {"commit_sha": "b11c4477d973b0cc87a296f6b028eaf9abab4686", "sha": "218331b83a5d843d4d5e02c5b93fc131d79171c8", "filename": "meta/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "galaxy_info:\n  author: Bjorn Oscarsson\n  description: \"Installs and configures Docker Community Edition (CE)\"\n  min_ansible_version: 2.1\n  license: MIT\n  platforms:\n  - name: Fedora\n    versions:\n      - 24\n      - 25\n  - name: EL\n    versions:\n      - 7\n\n  galaxy_tags:\n    - docker\n    - docker-ce\n\ndependencies: []\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "04170eed0bee26655c7b520eb9d591f66ec97246", "filename": "roles/dns/manage-dns-zones/tasks/route53/loop-zones.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: \"Loop over all zones\"\n  include_tasks: loop-records.yml\n  with_items:\n    - \"{{ zones_records.results }}\"\n  loop_control:\n    loop_var: r53_zone\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a1fc85fefd43a8142ea5d563721294393c8382f5", "filename": "roles/config-libvirt/tasks/libvirt.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install the Virtualization Host group\"\n  yum:\n    name: \"@^virtualization-host-environment\"\n    state: present\n\n- name: \"Ensure libvirt is running\"\n  service:\n    name: libvirtd \n    state: started\n    enabled: yes \n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "bca20688255ef4e76507d74d1d5fbc36264b181a", "filename": "reference-architecture/vmware-ansible/playbooks/cns-node-setup.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: cns\n  gather_facts: yes\n  become: no\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - instance-groups\n  - rhsm\n  - vmware-guest-setup\n  - docker-storage-setup\n  - openshift-volume-quota\n  - gluster-ports\n- include: add-node-prerequisite.yaml\n"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "bf66a616d63be1d04341b2b6023507c09dd3c747", "filename": "tasks/fetch/oracle-fallback.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Detecting system options for fetching artifact\"\n  set_fact:\n    distro: \"{{ ansible_facts.system | lower }}\"\n    arch: \"{{ 'i586' if ansible_architecture == 'i386' else 'x64' }}\"\n\n- name: \"Download artifact from Oracle OTN\"\n  get_url:\n    url: \"{{ fallback_oracle_artifacts[distro][java_major_version][java_minor_version][arch][java_package] }}\"\n    dest: \"{{ download_path }}\"\n    mode: \"0755\"\n    headers: \"Cookie:gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie; --no-check-certificate\"\n  until: \"'OK' in FILE_DOWNLOADED.msg\"\n  # I'm convinced that about half of what separates the successful entrepreneurs\n  # from the non-successful ones is pure perseverance. It is so hard.\n  # https://en.wikiquote.org/wiki/Steve_Jobs\n  retries: 15\n  delay: 5\n  delegate_to: \"localhost\"\n  connection: \"local\"\n  become: False\n  register: FILE_DOWNLOADED\n\n- name: \"Downloaded artifact\"\n  set_fact:\n    oracle_artifact: \"{{ FILE_DOWNLOADED.dest }}\"\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "57d611e04619637c4c73be92506afe85d40b6a9d", "filename": "roles/ipaserver/defaults/main.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "---\n# defaults file for ipaserver\n\n### basic ###\nipaserver_no_host_dns: no\n### server ###\nipaserver_setup_adtrust: no\nipaserver_setup_kra: no\nipaserver_setup_dns: no\nipaserver_no_hbac_allow: no\nipaserver_no_pkinit: no\nipaserver_no_ui_redirect: no\n### ssl certificate ###\n### client ###\nipaclient_mkhomedir: no\nipaclient_no_ntp: no\n#ipaclient_ssh_trust_dns: no\n#ipaclient_no_ssh: no\n#ipaclient_no_sshd: no\n#ipaclient_no_dns_sshfp: no\n### certificate system ###\nipaserver_external_ca: no\n### dns ###\nipaserver_allow_zone_overlap: no\nipaserver_no_reverse: no\nipaserver_auto_reverse: no\nipaserver_no_forwarders: no\nipaserver_auto_forwarders: no\nipaserver_no_dnssec_validation: no\n### ad trust ###\nipaserver_enable_compat: no\nipaserver_setup_ca: yes\n### packages ###\nipaserver_install_packages: yes\n### firewalld ###\nipaserver_setup_firewalld: yes\n\n### additional ###\nipaserver_allow_missing: [ ]\n\n### uninstall ###\nipaserver_ignore_topology_disconnect: no\nipaserver_ignore_last_of_role: no\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "5cc14a2c7060d2dc147ab9a06fe377828216c25b", "filename": "roles/ovirt-engine-backup/meta/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\ngalaxy_info:\n  author: \"Petr Kubica\"\n  description: \"oVirt backup role\"\n  company: \"Red Hat\"\n  license: \"GPLv3\"\n  min_ansible_version: 1.9\n  platforms:\n  - name: EL\n    versions:\n    - all\n  galaxy_tags:\n    - installer\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "a865dfb422953c02990fd4fa3db660fff386e626", "filename": "roles/vpn/defaults/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\nstrongswan_shell: /usr/sbin/nologin\nstrongswan_home: /var/lib/strongswan\nBetweenClients_DROP: true\nwireguard_config_path: \"configs/{{ IP_subject_alt_name }}/wireguard/\"\nwireguard_interface: wg0\nwireguard_network_ipv4:\n  subnet: 10.19.49.0\n  prefix: 24\n  gateway: 10.19.49.1\n  clients_range: 10.19.49\n  clients_start: 2\nwireguard_network_ipv6:\n  subnet: 'fd9d:bc11:4021::'\n  prefix: 48\n  gateway: 'fd9d:bc11:4021::1'\n  clients_range: 'fd9d:bc11:4021::'\n  clients_start: 2\nwireguard_vpn_network: \"{{ wireguard_network_ipv4['subnet'] }}/{{ wireguard_network_ipv4['prefix'] }}\"\nwireguard_vpn_network_ipv6: \"{{ wireguard_network_ipv6['subnet'] }}/{{ wireguard_network_ipv6['prefix'] }}\"\nkeys_clean_all: false\nwireguard_dns_servers: >-\n  {% if local_dns|default(false)|bool or dns_encryption|default(false)|bool == true %}\n  {{ local_service_ip }}\n  {% else %}\n  {% for host in dns_servers.ipv4 %}{{ host }}{% if not loop.last %},{% endif %}{% endfor %}{% if ipv6_support %},{% for host in dns_servers.ipv6 %}{{ host }}{% if not loop.last %},{% endif %}{% endfor %}{% endif %}\n  {% endif %}\n\nalgo_ondemand_cellular: false\nalgo_ondemand_wifi: false\nalgo_ondemand_wifi_exclude: '_null'\nalgo_windows: false\nalgo_store_cakey: false\nalgo_local_dns: false\nipv6_support: false\ndns_encryption: true\ndomain: false\nsubjectAltName_IP: \"IP:{{ IP_subject_alt_name }}\"\nsubjectAltName_USER: \"{% if '@' in item %}email:{{ item }}{% else %}DNS:{{ item }}{% endif %}\"\nopenssl_bin: openssl\nstrongswan_enabled_plugins:\n  - aes\n  - gcm\n  - hmac\n  - kernel-netlink\n  - nonce\n  - openssl\n  - pem\n  - pgp\n  - pkcs12\n  - pkcs7\n  - pkcs8\n  - pubkey\n  - random\n  - revocation\n  - sha2\n  - socket-default\n  - stroke\n  - x509\n\nciphers:\n  defaults:\n    ike: aes256gcm16-prfsha512-ecp384!\n    esp: aes256gcm16-ecp384!\n  compat:\n    ike: aes256gcm16-prfsha512-ecp384,aes256-sha2_512-prfsha512-ecp384,aes256-sha2_384-prfsha384-ecp384!\n    esp: aes256gcm16-ecp384,aes256-sha2_512-prfsha512-ecp384!\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "75ce0637bf611b0d9c41eb2cc4dd9ff7bd0dbe61", "filename": "roles/cfme-ocp-provider/tasks/query_provider.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: Query Provider\n  uri:\n    url: \"{{ provider_url.href }}\"\n    method: GET\n    user: \"{{ cfme_username }}\"\n    password: \"{{ cfme_password }}\"\n    force_basic_auth: true\n    validate_certs: false\n    status_code: 200\n  register: provider_result\n\n- name: Set Provider Facts When Match Found\n  set_fact:\n    ocp_provider_found: True\n    ocp_provider_id: \"{{ provider_result.json.id }}\"\n  when: \"provider_result.json.name == ocp_container_provier_name\""}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "941f6347e2697f30510c2da5137f99b16d751c69", "filename": "roles/7-edu-apps/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# Educational Apps\n\n- name: ...IS BEGINNING ========================================\n  command: echo\n\n- name: KALITE\n  include_role:\n    name: kalite\n  when: kalite_install\n  tags: kalite\n\n- name: KOLIBRI\n  include_role:\n    name: kolibri\n  when: kolibri_install\n  tags: kolibri\n\n- name: KIWIX\n  include_role:\n    name: kiwix\n  when: kiwix_install\n  tags: kiwix\n\n- name: MOODLE\n  include_role:\n    name: moodle\n  when: moodle_install\n  tags: olpc, moodle\n\n- name: OSM\n  include_role:\n    name: osm\n  when: osm_install\n  tags: osm\n\n- name: PATHAGAR\n  include_role:\n    name: pathagar\n  when: pathagar_install\n  tags: pathagar\n\n- name: SUGARIZER\n  include_role:\n    name: sugarizer\n  when: sugarizer_install\n  tags: sugarizer\n\n- name: Recording STAGE 7 HAS COMPLETED ========================\n  lineinfile:\n    dest: /etc/iiab/iiab.env\n    regexp: '^STAGE=*'\n    line: 'STAGE=7'\n    state: present\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "3b4916dca9d043bea827e0ebe603d00a33425e8c", "filename": "roles/user-management/manage-idm-users/tasks/configure_group.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n  - name: Create IPA group\n    ipa_group:\n      ipa_host: \"{{ ipa_host | default(ansible_host)}}\"\n      ipa_user: \"{{ ipa_admin_user }}\"\n      ipa_pass: \"{{ ipa_admin_password }}\"\n      validate_certs: \"{{ ipa_validate_certs | default(True) }}\"\n      name: \"{{ item.name | trim }}\"\n      state: \"{{ item.state | default('present') }}\"\n      user: \"{{ item.members | default('[]') }}\"\n    with_items: \"{{ user_groups }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8adf5d58307caf1e5fdfc31a2418d43ab6f91947", "filename": "playbooks/provision-dns-server/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_playbook: ../prep.yml\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../osp/manage-user-network.yml\n  when:\n  - hosting_infrastructure == 'openstack'\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../osp/provision-osp-instance.yml\n  when:\n  - hosting_infrastructure == 'openstack'\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../rhsm.yml\n  tags:\n  - 'never'\n  - 'install'\n\n- hosts: dns-server\n  roles:\n  - role: update-host\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: configure-dns-server.yml\n  tags:\n  - 'always'\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "524227c79994d92f5bfc6e9f9d8e8547c4defb95", "filename": "roles/load-balancers/manage-haproxy/tasks/activate-config.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- block:\n  - name: 'Copy the HAproxy config file to a temp location for validity checking'\n    copy:\n      src: '{{ haproxy_temp_file }}'\n      dest: '{{ temp_new_file }}'\n\n  - name: 'Check the validity'\n    command: 'haproxy -c -f {{ temp_new_file }}'\n    notify: 'remove tmp new file'\n\n  - name: 'Copy and activate the HAproxy config file'\n    copy:\n      src: '{{ haproxy_temp_file }}'\n      dest: '/etc/haproxy/haproxy.cfg'\n      backup: 'yes'\n    notify: 'reload haproxy'\n\n  - name: 'Open Firewall for LB use (TCP only)'\n    firewalld:\n      port: \"{{ fe.lb_host_port }}/tcp\"\n      permanent: yes\n      state: enabled\n      immediate: yes\n    loop_control:\n      loop_var: fe\n    loop: \"{{ lb_config.frontends|flatten(levels=1) }}\"\n\n  - name: 'Tweak SELinux for LB use (TCP only)'\n    seport:\n      ports: \"{{ fe.lb_host_port }}\"\n      proto: tcp\n      setype: http_port_t\n      state: present\n    loop_control:\n      loop_var: fe\n    loop: \"{{ lb_config.frontends|flatten(levels=1) }}\"\n\n  become: True\n\n- name: 'Clean-up the temp file'\n  file:\n    path: \"{{ haproxy_temp_file }}\"\n    state: absent\n  when:\n  - (clean_up_temp|default('yes'))|lower == 'yes'\n  delegate_to: localhost\n  run_once: True\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "050a460ff737bd7af54c7325e55a935a5e3d2480", "filename": "roles/config-vnc-server/tasks/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Determine required python firewall package'\n  set_fact:\n    python_firewall_package: \"python3-firewall\"\n  when:\n  - ansible_python_version is match(\"3.*\")\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - firewalld\n  - \"{{ python_firewall_package | default('python-firewall') }}\"\n\n- name: 'Ensure firewalld is running'\n  service:\n    name: firewalld\n    state: started\n    enabled: yes\n\n- name: 'Open Firewall for NFS use'\n  firewalld:\n    port: \"{{ item }}\"\n    permanent: yes\n    state: enabled\n    immediate: yes\n  with_items:\n  - 5900/tcp\n  - 5901/tcp\n  - 5902/tcp\n  - 5903/tcp\n  - 5904/tcp\n  - 5905/tcp\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "381e479fe6fbcc683a9896d2ab60522eb0980c9a", "filename": "reference-architecture/aws-ansible/playbooks/library/cloudformation_facts.py", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#!/usr/bin/python\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\nDOCUMENTATION = '''\n---\nmodule: cloudformation_facts\nshort_description: Obtain facts about an AWS CloudFormation stack\ndescription:\n  - Gets information about an AWS CloudFormation stack\nrequirements:\n  - boto3 >= 1.0.0\n  - python >= 2.6\nversion_added: \"2.2\"\nauthor: Justin Menga (@jmenga)\noptions:\n    stack_name:\n        description:\n          - The name or id of the CloudFormation stack\n        required: true\n    all_facts:\n        description:\n            - Get all stack information for the stack\n        required: false\n        default: false\n    stack_events:\n        description:\n            - Get stack events for the stack\n        required: false\n        default: false\n    stack_template:\n        description:\n            - Get stack template body for the stack\n        required: false\n        default: false\n    stack_resources:\n        description:\n            - Get stack resources for the stack\n        required: false\n        default: false\n    stack_policy:\n        description:\n            - Get stack policy for the stack\n        required: false\n        default: false\nextends_documentation_fragment:\n    - aws\n    - ec2\n'''\n\nEXAMPLES = '''\n# Note: These examples do not set authentication details, see the AWS Guide for details.\n\n# Get summary information about a stack\n- cloudformation_facts:\n    stack_name: my-cloudformation-stack\n\n# Facts are published in ansible_facts['cloudformation'][<stack_name>]\n- debug: msg={{ ansible_facts['cloudformation']['my-cloudformation-stack'] }}\n\n# Get all stack information about a stack\n- cloudformation_facts:\n    stack_name: my-cloudformation-stack\n    all_facts: true\n\n# Get stack resource and stack policy information about a stack\n- cloudformation_facts:\n    stack_name: my-cloudformation-stack\n    stack_resources: true\n    stack_policy: true\n\n# Example dictionary outputs for stack_outputs, stack_parameters and stack_resources:\n\"stack_outputs\": {\n    \"ApplicationDatabaseName\": \"dazvlpr01xj55a.ap-southeast-2.rds.amazonaws.com\",\n    ...\n},\n\"stack_parameters\": {\n    \"DatabaseEngine\": \"mysql\",\n    \"DatabasePassword\": \"****\",\n    ...\n},\n\"stack_resources\": {\n    \"AutoscalingGroup\": \"dev-someapp-AutoscalingGroup-1SKEXXBCAN0S7\",\n    \"AutoscalingSecurityGroup\": \"sg-abcd1234\",\n    \"ApplicationDatabase\": \"dazvlpr01xj55a\",\n    \"EcsTaskDefinition\": \"arn:aws:ecs:ap-southeast-2:123456789:task-definition/dev-someapp-EcsTaskDefinition-1F2VM9QB0I7K9:1\"\n    ...\n}\n'''\n\nRETURN = '''\nstack_description:\n    description: Summary facts about the stack\n    returned: always\n    type: dict\nstack_outputs:\n    description: Dictionary of stack outputs keyed by the value of each output 'OutputKey' parameter and corresponding value of each output 'OutputValue' parameter\n    returned: always\n    type: dict\nstack_parameters:\n    description: Dictionary of stack parameters keyed by the value of each parameter 'ParameterKey' parameter and corresponding value of each parameter 'ParameterValue' parameter\n    returned: always\n    type: dict\nstack_events:\n    description: All stack events for the stack\n    returned: only if all_facts or stack_events is true\n    type: list of events\nstack_policy:\n    description: Describes the stack policy for the stack\n    returned: only if all_facts or stack_policy is true\n    type: dict\nstack_template:\n    description: Describes the stack template for the stack\n    returned: only if all_facts or stack_template is true\n    type: dict\nstack_resource_list:\n    description: Describes stack resources for the stack\n    returned: only if all_facts or stack_resourses is true\n    type: list of resources\nstack_resources:\n    description: Dictionary of stack resources keyed by the value of each resource 'LogicalResourceId' parameter and corresponding value of each resource 'PhysicalResourceId' parameter\n    returned: only if all_facts or stack_resourses is true\n    type: dict\n'''\n\ntry:\n    import boto3\n    import botocore\n    HAS_BOTO3 = True\nexcept ImportError:\n    HAS_BOTO3 = False\n\nfrom ansible.module_utils.ec2 import get_aws_connection_info, ec2_argument_spec\nfrom ansible.module_utils.basic import AnsibleModule\nfrom functools import partial\nimport json\nimport traceback\n\nclass CloudFormationServiceManager:\n    \"\"\"Handles CloudFormation Services\"\"\"\n\n    def __init__(self, module):\n        self.module = module\n\n        try:\n            region, ec2_url, aws_connect_kwargs = get_aws_connection_info(module, boto3=True)\n            self.client = boto3_conn(module, conn_type='client',\n                                     resource='cloudformation', region=region,\n                                     endpoint=ec2_url, **aws_connect_kwargs)\n        except botocore.exceptions.NoRegionError:\n            self.module.fail_json(msg=\"Region must be specified as a parameter, in AWS_DEFAULT_REGION environment variable or in boto configuration file\")\n        except Exception as e:\n            self.module.fail_json(msg=\"Can't establish connection - \" + str(e), exception=traceback.format_exc(e))\n\n    def describe_stack(self, stack_name):\n        try:\n            func = partial(self.client.describe_stacks,StackName=stack_name)\n            response = self.paginated_response(func, 'Stacks')\n            if response:\n                return response[0]\n            self.module.fail_json(msg=\"Error describing stack - an empty response was returned\")\n        except Exception as e:\n            self.module.fail_json(msg=\"Error describing stack - \" + str(e), exception=traceback.format_exc(e))\n\n    def list_stack_resources(self, stack_name):\n        try:\n            func = partial(self.client.list_stack_resources,StackName=stack_name)\n            return self.paginated_response(func, 'StackResourceSummaries')\n        except Exception as e:\n            self.module.fail_json(msg=\"Error listing stack resources - \" + str(e), exception=traceback.format_exc(e))\n\n    def describe_stack_events(self, stack_name):\n        try:\n            func = partial(self.client.describe_stack_events,StackName=stack_name)\n            return self.paginated_response(func, 'StackEvents')\n        except Exception as e:\n            self.module.fail_json(msg=\"Error describing stack events - \" + str(e), exception=traceback.format_exc(e))\n\n    def get_stack_policy(self, stack_name):\n        try:\n            response = self.client.get_stack_policy(StackName=stack_name)\n            stack_policy = response.get('StackPolicyBody')\n            if stack_policy:\n                return json.loads(stack_policy)\n            return dict()\n        except Exception as e:\n            self.module.fail_json(msg=\"Error getting stack policy - \" + str(e), exception=traceback.format_exc(e))\n\n    def get_template(self, stack_name):\n        try:\n            response = self.client.get_template(StackName=stack_name)\n            return response.get('TemplateBody')\n        except Exception as e:\n            self.module.fail_json(msg=\"Error getting stack template - \" + str(e), exception=traceback.format_exc(e))\n\n    def paginated_response(self, func, result_key, next_token=None):\n        '''\n        Returns expanded response for paginated operations.\n        The 'result_key' is used to define the concatenated results that are combined from each paginated response.\n        '''\n        args=dict()\n        if next_token:\n            args['NextToken'] = next_token\n        response = func(**args)\n        result = response.get(result_key)\n        next_token = response.get('NextToken')\n        if not next_token:\n           return result\n        return result + self.paginated_response(func, result_key, next_token)\n\ndef to_dict(items, key, value):\n    ''' Transforms a list of items to a Key/Value dictionary '''\n    if items:\n        return dict(zip([i[key] for i in items], [i[value] for i in items]))\n    else:\n        return dict()\n\ndef main():\n    argument_spec = ec2_argument_spec()\n    argument_spec.update(dict(\n        stack_name=dict(required=True, type='str' ),\n        all_facts=dict(required=False, default=False, type='bool'),\n        stack_policy=dict(required=False, default=False, type='bool'),\n        stack_events=dict(required=False, default=False, type='bool'),\n        stack_resources=dict(required=False, default=False, type='bool'),\n        stack_template=dict(required=False, default=False, type='bool'),\n    ))\n\n    module = AnsibleModule(argument_spec=argument_spec, supports_check_mode=False)\n\n    if not HAS_BOTO3:\n      module.fail_json(msg='boto3 is required.')\n\n    # Describe the stack\n    service_mgr = CloudFormationServiceManager(module)\n    stack_name = module.params.get('stack_name')\n    result = {\n        'ansible_facts': { 'cloudformation': { stack_name:{} } }\n    }\n    facts = result['ansible_facts']['cloudformation'][stack_name]\n    facts['stack_description'] = service_mgr.describe_stack(stack_name)\n\n    # Create stack output and stack parameter dictionaries\n    if facts['stack_description']:\n        facts['stack_outputs'] = to_dict(facts['stack_description'].get('Outputs'), 'OutputKey', 'OutputValue')\n        facts['stack_parameters'] = to_dict(facts['stack_description'].get('Parameters'), 'ParameterKey', 'ParameterValue')\n\n    # normalize stack description API output\n    facts['stack_description'] = camel_dict_to_snake_dict(facts['stack_description'])\n    # camel2snake doesn't handle NotificationARNs properly, so let's fix that\n    facts['stack_description']['notification_arns'] = facts['stack_description'].pop('notification_ar_ns', [])\n\n    # Create optional stack outputs\n    all_facts = module.params.get('all_facts')\n    if all_facts or module.params.get('stack_resources'):\n        facts['stack_resource_list'] = service_mgr.list_stack_resources(stack_name)\n        facts['stack_resources'] = to_dict(facts.get('stack_resource_list'), 'LogicalResourceId', 'PhysicalResourceId')\n    if all_facts or module.params.get('stack_template'):\n        facts['stack_template'] = service_mgr.get_template(stack_name)\n    if all_facts or module.params.get('stack_policy'):\n        facts['stack_policy'] = service_mgr.get_stack_policy(stack_name)\n    if all_facts or module.params.get('stack_events'):\n        facts['stack_events'] = service_mgr.describe_stack_events(stack_name)\n\n    result['changed'] = False\n    module.exit_json(**result)\n\n# import module snippets\nfrom ansible.module_utils.basic import *\nfrom ansible.module_utils.ec2 import *\n\nif __name__ == '__main__':\n    main()\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "33c843d4d6ec832e24f12ed005e16091547347a8", "filename": "playbooks/roles/docket/tasks/prereqs.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# prepreqs checks for rocknsm.docket\n\n# Validate hosts exist in stenographer group\n- name: docket | check for docket and stenographer hosts\n  assert:\n    that:\n      - \"{{ ('docket' in groups) and (groups['docket'] | length) > 0 }}\"\n      - \"{{ ('stenographer' in groups) and (groups['stenographer'] | length) > 0 }}\"\n    msg: \"The [docket] and [stenographer] inventory groups must each have at least one host.\"\n\n- name: docket | check docket and stenographer hosts for pyopenssl\n  yum:\n    list=*pyOpenSSL\n  register: pyopenssl_status\n\n- name: docket | validate pyopenssl >= 15.0 is installed\n  assert:\n    that:\n      - \"{{pyopenssl_status.results|selectattr('yumstate', 'match', 'installed')|map(attribute='version')| version_compare('15.0.0', '>=' )}}\"\n    msg: \"Docket hosts require PyOpenSSL greater than 15.0 prior to executing this play\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "0f3b9665827c6032c75ce47636c9f6ddc47676c5", "filename": "roles/monit/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install Monit package\n  package:\n    name: monit\n    state: present\n  tags:\n    - download\n\n- name: Install chkconfig package -- not in Debian 8\n  package:\n    name: chkconfig\n    state: present\n  when: is_debian and ansible_distribution_major_version == \"8\"\n  tags:\n    - download\n\n- name: Update main config file\n  template:\n    backup: yes\n    src: monitrc\n    dest: /etc/monitrc\n    owner: root\n    group: root\n    mode: 0600\n\n- name: Update config files\n  template:\n    src: \"{{ item }}\"\n    dest: \"/etc/monit.d/{{ item }}\"\n    owner: root\n    group: root\n    force: yes\n    mode: 0755\n  with_items: watchdog\n  register: monit_config\n  when: false\n  until: monit_config | success\n  retries: 5\n  delay: 1\n\n#TODO: create systemd script\n- name: Enable 'monit' service\n  command: chkconfig monit on\n  when: is_debian and ansible_local.local_facts.os_ver == \"debian-8\"\n\n#- name: Restart monit service\n#  command: service monit restart\n\n- name: Add 'monit' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: monit\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: Monit\n    - option: description\n      value: '\"Monit is a background service monitor which can correct problems, send email, restart services.\"'\n    - option: enabled\n      value: \"{{ monit_enabled }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "414895820e4a8ebb7bd8de7c93051ecdf816cd8d", "filename": "roles/config-docker/tasks/docker.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: \"Install additional packages for Docker\"\n  package:\n    name: \"{{ item }}\"\n    state: latest\n  with_items:\n  - docker\n  notify: \n  - restart docker\n\n- name: \"Enable docker\"\n  service:\n    name: docker\n    enabled: yes\n    state: started\n\n# This task will error out if the user doesn't exist\n- name: \"Check to see if the targeted docker user exists\"\n  command: id \"{{ docker_username }}\"\n  changed_when: false\n\n- name: \"Add docker group\"\n  group:\n    name: docker\n    state: present\n  notify: restart docker\n\n- name: \"Add username to the docker group\" \n  user:\n    name: \"{{ docker_username }}\"\n    groups: docker\n    append: yes\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "7e36aff2c1dc2479bc12a6e30c7ed044f8d79e9c", "filename": "roles/openshift-defaults/defaults/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\nopenshift:\n  common:\n    client_binary: \"oc\"\n    admin_binary: \"oc adm\"\ndocker:\n  common:\n    client_binary: \"docker\"\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "7e3b399fe88cf8898024fecc8e25be04cdf726d2", "filename": "tasks/check_requirements.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: Check if OS is Debian-based (we do not support others)\n    debug: \"Check OS family\"\n    failed_when: ansible_os_family != \"Debian\"\n\n  - name: Check if Ansible version is supported\n    debug: \"Check Ansible version\"\n    failed_when: ansible_version.full | version_compare('1.8', '<')\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "69d3156843e7052afecabf7398ca6992261825f2", "filename": "roles/osp/packstack-post/tasks/mariadb.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Ensure target directory exists\"\n  file:\n    path: \"/etc/systemd/system/mariadb.service.d\"\n    state: directory\n\n- name: \"Update mariadb/MySQL to handle additional open files\"\n  copy:\n    src: \"mariadb-limits.conf\"\n    dest: \"/etc/systemd/system/mariadb.service.d/limits.conf\"\n  notify:\n  - 'restart mariadb'\n\n- name: \"Perform daemon-reload to ensure the changes are picked up\"\n  command: 'systemctl daemon-reload'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "93080cdaea917573cd5a5b6c59862ab3d82d4b9a", "filename": "roles/dns/manage-dns-zones/tasks/named/generate_keys.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Build a list of view/zone items\n  set_fact:\n    dns_views: \"{{ dns_views | default([]) + [ item.0.name + '-' + item.1.dns_domain ] }}\"\n  with_subelements:\n    - \"{{ dns_data.views }}\"\n    - zones\n\n- name: Get existing key files\n  shell: 'ls -1 K{{ item }}*.key | sed -ne \"s/K\\({{ item }}\\).*.key/\\1/p\"'\n  args:\n    chdir: /var/named/\n  register: key_files\n  ignore_errors: yes\n  with_items:\n    - \"{{ dns_views }}\"\n\n- name: Build list of existing key files\n  set_fact:\n    existing_key_files: \"{{ existing_key_files | default([]) + [ item.stdout ] }}\"\n  with_items:\n    - \"{{ key_files.results }}\"\n\n- name: Generate keys for nsupdate\n  command: >\n    /sbin/dnssec-keygen\n      -a {{ dnssec_keygen_algorithm | default(default_dnssec_keygen_algorithm) }}\n      -b {{ dnssec_keygen_size | default(default_dnssec_keygen_size) }}\n      -n USER\n      -r /dev/urandom\n      -K /var/named {{ item }}\n  with_items:\n    - \"{{ dns_views }}\"\n  when:\n    - item not in existing_key_files\n\n- name: Gather keys for nsupdate\n  shell: \"grep Key: /var/named/K{{ item }}*.private | cut -d ' ' -f 2\"\n  register: nsupdate_keys_captured\n  with_items:\n    - \"{{ dns_views }}\"\n\n# Build the dict with the proper keys, i.e.:\n#    private-view.example.com:\n#      algorithm: HMAC-MD5\n#      secret: SKqKNdpfk7llKxZ57bbxUnUDobaaJp9t8CjXLJPl+fRI5mPcSBuxTAyvJPa6Y9R7vUg9DwCy/6WTpgLNqnV4Hg==\n#    public-view.example.com:\n#      algorithm: HMAC-MD5\n#      secret: kVE2bVTgZjrdJipxPhID8BEZmbHD8cExlVPR+zbFpW6la8kL5wpXiwOh8q5AAosXQI5t95UXwq3Inx8QT58duw==\n- name: Set nsupdate_keys fact\n  set_fact:\n    nsupdate_keys: \"{{ nsupdate_keys | default({}) | combine({ item.item : { 'key_algorithm': ( dnssec_keygen_algorithm | default(default_dnssec_keygen_algorithm) ), 'key_secret': item.stdout } }) }}\"\n  with_items: \"{{ nsupdate_keys_captured.results }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "853ef5b5f583d0449e6b5b64e4e959a7908dce4d", "filename": "roles/config-vnc-server/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nvnc_home_dir: '/home'\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "7b4944973aea24d80d7e12c09032cd3aceca74aa", "filename": "playbooks/openshift-storage.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: schedulable_nodes\n  gather_facts: no\n  become: yes\n  roles:\n  - docker-storage-setup\n  - openshift-volume-quota\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "cff2e9c1e9a4f4509275694b646dad8e5f6f856a", "filename": "playbooks/roles/suricata/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# handlers file for suricata"}, {"commit_sha": "bf6e08dcb2440421477b6536ff6a8d11adc2be17", "sha": "b6f4b97021cd5fc1dcc2006ec37291a02c0e375b", "filename": "roles/marathon/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for marathon\nmarathon_port: 8080\nconsul_dir: /etc/consul.d\nmarathon_hostname: \"{{ inventory_hostname }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9ef8b0d3f81c6ccb42c498a3624284dfa9501998", "filename": "roles/ansible/tower/config-ansible-tower/tasks/install.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- block: # become: True\n\n  - name: \"install epel-release\"\n    package:\n      name: \"{{ ansible_tower_epel_download_url }}\"\n      state: present\n\n  - name: \"Set installation dir fact\"\n    set_fact:\n      ansible_tower_dir: \"{{ ansible_tower_download_url |\u00a0basename |\u00a0regex_replace('.tar.gz','') }}\"\n\n  - name: \"Check if installer dir exists\"\n    stat:\n      path: \"{{ ansible_tower_dir }}\"\n    register: installer_dir\n\n  - name: \"Download & Unpack Ansible Tower installer\"\n    shell: curl {{\u00a0ansible_tower_download_url }} | tar xzf -\n    args:\n      warn: False\n    when: not installer_dir.stat.exists\n\n  - name: \"Set up the Ansible Tower inventory\"\n    template:\n      src: inventory.j2\n      dest: \"{{ ansible_tower_dir }}/inventory\"\n    register: inventory\n\n  - name: \"run tower installer\"\n    shell: ./setup.sh\n    args:\n      chdir: \"{{ ansible_tower_dir }}\"\n\n  - name: \"Wait for Tower to become available before proceeding (30 sec max)\"\n    uri:\n      url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v1/config/\"\n      user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      password: \"{{ ansible_tower.admin_password }}\"\n      force_basic_auth: yes\n      method: GET\n      validate_certs: no\n    register: status_output\n    until: status_output.status == 200\n    retries: 6\n    delay: 5\n\n  - name: \"Add Tower license\"\n    uri:\n      url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v1/config/\"\n      user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      password: \"{{ ansible_tower.admin_password }}\"\n      force_basic_auth: yes\n      method: POST\n      body: '{{\u00a0lookup(\"file\", ansible_tower.install.license_file) | from_json |\u00a0combine({\"eula_accepted\":\"true\"}) | to_json }}'\n      body_format: 'json'\n      headers:\n        Content-Type: \"application/json\"\n        Accept: \"application/json\"\n      validate_certs: no\n\n  - name: \"Download and Install the 'oc' client for OpenShift interactions\"\n    shell: curl {{ ansible_tower_oc_download_url }} | tar -C /bin/ -xzf -\n    args:\n      warn: False\n    when:\n    - ansible_tower_oc_download_url|trim != ''\n\n  - name: \"Copy custom Tower SSL certificate and Key\"\n    block:\n      - copy:\n          src: \"{{ ansible_tower.install.ssl_certificate.cert }}\"\n          dest: /etc/tower/tower.cert\n        notify:\n        - restart-tower\n      - copy:\n          src: \"{{ ansible_tower.install.ssl_certificate.key }}\"\n          dest: /etc/tower/tower.key\n        notify:\n        - restart-tower\n    when:\n    - ansible_tower.install.ssl_certificate is defined\n\n  become: True\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "9adbf7ad82df2f0f7bc0ce56aeaec6fac9e779cd", "filename": "roles/network/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "# Defaults for network detection\nwireless_lan_present: False\nstrict_networking: False\niiab_demo_mode: False\ngui_static_wan: False\nwan_cidr:\n\n# Set defaults for discovery process as strings\nwifi1: \"not found-1\"\nwifi2: \"not found-2\"\nap_device: \"none\"\ndevice_gw: \"none\"\ndevice_gw2: \"\"\n\niiab_wan_iface: \"none\"\niiab_lan_iface: \"none\"\ndiscovered_lan_iface: \"none\"\ndiscovered_wired_iface: \"none\"\ndiscovered_wireless_iface: \"none\"\n\n# Red Hat\n#iiab_wired_lan_iface: \"none\"\n#iiab_wireless_lan_iface: \"none\"\nhas_WAN: False\nhas_ifcfg_gw: \"none\"\nhas_wifi_gw: \"none\"\n\n# Debian\ndhcpcd_result: \"\"\nwan_in_interfaces: False\nnetwork_manager_active: False\nsystemd_networkd_active: False\n\n# The values here are default local variables.\ngui_wan_iface: \"unset\"\ngui_static_wan_ip: \"unset\"\ngui_desired_network_role: Gateway\nwondershaper_dspeed: \"4096\"\nwondershaper_upspeed: \"1024\"\n\n# Wi-Fi\nhost_ssid: IIAB\nhostapd_wait: 1\nhost_wifi_mode: g\nhost_channel: 6\nhost_wireless_n: False\n# Below moved to /etc/iiab/local_vars.yml: (so implementer sets this)\n#host_country_code: US\nhostapd_secure: True\nhostapd_password: \"iiab2017\"\ndriver_name: nl80211\nhostapd_enabled: True\n# Above is forcibly set to False (in roles/network/tasks/main.yml) if IIAB is\n# being WiFi-installed (run \"iiab-hotspot-on\" AFTER ./iiab-install completes\n# and content is downloaded, to enable the internal WiFi Access Point / AP!)\nreboot_to_AP: False\n# For those installing IIAB over WiFi: \"reboot_to_AP: True\" overrides the above\n# detection of WiFi-as-gateway, forcing \"hostapd_enabled: True\" regardless.\n\nnetwork_config_dir: /etc/network/interfaces.d\n#iiab_network_mode: \"Gateway\"\ndns_jail_enabled: False\nservices_externally_visible: False\n\n# Following variables set for old Apache, dhcpd, named usage\ndhcpd_install: True\ndhcpd_enabled: True\nnamed_install: True\nnamed_enabled: True\ndnsmasq_enabled: False\ndnsmasq_install: False\n\n# For @tim-moody's Nodogsplash approach to Captive Portal?  High experimental as of June 2018: github.com/iiab/iiab/issues/608\ncaptive_portal_enabled: False\n\n# Simple python Captive Portal, that @m-anish & @jvonau are experimenting with in July 2018: github.com/iiab/iiab/pull/870\npy_captive_portal_install: True\npy_captive_portal_enabled: True\npy_captive_portal_port: \"9090\"\npy_captive_portal_username: \"Admin\"\npy_captive_portal_password: \"changeme\"\n"}, {"commit_sha": "584d564218d6e2e63d3ecca157daf817fe4d533c", "sha": "f40da9be66a47c62e89654001417e792e2b74fca", "filename": "tasks/camera.yml", "repository": "mikolak-net/ansible-raspi-config", "decoded_content": "---\n- name: camera - ensure memory\n  pi_boot_config: config_vals=gpu_mem=128\n  when: raspi_config_enable_camera and raspi_config_memory_split_gpu < raspi_config_min_camera_mem\n- name: camera - set state\n  pi_boot_config: config_vals=start_x={{raspi_config_enable_camera|int}}\n  notify:\n    - apply raspi-config\n    - reboot"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "f80cf2ab93c1481049c8a2f558d517a27a4355f3", "filename": "ops/playbooks/config_monitoring.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- name: Deploy Grafana and Prometheus Stack for Swarm\n  hosts: vms\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - includes/internal_vars.yml\n\n  environment: \"{{ env }}\"\n\n  roles:\n\n# open the port used by Grafana\n    - role: hpe.openports\n      hpe_openports_ports:\n        - 3000/tcp\n      when: inventory_hostname in groups.ucp_lb or inventory_hostname in groups.loadbalancer\n\n  tasks:\n\n    - name: Configure all VMS to send their logs to the logger VM\n      template: src=../templates/rsyslog.conf.j2 dest=/etc/rsyslog.conf\n      when: inventory_hostname in groups.vms and inventory_hostname not in groups.logger\n      notify: Restart Rsyslog\n\n    - name: Copy monitoring files to UCP\n      copy: \n        src: ../monitoring\n        dest: /root\n        owner: root\n        group: root\n        mode: 0644\n      when: inventory_hostname in groups.ucp\n\n    - name: Copy docker compose file\n      template: src=../monitoring/docker-compose.yml.j2 dest=/root/monitoring/docker-compose.yml\n      when: inventory_hostname in groups.ucp\n\n    - name: Deploy monitoring stack\n      command: docker stack deploy --compose-file docker-compose.yml prom\n      args:\n        chdir: /root/monitoring\n      when: inventory_hostname in groups.ucp_main\n\n    - name: Wait for Prometheus to be available\n      wait_for:\n        host: \"{{ inventory_hostname }}\"\n        port: 9090\n        delay: 15\n      when: inventory_hostname in groups.ucp_main\n\n    - name: Wait for Grafana to be available\n      wait_for:\n        host: \"{{ inventory_hostname }}\"\n        port: 3000\n        delay: 15\n      when: inventory_hostname in groups.ucp_main\n\n    - name: Check if data source already available\n      uri:\n        url: \"http://{{ inventory_hostname }}.{{ domain_name }}:3000/api/datasources/name/Prometheus\"\n        method: GET\n        user: admin\n        password: admin\n        status_code: 200,404\n        body_format: json\n        force_basic_auth: yes\n      when: inventory_hostname in groups.ucp_main\n      register: ds\n\n    - name: Create Grafana Data Source\n      uri:\n        url: \"http://{{ inventory_hostname }}.{{ domain_name }}:3000/api/datasources\"\n        method: POST\n        user: admin\n        password: admin\n        body: \"{{ lookup('file','../monitoring/datasource.json') }}\"\n        status_code: 200\n        body_format: json\n        force_basic_auth: yes\n      when: inventory_hostname in groups.ucp_main and ds.status == 404\n\n    - name: Import Dashboard\n      uri:\n        url: \"http://{{ inventory_hostname }}.{{ domain_name }}:3000/api/dashboards/db\"\n        method: POST\n        user: admin\n        password: admin\n        body: {\"dashboard\": \"{{ lookup('file','../monitoring/Docker-Swarm-Dashboard.json') }}\",\"overwrite\": true}\n        status_code: 200\n        body_format: json\n        force_basic_auth: yes\n      when: inventory_hostname in groups.ucp_main\n\n    # not really needed but if someone makes a change to haproxy\n    - name: Update haproxy.cfg on UCP Load Balancer\n      template:\n        src: ../templates/haproxy.cfg.ucp.j2\n        dest: /etc/haproxy/haproxy.cfg\n        owner: root\n        group: root\n        mode: 0644\n      when: inventory_hostname in groups.ucp_lb or inventory_hostname in groups.loadbalancer\n      notify: Enable and start haproxy service\n\n  handlers:\n    - name: Enable and start haproxy service\n      systemd:\n        name: haproxy\n        enabled: yes\n        state: restarted\n\n    - name: Restart Rsyslog\n      systemd:\n        name: rsyslog\n        enabled: yes\n        state: restarted\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "3dafed891f34957be81df12d11d04e557ed01bfc", "filename": "roles/load-balancers/manage-haproxy/tasks/install.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- block:\n\n  - name: 'Install required packages'\n    package:\n      name: '{{ item }}'\n      state: installed\n    with_items:\n    - haproxy\n    - openssl-devel\n    - firewalld\n    - python-firewall\n    - libsemanage-python\n    - policycoreutils-python\n    notify: 'enable and start service(s)'\n\n  - name: 'Start firewalld'\n    service:\n      name: firewalld\n      state: started\n      enabled: yes\n\n  - name: 'Enable syslog logging'\n    copy:\n      src: rsyslog_haproxy.conf\n      dest: /etc/rsyslog.d/haproxy.conf\n    notify: 'restart rsyslog'\n\n  become: True\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "adf78c9668eca0e33a7cfd6ea14118c622ea84bd", "filename": "roles/static_inventory/tasks/openstack.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- no_log: true\n  block:\n    - name: fetch all nodes from openstack shade dynamic inventory\n      command: shade-inventory --list\n      register: registered_nodes_output\n      when: refresh_inventory|bool\n\n    - name: set fact for openstack inventory cluster nodes\n      set_fact:\n        registered_nodes: \"{{ (registered_nodes_output.stdout | from_json) | json_query(q) }}\"\n      vars:\n        q: \"[] | [?metadata.clusterid=='{{stack_name}}']\"\n      when:\n        - refresh_inventory|bool\n\n    - name: set_fact for openstack inventory nodes\n      set_fact:\n        registered_bastion_nodes: \"{{ (registered_nodes_output.stdout | from_json) | json_query(q) }}\"\n        registered_nodes_floating: \"{{ (registered_nodes_output.stdout | from_json) | json_query(q2) }}\"\n      vars:\n        q: \"[] | [?metadata.group=='infra.{{stack_name}}']\"\n        q2: \"[] | [?metadata.clusterid=='{{stack_name}}'] | [?public_v4!='']\"\n      when:\n        - refresh_inventory|bool\n\n    - name: set_fact for openstack inventory nodes with provider network\n      set_fact:\n        registered_nodes_floating: \"{{ (registered_nodes_output.stdout | from_json) | json_query(q) }}\"\n      vars:\n        q: \"[] | [?metadata.clusterid=='{{stack_name}}'] | [?public_v4=='']\"\n      when:\n        - refresh_inventory|bool\n        - openstack_provider_network_name|default(None)\n\n    - name: Add cluster nodes w/o floating IPs to inventory\n      with_items: \"{{ registered_nodes|difference(registered_nodes_floating) }}\"\n      add_host:\n        name: '{{ item.name }}'\n        ansible_host: >-\n          {% if use_bastion|bool -%}\n          {{ item.name }}\n          {%- else -%}\n          {%- set node = registered_nodes | json_query(\"[?name=='\" + item.name + \"']\") -%}\n          {{ node[0].addresses[openstack_private_network|quote][0].addr }}\n          {%- endif %}\n        ansible_fqdn: '{{ item.name }}'\n        ansible_user: '{{ ssh_user }}'\n        ansible_private_key_file: '{{ private_ssh_key }}'\n        ansible_ssh_extra_args: '-F {{ ssh_config_path }}'\n        private_v4: >-\n          {% set node = registered_nodes | json_query(\"[?name=='\" + item.name + \"']\") -%}\n          {{ node[0].addresses[openstack_private_network|quote][0].addr }}\n\n    - name: Add cluster nodes with floating IPs to inventory\n      with_items: \"{{ registered_nodes_floating }}\"\n      add_host:\n        name: '{{ item.name }}'\n        ansible_host: >-\n          {% if use_bastion|bool -%}\n          {{ item.name }}\n          {%- elif openstack_provider_network_name|default(None) -%}\n          {{ item.private_v4 }}\n          {%- else -%}\n          {{ item.public_v4 }}\n          {%- endif %}\n        ansible_fqdn: '{{ item.name }}'\n        ansible_user: '{{ ssh_user }}'\n        ansible_private_key_file: '{{ private_ssh_key }}'\n        ansible_ssh_extra_args: '-F {{ ssh_config_path }}'\n        private_v4: >-\n          {% set node = registered_nodes | json_query(\"[?name=='\" + item.name + \"']\") -%}\n          {{ node[0].addresses[openstack_private_network|quote][0].addr }}\n        public_v4: >-\n          {% if openstack_provider_network_name|default(None) -%}\n          {{ item.private_v4 }}\n          {%- else -%}\n          {{ item.public_v4 }}\n          {%- endif %}\n\n    # Split registered_nodes into old nodes and new app nodes\n    # Add new app nodes to new_nodes host group for upscaling\n    - name: Create new_app_nodes variable\n      set_fact:\n        new_app_nodes: []\n\n    - name: Filter new app nodes out of registered_nodes\n      include: filter_out_new_app_nodes.yaml\n      with_items: \"{{ registered_nodes }}\"\n      loop_control:\n        loop_var: node\n\n    - name: Add new app nodes to the new_nodes section (if a deployment already exists)\n      with_items: \"{{ new_app_nodes }}\"\n      add_host:\n        name: \"{{ item.name }}\"\n        groups: new_nodes, app\n\n    - name: Add the rest of cluster nodes to their corresponding groups\n      with_items: \"{{ registered_nodes }}\"\n      add_host:\n        name: '{{ item.name }}'\n        groups: '{{ item.metadata.group }}'\n\n    - name: Add bastion node to inventory\n      add_host:\n        name: bastion\n        groups: bastions\n        ansible_host: '{{ registered_bastion_nodes[0].public_v4 }}'\n        ansible_fqdn: '{{ registered_bastion_nodes[0].name }}'\n        ansible_user: '{{ ssh_user }}'\n        ansible_private_key_file: '{{ private_ssh_key }}'\n        ansible_ssh_extra_args: '-F {{ ssh_config_path }}'\n        private_v4: >-\n          {% set node = registered_nodes | json_query(\"[?name=='\" + registered_bastion_nodes[0].name + \"']\") -%}\n          {{ node[0].addresses[openstack_private_network|quote][0].addr }}\n        public_v4: '{{ registered_bastion_nodes[0].public_v4 }}'\n      when:\n        - registered_bastion_nodes is defined\n        - use_bastion|bool\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "5803cff94f79faa2c4ada02d1d2f1c7a22634e20", "filename": "roles/local/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- block:\n  - name: Include prompts\n    import_tasks: prompts.yml\n\n  rescue:\n    - debug: var=fail_hint\n      tags: always\n    - fail:\n      tags: always\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "7de16555d0177a4322801c8d247bc41a7247c03d", "filename": "roles/zookeeper/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for zookeeper\nzookeeper_client_port: 2181\nzookeeper_leader_connect_port: 2888\nzookeeper_leader_election_port: 3888\nzookeeper_server_group: zookeeper_servers\nzookeeper_id: \"\n\t{%- if zookeeper_host_list is defined -%}\n\t\t{%- for host in zookeeper_host_list.split() -%}\n\t\t\t{%- if host == ansible_eth0.ipv4.address -%}\n\t        \t{{ loop.index }}\n\t\t\t{%- endif -%}\n\t\t{%- endfor -%}\n\t{%- else -%}\n    \t{%- for host in groups[zookeeper_server_group] -%}\n      \t\t{%- if host == 'default' or host == inventory_hostname or host == ansible_fqdn or host in ansible_all_ipv4_addresses -%}\n        \t\t{{ loop.index }}\n  \t\t\t{%- endif -%}\n    \t{%- endfor -%}\n    {%- endif -%}\n\"\nconsul_dir: /etc/consul.d\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "7d3abb25197d2b9b6679052d6350e3e8b6efde2a", "filename": "playbooks/roles/docket/playbook.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- hosts: docket:stenographer\n  become: true\n  vars:\n    docket_webserver_type: 'lighttpd'\n  roles:\n    - role: ansible-ntp\n    - role: ansible-timezone\n    - role: ansible-lighttpd\n      when: docket_web_server == \"lighttpd\"\n    - role: ansible-nginx\n      when: docket_web_server == \"nginx\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "06cbca65aa3c3836b3ae6b2bf4de39f3ab49459c", "filename": "roles/user-management/manage-atlassian-users/tasks/create_atlassian_users.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Create User\n  uri:\n    url: '{{ atlassian.url }}/rest/api/2/user'\n    method: POST\n    user: '{{ atlassian.username }}'\n    password: '{{ atlassian.password }}'\n    force_basic_auth: yes\n    status_code: 201\n    body_format: json\n    body: \"{'name': '{{ atlassian_user.email.split(\\\"@\\\") | first }}', \n            'password': '{{ atlassian_user.password }}', \n            'emailAddress': '{{ atlassian_user.email }}', \n            'displayName': '{{ atlassian_user.firstname }} {{ atlassian_user.lastname }}' }\"\n    return_content: yes\n  register: user_data\n  when: \n    - atlassian_user.state|default('present') == 'present'"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "cc87984a790101bf764904982ad10f3a1ddad4b4", "filename": "roles/ovirt-engine-setup/defaults/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_engine_dwh: True\novirt_engine_type: 'ovirt-engine'\novirt_engine_version: '4.1'\novirt_engine_admin_password: '123456'\n\novirt_engine_db_host: 'localhost'\novirt_engine_db_port: 5432\novirt_engine_db_name: 'engine'\novirt_engine_db_user: 'engine'\novirt_engine_db_password: 'AqbXg4dpkbcVRZwPbY8WOR'\n\novirt_engine_dwh_db_host: 'localhost'\novirt_engine_dwh_db_port: 5432\novirt_engine_dwh_db_name: 'ovirt_engine_history'\novirt_engine_dwh_db_user: 'ovirt_engine_history'\novirt_engine_dwh_db_password: '37xmBKECANQGm0z3SfylMp'\n\novirt_engine_configure_iso_domain: false\novirt_engine_iso_domain_path: '/var/lib/exports/iso'\novirt_engine_iso_domain_name: 'ISO_DOMAIN'\novirt_engine_iso_domain_acl: '0.0.0.0/0.0.0.0(rw)'\n\novirt_engine_firewall_manager: 'firewalld'\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "b8a20d83d8d9527956273938920a3e758629c28b", "filename": "playbooks/cluster/openshift/config.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- hosts: all\n  remote_user: root\n  vars:\n    epel_release_rpm_url: \"https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\"\n    openshift_ansible_dir: \"openshift-ansible/\"\n    openshift_playbook_path: \"playbooks/byo/config.yml\"\n    openshift_version: \"3.7\"\n  pre_tasks:\n    - name: include_vars\n      include_vars:\n        file: \"{{ item }}\"\n      with_items:\n        - \"{{ playbook_dir }}/../../../vars/all.yml\"\n        - \"{{ playbook_dir }}/vars/{{ openshift_version }}.yml\"\n\n    - name: Include lib_utils\n      import_role:\n        name: lib_utils\n\n    - name: set SELinux to permissive mode\n      command: setenforce 0\n\n    - name: set SELinux to permissive mode under configuration file\n      selinux:\n        policy: targeted\n        state: permissive\n\n    - name: stop and disable firewalld\n      register: result\n      service:\n        state: stopped\n        enabled: no\n        name: firewalld\n      failed_when: \"result|failed and not 'Could not find the requested service' in result.msg\"\n\n    - name: install epel repository\n      package:\n        name: \"{{ epel_release_rpm_url }}\"\n        state: present\n      when:\n        - ansible_distribution in [\"CentOS\",\"RedHat\"]\n\n    - name: Install openshift_facts requirements\n      package:\n        name: \"{{ item }}\"\n      with_items:\n        - python-yaml\n        - python-ipaddress\n        - wget\n        - git\n        - net-tools\n        - bind-utils\n        - iptables-services\n        - bridge-utils\n        - bash-completion\n        - kexec-tools\n        - sos\n        - psacct\n        - docker\n        - NetworkManager\n\n    - name: Include openshift_facts module\n      import_role:\n        name: \"{{ openshift_ansible_dir }}/roles/openshift_facts\"\n\n    - name: Load openshift facts\n      openshift_facts:\n        role: common\n\n    - name: Set facts for docker-storage-setup\n      block:\n        - name: Detect Operating System from ostree_booted\n          stat:\n            path: /run/ostree-booted\n          register: ostree_booted\n        - name: initialize_facts set fact openshift_is_atomic\n          set_fact:\n            openshift: \"{{ openshift | combine({'common':{'is_atomic': ostree_booted.stat.exists}}, recursive=True) }}\"\n      when: \"'is_atomic' not in openshift.common\"\n\n  roles:\n    - \"openshift-contrib/roles/docker-storage-setup\"\n\n  post_tasks:\n    - name: Enable and start docker service\n      service:\n        state: started\n        enabled: yes\n        name: docker\n\n    - name: Enable and start NetworkManager service\n      service:\n        state: started\n        enabled: yes\n        name: NetworkManager\n\n- import_playbook: \"{{ openshift_ansible_dir }}/playbooks/prerequisites.yml\"\n- import_playbook: \"{{ openshift_ansible_dir | join_paths(openshift_playbook_path | default('playbooks/byo/config.yml')) }}\"\n\n- hosts: masters\n  tasks:\n    - name: Configure oc admin user for testing\n      shell: |\n        user_name=\"{{ admin_test_user_name | default('test_admin') }}\"\n        kubeconfig=\"/etc/origin/master/admin.kubeconfig\"\n        oc login -u system:admin --config=$kubeconfig\n        oc get user \"$user_name\" --config=$kubeconfig || oc create user \"$user_name\" --config=$kubeconfig\n        oc adm policy add-cluster-role-to-user cluster-admin \"$user_name\" --config=$kubeconfig\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "5c5bfcc97d72a92b10b4776033ab1de8c03492c2", "filename": "roles/dns/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: \"Remove any deleted DNS A records\"\n  nsupdate:\n    key_name: \"{{ item.0.key_name }}\"\n    key_secret: \"{{ item.0.key_secret }}\"\n    key_algorithm: \"{{ item.0.key_algorithm }}\"\n    server: \"{{ item.0.server }}\"\n    zone: \"{{ item.0.zone }}\"\n    record: \"{{ item.1.hostname }}\"\n    type: \"{{ item.1.type }}\"\n    state: absent\n  with_subelements:\n  - \"{{ dns_records_rm | default({}) }}\"\n  - entries\n  register: nsupdate_remove_result\n  until: nsupdate_remove_result|succeeded\n  retries: 10\n  delay: 1\n\n- name: \"Add DNS A records\"\n  nsupdate:\n    key_name: \"{{ item.0.key_name }}\"\n    key_secret: \"{{ item.0.key_secret }}\"\n    key_algorithm: \"{{ item.0.key_algorithm }}\"\n    server: \"{{ item.0.server }}\"\n    zone: \"{{ item.0.zone }}\"\n    record: \"{{ item.1.hostname }}\"\n    value: \"{{ item.1.ip }}\"\n    type: \"{{ item.1.type }}\"\n    state: present\n  with_subelements:\n  - \"{{ dns_records_add | default({}) }}\"\n  - entries\n  register: nsupdate_add_result\n  until: nsupdate_add_result|succeeded\n  retries: 10\n  delay: 1\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "c46bfe997b15292949c3286145ee64973310ed2e", "filename": "roles/1-prep/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# Preparations (Hardware Level)\n\n- name: ...IS BEGINNING ============================================\n  command: echo\n\n- name: Install uuidgen program (debuntu)\n  package:\n    name: uuid-runtime\n    state: present\n  when: is_debuntu\n\n- name: Test for /etc/iiab/uuid file\n  stat:\n    path: /etc/iiab/uuid\n  register: uuid_file\n\n- name: Create folder to hold uuid\n  file:\n    path: /etc/iiab\n    state: directory\n  when: not uuid_file.stat.exists\n\n- name: If no uuid exists, create one\n  shell: uuidgen\n  register: uuid_response\n  when: not uuid_file.stat.exists\n\n- name: Put the uuid in place\n  shell: echo {{ uuid_response.stdout_lines[0] }} > /etc/iiab/uuid\n  when: not uuid_file.stat.exists\n\n- name: Get the uuid\n  command: cat /etc/iiab/uuid\n  register: stored_uuid\n\n- name: Get the value into a variable\n  set_fact:\n    uuid: \"{{ stored_uuid.stdout_lines[0] }}\"\n\n# for rpi, without rtc, we need time as soon as possible\n- name: Install chrony package\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - chrony\n  tags:\n    - download\n\n#TODO: Use regexp filter instead of hard-code ip\n- name: Update chrony config file\n  template:\n    backup: no\n    dest: /etc/chrony.conf\n    src: chrony.conf.j2\n\n- name: Disable AppArmor -- override OS default (ubuntu)\n  service:\n    name: apparmor\n    enabled: False\n    state: stopped\n  when: is_ubuntu\n  ignore_errors: true\n\n- name: Disable SELinux on next boot (OS's other than debuntu)\n  selinux:\n    state: disabled\n  register: selinux_disabled\n  when: not is_debuntu\n\n- name: Disable SELinux for this session (OS's other than debuntu, if needed)\n  command: setenforce Permissive\n  when: not is_debuntu and selinux_disabled is defined and selinux_disabled.changed\n\n##  DISCOVER PLATFORMS ######\n# Put conditional actions for hardware platforms here\n- include_tasks: raspberry_pi.yml\n  when: first_run and rpi_model != \"none\"\n\n- name: Check if the identifier for Intel's NUC6 builtin WiFi is present\n  shell: \"lsusb | grep 8087:0a2b | wc | awk '{print $1}'\"\n  register: usb_NUC6\n  ignore_errors: true\n\n- name: Download the firmware for built-in WiFi on NUC6\n  get_url:\n    url: \"{{ iiab_download_url }}/iwlwifi-8000C-13.ucode\"\n    dest: /lib/firmware\n    timeout: \"{{ download_timeout }}\"\n  when: internet_available and usb_NUC6.stdout|int > 0\n\n# this script can be sourced to get IIAB location\n- name: Recording STAGE 1 HAS COMPLETED ============================\n  template:\n    src: roles/1-prep/templates/iiab.env.j2\n    dest: /etc/iiab/iiab.env\n    owner: root\n    group: root\n    mode: 0644\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "bb58659e1a5ed140ccdcafabcb7ea5270087e18d", "filename": "tasks/delete_blobstore_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include_tasks: call_script.yml\n  vars:\n    script_name: delete_blobstore\n    args: \"{{ item }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ad0511cb26773bd2cd2d314acdb415fe7ece7a0b", "filename": "playbooks/provisioning/openstack/sample-inventory/group_vars/all.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\nenv_id: \"openshift\"\npublic_dns_domain: \"example.com\"\npublic_dns_nameservers: []\n\n# Used Hostnames\n# - set custom hostnames for roles by uncommenting corresponding lines\n#openstack_master_hostname: \"master\"\n#openstack_infra_hostname: \"infra-node\"\n#openstack_node_hostname: \"app-node\"\n#openstack_lb_hostname: \"lb\"\n#openstack_etcd_hostname: \"etcd\"\n#openstack_dns_hostname: \"dns\"\n\n# The Nova keypair name to use when provisioning VMs in OpenStack.\nopenstack_ssh_public_key: \"openshift\"\nopenstack_external_network_name: \"public\"\n#openstack_private_network_name:  \"openshift-ansible-{{ stack_name }}-net\"\n# # A dedicated Neutron network name for containers data network\n# # Configures the data network to be separated from openstack_private_network_name\n# # NOTE: this is only supported with Flannel SDN yet\n#openstack_private_data_network_name: \"openshift-ansible-{{ stack_name }}-data-net\"\n\n# If you want to use a provider network, set the name here.\n# NOTE: the `openstack_external_network_name` and\n# `openstack_private_network_name` options will be ignored when using a\n# provider network.\n#openstack_provider_network_name: \"provider\"\n\n# Used Images\n# - set specific images for roles by uncommenting corresponding lines\n# - note: do not remove openstack_default_image_name definition\n#openstack_master_image_name: \"centos7\"\n#openstack_infra_image_name: \"centos7\"\n#openstack_node_image_name: \"centos7\"\n#openstack_lb_image_name: \"centos7\"\n#openstack_etcd_image_name: \"centos7\"\n#openstack_dns_image_name: \"centos7\"\nopenstack_default_image_name: \"centos7\"\n\nopenstack_num_masters: 1\nopenstack_num_infra: 1\nopenstack_num_nodes: 2\n\n# Used Flavors\n# - set specific flavors for roles by uncommenting corresponding lines\n# - note: do note remove openstack_default_flavor definition\n#openstack_master_flavor: \"m1.medium\"\n#openstack_infra_flavor: \"m1.medium\"\n#openstack_node_flavor: \"m1.medium\"\n#openstack_lb_flavor: \"m1.medium\"\n#openstack_etcd_flavor: \"m1.medium\"\n#openstack_dns_flavor: \"m1.medium\"\nopenstack_default_flavor: \"m1.medium\"\n\n# Numerical index of nodes to remove\n#openstack_nodes_to_remove: []\n\n# Docker volume size\n# - set specific volume size for roles by uncommenting corresponding lines\n# - note: do not remove docker_default_volume_size definition\n#docker_master_volume_size: \"15\"\n#docker_infra_volume_size: \"15\"\n#docker_node_volume_size: \"15\"\n#docker_etcd_volume_size: \"2\"\n#docker_dns_volume_size: \"1\"\n#docker_lb_volume_size: \"5\"\ndocker_volume_size: \"15\"\n\n# Specify server group policies for master and infra nodes. Nova must be configured to\n# enable these policies. 'anti-affinity' will ensure that each VM is launched on a\n# different physical host.\n#openstack_master_server_group_policies: [anti-affinity]\n#openstack_infra_server_group_policies: [anti-affinity]\n\n# Create a Cinder volume and use it for the OpenShift registry.\n# NOTE: the openstack credentials and hosted registry options must be set in OSEv3.yml!\n#cinder_hosted_registry_name: cinder-registry\n#cinder_hosted_registry_size_gb: 10\n\n# Set up a filesystem on the cinder volume specified in `OSEv3.yaml`.\n# You need to specify the file system and volume ID in OSEv3 via\n# `openshift_hosted_registry_storage_openstack_filesystem` and\n# `openshift_hosted_registry_storage_openstack_volumeID`.\n# WARNING: This will delete any data on the volume!\n#prepare_and_format_registry_volume: False\n\nopenstack_subnet_prefix: \"192.168.99\"\n\n# Red Hat subscription defaults to false which means we will not attempt to\n# subscribe the nodes.\n#rhsm_register: False\n\n# Using Red Hat Satellite:\n#rhsm_register: True\n#rhsm_satellite: 'sat-6.example.com'\n#rhsm_org: 'OPENSHIFT_ORG'\n#rhsm_activationkey: '<activation-key>'\n\n# Or using RHN username, password and optionally pool:\n#rhsm_register: True\n#rhsm_username: '<username>'\n#rhsm_password: '<password>'\n#rhsm_pool: '<pool id>'\n\n# Red Hat Subscription Management repositories.\n#rhsm_repos:\n# - \"rhel-7-server-rpms\"\n# - \"rhel-7-server-ose-3.5-rpms\"\n# - \"rhel-7-server-extras-rpms\"\n# - \"rhel-7-fast-datapath-rpms\"\n\n# Roll-your-own DNS\n#openstack_num_dns: 0\n#external_nsupdate_keys:\n#  public:\n#    key_secret: 'SKqKNdpfk7llKxZ57bbxUnUDobaaJp9t8CjXLJPl+fRI5mPcSBuxTAyvJPa6Y9R7vUg9DwCy/6WTpgLNqnV4Hg=='\n#    key_algorithm: 'hmac-md5'\n#    server: '192.168.1.1'\n#  private:\n#    key_secret: 'kVE2bVTgZjrdJipxPhID8BEZmbHD8cExlVPR+zbFpW6la8kL5wpXiwOh8q5AAosXQI5t95UXwq3Inx8QT58duw=='\n#    key_algorithm: 'hmac-md5'\n#    server: '192.168.1.2'\n\n# Customize DNS server security options\n#named_public_recursion: 'no'\n#named_private_recursion: 'yes'\n\n# NOTE(shadower): Do not change this value. The Ansible user is currently\n# hardcoded to `openshift`.\nansible_user: openshift\n\n# Use a single security group for a cluster (default: false)\n#openstack_flat_secgrp: false\n\n# OpenStack inventory type and cluster nodes access pattern\n# Defaults to 'static'.\n# Use 'dynamic' to access cluster nodes directly, via floating IPs\n# and given a dynamic inventory script, like openstack.py\n#openstack_inventory: static\n# The path to checkpoint the static inventory from the in-memory one\n#openstack_inventory_path: ../../../../inventory\n\n# Use bastion node to access cluster nodes (Defaults to False).\n# Requires a static inventory.\n#openstack_use_bastion: False\n#bastion_ingress_cidr: \"{{openstack_subnet_prefix}}.0/24\"\n\n# The Nova keypair's private SSH key to access inventory nodes.\n#openstack_private_ssh_key: ~/.ssh/id_rsa\n# The path for the SSH config to access all nodes\n#openstack_ssh_config_path: /tmp/ssh.config.openshift.ansible.{{ env_id }}.{{ public_dns_domain }}\n\n# If you want to use the VM storage instead of Cinder volumes, set this to `true`.\n# NOTE: this is for testing only! Your data will be gone once the VM disappears!\n#ephemeral_volumes: false\n\n# OpenShift node labels\n# - in order to customise node labels for app and/or infra group, set the\n#   openshift_cluster_node_labels variable\n#openshift_cluster_node_labels:\n#  app:\n#    region: primary\n#  infra:\n#    region: infra\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "997be1e5821186115e1b58bfe21dc502e91ac7ff", "filename": "roles/config-pxe/tasks/pxe-target.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Generate the ip line - if applicable\"\n  set_fact:\n    ip_config: \"{{ target_entry.ip }}::{{ target_entry.gateway }}:{{ target_entry.netmask }}::{{ target_entry.interface }}:none\"\n  when: \n  - target_entry.ip is defined\n  - target_entry.ip|trim != ''\n\n- name: \"(re)set the pxe_entries\"\n  set_fact:\n    pxe_entries: \"{{ target_entry.pxe_entries }}\"\n\n- name: \"Generate the mac-address specific grub.cfg\"\n  set_fact:\n    target_file: \"{{ tftpserver_root_dir }}/pxelinux/grub.cfg-01-{{ target_entry.mac|regex_replace(':', '-')|lower }}\"\n\n- name: \"Populate the host specific grub.cfg (UEFI) file\"\n  template:\n    src: pxelinux_uefi.j2\n    dest: \"{{ target_file }}\"\n\n- name: \"Workaround for a RHEL 7.4 uEFI bug where it expects a '-' at the end of the filename\"\n  file:\n    src: \"{{ target_file|basename }}\"\n    dest: \"{{ target_file }}-\"\n    state: link \n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "0348d9dc8ed13195ac7c462b7e09517253645122", "filename": "tasks/section_05_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 5.1.1 Ensure NIS is not installed (Scored)\n    apt: name=nis state=absent purge=yes\n    tags:\n    - section5\n    - section5.1\n    - section5.1.1\n\n  - name: 5.1.2 Ensure rsh, rlogin, rexec, talk, telnet, chargen, daytime, echo, discard, time is not enabled (stat inetd) (Scored)\n    stat: path=/etc/inetd.conf\n    register: inetd_path\n    tags:\n    - section5\n    - section5.1\n    - section5.1.2\n\n  - name: 5.1.2-.6 Ensure rsh, rlogin, rexec, talk, telnet, chargen, daytime, echo, discard, time is not enabled (Scored)\n    lineinfile: >\n        dest=/etc/inetd.conf\n        regexp='^({{ item }}.*)'\n        line='#\\1'\n        state=present\n        backrefs=yes\n        backup=yes\n    with_items:\n        - shell\n        - login\n        - exec\n        - talk\n        - ntalk\n        - telnet\n        - chargen\n        - daytime\n        - echo\n        - discard\n        - time\n    when: inetd_path.stat.exists == True\n    tags:\n    - section5\n    - section5.1\n    - section5.1.2\n\n  - name: 5.1.3.1 Ensure rsh client is not installed (Scored)\n    apt: name=rsh-client state=absent purge=yes\n    tags:\n    - section5\n    - section5.1\n    - section5.1.3\n    - section5.1.3.1\n\n  - name: 5.1.3.2 Ensure rsh client is not installed (Scored)\n    apt: name=rsh-redone-client state=absent purge=yes\n    tags:\n    - section5\n    - section5.1\n    - section5.1.3\n    - section5.1.3.2\n\n  - name: 5.1.5 Ensure talk client is not installed (Scored)\n    apt: name=talk state=absent purge=yes\n    tags:\n    - section5\n    - section5.1\n    - section5.1.5\n\n  - name: 5.1.8 Ensure xinetd is not enabled (stat xinetd) (Scored)\n    stat: path=/etc/init/xinetd.conf\n    register: xinetd_path\n    tags:\n    - section5\n    - section5.1\n    - section5.1.8\n\n  - name: 5.1.8 Ensure xinetd is not enabled (Scored)\n    lineinfile: >\n        dest=/etc/init/xinetd.conf\n        regexp='start on runlevel'\n        state=present\n        line='#start on runlevel [2345]'\n    when: xinetd_path.stat.exists == True\n    tags:\n    - section5\n    - section5.1\n    - section5.1.8\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "95cc1a3dcf22e421205d100d3a17c114e38c0c40", "filename": "roles/5-xo-services/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# XO Services\n\n- name: ...IS BEGINNING =====================================\n  command: echo\n\n- name: ACTIVITY-SERVER\n  include_role:\n    name: activity-server\n  when: activity_server_install\n  tags: olpc, activity-server\n\n- name: EJABBERD_XS\n  include_role:\n    name: ejabberd_xs\n  when: ejabberd_xs_install\n  tags: olpc, ejabberd-xs\n\n- name: IDMGR\n  include_role:\n    name: idmgr\n  when: idmgr_install\n  tags: olpc, idmgr\n\n- name: Recording STAGE 5 HAS COMPLETED =====================\n  lineinfile:\n    dest: /etc/iiab/iiab.env\n    regexp: '^STAGE=*'\n    line: 'STAGE=5'\n    state: present\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a3d169f23fcb6326156d922ee50836f94839582c", "filename": "roles/idm-host-cert/tasks/retrieve-ca-cert.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Retrieve the CA cert\"\n  uri: \n    url: \"https://{{ idm_fqdn }}/ipa/session/json\"\n    method: POST\n    body: '{\"method\": \"cert_show\", \"params\":[[\"{{ retrieve_cert_serialnumber|default(1) }}\"],{ \"version\": \"{{ api_version }}\" }],\"id\":0}'\n    body_format: json\n    validate_certs: no\n    return_content: yes\n    headers:\n      Cookie: \"{{ idm_session.set_cookie }}\"\n      referer: \"https://{{ idm_fqdn }}/ipa\"\n      Content-Type: \"application/json\"\n      Accept: \"application/json\"\n  register: ca_cert\n\n- name: \"Error out if the request returned an error\"\n  fail:\n    msg: \"ERROR: request failed with message: {{ ca_cert.json.error.message }}\"\n  when:\n  - ca_cert.json.error is defined\n  - ca_cert.json.error.message is defined\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "90a86ee3ad1381f1a0fb4e41e0ddc675d08fe82c", "filename": "roles/dns_adblocking/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: Gather Facts\n  setup:\n\n- name: Dnsmasq installed\n  package: name=dnsmasq\n\n- name: Ensure that the dnsmasq user exist\n  user: name=dnsmasq groups=nogroup append=yes state=present\n\n- name: The dnsmasq directory created\n  file: dest=/var/lib/dnsmasq state=directory mode=0755 owner=dnsmasq group=nogroup\n\n- include: ubuntu.yml\n  when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'\n\n- include: freebsd.yml\n  when: ansible_distribution == 'FreeBSD'\n\n- meta: flush_handlers\n\n- name: Dnsmasq configured\n  template: src=dnsmasq.conf.j2 dest=\"{{ config_prefix|default('/') }}etc/dnsmasq.conf\"\n  notify:\n    - restart dnsmasq\n\n- name: Adblock script created\n  template: src=adblock.sh dest=/usr/local/sbin/adblock.sh owner=root group=\"{{ root_group|default('root') }}\" mode=0755\n\n- name: Adblock script added to cron\n  cron:\n    name: Adblock hosts update\n    minute: 10\n    hour: 2\n    job: /usr/local/sbin/adblock.sh\n    user: dnsmasq\n\n- name: Update adblock hosts\n  shell: >\n    sudo -u dnsmasq \"/usr/local/sbin/adblock.sh\"\n\n- name: Dnsmasq enabled and started\n  service: name=dnsmasq state=started enabled=yes\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "d7fb9ca0729c3ed084fec7ea7c061144305a19ef", "filename": "roles/debian_schooltool/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: get the required build packages \n  package: name={{ item }}\n           state=present\n  with_items:\n    - build-essential \n    - gettext\n    - python-dev\n    - libicu-dev \n    - libxslt1-dev \n    - libfreetype6-dev \n    - libjpeg-dev \n    - enscript\n    - python-virtualenv \n    - ttf-liberation\n    - redis-server\n    - libjpeg-dev\n    - xvfb\n  when: debian_schooltool_install and is_debuntu\n\n- name: Create the font directory\n  file: path=/usr/share/fonts/truetype/ttf-ubuntu\n        state=directory\n  \n- name: get the ttf-ubuntu-font-family\n  get_url: url={{ iiab_download_url }}/ubuntu-font-family-0.83.zip\n           dest={{ downloads_dir }}\n\n- name: expand the ttf fonts to the right place\n  unarchive: src={{ downloads_dir }}/ubuntu-font-family-0.83.zip\n             dest=/usr/share/fonts/truetype/ttf-ubuntu/\n\n- name: get the schooltool source\n  get_url: url={{ iiab_download_url }}/{{ schooltool_src }}\n           dest={{ downloads_dir }}\n\n- name: expand source to dest\n  unarchive: src={{ downloads_dir }}/{{ schooltool_src }}\n             dest={{ iiab_base }}\n\n- name: create a link for schooltool\n  file: src={{ iiab_base }}/{{ schooltool_version }}\n        dest={{ iiab_base }}/schooltool\n        state=link\n\n- name: build the schooltool from source\n  shell: command='$( cd {{ iiab_base }}/schooltool; /usr/bin/make ) '\n\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "f6415cffff925b9f267265eb455fc21477b80faa", "filename": "tasks/main.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n\n- name: Check requirements and deprecated features/vars\n  import_tasks: \"requirements_and_deprecated.yml\"\n\n- name: Include OS specific variables.\n  include_vars: \"configure-{{ ansible_os_family }}.yml\"\n\n- name: Include OS specific selinux libs and utils if needed\n  include_tasks: \"selinux-{{ ansible_os_family }}.yml\"\n  when: ansible_selinux.status is defined and ansible_selinux.status == \"enabled\"\n\n- name: Check if SystemD service is installed\n  stat:\n    path: /etc/systemd/system/nexus.service\n  register: nexus_systemd_service_file\n\n- name: Check if sysv service is installed\n  stat:\n    path: /etc/init.d/nexus\n  register: nexus_sysv_service_file\n\n- include_tasks: nexus_purge.yml\n  when: nexus_purge | default(false) | bool\n\n- import_tasks: nexus_install.yml\n\n- include_tasks: httpd_reverse_proxy_config.yml\n  when: httpd_setup_enable | bool\n\n- name: Configure nexus http proxy\n  include_tasks: call_script.yml\n  vars:\n    script_name: setup_http_proxy\n    args:\n      with_http_proxy: \"{{ nexus_with_http_proxy }}\"\n      http_proxy_host: \"{{ nexus_http_proxy_host }}\"\n      http_proxy_port: \"{{ nexus_http_proxy_port }}\"\n      http_proxy_username: \"{{ nexus_http_proxy_username }}\"\n      http_proxy_password: \"{{ nexus_http_proxy_password }}\"\n      with_https_proxy: \"{{ nexus_with_https_proxy }}\"\n      https_proxy_host: \"{{ nexus_https_proxy_host }}\"\n      https_proxy_port: \"{{ nexus_https_proxy_port }}\"\n      https_proxy_username: \"{{ nexus_https_proxy_username }}\"\n      https_proxy_password: \"{{ nexus_https_proxy_password }}\"\n      proxy_exclude_hosts: \"{{ nexus_proxy_exclude_hosts }}\"\n  when: nexus_with_http_proxy or nexus_with_https_proxy\n\n- name: Deleting default repositories\n  include_tasks: delete_repo_each.yml\n  with_items:\n    - maven-central\n    - maven-public\n    - maven-releases\n    - maven-snapshots\n    - nuget-group\n    - nuget-hosted\n    - nuget.org-proxy\n  when: (nexus_data_dir_contents.stdout | length == 0) and nexus_delete_default_repos\n\n- name: Deleting default blobstore\n  include_tasks: delete_blobstore_each.yml\n  with_items:\n    - name: default\n    - name: \"{{ nexus_blob_names.raw.blob }}\"\n    - name: \"{{ nexus_blob_names.pypi.blob }}\"\n    - name: \"{{ nexus_blob_names.docker.blob }}\"\n    - name: \"{{ nexus_blob_names.ruby.blob }}\"\n    - name: \"{{ nexus_blob_names.bower.blob }}\"\n    - name: \"{{ nexus_blob_names.npm.blob }}\"\n    - name: \"{{ nexus_blob_names.nuget.blob }}\"\n    - name: \"{{ nexus_blob_names.mvn.blob }}\"\n    - name: \"{{ nexus_blob_names.gitlfs.blob }}\"\n    - name: \"{{ nexus_blob_names.yum.blob }}\"\n    - name: \"{{ nexus_blob_names.apt.blob }}\"\n  when: (nexus_data_dir_contents.stdout | length == 0) and nexus_delete_default_blobstore\n\n- block:\n    - include_tasks: setup_ldap_each.yml\n      with_items: \"{{ ldap_connections }}\"\n\n    - include_tasks: create_content_selector_each.yml\n      with_items: \"{{ nexus_content_selectors }}\"\n\n    - name: apply defaults to privileges\n      # @todo: fix with easier syntax once the flip filter is released\n      # See more comments on this issue in process_repos_list.yml\n      set_fact:\n        nexus_privileges: >-\n          {%- set result=[] -%}\n          {%- for privilege in nexus_privileges -%}\n            {{ result.append(_nexus_privilege_defaults | combine(privilege)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: \"Digest splited blob list var\"\n      include_vars: blob_vars.yml\n      when: nexus_blob_split | bool\n\n    - name: Create/Check blobstores\n      when: nexus_restore_point is undefined\n      block:\n\n        - name: Create directories for blob stores.\n          file:\n            path: \"{{ item['path'] }}\"\n            owner: \"{{ nexus_os_user }}\"\n            group: \"{{ nexus_os_group }}\"\n            state: directory\n            recurse: true\n          when: item.path is defined\n          loop: \"{{ nexus_blobstores }}\"\n\n        - name: Create/Check blobstores\n          include_tasks: call_script.yml\n          vars:\n            script_name: create_blobstores_from_list\n            args: \"{{ nexus_blobstores }}\"\n          when: nexus_blobstores | length > 0\n\n\n  when: nexus_run_provisionning | default(true) | bool\n\n- name: \"Restore nexus backup\"\n  include_tasks: nexus-restore.yml\n  when: nexus_restore_point is defined\n\n- block:\n    - name: Create/check cleanup policies\n      include_tasks: call_script.yml\n      vars:\n        script_name: create_cleanup_policies_from_list\n        args: \"{{ nexus_repos_cleanup_policies }}\"\n      when: nexus_repos_cleanup_policies | length > 0\n\n    - name: Apply defaults to repositories configurations and process a single list\n      include_tasks: process_repos_list.yml\n\n    - name: Create configured repositories\n      include_tasks: call_script.yml\n      vars:\n        script_name: create_repos_from_list\n        args: \"{{ _nexus_repos_global_list }}\"\n\n    - name: Create/check privileges\n      include_tasks: call_script.yml\n      vars:\n        script_name: setup_privileges_from_list\n        args: \"{{ nexus_privileges }}\"\n      when: nexus_privileges | length > 0\n\n    - name: Create/check roles\n      include_tasks: call_script.yml\n      vars:\n        script_name: setup_roles_from_list\n        args: \"{{ nexus_roles }}\"\n      when: nexus_roles | length > 0\n\n    - name: Create/check local users\n      include_tasks: call_script.yml\n      vars:\n        script_name: setup_users_from_list\n        args: \"{{ nexus_local_users }}\"\n      when: nexus_local_users | length > 0\n\n    - name: Create/check ldap users\n      include_tasks: call_script.yml\n      vars:\n        script_name: setup_ldap_users_from_list\n        args: \"{{ nexus_ldap_users }}\"\n      when: nexus_ldap_users | length > 0\n\n  when: nexus_run_provisionning | default(true) | bool\n\n- include_tasks: call_script.yml\n  vars:\n    script_name: setup_anonymous_access\n    args:\n      anonymous_access: \"{{ nexus_anonymous_access }}\"\n\n- include_tasks: call_script.yml\n  vars:\n    script_name: setup_base_url\n    args:\n      base_url: \"{{ nexus_public_scheme }}://{{ nexus_public_hostname }}/\"\n\n- include_tasks: call_script.yml\n  vars:\n    script_name: setup_realms\n    args:\n      nuget_api_key_realm: \"{{ nexus_nuget_api_key_realm }}\"\n      npm_bearer_token_realm: \"{{ nexus_npm_bearer_token_realm }}\"\n      rut_auth_realm: \"{{ nexus_rut_auth_realm }}\"\n      ldap_realm: \"{{ nexus_ldap_realm }}\"\n      docker_bearer_token_realm: \"{{ nexus_docker_bearer_token_realm }}\"\n\n- name: Configure RUT Auth header\n  include_tasks: call_script.yml\n  vars:\n    script_name: setup_capability\n    args:\n      capability_typeId: \"rutauth\"\n      capability_enabled: true\n      capability_properties:\n        httpHeader: \"{{ nexus_rut_auth_header }}\"\n  when: nexus_rut_auth_header is defined\n\n- include_tasks: call_script.yml\n  vars:\n    script_name: setup_email\n    args:\n      email_server_enabled: \"{{ nexus_email_server_enabled }}\"\n      email_server_host: \"{{ nexus_email_server_host }}\"\n      email_server_port: \"{{ nexus_email_server_port }}\"\n      email_server_username: \"{{ nexus_email_server_username }}\"\n      email_server_password: \"{{ nexus_email_server_password }}\"\n      email_from_address: \"{{ nexus_email_from_address }}\"\n      email_subject_prefix: \"{{ nexus_email_subject_prefix }}\"\n      email_tls_enabled: \"{{ nexus_email_tls_enabled }}\"\n      email_tls_required: \"{{ nexus_email_tls_required }}\"\n      email_ssl_on_connect_enabled: \"{{ nexus_email_ssl_on_connect_enabled }}\"\n      email_ssl_check_server_identity_enabled: \"{{ nexus_email_ssl_check_server_identity_enabled }}\"\n      email_trust_store_enabled: \"{{ nexus_email_trust_store_enabled }}\"\n\n- name: Configure branding capability\n  include_tasks: call_script.yml\n  vars:\n    script_name: setup_capability\n    args:\n      capability_typeId: \"rapture.branding\"\n      capability_enabled: \"{{ (nexus_branding_footer | length > 0) and (nexus_branding_header | length > 0) }}\"\n      capability_properties:\n        footerHtml: \"{{ nexus_branding_footer }}\"\n        headerHtml: \"{{ nexus_branding_header }}\"\n        footerEnabled: \"{{ nexus_branding_footer | length > 0 }}\"\n        headerEnabled: \"{{ nexus_branding_header | length > 0 }}\"\n\n- name: Configure audit capability\n  include_tasks: call_script.yml\n  vars:\n    script_name: setup_capability\n    args:\n      capability_typeId: \"audit\"\n      capability_enabled: \"{{ nexus_audit_enabled | bool }}\"\n      capability_properties: {}\n\n- include_tasks: create_task_each.yml\n  with_items: \"{{ nexus_scheduled_tasks }}\"\n  when: nexus_run_provisionning | default(true) | bool\n\n- name: Configure nexus backup task\n  include_tasks: call_script.yml\n  vars:\n    script_name: create_task\n    args:\n      name: db and blobstores backup\n      typeId: script\n      cron: \"{{ nexus_backup_cron }}\"\n      taskProperties:\n        language: groovy\n        source: \"{{ lookup('template', './templates/backup.groovy.j2') }}\"\n  when: nexus_backup_configure | bool\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "d930a02dfc239b0272d3caf19dadcf5e816aa1e0", "filename": "roles/registrator/meta/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\ngalaxy_info:\n  author: Alberto Garcia\n  description:\n  company: Capgemini\n  # Some suggested licenses:\n  # - BSD (default)\n  # - MIT\n  # - GPLv2\n  # - GPLv3\n  # - Apache\n  # - CC-BY\n  license: license (MIT)\n  min_ansible_version: 1.2\n  #\n  # Below are all platforms currently available. Just uncomment\n  # the ones that apply to your role. If you don't see your\n  # platform on this list, let us know and we'll get it added!\n  #\n  platforms:\n  #- name: EL\n  #  versions:\n  #  - all\n  #  - 5\n  #  - 6\n  #  - 7\n  #- name: GenericUNIX\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Fedora\n  #  versions:\n  #  - all\n  #  - 16\n  #  - 17\n  #  - 18\n  #  - 19\n  #  - 20\n  #- name: SmartOS\n  #  versions:\n  #  - all\n  #  - any\n  #- name: opensuse\n  #  versions:\n  #  - all\n  #  - 12.1\n  #  - 12.2\n  #  - 12.3\n  #  - 13.1\n  #  - 13.2\n  #- name: Amazon\n  #  versions:\n  #  - all\n  #  - 2013.03\n  #  - 2013.09\n  #- name: GenericBSD\n  #  versions:\n  #  - all\n  #  - any\n  #- name: FreeBSD\n  #  versions:\n  #  - all\n  #  - 8.0\n  #  - 8.1\n  #  - 8.2\n  #  - 8.3\n  #  - 8.4\n  #  - 9.0\n  #  - 9.1\n  #  - 9.1\n  #  - 9.2\n  - name: Ubuntu\n    versions:\n  #  - all\n  #  - lucid\n  #  - maverick\n  #  - natty\n  #  - oneiric\n  #  - precise\n  #  - quantal\n  #  - raring\n  #  - saucy\n     - trusty\n  #- name: SLES\n  #  versions:\n  #  - all\n  #  - 10SP3\n  #  - 10SP4\n  #  - 11\n  #  - 11SP1\n  #  - 11SP2\n  #  - 11SP3\n  #- name: GenericLinux\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Debian\n  #  versions:\n  #  - all\n  #  - etch\n  #  - lenny\n  #  - squeeze\n  #  - wheezy\n  #\n  # Below are all categories currently available. Just as with\n  # the platforms above, uncomment those that apply to your role.\n  #\n  categories:\n  - cloud\n  #- cloud:ec2\n  #- cloud:gce\n  #- cloud:rax\n  #- clustering\n  #- database\n  #- database:nosql\n  #- database:sql\n  #- development\n  #- monitoring\n  #- networking\n  #- packaging\n  - system\n  #- web\ndependencies:\n  - role: docker\n  - role: consul\n  # List your role dependencies here, one per line. Only\n  # dependencies available via galaxy should be listed here.\n  # Be sure to remove the '[]' above if you add dependencies\n  # to this list.\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "c9c0af7e25d8980c9fe08ae2e96d912023d57472", "filename": "roles/ovirt-engine-install-packages/defaults/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# default vars for ovirt-ovirt-engine-install-packages-packages role\novirt_engine_dwh: True\novirt_engine_version: 4.1\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "49aa435ea4e9b19e973d2c9708d47f49e4ecb372", "filename": "playbooks/roles/sensor-common/tasks/configure-pipelining.yml", "repository": "rocknsm/rock", "decoded_content": "# Remove requiretty to make ssh pipelining work\n\n- name: Remove require tty\n  lineinfile:\n    regexp: '^\\w+\\s+requiretty'\n    dest: /etc/sudoers\n    state: absent\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "59245c6c84c1ec6de481d3e9c3598df35b3dc6b8", "filename": "reference-architecture/gcp/ansible/playbooks/group_vars/all", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# Variables shared by multiple playbooks and roles, which are not interesting\n# to user\n\nopenshift_major_version: 6\nopenshift_ansible_branch: release-3.{{ openshift_major_version }}\nopenshift_required_repos:\n- rhel-7-server-rpms\n- rhel-7-server-extras-rpms\n- rhel-7-server-ose-3.{{ openshift_major_version }}-rpms\n- rhel-7-fast-datapath-rpms\n\ngcloud_region: '{{ gcloud_zone[:-2] }}'\ngcs_registry_bucket: '{{ gcloud_project }}-{{ prefix }}-registry-bucket'\ndeployment_type: '{{ openshift_deployment_type }}'\n\n# This is either a symlink to /usr/share/ansible/openshift-ansible or\n# git repo of openshift ansible installer from github\nopenshift_ansible_installer_dir: '{{ playbook_dir }}/../../../../../openshift-ansible'\n\nssh_config_file: '~/.ssh/config'\n\nrhel_image: '{{ rhel_image_path | basename | regex_replace(\"^(.*)\\.qcow2$\", \"\\1\") }}'\nrhel_image_gce: '{{ rhel_image | replace(\".\", \"-\") | replace(\"_\", \"-\") | lower }}'\ngold_image: '{{ rhel_image_gce + \"-gold\" if openshift_deployment_type == \"openshift-enterprise\" else \"centos-7-ocp-gold-image\" }}'\ngold_image_family: '{{ \"rhel-guest-gold\" if openshift_deployment_type == \"openshift-enterprise\" else \"centos-7-ocp-gold\" }}'\n\nconsole_port: 443\n\nvalidate_etcd_short_hostname: true\n\nservice_account: 'ocp-ansible'\nservice_account_id: '{{ service_account }}@{{ gcloud_project }}.iam.gserviceaccount.com'\nservice_account_name: 'OCP Ansible'\ncredentials_file: '{{ inventory_dir }}/gce/hosts/project.json'\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "0f766393c03e08465cda12b46f7fd4b729922ce1", "filename": "roles/storage-demo-nodeconfig/tasks/provision.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- name: Allow ceph OSD traffic\n  iptables:\n    action: insert\n    chain: INPUT\n    protocol: tcp\n    destination_port: 6789\n    jump: ACCEPT\n\n- name: Allow ceph MDS traffic\n  iptables:\n    action: insert\n    chain: INPUT\n    protocol: tcp\n    destination_port: 6800:7300\n    jump: ACCEPT\n\n- name: Save iptables configuration\n  command: iptables-save\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "c85344decca890aae396dd642a664a880ef78019", "filename": "playbooks/roles/suricata/tests/test.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- hosts: localhost\n  remote_user: root\n  roles:\n    - suricata"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "05f4d57a93cb085f1c4fd7b9be72e61b4c3396bc", "filename": "roles/ovirt-engine-backup/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# engine-backup\n- name: run engine-backup\n  shell: 'engine-backup --mode={{ovirt_backup_mode}} --file={{ovirt_backup_archive}} --log={{ovirt_backup_log_file}} --scope={{ovirt_backup_scope}}'\n  tags:\n    - skip_ansible_lint\n  register: ovirt_backup_status\n\n# download backup file to ansible-client node\n- name: download engine backup file\n  fetch:\n    src: \"{{ovirt_backup_archive}}\"\n    dest: \"{{ovirt_backup_archive}}\"\n  when: ovirt_backup_status|succeeded\n\n# download log file to ansible-client node\n- name: download log file\n  fetch:\n    src: \"{{ovirt_log_file}}\"\n    dest: \"{{ovirt_log_file}}\"\n  when: ovirt_backup_status|failed\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "43f506c2e579dd46f5b015237e2d314953a3e4ac", "filename": "roles/sugarizer/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# 0. CLEAN UP PRIOR VERSIONS OF SUGARIZER (NEEDS WORK!)\n\n- name: Wipe /library/www/html/sugarizer* if installing sugarizer-1.0\n  shell: \"rm -rf {{ doc_root }}/sugarizer*\"\n  when: sugarizer_version == \"sugarizer-1.0\"\n\n# 1. DOWNLOAD+LINK /opt/iiab/sugarizer\n\n- name: Clone llaske/sugarizer ({{ sugarizer_git_version }}) from GitHub to /opt/iiab (MAY DOWNLOAD 600+ MB)\n  git:\n    repo: https://github.com/llaske/sugarizer\n    dest: \"{{ sugarizer_location }}/{{ sugarizer_version }}\"\n    version: \"{{ sugarizer_git_version }}\"\n    force: yes\n    depth: 1\n  when: internet_available\n\n- name: Create symbolic link /opt/iiab/sugarizer -> /opt/iiab/{{ sugarizer_version }}\n  file:\n    src: \"{{ sugarizer_location }}/{{ sugarizer_version }}\"\n    dest: \"{{ sugarizer_location }}/sugarizer\"\n    state: link\n\n# 2. DOWNLOAD+LINK /opt/iiab/sugarizer-server\n\n# 2018-07-11: http://download.iiab.io/packages/sugarizer-server-1.0.tar.gz\n# was flawed, as documented at:\n#    https://github.com/iiab/iiab/pull/814#issuecomment-404211098\n# Versions of MongoDB, npm (& Node.js ?) matter!  Sugarizer 1.0 Context:\n#    https://github.com/iiab/iiab/issues/798\n# Going forward let's \"git clone\" IIAB's preferred versions, of sugarizer\n# AND sugarizer-server, as specified in roles/sugarizer/defaults/main.yml\n\n# 2018-07-14 BLOAT: git works well BUT even with \"depth: 1\"\n# - 229MB is unfort downloaded to /opt/iiab/sugarizer/.git\n# - 1.4MB is unfort downloaded to /opt/iiab/sugarizer-server/.git\n\n# CLARIF: during repeat runs of \"./runrole sugarizer\", this git sync shows\n# \"changed\" (whereas above git sync shows \"ok\").  Reason: \"npm install\"\n# (below) modifies /opt/iiab/sugarizer-server/node_modules\n- name: Clone llaske/sugarizer-server ({{ sugarizer_server_git_version }}) from GitHub to /opt/iiab\n  git:\n    repo: https://github.com/llaske/sugarizer-server\n    dest: \"{{ sugarizer_location }}/{{ sugarizer_server_version }}\"\n    version: \"{{ sugarizer_server_git_version }}\"\n    force: yes\n    depth: 1\n  when: internet_available\n\n- name: Create symbolic link /opt/iiab/sugarizer-server -> /opt/iiab/{{ sugarizer_server_version }}\n  file:\n    src: \"{{ sugarizer_location }}/{{ sugarizer_server_version }}\"\n    dest: \"{{ sugarizer_location }}/sugarizer-server\"\n    state: link\n\n# 3. INSTALL A GOOD VERSION OF Node.js AND npm\n\n- name: Set up Node.js 8.x apt sources (debuntu, but avoid ubuntu-18)\n  shell: curl -sL https://deb.nodesource.com/setup_8.x | bash -\n  when: internet_available and is_debuntu and not is_ubuntu_18\n\n- name: Install latest Node.js which includes /usr/bin/npm (debuntu, but avoid ubuntu-18)\n  package:\n    name: nodejs\n    # name: nodejs=8.x\n    state: latest\n    # state: present\n  when: internet_available and is_debuntu and not is_ubuntu_18\n\n# 2018-07-14: BOTH STEPS ABOVE TAKE TIME, but Raspbian (apt offers npm\n# 1.4.21) & Debian 9 (apt offers no npm!) STILL NEED the above\n# nodesource.com approach to get a version of npm that works with Sugarizer:\n# https://github.com/iiab/iiab/issues/798#issuecomment-404324530\n#\n# MORE POSITIVELY: this nodesource.com approach (brings in npm 5.6.0 with\n# nodejs 8.11.3 for now, to any OS) would also work on Ubuntu 18.04, and\n# might even bring about a sane consistency across mainline OS's?\n#\n# BUT FOR NOW: Ubuntu 18.04's apt (approach below) brings in npm 3.5.2,\n# which appears suffic \"SO FAR\"?  18.04's nodejs 8.10.0 is more reassuring!\n\n# CRAZY IDEA: most versions of npm can upgrade themselves to the latest\n# (6.2.0 for now) using command \"npm install -g npm\", if that helps us in\n# future, e.g. TK's memory issue etc?  If so, be CAREFUL this puts npm\n# in /usr/local/bin on Ubuntu 18.04 -- unlike Ubuntu 16.04 and Raspbian\n# where it upgrades /usr/bin/npm in place:\n# https://askubuntu.com/questions/1036278/npm-is-incorrect-version-on-latest-ubuntu-18-04-installation\n\n- name: Install Node.js and npm (ubuntu-18 or not debuntu)\n  package:\n    name: \"{{ item }}\"\n    state: latest\n  with_items:\n    - nodejs\n    - npm\n  when: internet_available and (is_ubuntu_18 or not is_debuntu)\n\n# 4. RUN \"npm install\" TO POPULATE ~35MB /opt/iiab/sugarizer-server/node_modules\n\n# Re-running \"npm install\" USED TO fail on Raspbian 9 if not other OS's ?\n# Strategies considered to avoid re-running it:\n# OLD WAY 1: test & set flag node_modules_exists: True\n# OLD WAY 2: \"creates: ...\" checks for non-existence of /opt/iiab/sugarizer-server-1.0/node_modules\n# OLD WAY 3: set \"register: git_sug_server_output\" above, then as nec delete /opt/iiab/sugarizer-server-1.0/node_modules \"when: git_sug_server_output.changed\" and as nec run \"npm install\"\n\n#- name: Check for /opt/iiab/{{ sugarizer_server_version }}/node_modules\n#  stat:\n#    path: \"{{ sugarizer_location }}/{{ sugarizer_server_version }}/node_modules\"\n#  register: nmtest\n#  ignore_errors: true\n#\n#- name: Set a flag to prevent re-running of \"npm install\"\n#  set_fact:\n#    node_modules_exists: True\n#  when: nmtest.stat is defined and nmtest.stat.exists\n\n# NEW WAY BELOW: run \"npm install --allow-root\" every time, as modern versions\n# of npm are incremental, with sanity checks (all 3 may work: but npm 6.2.0\n# is better than 5.6.0. which is better than Ubuntu 18.04's 3.5.2).\n\n# 2018-07-15: TK Kang & Holt confirmed sudo-driven \"npm install\" maxes out CPU\n# for hours, on diff OS's using npm 5.6.0 and 6.2.0.  Hours later you may get\n# error: code EACCES, errno -13 (permission denied),\n# \"Missing write access to /opt/iiab/sugarizer-server-1.0/node_modules\"\n#\n# SOLUTION: Implement '--allow-root --unsafe-perm=true' below, as is critical\n# for 1st run of sudo-driven 'npm install' especially:\n#\n# ON DEBIAN: npm 5.6.0's --allow-root would be sufficient: causing creation\n# of /root/.npm cache & lock files to owned by root:root instead of\n# iiab-admin:iiab-admin...thus permitting it & IIAB installs to complete!\n#\n# ON RASPBIAN: npm 5.6.0's --unsafe-perm=true is *required* so that npm\n# install actually finished (in about 5 minutes).  It's possible we should\n# remove --allow-root in favore of --unsafe-perm=true alone.  But this needs\n# testing on different Linuxes before proceeding.\n#\n# CLARIF 1: Something like 'chown -R root:root /root/.npm' would do the job,\n# but cannot happen synchronously throughout the 1st run of 'npm install'\n# (when it's needed!)  Similar to what --allow-root does on Debian.\n#\n# CLARIF 2: Ubuntu 18.04 is currently unaffected due to its ancient\n# npm 3.5.2, which instead uses /home/iiab-admin/.npm (which remains owned\n# by iiab-admin:iiab-admin, even with '--allow-root', but thankfully still\n# gets the job done, for now!)\n\n#- name: Create the express framework for Node.js (OS's other than Fedora 18)\n- name: Run 'npm install --allow-root --unsafe-perm=true' to create /opt/iiab/{{ sugarizer_server_version }}/node_modules (CAN TAKE ~3 MINUTES)\n  command: npm install --allow-root --unsafe-perm=true    # \"command:\" a bit safer than \"shell:\"\n  args:\n    chdir: \"{{ sugarizer_location }}/{{  sugarizer_server_version }}\"\n    #creates: \"{{ sugarizer_location }}/{{ sugarizer_server_version }}/node_modules\"    # OLD WAY 2\n  when: internet_available    # \"npm install\" generally requires Internet access\n# when: internet_available and git_sug_server_output.changed    # OLD WAY 3\n# when: internet_available and not is_F18 and not node_modules_exists    # OLD WAY 1\n\n#- name: Create the express framework for Node.js (Fedora 18)\n#  shell: npm install\n#  args:\n#    chdir: \"{{ sugarizer_location }}/sugarizer/server\"\n#  when: internet_available and is_F18 and not node_modules_exists\n\n# 5. PLACE CONFIG FILES\n\n- name: Configure sugarizer.service (systemd), sugarizer.conf (Apache) and sugarizer.ini\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    mode: \"{{ item.mode }}\"\n    owner: root\n    group: root\n  with_items:\n    - { src: 'sugarizer.ini', dest: '{{ sugarizer_location }}/{{ sugarizer_server_version }}/env/sugarizer.ini', mode: '0644' }\n    - { src: 'sugarizer.conf', dest: '/etc/apache2/sites-available', mode: '0644' }\n    - { src: 'sugarizer.service.j2', dest: '/etc/systemd/system/sugarizer.service', mode: '0644' }\n\n- name: Create symlink for short URL http://box/sugarizer\n  file:\n    src: /etc/apache2/sites-available/sugarizer.conf\n    dest: /etc/apache2/sites-enabled/sugarizer.conf\n    state: link\n\n# 6. RESTART/STOP SYSTEMD SERVICE\n\n- name: Enable+restart systemd service if sugarizer_enabled, with \"systemctl daemon-reload\" (in case mongodb.service changed?)\n  systemd:\n    name: sugarizer\n    enabled: yes\n    state: restarted\n    daemon_reload: yes\n  when: sugarizer_enabled\n\n- name: 'Disable+stop systemd service if sugarizer_enabled: False'\n  systemd:\n    name: sugarizer\n    enabled: no\n    state: stopped\n  when: not sugarizer_enabled\n\n#- name: Enable services (all OS's)\n#  service:\n#    name: \"{{ item.name }}\"\n#    enabled: yes\n#    state: restarted\n#  with_items:\n##   - { name: mongodb }    # 2018-07-14: NICE TRY, but still doesn't bring http://box:8089 to life reliably, as a reboot usually does!  (Is a \"systemctl daemon-reload\" or some such nec?)\n#    - { name: sugarizer }\n#  when: sugarizer_enabled\n\n#- name: Disable service (all OS's)\n#  service:\n#    name: sugarizer\n#    enabled: no\n#    state: stopped\n#  when: not sugarizer_enabled\n\n- name: Add 'sugarizer' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: sugarizer\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: Sugarizer\n    - option: description\n      value: '\"The Sugar Learning Platform began with the famous One Laptop Per Child project, written in Python. Sugarizer is the new HTML/JavaScript implementation of Sugar, usable in most all browsers.\"'\n    - option: enabled\n      value: \"{{ sugarizer_enabled }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "6fd58ac7156bd395243a46d556dfb0be567e3802", "filename": "roles/config-versionlock/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Test versionlock of some packages\"\n  hosts: all\n  roles:\n  - role: config-versionlock \n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "fbcf7c886a203cf012f8c899b73cbcf3fa4e6add", "filename": "playbooks/libvirt/openshift-cluster/templates/user-data", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#cloud-config\ndisable_root: true\n\nhostname: {{ item[0] }}\nfqdn: {{ item[0] }}.example.com\n\nmounts:\n- [ sdb ]\n\nusers:\n  - default\n  - name: root\n    ssh_authorized_keys:\n    - {{ lookup('file', '~/.ssh/id_rsa.pub') }}\n\nsystem_info:\n  default_user:\n    name: openshift\n    sudo: [\"ALL=(ALL) NOPASSWD: ALL\"]\n\nssh_authorized_keys:\n  - {{ lookup('file', '~/.ssh/id_rsa.pub') }}\n\nwrite_files:\n  - path: /etc/sudoers.d/00-openshift-no-requiretty\n    permissions: 440\n    content: |\n        Defaults:openshift !requiretty\n  - path: /etc/sysconfig/docker-storage-setup\n    owner: root:root\n    permissions: '0644'\n    content: |\n      DEVS=/dev/sdb\n      VG=docker_vg\n      EXTRA_DOCKER_STORAGE_OPTIONS='--storage-opt dm.blkdiscard=true'\n  - path: /etc/systemd/system/fstrim.timer.d/hourly.conf\n    content: |\n      [Timer]\n      OnCalendar=hourly\n\nruncmd:\n  - NETWORK_CONFIG=/etc/sysconfig/network-scripts/ifcfg-eth0; if ! grep DHCP_HOSTNAME ${NETWORK_CONFIG}; then echo 'DHCP_HOSTNAME=\"{{ item[0] }}.example.com\"' >> ${NETWORK_CONFIG}; fi; pkill -9 dhclient; service network restart\n  - systemctl enable --now fstrim.timer\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "150a679118e706bd392a5853172c3fa88ea1bf54", "filename": "roles/dns/manage-dns-zones/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n###############################################################################\n# common defaults\nttl: 300\n\n\n###############################################################################\n# defaults for named\nnamed_processing: False\ndefault_dnssec_keygen_size: 256\ndefault_dnssec_keygen_algorithm: HMAC-SHA256\n\n\n###############################################################################\n# defaults for Route53\nroute53_processing: False\naws_access_key: \"{{ lookup('env','AWS_ACCESS_KEY_ID') }}\"\naws_secret_key: \"{{ lookup('env','AWS_SECRET_ACCESS_KEY') }}\"\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "7582feb687858ed7493682946858a7c5b4c480cd", "filename": "tasks/checks/distribution-checks.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Fail if this roles does not support the distribution\n  fail:\n    msg: \"Distribution {{ _docker_os_dist }} is not supported by this role!\"\n  when: _docker_os_dist != \"Fedora\" and\n        _docker_os_dist != \"CentOS\" and\n        _docker_os_dist != \"RedHat\" and\n        _docker_os_dist != \"Ubuntu\" and\n        _docker_os_dist != \"Debian\"\n\n- name: Fail if kernel version is lower than 3.10\n  fail:\n    msg: \"Kernel version 3.10 or later is required!\"\n  when: ansible_kernel is version_compare(\"3.10\", '<')\n\n- name: Include distribution check tasks for {{ _docker_os_dist }}\n  include_tasks: distribution-checks-{{ _docker_os_dist }}.yml\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "11fe2dd84f22d85a44b4623154b6fd712d7f7ede", "filename": "playbooks/provisioning/openstack/pre_tasks.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Generate Environment ID\n  set_fact:\n    env_random_id: \"{{ ansible_date_time.epoch }}\"\n  run_once: true\n  delegate_to: localhost\n\n- name: Set default Environment ID\n  set_fact:\n    default_env_id: \"openshift-{{ lookup('env','OS_USERNAME') }}-{{ env_random_id }}\"\n  delegate_to: localhost\n\n- name: Setting Common Facts\n  set_fact:\n    env_id: \"{{ env_id | default(default_env_id) }}\"\n  delegate_to: localhost\n\n- name: Updating DNS domain to include env_id (if not empty)\n  set_fact:\n    full_dns_domain: \"{{ (env_id|trim == '') | ternary(public_dns_domain, env_id + '.' + public_dns_domain) }}\"\n  delegate_to: localhost\n\n- name: Set the APP domain for OpenShift use\n  set_fact:\n    openshift_app_domain: \"{{ openshift_app_domain | default('apps') }}\"\n  delegate_to: localhost\n\n- name: Set the default app domain for routing purposes\n  set_fact:\n    openshift_master_default_subdomain: \"{{ openshift_app_domain }}.{{ full_dns_domain }}\"\n  delegate_to: localhost\n  when:\n  - openshift_master_default_subdomain is undefined\n\n# Check that openshift_cluster_node_labels has regions defined for all groups\n# NOTE(kpilatov): if node labels are to be enabled for more groups,\n#                 this check needs to be modified as well\n- name: Set openshift_cluster_node_labels if undefined (should not happen)\n  set_fact:\n    openshift_cluster_node_labels: {'app': {'region': 'primary'}, 'infra': {'region': 'infra'}}\n  when: openshift_cluster_node_labels is not defined\n\n- name: Set openshift_cluster_node_labels for the infra group\n  set_fact:\n    openshift_cluster_node_labels: \"{{ openshift_cluster_node_labels | combine({'infra': {'region': 'infra'}}, recursive=True) }}\"\n\n- name: Set openshift_cluster_node_labels for the app group\n  set_fact:\n    openshift_cluster_node_labels: \"{{ openshift_cluster_node_labels | combine({'app': {'region': 'primary'}}, recursive=True) }}\"\n\n- name: Set openshift_cluster_node_labels for auto-scaling app nodes\n  set_fact:\n    openshift_cluster_node_labels: \"{{ openshift_cluster_node_labels | combine({'app': {'autoscaling': 'app'}}, recursive=True) }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "d3e4331585d951384f0d0b9b449a69144017864b", "filename": "roles/ansible/tower/config-ansible-tower/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: install.yml\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "143612024688d8e798ddf4919f1249d26f43f905", "filename": "tasks/setup-repository-RedHat.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Include tasks from setup of repositories for CentOS\n  include_tasks: setup-repository-CentOS.yml\n\n- name: RHEL 7 repositories\n  block:\n    - name: Set internal facts for repository handling\n      set_fact:\n        _rhel_repo_check_cmd:\n          enabled:\n            sm: subscription-manager repos --list-enabled\n            yum: yum repolist enabled\n          disabled:\n            sm: subscription-manager repos --list-disabled\n            yum: yum repolist disabled\n        _rhel_cmd_enable_disable_repo:\n          enabled:\n            sm: subscription-manager repos --enable=\n            yum: \"yum-config-manager --enable \"\n          disabled:\n            sm: subscription-manager repos --disable=\n            yum: \"yum-config-manager --disable \"\n        _rhel_repos: \"{{ docker_rhel_repos }}\"\n\n    - name: Enable and disable repositories (RedHat)\n      become: true\n      shell: \"{{ _rhel_repo_check_cmd[item.state][item.repo_manager]  }} \\\n        | grep {{ item.id }} && exit 0 \\\n        || {{ _rhel_cmd_enable_disable_repo[item.state][item.repo_manager] }}{{ item.id }} \\\n        && exit 2\"\n      loop: \"{{ _rhel_repos }}\"\n      register: _cmd_rhel_repo_enabled_disabled\n      changed_when: _cmd_rhel_repo_enabled_disabled.rc == 2\n      failed_when: _cmd_rhel_repo_enabled_disabled.rc not in [ 0, 2 ]\n      tags:\n        - skip_ansible_lint\n\n  when:\n    - docker_network_access | bool\n    - _docker_os_dist_major_version | int == 7"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "43077ec815db6b45282c66354db8c9d98cc58386", "filename": "roles/config-quay-builder/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Validate Quay Hostname Provided\n  fail:\n    msg: \"Quay Hostname Must Be Provided!\"\n  when: quay_enterprise_hostname is undefined or quay_enterprise_hostname|trim == \"\"\n\n- name: Include Container Credentials\n  include_tasks: container_credentials.yml\n  when: (quay_registry_server | trim != \"\") and ((quay_registry_auth | trim != \"\") or (quay_registry_email | trim != \"\"))\n\n- name: Configure Configuration Directory\n  file:\n    state: directory\n    owner: root\n    group: root\n    mode: g+rw\n    path: \"{{ quay_builder_config_dir }}\"\n  \n- name: Configure Trusted SSL\n  block:\n    - name: Check if Trusted SSL file exists\n      become: false\n      stat:\n        path: \"{{ quay_builder_ssl_trust_src_file  }}\"\n      register: trusted_ssl_exists\n      changed_when: False\n      delegate_to: localhost\n    \n    - name: Fail if SSL source file does not exist\n      fail:\n        msg: \"Could not locate SSL trust certificate\"\n      when: trusted_ssl_exists.stat.exists == false\n  \n    - name: Copy SSL Certificate\n      copy:\n        src: \"{{ quay_builder_ssl_trust_src_file }}\"\n        dest: \"{{ quay_builder_ssl_trust_host_file }}\"\n        owner: root\n        group: root\n        mode: g+rw\n      notify: Restart Quay Builder Service\n  when: quay_builder_ssl_trust_configure|bool\n\n- name: Configure systemd environment files\n  template:\n    src: \"quay-builder.j2\"\n    dest: \"{{ systemd_environmentfile_dir}}/{{ quay_builder_name }}\"\n  notify: Restart Quay Builder Service\n\n- name: Configure systemd unit files\n  template:\n    src: \"quay-builder.service.j2\"\n    dest: \"{{ systemd_service_dir}}/{{ quay_builder_service }}\"\n  notify: Restart Quay Builder Service"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "04549619402ca202a960265fae672b3677d129d5", "filename": "roles/moodle/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "moodle_version: 35\n#moodle_repo_url: \"https://github.com/moodle/moodle.git\"\nmoodle_repo_url: \"git://git.moodle.org/moodle.git\"\nmoodle_base: \"{{ iiab_base }}/moodle\"\n#moodle_user: moodle\n#moodle_install: True\n#moodle_enabled: False\nmoodle_data: '{{ content_base }}/moodle'\nmoodle_database_name: moodle\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "2501eab533a301879d15b3027768a4aaf262fcaa", "filename": "roles/ipaclient/vars/RedHat-7.3.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# defaults file for ipaclient\n# vars/rhel.yml\nipaclient_packages: [ \"ipa-client\", \"ipa-admintools\", \"libselinux-python\" ]\n#ansible_python_interpreter: '/usr/bin/python2'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7f21f0379ba2290bf6c30dd188f8678898b7e080", "filename": "roles/ansible/tower/manage-job-templates/tasks/process-job-template.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Get the inventory id based on the inventory name\"\n  set_fact:\n    inventory_id: \"{{ item.id }}\"\n  when:\n  - item.name|trim == job_template.inventory|trim\n  with_items:\n  - \"{{ existing_inventories_output.rest_output }}\"\n\n- name: \"Get the project id based on the project name\"\n  set_fact:\n    project_id: \"{{ item.id }}\"\n  when:\n  - item.name|trim == job_template.project|trim\n  with_items:\n  - \"{{ existing_projects_output.rest_output }}\"\n\n- name: \"Get the credential id based on the credential name\"\n  set_fact:\n    credential_id: \"{{ item.id }}\"\n  when:\n  - item.name|trim == job_template.credential|trim\n  with_items:\n  - \"{{ existing_credentials_output.rest_output }}\"\n\n- name: \"Load up the job template\"\n  uri:\n    url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/job_templates/\"\n    user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n    password: \"{{ ansible_tower.admin_password }}\"\n    force_basic_auth: yes\n    method: POST\n    body: \"{{ lookup('template', 'job-template.j2') }}\"\n    body_format: 'json'\n    headers:\n      Content-Type: \"application/json\"\n      Accept: \"application/json\"\n    validate_certs: no\n    status_code: 200,201,400\n\n# Utilize the `rest_get` library routine to ensure REST pagination is handled\n- name: \"Obtain the current roles\"\n  rest_get:\n    host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n    rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n    rest_password: \"{{ ansible_tower.admin_password }}\"\n    api_uri: \"/api/v2/roles/\"\n  register: existing_roles_output\n\n- name: \"Assign the Team(s) Permission(s)\"\n  vars:\n    permissions_object: \"teams\"\n    permissions_value: \"{{ team }}\"\n  include_tasks: set-permissions.yml\n  when:\n  - job_template.permissions is defined\n  - job_template.permissions.teams is defined\n  with_items:\n  - \"{{ job_template.permissions.teams }}\"\n  loop_control:\n    loop_var: team\n\n- name: \"Assign the User(s) Permission(s)\"\n  vars:\n    permissions_object: \"users\"\n    permissions_value: \"{{ user }}\"\n  include_tasks: set-permissions.yml\n  when:\n  - job_template.permissions is defined\n  - job_template.permissions.users is defined\n  with_items:\n  - \"{{ job_template.permissions.users }}\"\n  loop_control:\n    loop_var: user\n\n- name: \"Clear/Update facts\"\n  set_fact:\n    inventory_id: ''\n    project_id: ''\n    processed_job_templates: \"{{ processed_job_templates + [ {'name': job_template.name } ] }}\"\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "b172da7454698ee8ad053442ecb00add712905b4", "filename": "playbooks/debug.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- debug: msg=\"Dumping variables for debug\"\n- debug: var=rock_debug\n- debug: var=rock_online_install\n- debug: var=rock_data_dir\n- debug: var=bro_data_dir\n- debug: var=suricata_data_dir\n- debug: var=stenographer_data_dir\n- debug: var=rock_data_user\n- debug: var=es_mem\n- debug: var=bro_cpu\n- debug: var=rock_monifs\n- debug: var=epel_baseurl\n- debug: var=epel_gpgurl\n- debug: var=elastic_baseurl\n- debug: var=elastic_gpgurl\n- debug: var=rocknsm_baseurl\n- debug: var=rocknsm_gpgurl\n- debug: var=rocknsm_local_baseurl\n- debug: var=http_proxy\n- debug: var=https_proxy\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "a9d9c9b5693be336cae5c73a792879c384844a3b", "filename": "roles/consul/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for consul\nconsul_is_server: no\nconsul_dc: dc1\nconsul_servers_group: consul_servers\nconsul_advertise: \"{{ ansible_default_ipv4.address }}\"\nconsul_bind_addr: \"{{ ansible_default_ipv4.address }}\"\nconsul_bootstrap_expect: \"{{ groups[consul_servers_group] | length }}\"\nconsul_client_addr: \"0.0.0.0\"\nconsul_atlas_join: false\n"}, {"commit_sha": "3c8d04f3e0875a9baf1f1282f6665b2e7d6871a8", "sha": "c69f1056565a4509b53a7d530353c10dab6322eb", "filename": "tasks/autoupdate-RedHat.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\n- name: Install yum-cron.\n  package: name=yum-cron state=present\n\n- name: Ensure yum-cron is running and enabled on boot.\n  service: name=yum-cron state=started enabled=yes\n\n- name: Configure autoupdates (RHEL 7).\n  lineinfile:\n    dest: \"/etc/yum/yum-cron.conf\"\n    regexp: '^apply_updates = .+'\n    line: 'apply_updates = yes'\n  when: security_autoupdate_enabled and ansible_distribution_major_version | int == 7\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "033314121352e22d4b859936a23ba67b3b1750a5", "filename": "roles/network/tasks/captive_portal.yml", "repository": "iiab/iiab", "decoded_content": "- name: Create directory for Captive Portal script\n  file:\n    path: /opt/iiab/captive-portal\n    state: directory\n  when: py_captive_portal_install\n\n- name: Copy Captive Portal script\n  template:\n    src: roles/network/templates/captive_portal/captive_portal.py.j2\n    dest: /opt/iiab/captive-portal/captive_portal.py\n    owner: root\n    group: root\n    mode: 0740\n  when: py_captive_portal_install\n\n- name: Copy Captive Portal service file\n  template:\n    src: roles/network/templates/captive_portal/captive_portal.service.j2\n    dest: /etc/systemd/system/captive_portal.service\n    owner: root\n    group: root\n    mode: 0644\n  when: py_captive_portal_install\n\n- name: Enable captive_portal after copying files\n  service:\n    name: captive_portal.service\n    enabled: yes\n  when: py_captive_portal_install and py_captive_portal_enabled\n\n- name: Start captive_portal after copying files\n  service:\n    name: captive_portal.service\n    state: started\n  when: py_captive_portal_install and py_captive_portal_enabled\n\n- name: Disable captive_portal after copying files\n  service:\n    name: captive_portal.service\n    enabled: no\n  when: py_captive_portal_install and py_captive_portal_enabled\n\n- name: Stop captive_portal after copying files\n  service:\n    name: captive_portal.service\n    state: started\n  when: py_captive_portal_install and py_captive_portal_enabled\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "68835dde42ba75a461df0e3e64e22a7205ac5ea3", "filename": "reference-architecture/gcp/ansible/playbooks/roles/master-http-proxy/templates/haproxy.cfg.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#---------------------------------------------------------------------\n# Example configuration for a possible web application.  See the\n# full configuration options online.\n#\n#   http://haproxy.1wt.eu/download/1.4/doc/configuration.txt\n#\n#---------------------------------------------------------------------\n\n#---------------------------------------------------------------------\n# Global settings\n#---------------------------------------------------------------------\nglobal\n    # to have these messages end up in /var/log/haproxy.log you will\n    # need to:\n    #\n    # 1) configure syslog to accept network log events.  This is done\n    #    by adding the '-r' option to the SYSLOGD_OPTIONS in\n    #    /etc/sysconfig/syslog\n    #\n    # 2) configure local2 events to go to the /var/log/haproxy.log\n    #   file. A line like the following can be added to\n    #   /etc/sysconfig/syslog\n    #\n    #    local2.*                       /var/log/haproxy.log\n    #\n    log         127.0.0.1 local2\n\n    chroot      /var/lib/haproxy\n    pidfile     /var/run/haproxy.pid\n    maxconn     4000\n    user        haproxy\n    group       haproxy\n    daemon\n\n    # turn on stats unix socket\n    stats socket /var/lib/haproxy/stats\n\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\n    mode                    http\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n    option forwardfor       except 127.0.0.0/8\n    option                  redispatch\n    retries                 3\n    timeout http-request    10s\n    timeout queue           1m\n    timeout connect         10s\n    timeout client          1m\n    timeout server          1m\n    timeout http-keep-alive 10s\n    timeout check           10s\n    maxconn                 3000\n\n#---------------------------------------------------------------------\n# main frontend which proxys to the backends\n#---------------------------------------------------------------------\nfrontend  http-proxy *:8080\n    acl          url_healthz  path_beg  -i /healthz\n    use_backend  ocp          if url_healthz\n\nbackend ocp\n    server       ocp localhost:{{ console_port }} ssl verify none\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "f687438e7114f5ce8a7807ba7ff24b38123c67c5", "filename": "reference-architecture/vmware-ansible/playbooks/crs-node-setup.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: crs\n  gather_facts: yes\n  become: no\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - instance-groups\n  - rhsm-subscription\n  - gluster-rhsm-repos\n  - vmware-guest-setup\n  - docker-storage-setup\n  - openshift-volume-quota\n  - gluster-crs-prerequisites\n  - gluster-ports\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c442458972b469a08427d021b2a17d8f59771707", "filename": "roles/config-idm-server/tasks/prep.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - ipa-server\n  - firewalld\n  - python-firewall\n\n- name: 'Ensure firewalld is running'\n  service:\n    name: firewalld\n    state: started\n    enabled: yes\n\n- name: 'Open Firewall for IdM use'\n  firewalld:\n    service: \"{{ item }}\"\n    permanent: yes\n    state: enabled\n    immediate: yes\n  with_items:\n  - ntp\n  - http \n  - https\n  - ldap\n  - ldaps\n  - kerberos\n  - kpasswd\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9f7c8909eca483da83ebff2090ab5def5aab3b5d", "filename": "roles/osp/admin-image/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Create the image\"\n  os_image:\n    cloud: \"{{ item.cloud | default(osp_default_cloud) | default(omit) }}\"\n    filename: \"{{ item.filename }}\"\n    disk_format: \"{{ item.disk_format | default(omit) }}\"\n    is_public: \"{{ item.is_public | default(omit) }}\"\n    name: \"{{ item.name }}\"\n  with_items:\n  - \"{{ osp_images | default([]) }}\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "9336dfa93efae813e0b578404cddd0c11d6e1722", "filename": "ops/playbooks/includes/get_dtr_version.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n\n    - name: Retrieve a token for the UCP API\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/auth/login\"\n        headers:\n          Content-Type: application/json\n        method: POST\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        body: '{\"username\":\"{{ ucp_username }}\",\"password\":\"{{ ucp_password }}\"}'\n      delegate_to: localhost\n      register: resp\n      until: resp.status == 200\n      retries: 20\n      delay: 5\n\n    - name: Remember the API's token\n      set_fact:\n        auth_token:  \"{{resp.json.auth_token}}\"\n\n    - name: Find DTR Detail\n      uri:\n        url: 'https://{{ ucp_instance }}.{{ domain_name }}/containers/json?filters={\"name\":[\"dtr-api*\"],\"status\":[\"running\"]}'\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        validate_certs: no\n      register: resp\n\n    - set_fact:\n        detected_dtr_version: \"{{ resp.json[0].Image.split(':') | last }}\"\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "cfe4b9b5fb0508a8d266a61fa38b59f3fe10085d", "filename": "roles/nfs-server/tests/nfs-server.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: nfs-server\n  roles: \n  - role: nfs-server\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "09e9b63c942a008e1867ad482d49b72b53213997", "filename": "tasks/section_08.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - include: section_08_level1.yml\n    tags:\n      - section08\n      - level1\n       \n\n  - include: section_08_level2.yml\n    tags:\n      - section08\n      - level2\n\n"}, {"commit_sha": "218cdc58f9fe9d7ece7d43e5f100fe9631fde5cc", "sha": "8abf7b7393e45780b9afe171e2de4ddca13e9db9", "filename": "meta/main.yml", "repository": "fubarhouse/ansible-role-golang", "decoded_content": "galaxy_info:\n  author: Karl Hepworth\n  description: Installs the Go language, including any configured packages and binaries.\n  license: MIT\n  min_ansible_version: 2.0\n  platforms:\n  #- name: MacOSX\n  #  versions:\n  #  - all\n  #  - 10.10\n  #  - 10.11\n  #  - 10.12\n  #  - 10.7\n  #  - 10.8\n  #  - 10.9\n  - name: Ubuntu\n    versions:\n    - precise\n    - trusty\n    - xenial\n  - name: EL\n    versions:\n  #  - 6\n    - 7\n  galaxy_tags:\n  - go\n  - development\n  - programming\n  - language\n  - compiler\n  - package\n  - manager\ndependencies:\n  - fubarhouse.commons"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "75502149757275a990e75a26f4d1889d1e86cb09", "filename": "tasks/selinux-Debian.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n\n- name: Make sure we have the necessary deb packages available for selinux\n  apt:\n    name: \"{{ item }}\"\n    update_cache: yes\n    state: present\n  with_items:\n    - python-selinux\n    - python-semanage\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "3b69c8386b006457e876b2f996f60a17db416e29", "filename": "roles/ansible/tower/config-ansible-tower/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: tower\n  roles:\n  - role: ansible/tower/config-ansible-tower\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "41223fd7bf8e7f06aeeedfd2902107d7b01eb859", "filename": "reference-architecture/aws-ansible/playbooks/add-crs.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  connection: local\n  gather_facts: no\n  become: no\n  vars_files:\n  - vars/main.yaml\n  roles:\n  # Group systems\n  - gluster-instance-groups\n\n- hosts: crs\n  gather_facts: yes\n  become: yes\n  serial: 1\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - rhsm-subscription\n\n- hosts: crs\n  gather_facts: yes\n  become: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - gluster-rhsm-repos\n  - gluster-crs-prerequisites\n  - gluster-ports\n"}, {"commit_sha": "01c4359d8ad17ba10149ac898663e598069b9055", "sha": "8477ddf3015cc617f72a640b114611819414ecf1", "filename": "meta/main.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\ndependencies: []\n\ngalaxy_info:\n  author: geerlingguy\n  description: Security software installation and configuration.\n  company: \"Midwestern Mac, LLC\"\n  license: \"license (BSD, MIT)\"\n  min_ansible_version: 1.4\n  platforms:\n  - name: EL\n    versions:\n    - all\n  - name: Debian\n    versions:\n    - all\n  - name: Ubuntu\n    versions:\n    - all\n  galaxy_tags:\n    - system\n    - security\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "e0067a6680c33f5e001c0d3a1eeb08ba09123f50", "filename": "roles/lighttpd/tasks/remove-user.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n- name: Remove a user from lighttpd\n  lineinfile:\n    path: /etc/lighttpd/rock-htpasswd.user\n    regex: \"^{{ lighttpd_user }}\"\n    state: absent\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b4b891ee94076bbf0fd3f4b6afd4971d9179598a", "filename": "reference-architecture/vmware-ansible/playbooks/library/vmware_resource_pool.py", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n# (c) 2017, Davis Phillips davis.phillips@gmail.com\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\nANSIBLE_METADATA = {'status': ['preview'],\n                    'supported_by': 'community',\n                    'version': '1.0'}\n\nDOCUMENTATION = '''\n---\nmodule: vmware_resource_pool\nshort_description: Add/remove resource pools to/from vCenter\ndescription:\n    - This module can be used to add/remove a resource pool to/from vCenter\nversion_added: 2.3\nauthor: \"Davis Phillips (@dav1x)\"\nnotes:\n    - Tested on vSphere 6.5\nrequirements:\n    - \"python >= 2.6\"\n    - PyVmomi\noptions:\n    datacenter:\n        description:\n            - Name of the datacenter to add the host\n        required: True\n    cluster:\n        description:\n            - Name of the cluster to add the host\n        required: True\n    resource_pool:\n        description:\n            - Resource pool name to manage\n        required: True\n    hostname:\n        description:\n            - ESXi hostname to manage\n        required: True\n    username:\n        description:\n            - ESXi username\n        required: True\n    password:\n        description:\n            - ESXi password\n        required: True\n    cpu_expandable_reservations:\n        description:\n            - In a resource pool with an expandable reservation, the reservation on a resource pool can grow beyond the specified value.\n        default: True\n    cpu_reservation:\n        description:\n            - Amount of resource that is guaranteed available to the virtual machine or resource pool.\n        default: 0\n    cpu_limit:\n        description:\n            - The utilization of a virtual machine/resource pool will not exceed this limit, even if there are available resources.\n        default: -1 (No limit)\n    cpu_shares:\n        description:\n            - Memory shares are used in case of resource contention.\n        choices:\n            - high\n            - custom\n            - low\n            - normal\n        default: Normal\n    mem_expandable_reservations:\n        description:\n            - In a resource pool with an expandable reservation, the reservation on a resource pool can grow beyond the specified value.\n        default: True\n    mem_reservation:\n        description:\n            - Amount of resource that is guaranteed available to the virtual machine or resource pool.\n        default: 0\n    mem_limit:\n        description:\n            - The utilization of a virtual machine/resource pool will not exceed this limit, even if there are available resources.\n        default: -1 (No limit)\n    mem_shares:\n        description:\n            - Memory shares are used in case of resource contention.\n        choices:\n            - high\n            - custom\n            - low\n            - normal\n        default: Normal\n    state:\n        description:\n            - Add or remove the resource pool\n        default: 'present'\n        choices:\n            - 'present'\n            - 'absent'\nextends_documentation_fragment: vmware.documentation\n'''\n\nEXAMPLES = '''\n# Create a resource pool\n  - name: Add resource pool to vCenter\n    vmware_resource_pool:\n      hostname: vcsa_host\n      username: vcsa_user\n      password: vcsa_pass\n      datacenter: datacenter\n      cluster: cluster\n      resource_pool: resource_pool\n      mem_shares: normal\n      mem_limit: -1\n      mem_reservation: 0\n      mem_expandable_reservations: True\n      cpu_shares: normal\n      cpu_limit: -1\n      cpu_reservation: 0\n      cpu_expandable_reservations: True\n      state: present\n'''\n\nRETURN = \"\"\"\ninstance:\n    descripton: metadata about the new resource pool\n    returned: always\n    type: dict\n    sample: None\n\"\"\"\n\ntry:\n    from pyVmomi import vim, vmodl\n    HAS_PYVMOMI = True\nexcept ImportError:\n    HAS_PYVMOMI = False\n\nfrom ansible.module_utils.vmware import get_all_objs, connect_to_api, vmware_argument_spec, find_datacenter_by_name, \\\n    find_cluster_by_name_datacenter, wait_for_task\nfrom ansible.module_utils.basic import AnsibleModule\n\nclass VMwareResourcePool(object):\n    def __init__(self, module):\n        self.module = module\n        self.datacenter = module.params['datacenter']\n        self.cluster = module.params['cluster']\n        self.resource_pool = module.params['resource_pool']\n        self.hostname = module.params['hostname']\n        self.username = module.params['username']\n        self.password = module.params['password']\n        self.state = module.params['state']\n        self.mem_shares = module.params['mem_shares']\n        self.mem_limit = module.params['mem_limit']\n        self.mem_reservation = module.params['mem_reservation']\n        self.mem_expandable_reservations = module.params['cpu_expandable_reservations']\n        self.cpu_shares = module.params['cpu_shares']\n        self.cpu_limit = module.params['cpu_limit']\n        self.cpu_reservation = module.params['cpu_reservation']\n        self.cpu_expandable_reservations = module.params['cpu_expandable_reservations']\n        self.dc_obj = None\n        self.cluster_obj = None\n        self.host_obj = None\n        self.resource_pool_obj = None\n        self.content = connect_to_api(module)\n\n    def find_host_by_cluster_datacenter(self):\n        self.dc_obj = find_datacenter_by_name(self.content, self.datacenter)\n        self.cluster_obj = find_cluster_by_name_datacenter(self.dc_obj, self.cluster)\n\n        for host in self.cluster_obj.host:\n            if host.name == self.hostname:\n                return host, self.cluster\n\n        return None, self.cluster\n\n\n    def select_resource_pool(self, host):\n        pool_obj = None\n\n        resource_pools = get_all_objs(self.content, [vim.ResourcePool])\n\n        pool_selections = self.get_obj(\n            [vim.ResourcePool],\n            self.resource_pool,\n            return_all = True\n        )\n        if pool_selections:\n            for p in pool_selections:\n                if p in resource_pools:\n                    pool_obj = p\n                    break\n        return pool_obj\n\n    def get_obj(self, vimtype, name, return_all = False):\n        obj = list()\n        container = self.content.viewManager.CreateContainerView(\n            self.content.rootFolder, vimtype, True)\n\n        for c in container.view:\n            if name in [c.name, c._GetMoId()]:\n                if return_all is False:\n                    return c\n                    break\n                else:\n                    obj.append(c)\n\n        if len(obj) > 0:\n            return obj\n        else:\n            # for backwards-compat\n            return None\n\n    def process_state(self):\n        try:\n            rp_states = {\n                'absent': {\n                    'present': self.state_remove_rp,\n                    'absent': self.state_exit_unchanged,\n                },\n                'present': {\n                    'present': self.state_exit_unchanged,\n                    'absent': self.state_add_rp,\n                }\n            }\n\n            rp_states[self.state][self.check_rp_state()]()\n\n        except vmodl.RuntimeFault as runtime_fault:\n            self.module.fail_json(msg = runtime_fault.msg)\n        except vmodl.MethodFault as method_fault:\n            self.module.fail_json(msg = method_fault.msg)\n        except Exception as e:\n            self.module.fail_json(msg = str(e))\n\n    def state_exit_unchanged(self):\n        self.module.exit_json(changed = False)\n\n    def state_remove_rp(self):\n        changed = True\n        result = None\n        resource_pool = self.select_resource_pool(self.host_obj)\n        try:\n            task = self.resource_pool_obj.Destroy()\n            success, result = wait_for_task(task)\n\n        except:\n            self.module.fail_json(msg = \"Failed to remove resource pool '%s' '%s'\" % (self.resource_pool,resource_pool))\n        self.module.exit_json(changed = changed, result = str(result))\n\n    def state_add_rp(self):\n        changed = True\n        result = None\n        root_resource_pool = None\n\n        rp_spec=vim.ResourceConfigSpec()\n        cpu_alloc=vim.ResourceAllocationInfo()\n        cpu_alloc.expandableReservation = self.cpu_expandable_reservations\n        cpu_alloc.limit = int(self.cpu_limit)\n        cpu_alloc.reservation = int(self.cpu_reservation)\n        cpu_alloc_shares = vim.SharesInfo()\n        cpu_alloc_shares.level = self.cpu_shares\n        cpu_alloc.shares =  cpu_alloc_shares\n        rp_spec.cpuAllocation = cpu_alloc\n        mem_alloc = vim.ResourceAllocationInfo()\n        mem_alloc.limit = int(self.mem_limit)\n        mem_alloc.expandableReservation = self.mem_expandable_reservations\n        mem_alloc.reservation = int(self.mem_reservation)\n        mem_alloc_shares = vim.SharesInfo()\n        mem_alloc_shares.level = self.mem_shares\n        mem_alloc.shares = mem_alloc_shares\n        rp_spec.memoryAllocation = mem_alloc\n\n        self.dc_obj = find_datacenter_by_name(self.content, self.datacenter)\n        self.cluster_obj = find_cluster_by_name_datacenter(self.dc_obj, self.cluster)\n        rootResourcePool = self.cluster_obj.resourcePool\n        task = rootResourcePool.CreateResourcePool(self.resource_pool, rp_spec)\n\n        self.module.exit_json(changed = changed)\n\n    def check_rp_state(self):\n\n        self.host_obj, self.cluster_obj = self.find_host_by_cluster_datacenter()\n        self.resource_pool_obj = self.select_resource_pool(self.host_obj)\n\n        if self.resource_pool_obj is None:\n            return 'absent'\n        else:\n            return 'present'\n\n\ndef main():\n    argument_spec = vmware_argument_spec()\n    argument_spec.update(dict(datacenter = dict(required = True, type = 'str'),\n                              cluster = dict(required = True, type = 'str'),\n                              resource_pool = dict(required=True, type='str'),\n                              hostname = dict(required = True, type = 'str'),\n                              username = dict(required = True, type = 'str'),\n                              password = dict(required = True, type = 'str', no_log = True),\n                              mem_shares = dict(type = 'str', default = \"normal\", choices = ['high','custom','normal', 'low']),\n                              mem_limit = dict(type = 'int',default = \"-1\"),\n                              mem_reservation = dict(type = 'int',default = \"0\"),\n                              mem_expandable_reservations = dict(type = 'bool',default = \"True\"),\n                              cpu_shares = dict(type = 'str', default = \"normal\", choices = ['high','custom','normal', 'low']),\n                              cpu_limit = dict(type = 'int',default = \"-1\"),\n                              cpu_reservation = dict(type = 'int',default = \"0\"),\n                              cpu_expandable_reservations = dict(type = 'bool',default = \"True\"),\n                              state = dict(default = 'present', choices = ['present', 'absent'], type = 'str')))\n\n    module = AnsibleModule(argument_spec = argument_spec, supports_check_mode = True)\n\n    if not HAS_PYVMOMI:\n        module.fail_json(msg = 'pyvmomi is required for this module')\n\n    vmware_rp = VMwareResourcePool(module)\n    vmware_rp.process_state()\n\nif __name__ == '__main__':\n    main()\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "ee0bf8886668f62656f26100ffd4aebfc7ce6fe6", "filename": "roles/filebeat/defaults/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n# defaults file for filebeat\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "a21f023eaceb17e7acde0f60d8abd6ccde50a2df", "filename": "playbooks/provisioning/openstack/ansible.cfg", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "# config file for ansible -- http://ansible.com/\n# ==============================================\n[defaults]\nansible_user = openshift\nforks = 50\n# work around privilege escalation timeouts in ansible\ntimeout = 30\nhost_key_checking = false\ninventory = inventory\ninventory_ignore_extensions = secrets.py, .pyc, .cfg, .crt\ngathering = smart\nretry_files_enabled = false\nfact_caching = jsonfile\nfact_caching_connection = .ansible/cached_facts\nfact_caching_timeout = 900\nstdout_callback = skippy\ncallback_whitelist = profile_tasks\nlookup_plugins = openshift-ansible-contrib/lookup_plugins\n\n\n[ssh_connection]\nssh_args = -o ControlMaster=auto -o ControlPersist=900s -o GSSAPIAuthentication=no\ncontrol_path = /var/tmp/%%h-%%r\npipelining = True\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "c1206f814f88e64e17b64a792549c459f56cb287", "filename": "reference-architecture/gcp/ansible/playbooks/roles/inventory-file-creation/templates/inventory.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "[OSEv3:children]\nmasters\netcd\nnodes\n\n[OSEv3:vars]\ndebug_level={{ debug_level }}\nopenshift_debug_level={{ debug_level }}\nopenshift_node_debug_level={{ debug_level }}\nopenshift_master_debug_level={{ debug_level }}\nconsole_port={{ console_port }}\ncontainerized={{ containerized }}\ndeployment_type={{ openshift_deployment_type }}\nopenshift_deployment_type={{ openshift_deployment_type }}\nosm_cluster_network_cidr={{ osm_cluster_network_cidr }}\nosm_default_node_selector='{{ osm_default_node_selector }}'\nosm_default_subdomain={{ wildcard_zone }}\nosm_use_cockpit={{ osm_use_cockpit }}\nos_sdn_network_plugin_name={{ os_sdn_network_plugin_name }}\nopenshift_cloudprovider_kind={{ openshift_cloudprovider_kind }}\nopenshift_master_api_port={{ console_port }}\nopenshift_master_console_port={{ console_port }}\nopenshift_master_cluster_method={{ openshift_master_cluster_method }}\nopenshift_master_cluster_hostname={{ openshift_master_cluster_hostname }}\nopenshift_master_cluster_public_hostname={{ openshift_master_cluster_public_hostname }}\nopenshift_master_default_subdomain={{ wildcard_zone }}\nopenshift_master_access_token_max_seconds={{ openshift_master_access_token_max_seconds }}\nopenshift_hosted_router_selector='{{ openshift_hosted_router_selector }}'\nopenshift_hosted_registry_selector='{{ openshift_hosted_registry_selector }}'\nopenshift_hosted_registry_storage_kind={{ openshift_hosted_registry_storage_kind }}\nopenshift_hosted_registry_storage_provider={{ openshift_hosted_registry_storage_provider }}\nopenshift_hosted_registry_storage_gcs_bucket={{ gcs_registry_bucket }}\nopenshift_hosted_registry_storage_gcs_rootdirectory='{{ openshift_hosted_registry_storage_gcs_rootdirectory }}'\nopenshift_hosted_registry_pullthrough={{ openshift_hosted_registry_pullthrough }}\nopenshift_hosted_registry_acceptschema2={{ openshift_hosted_registry_acceptschema2 }}\nopenshift_hosted_registry_enforcequota={{ openshift_hosted_registry_enforcequota }}\nopenshift_hosted_metrics_deploy={{ openshift_hosted_metrics_deploy }}\nopenshift_hosted_metrics_storage_volume_size={{ openshift_hosted_metrics_storage_volume_size }}\nopenshift_hosted_metrics_storage_kind={{ openshift_hosted_metrics_storage_kind }}\nopenshift_metrics_hawkular_nodeselector={{ openshift_metrics_hawkular_nodeselector | to_json }}\nopenshift_metrics_cassandra_nodeselector={{ openshift_metrics_cassandra_nodeselector | to_json }}\nopenshift_metrics_heapster_nodeselector={{ openshift_metrics_heapster_nodeselector | to_json }}\nopenshift_disable_check={{ openshift_disable_check }}\nopenshift_master_identity_providers={{ openshift_master_identity_providers | to_json }}\n\n[masters]\n{% for i in groups['tag_' + prefix + '-master'] %}\n{{ hostvars[i].gce_name }}\n{% endfor %}\n\n[etcd]\n{% for i in groups['tag_' + prefix + '-master'] %}\n{{ hostvars[i].gce_name }}\n{% endfor %}\n\n[nodes]\n{% for i in groups['tag_' + prefix + '-master'] %}\n{{ hostvars[i].gce_name }} openshift_node_labels=\"{'role': 'master'}\"\n{% endfor %}\n{% for i in groups['tag_' + prefix + '-infra-node'] %}\n{{ hostvars[i].gce_name }} openshift_node_labels=\"{'role': 'infra'}\"\n{% endfor %}\n{% for i in groups['tag_' + prefix + '-node'] %}\n{{ hostvars[i].gce_name }} openshift_node_labels=\"{'role': 'app'}\"\n{% endfor %}\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "0a7553e5c2761be4a190bdfc18ee361b340192a0", "filename": "roles/osp/packstack-install/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Prepare the OSP hosts\"\n  import_tasks: host-prep.yml\n\n- import_tasks: packstack-install-prep.yml\n  run_once: true\n  delegate_to: \"{{ ansible_play_hosts | first }}\"\n\n- import_tasks: sync-keys.yml\n\n- import_tasks: packstack-install.yml\n  run_once: true\n  delegate_to: \"{{ ansible_play_hosts | first }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "13b2679760c09af3964c4f4367c1dc368211cd1a", "filename": "playbooks/gce/openshift-cluster/service.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Call same systemctl command for openshift on all instance(s)\n  hosts: localhost\n  connection: local\n  become: no\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  - cluster_hosts.yml\n  tasks:\n  - fail: msg=\"cluster_id is required to be injected in this playbook\"\n    when: cluster_id is not defined\n\n  - add_host:\n      name: \"{{ item }}\"\n      groups: g_service_nodes\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    with_items: \"{{ node_hosts | default([]) | difference(['localhost']) | difference(groups.status_terminated) }}\"\n\n  - add_host:\n      name: \"{{ item }}\"\n      groups: g_service_masters\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    with_items: \"{{ master_hosts | default([]) | difference(['localhost']) | difference(groups.status_terminated) }}\"\n\n- include: ../../common/openshift-node/service.yml\n- include: ../../common/openshift-master/service.yml\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "721472d64b008cd88ea32a8d1d95df8f4128d3f6", "filename": "roles/consul/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for consul\nconsul_dc: dc1\nconsul_servers_group: consul_servers\nconsul_advertise: \"{{ ansible_ssh_host }}\"\nconsul_bind_addr: \"{{ ansible_default_ipv4.address }}\"\nconsul_retry_join: \"{% for host in groups[consul_servers_group] %}\\\"{{ hostvars[host].ansible_default_ipv4.address }}\\\"{% if not loop.last %}, {% endif %}{% endfor %}\"\nconsul_bootstrap_expect: \"{{ groups[consul_servers_group] | length }}\"\nconsul_client_addr: \"0.0.0.0\"\nconsul_atlas_join: false\nconsul_node_name: \"{{ ansible_hostname }}\"\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "370c85658be9bb34cbaf1345becf1be4872f4a15", "filename": "roles/bro/tests/test.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- hosts: localhost\n  remote_user: root\n  roles:\n    - bro"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "1d25ceb676c6c0716597e7b295893975c5d3af59", "filename": "roles/user-management/manage-idm-users/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: configure_user.yml\n\n- import_tasks: configure_group.yml\n\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "4a10624d76f14ad37069ecc636841c52fa862122", "filename": "tasks/create_repo_docker_hosted_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_docker_hosted\n    args: \"{{ _nexus_repos_docker_defaults|combine(item) }}\"\n"}, {"commit_sha": "6d10af54bdbf8e81c3d90a70ffea87b4d2c20eb2", "sha": "7e3f9444199129cf794c40a5ab77b3dfeb123318", "filename": "tasks/users.yml", "repository": "Oefenweb/ansible-wordpress", "decoded_content": "# tasks file for wordpress, users\n---\n- name: create (data) directory\n  file:\n    path: \"{{ wordpress_data_path }}\"\n    state: directory\n    owner: root\n    group: root\n    mode: 0755\n  tags: [configuration, wordpress, wordpress-users]\n\n- name: copy file (users)\n  copy:\n    src: \"{{ item.users.src }}\"\n    dest: \"{{ wordpress_data_path }}/{{ item.dbname }}.csv\"\n    owner: root\n    group: root\n    mode: 0644\n  register: check_copy_users\n  with_items: wordpress_installs\n  when: item.users.src is defined\n  tags: [configuration, wordpress, wordpress-users]\n\n- name: install (users)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.item.path }}' user import-csv {{ wordpress_data_path }}/{{ item.item.dbname }}.csv {{ '--skip-update' if item.item.users.skip_update | default(true) else '' }} --send-email\"\n  register: check_installation_users\n  changed_when: \"'Success' in check_installation_users.stdout\"\n  with_items: check_copy_users.results\n  when: check_copy_users is defined and item.changed\n  tags: [configuration, wordpress, wordpress-users]\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "2625d4d0539bec896c398b37c64c07bcd7dd4eda", "filename": "playbooks/gce/openshift-cluster/config.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: no\n  tasks:\n  - include_vars: vars.yml\n  - include_vars: cluster_hosts.yml\n  - add_host:\n      name: \"{{ item }}\"\n      groups: l_oo_all_hosts\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    with_items: \"{{ g_all_hosts | default([]) }}\"\n\n- hosts: l_oo_all_hosts\n  gather_facts: no\n  tasks:\n  - include_vars: vars.yml\n  - include_vars: cluster_hosts.yml\n\n- include: ../../common/openshift-cluster/config.yml\n  vars:\n    g_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n    g_sudo: \"{{ deployment_vars[deployment_type].become }}\"\n    g_nodeonmaster: true\n    openshift_cluster_id: \"{{ cluster_id }}\"\n    openshift_debug_level: \"{{ debug_level }}\"\n    openshift_deployment_type: \"{{ deployment_type }}\"\n    openshift_hostname: \"{{ gce_private_ip }}\"\n    openshift_hosted_registry_selector: 'type=infra'\n    openshift_hosted_router_selector: 'type=infra'\n    openshift_master_cluster_method: 'native'\n    openshift_use_openshift_sdn: \"{{ lookup('oo_option', 'use_openshift_sdn') }}\"\n    os_sdn_network_plugin_name: \"{{ lookup('oo_option', 'sdn_network_plugin_name') }}\"\n    openshift_use_flannel: \"{{ lookup('oo_option', 'use_flannel') }}\"\n    openshift_use_calico: \"{{ lookup('oo_option', 'use_calico') }}\"\n    openshift_use_fluentd: \"{{ lookup('oo_option', 'use_fluentd') }}\"\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "6075ddacb80cef74ca3806932a89802353021455", "filename": "tasks/section_11_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 11.1.1 Set Warning Banner for Standard Login Services (Scored)\n    lineinfile: >\n        dest={{ item }}\n        create=yes\n        line='Authorized uses only. All activity may be monitored and reported.'\n        state=present\n        mode=644\n        owner=root\n        group=root\n    with_items:\n        - /etc/motd\n        - /etc/issue\n        - /etc/issue.net\n    tags:\n      - section11\n      - section11.1\n\n  - name: 11.2.1 Remove OS Information from Login Warning Banners (Scored)\n    shell: egrep '(\\\\v|\\\\r|\\\\m|\\\\s)' {{ item }}\n    register: egrep_os_infos\n    failed_when: egrep_os_infos.rc == 0\n    changed_when: False\n    with_items:\n        - /etc/motd\n        - /etc/issue\n        - /etc/issue.net\n    tags:\n      - section11\n      - section11.2\n\n  - name: 11.3.1 Set Graphical Warning Banner (Not Scored)\n    debug: msg=\"*** Set a banner for the display manager ***\"\n    tags:\n      - section11\n      - section11.3\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "860e8ad8e798a58b2152bc0e12ab53680353bc74", "filename": "roles/config-satellite/tests/group_vars/satellite_servers.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# NOTE: Make sure a valid 'manifest.zip' file exists in the `files` directory before running this test\nmanifest_file_path: \"{{ inventory_dir }}/files/manifest.zip\"\n\nsatellite_organization: \"Example Org\"\nsatellite_location: \"example\"\nsatellite_username: \"admin\"\nsatellite_password: \"admin01\"\n\nsatellite_repositories:\n- product: \"Red Hat Enterprise Linux Server\"\n  name: \"Red Hat Enterprise Linux 7 Server (RPMs)\"\n  release_version:\n  - \"7.3\"\n  - \"7Server\"\n  base_arch: \"x86_64\"\n- product: \"Red Hat Enterprise Linux Server\"\n  name: \"Red Hat Enterprise Linux 7 Server - Extras (RPMs)\"\n  release_version: []\n  base_arch: \"x86_64\"\n\nsatellite_sync_plan: \"example-plan\"\n\n\n# NOTE: Make sure to update the 'Your name  here' below with a matching \n#       name for your subscription - i.e.: something that will match a \"grep\" \nsatellite_activation_keys:\n  rhel-7-example\n    subscription: \"Your name here\"\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6e9d1c2e721e9b4437360782fea8f08d147042ad", "filename": "roles/openstack-create-cinder-registry/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- os_volume:\n    display_name: \"{{ cinder_hosted_registry_name }}\"\n    size: \"{{ cinder_hosted_registry_size_gb }}\"\n  register: cinder_registry_volume\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "16c9297d8c13fbd79a33616228d21edea129f72f", "filename": "reference-architecture/gcp/ansible/playbooks/roles/restrict-gce-metadata/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: block access to the GCP metadata server\n  command: iptables --wait -4 --insert OUTPUT -d 169.254.169.254 -m comment --comment \"Prevent all users from reaching GCP API server\" -j REJECT --reject-with icmp-host-prohibited\n\n- name: enable root user access to the GCP metadata server\n  command: iptables --wait -4 --insert OUTPUT -d 169.254.169.254 -j ACCEPT -m comment --comment \"Enable root user to reach GCP API server\" -m owner --uid-owner 0\n\n- name: enable udp access to the GCP DNS\n  command: iptables --wait -4 --insert OUTPUT -p udp -d 169.254.169.254 -j ACCEPT --destination-port 53 -m comment --comment \"Enable udp access to the GCP DNS\"\n\n- name: enable tcp access to the GCP DNS\n  command: iptables --wait -4 --insert OUTPUT -p tcp -d 169.254.169.254 -j ACCEPT --destination-port 53 -m comment --comment \"Enable tcp access to the GCP DNS\"\n\n- name: block containers from access to the GCP metadata server\n  command: iptables --wait -4 --insert FORWARD -d 169.254.169.254 -m comment --comment \"Prevent containers from reaching GCP API server\" -j REJECT --reject-with icmp-host-prohibited\n\n- name: save iptables rules\n  command: /usr/libexec/iptables/iptables.init save\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "d804721933b3082a7fc3b34aad3f687efd7ebf08", "filename": "roles/openstack-stack/test/stack-create-test.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: True\n  become: False\n  roles:\n  - role: openstack-stack\n    stack_name: test-stack\n    dns_domain: \"{{ public_dns_domain }}\"\n    dns_nameservers: \"{{ public_dns_nameservers }}\"\n    subnet_prefix: \"{{ openstack_subnet_prefix }}\"\n    ssh_public_key: \"{{ openstack_ssh_public_key }}\"\n    openstack_image: \"{{ openstack_default_image_name }}\"\n    etcd_flavor: \"{{ openstack_default_flavor }}\"\n    master_flavor: \"{{ openstack_default_flavor }}\"\n    node_flavor: \"{{ openstack_default_flavor }}\"\n    infra_flavor: \"{{ openstack_default_flavor }}\"\n    dns_flavor: \"{{ openstack_default_flavor }}\"\n    external_network: \"{{ openstack_external_network_name }}\"\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "665e710bdbf2363c365035a76e691311008dc21b", "filename": "roles/docker/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for docker\ndocker_graph_dir: /var/lib/docker\ndocker_tmp_dir: /var/lib/docker/tmp"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "feb07f0535a1ddfa2501efe2c27a9aaef78ddb33", "filename": "ops/playbooks/roles/hpe.openports/defaults/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n\nhpe_openports_ports:\n  - 80/tcp\n  - 443/tcp\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "5941cc56523dd600c658a46df5231799f8d85f9c", "filename": "ops/playbooks/includes/config_client.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n    - name: Copy kubectl client\n      copy:\n        src: ../files/k8s/client/kubectl\n        dest: /usr/local/bin/kubectl\n        mode: 0744\n\n    - name: Retrieve a token for the UCP API\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/auth/login\"\n        headers:\n          Content-Type: application/json\n        method: POST\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        body: '{\"username\":\"{{ ucp_username }}\",\"password\":\"{{ ucp_password }}\"}'\n      delegate_to: localhost\n      register: resp\n      until: resp.status == 200\n      retries: 20\n      delay: 5\n      no_log: yes\n\n    - name: Remember the API's token\n      set_fact:\n        auth_token:  \"{{resp.json.auth_token}}\"\n\n    - name: Get the client bundle\n      get_url:\n        url: 'https://{{ ucp_instance }}.{{ domain_name }}/api/clientbundle'\n        headers: 'Authorization: Bearer {{ auth_token }}'\n        validate_certs: no\n        force: yes\n        dest: /tmp/bundle.zip\n      register: resp\n\n    - name: Create UCP Bundle dir\n      file:\n        path: ~/certs.{{ ucp_username }}\n        state: directory\n\n    - name: unarchive the client bundle\n      unarchive:\n        src: /tmp/bundle.zip\n        dest: ~/certs.{{ ucp_username }}\n        force: yes\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "fc0f0c8493cfd25e3b74c1db31296d10cd970dc9", "filename": "roles/user-management/manage-atlassian-users/tasks/create_atlassian_groups.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Create Group\n  uri:\n    url: '{{ atlassian_url }}/rest/api/2/group'\n    method: POST\n    status_code: [201, 400]\n    user: '{{ atlassian_username }}'\n    password: '{{ atlassian_password }}'\n    force_basic_auth: yes\n    body_format: json\n    body: \"{'name': '{{ group }}' }\"\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "201739739fb88fac04df79b29d398f6bb0b43675", "filename": "roles/suricata/meta/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\ngalaxy_info:\n  author: RockNSM Contributors\n  description: A role to deploy Suricata to a RockNSM sensor\n  company: RockNSM Foundation\n\n  license: Apache\n\n  min_ansible_version: 2.7\n\n  platforms:\n    - name: CentOS\n      versions:\n        - 7\n    - name: RedHat\n      versions:\n        - 7\n\n  galaxy_tags:\n    - suricata\n    - rocknsm\n    - ids\n\ndependencies:\n  - name: Install and configure filebeat\n    role: filebeat\n    vars:\n      filebeat_configs:\n        - { src: 'fb-suricata.yml.j2', dest: 'suricata.yml' }\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ff1d43f3ad7f46254a69892dff6733675b187af3", "filename": "playbooks/osp/inventory/openstack.py", "repository": "redhat-cop/infra-ansible", "decoded_content": "../../../files/openstack.py"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "c0407e0e00d60ec621cb461fdb22b5a0830e0bae", "filename": "roles/docket/defaults/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# defaults file for rocknsm.docket\nrocknsm_conf_dir: /etc/rocknsm\nrocknsm_conf_user: root\nrocknsm_conf_group: root\n\n# How are we going to install docket?\n# Currently supports one of: yumrepo, offline\n# yumrepo sets up the RockNSM repo that hosts docket\n# offline skips repo setup and assumes it's already configured\ndocket_install: yumrepo\ndocket_enable: true\n\n# Application config\ndocket_debug: false\ndocket_testing: false\ndocket_secret: \"{{ lookup('password', '/dev/null chars=letters,digits length=64') }}\"\ndocket_session_cookie: DOCKET_SESSION\ndocket_sendfile: true\ndocket_logger: docket\ndocket_celery_url: redis://localhost:6379\ndocket_spool_dir: /var/spool/docket\ndocket_frontend_dir: /opt/rocknsm/docket/frontend\ndocket_uwsgi_socket: /run/docket/docket.socket\ndocket_no_redis: false\ndocket_long_ago: 24h\n\n# Web server config\ndocket_tls: true\ndocket_user: docket\ndocket_group: docket\n\n# An empty string defaults to all interfaces on IPv4\ndocket_listen_ip: \"0.0.0.0\"\ndocket_listen_port: \"{{ 8443 if docket_tls else 8080 }}\"\ndocket_host: \"{{ hostvars[groups['docket'][0]]['ansible_hostname'] }}\"\n\ndocket_web_server: lighttpd\ndocket_web_pemfile: \"/etc/pki/tls/private/lighttpd_docket.pem\"\ndocket_web_dhparams: \"/etc/pki/tls/misc/lighttpd_dh.pem\"\ndocket_web_server_name: \"{{ ansible_fqdn }}\"\ndocket_web_user: lighttpd\ndocket_url_apppath: /app/docket\ndocket_url_resultspath: /results\ndocket_url_pattern: \"(^/results/|^/app/docket/)\"\n\n# Vars to generate keys/certs\ndocket_x509_dir: /etc/pki/docket\ndocket_x509_key: \"{{ docket_x509_dir }}/docket_{{ hostvars[groups['docket'][0]]['ansible_default_ipv4']['address'] }}_key.pem\"\ndocket_x509_cn: \"{{ docket_host }}-docket\"\ndocket_x509_o: Stenographer\ndocket_x509_c: XX\ndocket_x509_user: root\ndocket_x509_group: docket\n\n# These should be overridden by host-specific vars\nsteno_host: \"{{ hostvars[groups['stenographer'][0]]['ansible_default_ipv4']['address'] }}\"\nsteno_sensor: \"{{ hostvars[groups['stenographer'][0]]['ansible_hostname'] }}\"\nsteno_port: 1234\nsteno_certs_dir: /etc/stenographer/certs\nsteno_ca_cert: \"{{ steno_certs_dir }}/ca_cert.pem\"\nsteno_ca_key: \"{{ steno_certs_dir }}/ca_key.pem\"\n\n# This is used to generate the config for docket on\n# where to connect and how to authenticate\ndocket_steno_instances:\n  - { host: \"{{ steno_host }}\", sensor: \"{{ steno_sensor }}\", port: \"{{ steno_port }}\", key: \"{{ docket_x509_key }}\", cert: \"{{ docket_x509_dir }}/docket-{{ docket_host }}_sensor-{{ steno_sensor }}_cert.pem\", ca: \"{{ docket_x509_dir }}/{{ steno_sensor }}_ca_cert.pem\" }\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ffb8e14c96bd59cf607693735bf79f275d09908a", "filename": "roles/config-redis/tasks/install_containerized.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Configure Storage Directory\n  file:\n    state: directory\n    owner: root\n    group: root\n    mode: g+rw\n    path: \"{{ redis_storage_dir }}\"\n  notify: \"Restart Redis Service\"\n\n- name: Configure systemd environment files\n  template:\n    src: \"{{ redis_name }}.j2\"\n    dest: \"{{ systemd_environmentfile_dir}}/{{ redis_name }}\"\n  notify: \"Restart Redis Service\"\n\n- name: Configure systemd unit files\n  template:\n    src: \"{{ redis_service }}.j2\"\n    dest: \"{{ systemd_service_dir}}/{{ redis_service }}\"\n  notify: \"Restart Redis Service\"\n\n- name: Include firewall tasks\n  include_tasks: firewall.yml"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "28dd21fbc364a3e144178c7b2996562b425b9bc4", "filename": "archive/roles/cicd/tasks/java.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n  \n- name: Install Java\n  yum:\n    enablerepo: rhel-7-server-thirdparty-oracle-java-rpms\n    name: java-1.8.0-oracle-devel\n    state: present\n  tags: java\n  \n- name: Determine Java Home Location\n  shell: alternatives --list | grep jre_oracle | awk '{ print $3 }'\n  register: java_root\n  failed_when: java_root == \"\"\n  tags: java\n\n\n# TODO: Configure Custom Certificates \n "}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "12bdfe91a3dd80de6a92db1dccebf34937c355d8", "filename": "playbooks/win_script_rebuild.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n# This playbook is designed to help when modifying the Windows script template\n# in roles/vpn/templates/client_windows.ps1.j2\n# It rebuilds the client_USER.ps1 scripts for each user defined in config.cfg,\n# without redeploying users or opening an SSH connection to the Algo server at\n# all.\n#\n# This playbook is _not_ part of a normal Algo deployment.\n# It is only intended to speed up development of the client_USER.ps1 Windows\n# Algo install scripts.\n#\n# REQUIREMENTS\n# - Algo must have been deployed once\n# - Windows users must have been enabled at deployment time\n# - All users defined in config.cfg must not have changed\n# - Only one Algo deployment exists in the configs/ directory\n# - There must be exactly one subfolder in the configs/ directory:\n#   the folder named after the IP of the algo server\n\n- hosts: localhost\n  gather_facts: False\n  tags: always\n  vars_files:\n    - ../config.cfg\n\n  tasks:\n\n  - name: Get config subdir\n    shell: find ../configs/* -maxdepth 0 -type d | sed 's/.*\\///'\n    register: config_subdir_result\n  - fail:\n      msg:\n        - \"Found wrong number of config subdirs... stdout:\"\n        - \"{{ config_subdir_result.split('\\n') }}\"\n    when: config_subdir_result.stdout.split('\\n') | length != 1\n  - set_fact:\n      IP_subject_alt_name: \"{{ config_subdir_result.stdout }}\"\n  - debug:\n      var: IP_subject_alt_name\n\n  - name: Register p12 PayloadContent\n    shell: cat private/{{ item }}.p12 | base64\n    register:  PayloadContent\n    args:\n      chdir: \"../configs/{{ IP_subject_alt_name }}/pki/\"\n    with_items: \"{{ users }}\"\n\n  - name: Set facts for mobileconfigs\n    set_fact:\n      proxy_enabled: false\n      PayloadContentCA: \"{{ lookup('file' , '../configs/{{ IP_subject_alt_name }}/pki/cacert.pem')|b64encode }}\"\n\n  - name: Build the windows client powershell script\n    template:\n      src: ../roles/vpn/templates/client_windows.ps1.j2\n      dest: ../configs/{{ IP_subject_alt_name }}/windows_{{ item.0 }}.ps1\n      mode: 0600\n    with_together:\n      - \"{{ users }}\"\n      - \"{{ PayloadContent.results }}\"\n\n  - name: List windows client powershell scripts\n    debug:\n      msg: \"configs/{{ IP_subject_alt_name }}/windows_{{ item }}.ps1\"\n    with_items:\n      - \"{{ users }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "27b0666c55f05d27ab4e764be4f84f7321388d13", "filename": "roles/config-satellite/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\ndefault_manifest_file_path: manifest.zip\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "8ff9012f61ad92303242df8c4318173aa51c0603", "filename": "roles/manage-jira/tasks/create_project_category.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Create Jira Project Category\n  uri:\n    url: \"{{ atlassian.jira.url }}/rest/api/2/projectCategory\"\n    method: POST\n    user: \"{{ atlassian.jira.username }}\"\n    password: \"{{ atlassian.jira.password }}\"\n    return_content: yes\n    force_basic_auth: yes\n    body_format: json\n    header:\n      - Accept: 'application/json'\n      - Content-Type: 'application/json'\n    body: \"{ 'name': '{{ atlassian.jira.project.category_name }}',\n              'description': '{{ atlassian.jira.project.category_description }}' }\"\n    status_code: 201\n  register: category\n\n- name: Set fact for Category ID\n  set_fact:\n    CategoryID: \"{{ category.json.id }}\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "741221ad05498d84100cf863ef451008128c8a76", "filename": "playbooks/openshift/provision-openstack.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- hosts: localhost\n  pre_tasks:\n  - import_tasks: pre-tasks.yml\n  roles:\n  - role: openshift-ansible-contrib/roles/openstack-stack\n    stack_name: \"{{ env_id }}.{{ dns_domain }}\"\n    subnet_prefix: \"{{ openstack_subnet_prefix }}\"\n    ssh_public_key: \"{{ openstack_ssh_public_key }}\"\n    openstack_image: \"{{ openstack_default_image_name }}\"\n    lb_flavor: \"{{ openstack_lb_flavor | default('m1.small') }}\"\n    etcd_flavor: \"{{ openstack_etcd_flavor | default( openstack_default_flavor ) }}\"\n    master_flavor: \"{{ openstack_master_flavor | default( openstack_default_flavor ) }}\"\n    node_flavor: \"{{ openstack_node_flavor | default( openstack_default_flavor ) }}\"\n    infra_flavor: \"{{ openstack_infra_flavor | default( openstack_default_flavor ) }}\"\n    dns_flavor: \"{{ openstack_dns_flavor | default('m1.small') }}\"\n    external_network: \"{{ openstack_external_network_name }}\"\n    num_etcd: \"{{ openstack_num_etcd | default(0) }}\"\n    num_masters: \"{{ openstack_num_masters }}\"\n    num_nodes: \"{{ openstack_num_nodes }}\"\n    num_infra: \"{{ openstack_num_infra }}\"\n    num_dns: \"{{ openstack_num_dns | default(0) }}\"\n    master_volume_size: \"{{ docker_volume_size }}\"\n    node_volume_size: \"{{ docker_volume_size }}\"\n    infra_volume_size: \"{{ docker_volume_size }}\"\n\n- name: Refresh Server inventory\n  hosts: localhost\n  connection: local\n  gather_facts: False\n  tasks:\n  - meta: refresh_inventory\n\n- hosts: cluster_hosts\n  gather_facts: false\n  tasks:\n  - name: Debug hostvar\n    debug:\n      msg: \"{{ hostvars[inventory_hostname] }}\"\n      verbosity: 2\n  - name: waiting for server to come back\n    local_action: wait_for host={{ hostvars[inventory_hostname]['ansible_ssh_host'] }} port=22 delay=30 timeout=300\n    become: false\n\n- hosts: cluster_hosts\n  tasks:\n  - name: Copy the 'private_v4' address out of the 'openstack' section to make it generic\n    set_fact:\n      private_v4: \"{{ openstack.private_v4 }}\"\n  - name: Copy the 'public_v4' address out of the 'openstack' section to make it generic\n    set_fact:\n      public_v4: \"{{ openstack.public_v4 }}\"\n\n- import_playbook: post-provision-openstack.yml\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "350abed378b37f60a55da399ae5d6ed05ee4d19d", "filename": "playbooks/cdi.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- hosts: localhost\n  connection: local\n  gather_facts: False\n  # unset http_proxy. required for running in the CI\n  environment:\n    http_proxy: \"\"\n  roles:\n    - role: \"cdi\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "947e4646c3744ae0e2e135de7f812dfb57477103", "filename": "roles/config-repo-server/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- include_role:\n    name: config-httpd\n\n- import_tasks: mount-iso.yml\n\n"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "d4830e1d686f31a604c36ae8cac0a9b274f4c82a", "filename": "tasks/fetch/s3.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Download artifact from s3\"\n  aws_s3:\n    bucket: \"{{ transport_s3_bucket }}\"\n    object: \"{{ transport_s3_path }}\"\n    dest: \"{{ download_path }}/{{ transport_s3_path|basename }}\"\n    aws_access_key: \"{{ transport_s3_aws_access_key }}\"\n    aws_secret_key: \"{{ transport_s3_aws_secret_key }}\"\n    mode: get\n  retries: 5\n  delegate_to: \"localhost\"\n  connection: \"local\"\n\n- name: \"Downloaded artifact\"\n  set_fact:\n    oracle_artifact: \"{{ download_path }}/{{ transport_s3_path|basename }}\"\n    oracle_artifact_basename: \"{{ transport_s3_path|basename }}\"\n\n- name: \"Getting java version variables\"\n  set_fact:\n    java_package: \"{{ oracle_artifact_basename.split('-')[0] }}\"\n    java_major_version: \"{{ oracle_artifact_basename.split('-')[1].split('u')[0] }}\"\n    java_minor_version: \"{{ oracle_artifact_basename.split('-')[1].split('u')[1] }}\"\n    java_arch: \"{{ oracle_artifact_basename.split('-')[3] }}\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "e049b4e15f57803a90c77d08ce1cef5bf2b6393f", "filename": "playbooks/nagios/README.md", "repository": "redhat-cop/casl-ansible", "decoded_content": "# Nagios Example Run\n\nThe `nagios-target` and `nagios-server` roles can be used to setup a complete Nagios monitoring for any environment. The `nagios-target` role will prepare the targets with the correct monitoring configuratoin for use with the NRPE (Nagios Remote Plugin Executor). The target role will selectively enable the correct monitoring plugins (and correctly configured) per targets.\n\nBelow is an example inventory file for setting up the Nagios server and targets. Before executing, ensure that access to the target servers is enabled - i.e.: SSH key login for root. \n\nExample run of the playbook:\n> ansible-playbook -i \\<path_to_inventory\\> setup_nagios.yml\n\n\n```\n[all:vars]\nansible_ssh_user=root\n\n# Infrastructure Server\n[infra]\ndns.infra.example.com nagios_services=dns\nnfs.infra.example.com nagios_services=nfs\n\n[infra:vars]\nhostgroup_name=infra-servers\nhostgroup_alias=Infrastructure Servers\n\n# OpenShift 3 environment\n[openshift]\nmaster.openshift.example.com nagios_services=docker,openshift-master,openshift-node\nnode1.openshift.example.com nagios_services=docker,openshift-node\nnode2.openshift.example.com nagios_services=docker,openshift-node\nnode3.openshift.example.com nagios_services=docker,openshift-node\ndns.openshift.example.com nagios_services=dns\nnfs.openshift.example.com nagios_services=nfs\n\n[openshift:vars]\nhostgroup_name=openshift-cluster\nhostgroup_alias=OpenShift Cluster (Environment #2)\n\n\n###############################################################################\n# The 'nagios-targets' definition is required\n[nagios-targets:children]\ninfra\nopenshift\n\n# The 'nagios-servers' definition is required\n[nagios-servers]\nnagios.example.com\n```\n\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "79975ce10bfe111053aedf85eefa68be689576bd", "filename": "roles/config-docker-compose/tasks/docker-compose.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: \"Install additional packages for Docker Compose\"\n  package:\n    name: \"{{ item }}\"\n    state: latest\n  with_items:\n  - docker-compose\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "57174bdd5ab6ae279d6c9ab53ed6edf435f2944c", "filename": "reference-architecture/aws-ansible/playbooks/roles/inventory-file-creation/files/inventory", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "[OSEv3:children]\nmasters\netcd\nnodes\n\n[OSEv3:vars]\ndebug_level=2\nopenshift_debug_level=2\nopenshift_node_debug_level=2\nopenshift_master_debug_level=2\nopenshift_master_access_token_max_seconds=2419200\nopenshift_master_api_port={{ console_port }}\nopenshift_master_console_port={{ console_port }}\nosm_cluster_network_cidr=172.16.0.0/16\nopenshift_registry_selector=\"role=infra\"\nopenshift_router_selector=\"role=infra\"\nopenshift_hosted_router_replicas=3\nopenshift_hosted_registry_replicas=3\nopenshift_master_cluster_method=native\nopenshift_node_local_quota_per_fsgroup=512Mi\nopenshift_cloudprovider_kind=aws\nopenshift_master_cluster_hostname=internal-openshift-master.{{ public_hosted_zone }}\nopenshift_master_cluster_public_hostname=openshift-master.{{ public_hosted_zone }}\nosm_default_subdomain={{ wildcard_zone }}\nopenshift_master_default_subdomain={{ wildcard_zone }}\nosm_default_node_selector=\"role=app\"\ndeployment_type={{ deployment_type | default('openshift-enterprise') }}\nos_sdn_network_plugin_name={{ openshift_sdn | default('redhat/openshift-ovs-subnet') }}\nopenshift_master_identity_providers=[{'name': 'github', 'challenge': 'false', 'login': 'true', 'kind': 'GitHubIdentityProvider', 'mapping_method': 'claim', 'clientID': '{{ github_client_id }}', 'clientSecret': '{{ github_client_secret }}', 'organizations': {'['{{ github_organization }}']'}}]\nosm_use_cockpit=true\ncontainerized={{ containerized | default('false') }}\nopenshift_hosted_registry_storage_kind=object\nopenshift_hosted_registry_storage_provider=s3\nopenshift_hosted_registry_storage_s3_accesskey={{ hostvars['localhost']['s3user_id'] }}\nopenshift_hosted_registry_storage_s3_secretkey={{ hostvars['localhost']['s3user_secret'] }}\nopenshift_hosted_registry_storage_s3_bucket={{ hostvars['localhost']['s3_bucket_name'] }}\nopenshift_hosted_registry_storage_s3_region={{ hostvars['localhost']['region'] }}\nopenshift_hosted_registry_storage_s3_chunksize=26214400\nopenshift_hosted_registry_storage_s3_rootdirectory=/registry\nopenshift_hosted_registry_pullthrough=true\nopenshift_hosted_registry_acceptschema2=true\nopenshift_hosted_registry_enforcequota=true\n\n[masters]\n\n[etcd]\n\n[nodes]\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "d0beae0ad0e29317ba3c83fd8eff53f3b6c5d3eb", "filename": "roles/common/tasks/unattended-upgrades.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Install unattended-upgrades\n  apt:\n    name: unattended-upgrades\n    state: latest\n\n- name: Configure unattended-upgrades\n  template:\n    src: 50unattended-upgrades.j2\n    dest: /etc/apt/apt.conf.d/50unattended-upgrades\n    owner: root\n    group: root\n    mode: 0644\n\n- name: Periodic upgrades configured\n  template:\n    src: 10periodic.j2\n    dest: /etc/apt/apt.conf.d/10periodic\n    owner: root\n    group: root\n    mode: 0644\n\n- name: Unattended reboots configured\n  template:\n    src: 60unattended-reboot.j2\n    dest: /etc/apt/apt.conf.d/60unattended-reboot\n    owner: root\n    group: root\n    mode: 0644\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "906d425322371df3b71c25b1c6465fbaffc026eb", "filename": "tasks/Win32NT/install/chocolatey.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Check choco\n  win_chocolatey:\n    name: chocolatey\n    state: present\n\n- name: 'Install {{ choco_java_package }} from chocolatey'\n  win_chocolatey:\n    name: '{{ choco_java_package }}'\n  register: choco_install\n  retries: 15\n  delay: 5\n  until: choco_install is succeeded\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "9dcd122d997788a0d0a9aea7244f7aee308042ef", "filename": "roles/logging/handlers/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: restart rsyslog\n  service: name=rsyslog state=restarted\n\n- name: restart auditd\n  service: name=auditd state=restarted\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "d84900650e82a01c678413f3120636add2fb6f03", "filename": "roles/2-common/tasks/iptables.yml", "repository": "iiab/iiab", "decoded_content": "- name: Disable firewalld service (OS's other than debuntu)\n  service:\n    name: firewalld\n    enabled: no\n  when: not is_debuntu\n\n- name: Use larger hammer to disable firewalld -- 2 symbolic links involved (OS's other than debuntu)\n  shell: \"systemctl disable firewalld.service\"\n  when: not is_debuntu\n\n- name: Mask firewalld service (OS's other than debuntu)\n  shell: 'systemctl mask firewalld'\n  ignore_errors: yes\n  when: not installing and not is_debuntu\n\n- name: Stop firewalld service (OS's other than debuntu)\n  service:\n    name: firewalld\n    state: stopped\n  ignore_errors: yes\n  when: not installing and not is_debuntu\n\n- name: Remove iptables.service file from /etc\n  file:\n    path: /etc/systemd/system/iptables.service\n    state: absent\n\n- name: Remove iptables-xs.service file from /etc\n  file:\n    path: /etc/systemd/system/iptables-xs.service\n    state: absent\n\n- name: Install iptables service package (debuntu)\n  package:\n    name: iptables-persistent\n    state: present\n  when: is_debuntu\n  tags:\n    - download\n\n- name: Install iptables service package (OS's other than debuntu)\n  package:\n    name: iptables-services\n    state: present\n  when: not is_debuntu\n  tags:\n    - download\n\n- name: Install iptables services\n  template:\n    src: \"{{ item.0 }}\"\n    dest: \"{{ item.1 }}\"\n    owner: root\n    group: root\n    mode: \"{{ item.2 }}\"\n  with_items:\n   - { 0: 'iptables-config', 1: '/etc/sysconfig/iptables-config', 2: '0644' }\n\n- name: Install Debian config (debuntu)\n  template:\n    src: iptables\n    dest: /etc/network/if-pre-up.d/iptables\n    mode: 0755\n  when: is_debuntu\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "ec017fbd958ddbe48c01b4a6e1e6e0fbcbabe7bc", "filename": "tasks/Linux/fetch/repositories.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: 'Install {{ java_distribution }} from repositories'\n  debug:\n    msg: 'Install {{ java_distribution }} from repositories'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "594430a0b747e5d7ec429146dc1b59aa569140d4", "filename": "roles/dns/manage-dns-records/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: localhost\n  roles:\n  - role: dns/manage-dns-records\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "dd667de26f28e08c443f0a1b935218c028828af1", "filename": "dev/playbooks/config_dummy_vms_for_docker_volumes_backup.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: ucp_main\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  environment: \"{{ env }}\"\n\n  vars:\n    powercli_script: '/tmp/vols.ps1'\n\n  tasks:\n    - name: Create dummy VMs\n      local_action:\n        module: vmware_guest\n        hostname: \"{{ vcenter_hostname }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        validate_certs: no\n        esxi_hostname: \"{{ esxi_host }}\"\n        datacenter: \"{{ datacenter }}\"\n        folder: \"{{ folder_name }}\"\n        name: \"{{ dummy_vm_prefix }}-{{ item }}\"\n        guest_id: \"dosGuest\"\n        state: poweredoff\n        hardware:\n          memory_mb: \"128\"\n          num_cpus: \"1\"\n        disk:\n        - size_gb: 1\n          datastore: \"{{ item }}\"\n          type: thin\n      with_items: \"{{ datastores }}\"\n\n    - name: Generate powercli script\n      template: src=../templates/powercli_script.j2 dest={{ powercli_script }}\n\n    - name: Run powercli on temporary docker container\n      command: docker run --rm --entrypoint='/usr/bin/powershell' -v /tmp:/tmp vmware/powerclicore {{ powercli_script }}\n\n    - name: Delete powercli script from docker host\n      file: state=absent path={{ powercli_script }}\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "12b3f19d0f403a8747dd5c59e1e9d17c809c19ee", "filename": "roles/cloud-ec2/defaults/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\nami_search_encrypted: omit\nencrypted: \"{{ cloud_providers.ec2.encrypted }}\"\nec2_vpc_nets:\n  cidr_block: 172.16.0.0/16\n  subnet_cidr: 172.16.254.0/23\nec2_venv: \"{{ playbook_dir }}/configs/.venvs/aws\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "20df1cf54c91f12ada571584a10e4e5606df7b5c", "filename": "roles/disconnected-git/tasks/main.yaml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: Install Packages\n  action: \"{{ ansible_pkg_mgr }} name={{ item }} state=present\"\n  with_items:\n    - git\n    - httpd\n    - firewalld\n    - libsemanage-python\n    \n- name: Create Git User\n  user: name=\"{{ git_user }}\"\n\n- name: Create Git User Authorized Keys\n  authorized_key:\n    user: \"{{ git_user}}\"\n    key: \"{{ item }}\"\n  with_items:\n    - \"{{ git_user_authorized_keys }}\"\n  \n- name: Create OSE Git Repository Content Home\n  file:\n    path: \"{{ item }}\"\n    state: directory\n    owner: \"{{ git_user }}\"\n    group: \"{{ git_user }}\"\n  with_items:\n    - \"{{ git_repo_home }}\"\n    - \"{{ ose_git_repo_home }}\"\n\n- name: Clone OSE Examples\n  git:\n    repo: \"{{ item }}\"\n    bare: yes\n    dest: \"{{ ose_git_repo_home }}/{{ item | basename }}\"\n  with_items:\n    - \"{{ ose_example_repos }}\"\n  tags: test\n  become: yes\n  become_user: \"{{ git_user }}\"\n  \n- name: Configure Git HTTP Configuration\n  template:\n    src: \"{{ role_path }}/templates/git.conf.j2\"\n    dest: \"/etc/httpd/conf.d/git.conf\"\n    owner: root\n    group: root\n  notify: restart httpd\n\n- name: Enable Services\n  service: name={{ item }} enabled=yes state=started   \n  with_items:\n    - firewalld\n    - httpd \n    \n- name: HTTPD SELinux Configurations\n  seboolean: \n    name: httpd_can_network_connect\n    state: yes\n    persistent: yes\n\n- name: Open Firewall for HTTPD\n  firewalld: port=80/tcp permanent=yes state=enabled immediate=yes zone=public\n  \n\n\n  "}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "681a0302c4989bedbb27c37bd9f0cd1be5ae7736", "filename": "roles/manage-sshd-config/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: 'sshd-update.yml'\n\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "e145b9c5a1817963f4affa16c47313e811f3a05a", "filename": "tasks/main.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Load a system variables file based on distribution or OS family\n  include_vars: '{{ distribution }}'\n  with_first_found:\n    - '{{ ansible_os_family }}.yml'\n    - default.yml\n  loop_control:\n    loop_var: distribution\n\n- name: Set base variables based on java distribution\n  include_vars: 'java_distro_configs/{{ java_distribution }}_vars.yml'\n\n- name: 'Fetch oracle artifact with {{ transport }} transport'\n  include_tasks: '{{ transport_driver }}'\n  with_first_found:\n    - '{{ ansible_system }}/fetch/{{ transport }}.yml'\n    - unknown-transport.yml\n  loop_control:\n    loop_var: transport_driver\n\n- name: Set parse variables based on java distribution\n  include_vars: java_parts.yml\n  when:\n    - transport != 'repositories'\n    - java_binary_type != 'chocolatey'\n\n- name: Choose platform based task\n  include_tasks: '{{ platform }}'\n  with_first_found:\n    - '{{ ansible_system }}/system.yml'\n    - not-supported.yml\n  loop_control:\n    loop_var: platform\n\n- name: Apply security policy patch\n  include_tasks: '{{ platform }}'\n  with_first_found:\n    - '{{ ansible_system }}/security_policy.yml'\n    - not-supported.yml\n  loop_control:\n    loop_var: platform\n  when:\n    - java_unlimited_policy_enabled\n    - java_distribution == 'oracle_java'\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "13a04cbfb81bbff451a36d0550e7f94edd9e92ad", "filename": "roles/sugar-stats/tasks/statistics-consolidation.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install python-pip package\n  package: name=python-pip\n           state=present\n\n- name: Install statistics-consolidation with pip\n  pip: name=stats-consolidation version=2.1.2\n  when: internet_available\n\n- name: Install required libraries\n  package: name={{ item }}\n           state=present\n  with_items:\n    - rrdtool-python\n    - python-sqlalchemy\n    - python-psycopg2\n\n- name: Enable postgresl access by md5 method\n  lineinfile: backup=yes\n              dest=/library/pgsql-iiab/pg_hba.conf\n              regexp=\"^host\\s+statsconso\"\n              line=\"host     statsconso     statsconso     samehost     md5\"\n              state=present\n              insertafter=\"^# IPv4 local connections\"\n              owner=postgres\n              group=postgres\n\n- name: Restart postgresql service\n  service: name=postgresql-iiab\n           state=restarted\n\n- name: Create postgres user\n  postgresql_user: user=statsconso password=statsconso\n  become: yes\n  become_user: postgres\n\n- name: Create postgres database\n  postgresql_db: db=statsconso owner=statsconso\n  sudo: yes\n  sudo_user: postgres\n\n- name: Install conf file\n  template: backup=yes\n            src=statistics-consolidation/stats-consolidation.conf\n            dest=/etc/stats-consolidation.conf\n            owner=root\n            group=root\n            mode=0644\n\n- name: Create log directory\n  file: path=/var/log/statistics-consolidation\n        state=directory\n        group=sugar-stats\n        owner=sugar-stats\n        mode=0755\n\n- name: Enable logrotate\n  template: backup=yes\n            src=statistics-consolidation/stats-consolidation.logrotate\n            dest=/etc/logrotate.d/stats-consolidation\n            group=root\n            owner=root\n            mode=0644\n\n- name: Install cron file\n  template: backup=yes\n            src=statistics-consolidation/stats-consolidation.cron\n            dest=/etc/cron.d/stats-consolidation\n            owner=root\n            group=root\n            mode=0644\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "8d0c1b01a2db9a42e1a1a94a18a4adebd8543525", "filename": "roles/0-init/tasks/fl.yml", "repository": "iiab/iiab", "decoded_content": "- name: Create various library directories\n  file:\n    path: \"{{ item }}\"\n    owner: root\n    group: root\n    mode: 0755\n    state: directory\n  with_items:\n    - /etc/iiab\n    - \"{{ yum_packages_dir }}\"\n    - \"{{ pip_packages_dir }}\"\n    - \"{{ downloads_dir }}\"\n    - /library/downloads/zims\n    - /library/downloads/rachel\n    - /library/working/zims\n    - /library/working/rachel\n    - \"{{ iiab_zim_path }}/content\"\n    - \"{{ iiab_zim_path }}/index\"\n    - \"{{ doc_root }}/local_content\"\n    - \"{{ doc_root }}/modules\"\n    - \"{{ doc_root }}/common/css\"\n    - \"{{ doc_root }}/common/js\"\n    - \"{{ doc_root }}/common/fonts\"\n    - \"{{ doc_root }}/common/html\"\n    - \"{{ doc_root }}/common/images\"\n    - \"{{ doc_root }}/common/assets\"\n    - \"{{ doc_root }}/common/services\"\n    - /etc/sysconfig/olpc-scripts/\n    - /etc/sysconfig/olpc-scripts/setup.d/installed/\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "3fc8e213463d90eb5bbf497b05d02bd040b57133", "filename": "tasks/Linux/install/sapjvm_tarball.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Check that the java_folder exists\n  stat:\n    path: '{{ java_path }}/{{ java_folder }}/bin'\n  register: java_folder_bin\n\n- name: 'Install java {{ java_full_version }} from tarball'\n  block:\n  - name: Mkdir for java installation\n    file:\n      path: '{{ java_path }}/{{ java_folder }}'\n      state: directory\n\n  - name: Create temporary directory\n    tempfile:\n      state: directory\n    register: temp_dir\n\n  - name: Unarchive to temporary directory\n    unarchive:\n      src: '{{ java_artifact }}'\n      dest: '{{ temp_dir.path }}'\n      remote_src: true\n      list_files: true\n    register: unarchived_folder\n\n  - name: Sync from temporary directory\n    synchronize:\n      src: '{{ temp_dir.path }}/{{ unarchived_folder.files[0].split(\"/\")[0] }}/'\n      dest: '{{ java_path }}/{{ java_folder }}'\n      recursive: true\n      archive: false\n      checksum: true\n    delegate_to: '{{ inventory_hostname }}'\n\n  - name: Set permissions for java installation\n    file:\n      path: '{{ java_path }}/{{ java_folder }}'\n      recurse: true\n      owner: root\n      group: root\n      mode: 0755\n  when: not java_folder_bin.stat.exists\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "fdce04291c842b80d48e171f1eaa8ba8886e3305", "filename": "tasks/replication/slave/ssh.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n- name: SHELL | Create SSH key if needed on slave\n  shell: \"ssh-keygen -b 2048 -t rsa -f {{ ansible_env.HOME }}/.ssh/id_rsa -q -N ''\"\n  args:\n    creates: \"{{ ansible_env.HOME }}/.ssh/id_rsa\"\n\n- name: COMMAND | Get pub key\n  command: cat {{ ansible_env.HOME }}/.ssh/id_rsa.pub\n  register: pub_key\n  changed_when: false\n\n- name: AUTHORIZED_KEY | Auth slave to backup host\n  authorized_key:\n    user: \"{{ mariadb_backup_user }}\"\n    state: present\n    key: \"{{ pub_key.stdout }}\"\n  delegate_to: \"{{ mariadb_slave_import_from }}\"\n  become: yes\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "cf4b38278e38140e4114df53f42637a614a5d367", "filename": "roles/network/tasks/hosts.yml", "repository": "iiab/iiab", "decoded_content": "#TODO: Use vars instead of hardcoded values\n- name: Remove fqdn in /etc/hosts without LAN\n  lineinfile: dest=/etc/hosts\n              regexp='^172\\.18\\.96\\.1'\n              state=absent\n  when: iiab_lan_iface == \"none\" and not installing\n\n- name: Configure fqdn in /etc/hosts with LAN\n  lineinfile: dest=/etc/hosts\n              regexp='^172\\.18\\.96\\.1'\n              line='172.18.96.1            {{ iiab_hostname }}.{{ iiab_domain }} {{ iiab_hostname }} box'\n              state=present\n  when: iiab_lan_iface != \"none\" and not installing\n\n- name: Configure fqdn in /etc/hosts appliance mode\n  lineinfile: dest=/etc/hosts\n              regexp='^127\\.0\\.0\\.1'\n              line='127.0.0.1            localhost.localdomain localhost {{ iiab_hostname }}.{{ iiab_domain }} {{ iiab_hostname }} box '\n              owner=root\n              group=root\n              mode=0644\n  when: iiab_lan_iface == \"none\" and not installing\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "96ca66c4804a16c1443a92d7de05d6c904e8dc9e", "filename": "roles/virt-install/tasks/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - virt-install\n  - httpd\n  - libselinux-python\n\n- name: 'Enable and start libvirtd'\n  service:\n    name: libvirtd\n    enabled: yes\n    state: started\n\n- name: 'Enable and start httpd'\n  service:\n    name: httpd\n    enabled: no\n    state: started\n\n- name: 'Enable and start firewalld'\n  service:\n    name: firewalld\n    enabled: yes\n    state: started\n\n- name: 'Ensure firewalld is open for httpd traffic'\n  firewalld:\n    service: http\n    state: enabled\n    permanent: no\n    immediate: yes\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "6d6ba89960ce053ff206a37dbbe229c38fd3356d", "filename": "roles/load-balancers/manage-haproxy/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\ntemp_new_file: '/etc/haproxy/haproxy.cfg.new'\nlb_https_backends: {}\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "821a0f30ec36d185bf6377e5165188aca8eb4750", "filename": "playbooks/aws/openshift-cluster/config.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: no\n  tasks:\n  - include_vars: vars.yml\n  - include_vars: cluster_hosts.yml\n  - add_host:\n      name: \"{{ item }}\"\n      groups: l_oo_all_hosts\n    with_items: \"{{ g_all_hosts | default([]) }}\"\n\n- hosts: l_oo_all_hosts\n  gather_facts: no\n  tasks:\n  - include_vars: vars.yml\n  - include_vars: cluster_hosts.yml\n\n- include: ../../common/openshift-cluster/config.yml\n  vars:\n    g_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n    g_sudo: \"{{ deployment_vars[deployment_type].become }}\"\n    g_nodeonmaster: true\n    openshift_cluster_id: \"{{ cluster_id }}\"\n    openshift_debug_level: \"{{ debug_level }}\"\n    openshift_deployment_type: \"{{ deployment_type }}\"\n    openshift_public_hostname: \"{{ ec2_ip_address }}\"\n    openshift_hosted_registry_selector: 'type=infra'\n    openshift_hosted_router_selector: 'type=infra'\n    openshift_node_labels:\n      region: \"{{ deployment_vars[deployment_type].region }}\"\n      type: \"{{ hostvars[inventory_hostname]['ec2_tag_sub-host-type'] }}\"\n    openshift_master_cluster_method: 'native'\n    openshift_use_openshift_sdn: \"{{ lookup('oo_option', 'use_openshift_sdn') }}\"\n    os_sdn_network_plugin_name: \"{{ lookup('oo_option', 'sdn_network_plugin_name') }}\"\n    openshift_use_flannel: \"{{ lookup('oo_option', 'use_flannel') }}\"\n    openshift_use_calico: \"{{ lookup('oo_option', 'use_calico') }}\"\n    openshift_use_fluentd: \"{{ lookup('oo_option', 'use_fluentd') }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e517e4f3b2bca977494bbfbadded1733d4501393", "filename": "roles/config-software-src/tasks/mount-software.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Mount the software repository\" \n  mount:\n    path: \"{{ iso_repo_dir }}\"\n    src: \"{{ iso_repo_nfs }}\"\n    fstype: nfs\n    state: mounted\n\n\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "01d495c7da50b339562c9b7b6a48b7c746886fa5", "filename": "playbooks/cluster/kubernetes/vars/default_vars.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\nkubernetes_token: \"abcdef.1234567890123456\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9b78c918fc809e1e14a485ece767e6d1b00bd208", "filename": "roles/osp/admin-network/tasks/manage-routers.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Set up router(s)\"\n  os_router:\n    cloud: \"{{ item.cloud | default(osp_default_cloud) | default(omit) }}\"\n    project: \"{{ item.project | default(omit) }}\"\n    state: \"{{ item.state | default(osp_resource_state) | default('present') }}\"\n    name: \"{{ item.name }}\"\n    network: \"{{ item.external_gateway }}\"\n    interfaces:\n    - \"{{ item.subnet }}\"\n  with_items:\n  - \"{{ osp_routers | default([]) }}\"\n"}, {"commit_sha": "584d564218d6e2e63d3ecca157daf817fe4d533c", "sha": "639e439fd2d82ec886cdb75f534000ad22267f3c", "filename": "handlers/main.yml", "repository": "mikolak-net/ansible-raspi-config", "decoded_content": "---\n- name: apply raspi-config\n  command: raspi-config --apply-os-config\n  # using the Ansible blog post solution:\n  # https://support.ansible.com/hc/en-us/articles/201958037-Reboot-a-server-and-wait-for-it-to-come-back\n- name: reboot\n  command: shutdown -r now\n  async: 0\n  poll: 0\n  ignore_errors: True\n  notify:\n    - wait for reboot\n- name: wait for reboot\n  local_action: wait_for host={{ inventory_hostname }}\n                state=started\n                timeout=30 # doesn't appear to work correctly now, instead a simple delay is imposed - that's fine for now\n  sudo: false\n- name: remove default user\n  when: \"raspi_config_replace_user['name'] != raspi_config_auth_test_username\"\n  user: name={{raspi_config_auth_test_username}} state=absent force=yes\n  async: 0\n  poll: 0\n  ignore_errors: True"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8dd11bd89dadcd410bdaf32dd7ce68405f54cc16", "filename": "roles/scm/add-webhooks-github/tasks/add-webhook.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n# Helper for main.yml\n\n- set_fact:\n    payload:\n      name: \"web\"\n      active: \"{{ is_active }}\"\n      events: \"{{ events }}\"\n      config:\n        url: \"{{ url }}\"\n        content_type: \"json\"\n        insecure_ssl: \"1\"\n\n- uri:\n    url: https://api.github.com/repos/{{ owner }}/{{ repo }}/hooks\n    method: POST\n    headers:\n      Authorization: token {{ api_token }}\n    body_format: json\n    body: \"{{ payload | to_json }}\"\n    status_code: 201, 422\n  register: request_output\n  changed_when: request_output.status == 201\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "2727d85eaad0297158daa30970875f98fcee5050", "filename": "reference-architecture/vmware-ansible/playbooks/crs-storage.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- include: prod-ose-crs.yaml\n  tags: ['vms']\n\n- include: crs-node-setup.yaml\n  tags: [ 'node-setup' ]\n\n- include: heketi-setup.yaml\n  tags: [ 'heketi-setup']\n\n- include: heketi-ocp.yaml\n  tags: ['heketi-ocp']\n\n- include: cleanup-crs.yaml\n  tags: ['clean']\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "994c88b3ebf8d93b2b985c6fd3d46786c3a405d3", "filename": "dev/playbooks/install_ucp_nodes.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: ucp\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n   \n  environment: \"{{ env }}\"\n\n  vars:\n      san_fqdns: \"{% for host in groups['ucp']+groups['ucp_lb'] %}--san {{ host }}.{{ domain_name }} {% endfor %}\" \n      san_ips: \"{% for host in groups['ucp']+groups['ucp_lb'] %} --san {{ hostvars[host].ip_addr | ipaddr('address') }} {% endfor %}\" \n      san_all_formatted: \"{{ san_fqdns }} {{ san_ips }}\" \n  \n  tasks:\n    - name: Open required ports for UCP\n      command: firewall-cmd --permanent --zone=public --add-port=443/tcp --add-port=2376/tcp --add-port=2377/tcp --add-port=4789/tcp --add-port=4789/udp --add-port=7946/tcp --add-port=7946/udp --add-port=12376/tcp --add-port=12379/tcp --add-port=12380/tcp --add-port=12381/tcp --add-port=12382/tcp --add-port=12383/tcp --add-port=12384/tcp --add-port=12385/tcp --add-port=12386/tcp --add-port=12387/tcp\n\n    - name: Reload firewalld configuration\n      command: firewall-cmd --reload\n\n    - name: Restart docker service\n      systemd:\n        name: docker\n        state: restarted\n\n    - name: Check if node already belongs to the swarm\n      shell: docker info | grep \"Swarm{{ \":\" }} inactive\" | wc -l\n      register: swarm_inactive\n\n    - name: Copy the license\n      copy: src=\"{{ license_file }}\" dest=\"/tmp/{{ license_file | basename }}\"\n\n    - name: Install swarm leader and first UCP node\n      shell: docker run --rm --name ucp -v /var/run/docker.sock:/var/run/docker.sock docker/ucp:{{ ucp_version }} install --host-address {{ ip_addr | ipaddr('address') }} --admin-username={{ ucp_username }} --admin-password={{ ucp_password }} --license \"$(cat /tmp/{{ license_file | basename }})\" {{ san_all_formatted }}\n      register: output\n      when: inventory_hostname in groups.ucp_main and swarm_inactive.stdout == \"1\"\n\n    - name: Get swarm manager token\n      shell: echo `docker swarm join-token manager` | cut -f2 -d':' | sed 's|\\\\||g'\n      register: manager_token\n      when: inventory_hostname in groups.ucp_main\n\n    - name: Get swarm worker token\n      shell: echo `docker swarm join-token worker` | cut -f2 -d':' | sed 's|\\\\||g'\n      register: worker_token\n      when: inventory_hostname in groups.ucp_main\n\n    - name: Save worker token\n      local_action: copy content=\"token{{ \":\" }} {{ hostvars[groups['ucp_main'][0]]['worker_token']['stdout'] }}\" dest=/tmp/worker_token\n\n    - name: Add additional UCP nodes to the swarm\n      command: \"{{ hostvars[groups['ucp_main'][0]]['manager_token']['stdout'] }}\"\n      when: inventory_hostname not in groups.ucp_main and swarm_inactive.stdout == \"1\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "3d4683c0f50bc3b9bd68082e7df00e9bb31d2b93", "filename": "roles/kalite/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "# The values here are defaults.\n# To override them edit /etc/iiab/local_vars.yml\n\nkalite_install: True\nkalite_enabled: False\n\nkalite_version: \"0.17.5\"\nkalite_repo_url: \"https://github.com/learningequality/ka-lite.git\"\nkalite_requirements: \"https://raw.githubusercontent.com/learningequality/ka-lite/master/requirements.txt\"\n\nkalite_venv: \"/usr/local/kalite/venv\"\nkalite_program: \"{{ kalite_venv }}/bin/kalite\"\nkalite_root: \"/library/ka-lite\"\n\nkalite_server_port: 8008\nkalite_admin_user: Admin\nkalite_admin_password: changeme\n\n# Unused in 2018; but remain as placeholders for Fedora 18 legacy (XO laptops)\nkalite_cron_enabled: False\nkalite_user: kalite\n# obtain a password hash with - python -c 'import crypt; print crypt.crypt(\"<plaintext>\", \"$6$<salt>\")'\nkalite_password_hash: $6$<salt>$KHET0XRRsgAY.wOWyTOI3W7dyDh0ESOr48uI5vtk2xdzsU7aw0TF4ZkNuM34RmHBGMJ1fTCmOyVobo0LOhBlJ/\nkalite_password: kalite\n\n# Unused in 2018\n# kalite_server_name: kalite\n# khan_assessment_install: True\n# khan_assessment_url: \"http://pantry.learningequality.org/downloads/ka-lite/0.16/content/khan_assessment.zip\"\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "a91d851372ffca8a92ee7f12834e511a14049025", "filename": "tasks/Win32NT/fetch/security-fetch/security-winfetch-web.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Download security policy artifact from web\n  win_get_url:\n    url: '{{ java_unlimited_policy_url }}'\n    dest: >-\n      {{ java_download_path }}\\{{ (java_unlimited_policy_url\n        | urlsplit('path')).split('/')[-1] }}\n    force: false\n  register: policy_file_downloaded\n  retries: 3\n  delay: 2\n  until: policy_file_downloaded is succeeded\n\n- name: Downloaded security policy artifact\n  set_fact:\n    security_policy_java_artifact: '{{ policy_file_downloaded.dest }}'\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "ccf36c43c90cf228f18543c10d1fab4021bd69a8", "filename": "handlers/main.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- name: systemd-reload\n  systemd:\n    daemon-reload: yes\n    name: nexus.service\n\n- name: nexus systemd service restart\n  systemd:\n    name: nexus.service\n    state: restarted\n    no_block: yes\n  listen: nexus-service-restart\n  when: \"ansible_service_mgr == 'systemd'\"\n\n- name: nexus sysv service restart\n  service:\n    name: nexus\n    state: restarted\n  listen: nexus-service-restart\n  when: \"ansible_service_mgr != 'systemd'\"\n\n- name: nexus systemd service stop\n  systemd:\n    name: nexus.service\n    state: stopped\n  listen: nexus-service-stop\n  when: nexus_systemd_service_file.stat.exists\n\n- name: nexus sysv service stop\n  service:\n    name: nexus\n    state: stopped\n  listen: nexus-service-stop\n  when: nexus_sysv_service_file.stat.exists\n\n- name: wait-for-nexus\n  wait_for:\n    path: \"{{ nexus_data_dir }}/log/nexus.log\"\n    search_regex: \"Started Sonatype Nexus .*\"\n    timeout: 1800\n\n- name: wait-for-nexus-port\n  wait_for:\n    port: \"{{ nexus_default_port }}\"\n    timeout: \"{{ nexus_wait_for_port_timeout | default(omit) }}\"\n  retries: \"{{ nexus_wait_for_port_retries | default(omit) }}\"\n  register: wait_for_result\n  until: wait_for_result is success\n\n- name: httpd-service-reload\n  systemd:\n    name: \"{{ httpd_package_name }}.service\"\n    state: reloaded\n    enabled: yes\n    no_block: yes\n\n- name: wait-for-httpd\n  wait_for:\n    port: 443\n    delay: 5\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "87202c34df5ab2e436cc4df8f432a3852113eb4b", "filename": "roles/manage-aws-infra/tasks/pre-reqs.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: Fail when delete_vpc is not defined\n  debug:\n    msg: \"delete_vpc boolean variable must be specified to remove the Cluster\"\n  failed_when:\n    - delete_vpc is not defined\n    - operation == \"absent\"\n\n- name: Fail when AWS KEY environment variables are not defined\n  debug:\n    msg: \"Both 'AWS_ACCESS_KEY_ID' and 'AWS_SECRET_ACCESS_KEY' environment variables must be defined\"\n  failed_when: (lookup('env','AWS_ACCESS_KEY_ID')  == \"\") or\n               (lookup('env','AWS_SECRET_ACCESS_KEY')  == \"\")\n\n- name: Set Default values\n  set_fact:\n    master_root_volume: \"{{ master_root_volume | default(default_root_volume) }}\"\n    master_root_volume_size: \"{{ master_root_volume_size | default(default_root_volume_size) }}\"\n    infra_node_root_volume: \"{{ infra_node_root_volume | default(default_root_volume) }}\"\n    infra_node_root_volume_size: \"{{ infra_node_root_volume_size | default(default_root_volume_size) }}\"\n    app_node_root_volume: \"{{ app_node_root_volume | default(default_root_volume) }}\"\n    app_node_root_volume_size: \"{{ app_node_root_volume_size | default(default_root_volume_size) }}\"\n    cns_node_root_volume: \"{{ cns_node_root_volume | default(default_root_volume) }}\"\n    cns_node_root_volume_size: \"{{ cns_node_root_volume_size | default(default_root_volume_size) }}\"\n    cns_node_glusterfs_volume: \"{{ cns_node_glusterfs_volume | default(default_cns_volume) }}\" \n    cns_node_glusterfs_volume_size: \"{{ cns_node_glusterfs_volume_size | default(default_cns_volume_size) }}\" \n    docker_storage_block_device: \"{{ docker_storage_block_device | default(default_docker_volume) }}\"\n    docker_storage_volume_size: \"{{ docker_storage_volume_size | default(default_docker_volume_size) }}\"\n \n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "f1f0bfd74b11027423ed1659cad317f035fb3872", "filename": "playbooks/roles/stenographer/tasks/prechecks.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# Insert any prerequisite checks here\n...\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "748c7d02a0102999902fba01153340f5b15a3092", "filename": "tasks/setup_role_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: setup_role\n    args: \"{{ item }}\""}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "e5b10619c7d95b14bd4b7389e236c0bbd562f372", "filename": "roles/vpn/tasks/iptables.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Iptables configured\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: root\n    group: root\n    mode: 0640\n  with_items:\n    - { src: rules.v4.j2, dest: /etc/iptables/rules.v4 }\n  notify:\n    - restart iptables\n\n- name: Iptables configured\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: root\n    group: root\n    mode: 0640\n  when: ipv6_support\n  with_items:\n    - { src: rules.v6.j2, dest: /etc/iptables/rules.v6 }\n  notify:\n    - restart iptables\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6c6f671beb7fe1bc50ecb9bf4a16ca58c2b73789", "filename": "playbooks/openstack/openshift-cluster/list.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Generate oo_list_hosts group\n  hosts: localhost\n  become: no\n  connection: local\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  tasks:\n  - set_fact: scratch_group=meta-clusterid_{{ cluster_id }}\n    when: cluster_id != ''\n  - set_fact: scratch_group=all\n    when: cluster_id == ''\n  - add_host:\n      name: \"{{ item }}\"\n      groups: oo_list_hosts\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_ssh_host: \"{{ hostvars[item].ansible_ssh_host | default(item) }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n      oo_public_ipv4: \"{{ hostvars[item].openstack.public_v4 }}\"\n      oo_private_ipv4: \"{{ hostvars[item].openstack.private_v4 }}\"\n    with_items: \"{{ groups[scratch_group] | default([]) | difference(['localhost']) }}\"\n  - debug:\n      msg: \"{{ hostvars | oo_select_keys(groups[scratch_group] | default([])) | oo_pretty_print_cluster('meta-') }}\"\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "dfdde2e568aca8260534ea48ae1637515b044a31", "filename": "roles/cloud-azure/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- set_fact:\n    resource_group: \"Algo_{{ region }}\"\n\n- name: Create a resource group\n  azure_rm_resourcegroup:\n    secret: \"{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\"\n    tenant: \"{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\"\n    client_id: \"{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\"\n    subscription_id: \"{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\"\n    name: \"{{ resource_group }}\"\n    location: \"{{ region }}\"\n    tags:\n      Environment: Algo\n\n- name: Create a virtual network\n  azure_rm_virtualnetwork:\n    secret: \"{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\"\n    tenant: \"{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\"\n    client_id: \"{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\"\n    subscription_id: \"{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\"\n    resource_group: \"{{ resource_group }}\"\n    name: algo_net\n    address_prefixes: \"10.10.0.0/16\"\n    tags:\n      Environment: Algo\n\n- name: Create a security group\n  azure_rm_securitygroup:\n    secret: \"{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\"\n    tenant: \"{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\"\n    client_id: \"{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\"\n    subscription_id: \"{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\"\n    resource_group: \"{{ resource_group }}\"\n    name: AlgoSecGroup\n    purge_rules: yes\n    rules:\n      - name: AllowSSH\n        protocol: Tcp\n        destination_port_range: 22\n        access: Allow\n        priority: 100\n        direction: Inbound\n      - name: AllowIPSEC500\n        protocol: Udp\n        destination_port_range: 500\n        access: Allow\n        priority: 110\n        direction: Inbound\n      - name: AllowIPSEC4500\n        protocol: Udp\n        destination_port_range: 4500\n        access: Allow\n        priority: 120\n        direction: Inbound\n\n- name: Create a subnet\n  azure_rm_subnet:\n    secret: \"{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\"\n    tenant: \"{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\"\n    client_id: \"{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\"\n    subscription_id: \"{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\"\n    resource_group: \"{{ resource_group }}\"\n    name: algo_subnet\n    address_prefix: \"10.10.0.0/24\"\n    virtual_network: algo_net\n    security_group_name: AlgoSecGroup\n    tags:\n      Environment: Algo\n\n- name: Create an instance\n  azure_rm_virtualmachine:\n    secret: \"{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\"\n    tenant: \"{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\"\n    client_id: \"{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\"\n    subscription_id: \"{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\"\n    resource_group: \"{{ resource_group }}\"\n    admin_username: ubuntu\n    virtual_network: algo_net\n    name: \"{{ azure_server_name }}\"\n    ssh_password_enabled: false\n    vm_size: Standard_D1\n    tags:\n      Environment: Algo\n    ssh_public_keys:\n      - { path: \"/home/ubuntu/.ssh/authorized_keys\", key_data: \"{{ lookup('file', '{{ SSH_keys.public }}') }}\" }\n    image:\n      offer: UbuntuServer\n      publisher: Canonical\n      sku: '16.04-LTS'\n      version: latest\n  register: azure_rm_virtualmachine\n\n- set_fact:\n    ip_address: \"{{ azure_rm_virtualmachine.ansible_facts.azure_vm.properties.networkProfile.networkInterfaces[0].properties.ipConfigurations[0].properties.publicIPAddress.properties.ipAddress }}\"\n    networkinterface_name: \"{{ azure_rm_virtualmachine.ansible_facts.azure_vm.properties.networkProfile.networkInterfaces[0].name }}\"\n\n- name: Ensure the network interface includes all required parameters\n  azure_rm_networkinterface:\n    secret: \"{{ azure_secret | default(lookup('env','AZURE_SECRET')) }}\"\n    tenant: \"{{ azure_tenant | default(lookup('env','AZURE_TENANT')) }}\"\n    client_id: \"{{ azure_client_id | default(lookup('env','AZURE_CLIENT_ID')) }}\"\n    subscription_id: \"{{ azure_subscription_id | default(lookup('env','AZURE_SUBSCRIPTION_ID')) }}\"\n    name: \"{{ networkinterface_name }}\"\n    resource_group: \"{{ resource_group }}\"\n    virtual_network_name: algo_net\n    subnet_name: algo_subnet\n    security_group_name: AlgoSecGroup\n\n- name: Add the instance to an inventory group\n  add_host:\n    name: \"{{ ip_address }}\"\n    groups: vpn-host\n    ansible_ssh_user: ubuntu\n    ansible_python_interpreter: \"/usr/bin/python2.7\"\n    ansible_ssh_private_key_file: \"{{ SSH_keys.private }}\"\n    cloud_provider: azure\n    ipv6_support: no\n\n- set_fact:\n    cloud_instance_ip: \"{{ ip_address }}\"\n\n- name: Ensure the group azure exists in the dynamic inventory file\n  lineinfile:\n    state: present\n    dest: configs/inventory.dynamic\n    line: '[azure]'\n\n- name: Populate the dynamic inventory\n  lineinfile:\n    state: present\n    dest: configs/inventory.dynamic\n    insertafter: '\\[azure\\]'\n    regexp: \"^{{ cloud_instance_ip }}.*\"\n    line: \"{{ cloud_instance_ip }}\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "8300062335c53d34114d2867f425a8a42dd974ee", "filename": "archive/roles/cicd/tasks/jenkins_install_plugins.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n    \n- name: Validate Jenkins Plugins\n  fail: msg=\"Plugins Not Passed\"\n  failed_when: plugins is not defined\n  tags: jenkins\n  \n- name: Cretate Jenkins Plugin Directory\n  file:\n    path: \"{{ jenkins_plugins_home_dir }}\"\n    group: \"{{ jenkins_group }}\"\n    owner: \"{{ jenkins_user }}\"\n    state: directory\n  tags: jenkins\n\n- name: \"Download Plugins\"\n  get_url:\n      url: \"{{ jenkins_plugins_base_url }}/{{ item.name }}/{{ item.version }}/{{ item.name }}.hpi\"\n      dest: \"{{ jenkins_plugins_home_dir }}/{{item.name}}.{{ (item.pinned is defined and item.pinned == true) | ternary('jpi','hpi') }}\"\n  register: jenkins_download_plugins\n  with_items: plugins\n  tags: jenkins\n  \n- name: \"Change Plugin Permissions\"\n  file:\n    group: \"{{ jenkins_group }}\"\n    owner: \"{{ jenkins_user }}\"\n    state: file\n    path: \"{{ item.dest }}\"\n  with_items: jenkins_download_plugins.results\n  tags: jenkins\n\n\n- name: \"Set Pinned Plugin Markers\"\n  file:\n    group: \"{{ jenkins_group }}\"\n    owner: \"{{ jenkins_user }}\"\n    state: touch\n    path: \"{{ jenkins_plugins_home_dir }}/{{ item.item.name }}.jpi.pinned\"\n  when: item.item.pinned is defined and item.item.pinned == true\n  with_items: jenkins_download_plugins.results\n  tags: jenkins"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "171ac89f4ba988dbc91893f9091d13354171af38", "filename": "archive/roles/cicd/tasks/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: Register Attached Block Storage\n  stat:\n    path: \"{{ cicd_storage_disk_volume }}\"\n  register: block_device\n  \n- name: Validate Block Device\n  fail: msg=\"Attached device at {{ cicd_storage_disk_volume }} is not present\"\n  failed_when: block_device.stat.exists == false\n  \n- name: Creates Temporary Directory\n  file:\n    path: \"{{cicd_temp_dir}}\"\n    state: directory\n  \n- include: prerequisites.yml\n\n- include: java.yml\n\n- include: groovy.yml\n\n- include: maven.yml\n\n- include: nexus.yml\n\n- include: httpd.yml\n\n- include: docker.yml\n\n- include: jenkins.yml"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "fb1d6a5e4e8e5a544dfe6d29d0490f349b4eab36", "filename": "roles/config-mysql/tasks/firewall.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Check if firewalld is installed\n  command: systemctl status firewalld\n  register: firewalld_status\n  failed_when: false\n  changed_when: false\n\n- name: Check if iptables is installed\n  command: systemctl status iptables\n  register: iptables_status\n  failed_when: false\n  changed_when: false\n\n- name: Open port in firewalld\n  firewalld:\n    port: \"{{ mysql_host_port }}/TCP\"\n    permanent: true\n    state: enabled\n  when: firewalld_status.rc == 0\n  notify:\n  - restart firewalld\n\n- name: Ensure iptables is correctly configured \n  lineinfile:\n    insertafter: \"^-A INPUT .* --dport {{ mysql_host_port }} .* ACCEPT\"\n    state: present\n    dest: /etc/sysconfig/iptables\n    regexp: \"^-A INPUT .* --dport {{ mysql_host_port }} .* ACCEPT\"\n    line: \"-A INPUT -p TCP -m state --state NEW -m TCP --dport {{ mysql_host_port }} -j ACCEPT\"\n  when: iptables_status.rc == 0 and firewalld_status.rc != 0 \n  notify:\n  - restart iptables\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "554a2aaf642a7610de2e56bba52ede5f448b2e36", "filename": "tasks/galera/bootstrap.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n- set_fact:\n    booboo: \"{{ mariadb_datadir }}/.ansible_galera_boostrap\"\n\n- name: STAT | Bootstrap mark\n  stat: path={{ booboo }}\n  register: s\n\n- block:\n    - name: SET_FACT | We must NOT restart after bootstrap!\n      set_fact:\n        mariadb_notify_restart: false\n\n    - name: SERVICE | Stop MariaDB\n      service: name=mysql state=stopped\n\n    - name: COMMAND | Bootstrap first node (systemd)\n      command: galera_new_cluster\n      when: ansible_service_mgr == 'systemd'\n\n    - name: SERVICE | Bootstrap first node (clean init)\n      service:\n        name: mysql\n        state: started\n        arguments: --wsrep-new-cluster\n      register: bootstrap_run\n      when: ansible_service_mgr != 'systemd'\n\n  when: not s.stat.exists or mariadb_galera_resetup\n\n- name: COMMAND | Create Bootstrap mark\n  command: \"touch {{ booboo }}\"\n  args:\n    creates: \"{{ booboo }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "51f959e9f1d1f9fc8d395f53b562affd46dc5106", "filename": "playbooks/provision-idm-server/configure-idm-server.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: idm-server\n  roles:\n  - role: config-idm-server\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "330905980a23ee07a89cbb0bcd8b37e9459f4c03", "filename": "roles/openshift-applier/tasks/process-one-entry.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: \"Set default values\"\n  set_fact:\n    target_namespace: ''\n    file: \"{{ content.file | default('') }}\"\n    file_f_option: ''\n    file_action: \"{{ content.file_action | default(default_file_action) }}\"\n    template: \"{{ content.template | default('') }}\"\n    template_f_option: ''\n    template_action: \"{{ content.template_action | default(default_template_action) }}\"\n    params: \"{{ content.params | default('') }}\"\n    process_local: ''\n\n- name: \"Set file_action and template_action to delete when in deprovision mode\"\n  set_fact:\n    file_action: \"delete\"\n    template_action: \"delete\"\n  when:\n  - not provision|bool\n\n- name: \"Fail if template but no params is specified\"\n  fail:\n    msg: \"Template specified, but no params file supplied\"\n  when:\n  - template|trim != ''\n  - params|trim == ''\n\n- name: \"Check if params file exists (if applicable)\"\n  stat:\n    path: \"{{ tmp_inv_dir }}{{ params }}\"\n  ignore_errors: yes\n  register: params_result\n  when:\n  - params|trim != ''\n\n- name: \"Fail if params file doesn't exist (if applicable)\"\n  fail:\n    msg: \"{{ params }} - params file doesn't exist.\"\n  when:\n  - params|trim != ''\n  - params_result.stat.exists == False\n\n- name: \"Determine location for the 'file' (if applicable)\"\n  vars:\n    path: \"{{ file }}\"\n  import_tasks: check-file-location.yml\n  when:\n  - file|trim != ''\n\n- name: \"Set 'file' values (if applicable)\"\n  set_fact:\n    file: \"{{ file_path }}\"\n    file_f_option: \"{{ option_f }}\"\n  when:\n  - file|trim != ''\n\n- name: \"Determine location for the 'template' (if applicable)\"\n  vars:\n    path: \"{{ template }}\"\n  import_tasks: check-file-location.yml\n  when:\n  - template|trim != ''\n\n- name: \"Set 'template' values (if applicable)\"\n  set_fact:\n    template: \"{{ file_path }}\"\n    template_f_option: \"{{ option_f }}\"\n  when:\n  - template|trim != ''\n\n- name: \"Set the target namespace option if supplied\"\n  set_fact:\n    target_namespace: \"-n {{ content.namespace }}\"\n  when:\n  - content.namespace is defined\n  - content.namespace|trim != ''\n\n- name: \"Include any pre-processing role(s) before applying file and/or template\"\n  include: pre-post-step.yml\n  with_items:\n  - \"{{ content.pre_steps | default('') }}\"\n  loop_control:\n    loop_var: step\n\n- name: \"Create OpenShift objects based on static files for '{{ entry.object}} : {{ content.name | default(file | basename) }}'\"\n  command: >\n    oc {{ file_action }} {{ target_namespace }} -f {{ file }}\n  register: command_result\n  failed_when:\n  - command_result.rc != 0\n  - \"'AlreadyExists' not in command_result.stderr\"\n  - \"'NotFound' not in command_result.stderr\"\n  when:\n  - file|trim != ''\n\n- name: \"Create OpenShift objects based on template with params for '{{ entry.object}} : {{ content.name | default(template | basename) }}'\"\n  shell: >\n    oc process \\\n      {{ process_local }} \\\n      {{ template_f_option }} {{ template }} \\\n      {{ target_namespace }} \\\n      --param-file={{ tmp_inv_dir }}{{ content.params }} | \\\n    oc {{ template_action }} {{ target_namespace }} -f -\n  register: command_result\n  failed_when:\n  - command_result.rc != 0\n  - \"'AlreadyExists' not in command_result.stderr\"\n  - \"'NotFound' not in command_result.stderr\"\n  when:\n  - template|trim != ''\n  - params|trim != ''\n\n- name: \"Include any post-processing role(s) after applying file and/or template\"\n  include: pre-post-step.yml\n  with_items:\n  - \"{{ content.post_steps | default('') }}\"\n  loop_control:\n    loop_var: step\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b94ff11d4af8efe8c77fced0639bcd81831a038b", "filename": "roles/config-packages/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Install additional Software packages/tools'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - \"{{ list_of_packages_to_install }}\"\n\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "ad31471d154264af299574cf527caa5085250938", "filename": "tasks/create_repo_raw_group_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_raw_group\n    args: \"{{ _nexus_repos_raw_defaults|combine(item) }}\""}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "ebdae5262483aa9e7e49b191a9f0ffa9bcc63318", "filename": "tasks/Win32NT/fetch/fetch_fallback_old.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Set artifact basename\n  set_fact:\n    artifact_url: '{{ release_url[0] }}'\n    artifact_basename: \"{{ (release_url[0] | urlsplit('path')).split('/')[-1] }}\"\n\n- name: 'Get {{ checksum_alg }} checksum of file'\n  win_stat:\n    path: '{{ java_download_path }}\\{{ artifact_basename }}'\n    get_checksum: true\n    checksum_algorithm: '{{ checksum_alg }}'\n  register: artifact\n\n- name: Download with checksum validation\n  include_tasks: fetch_checksum.yml\n  when: |\n    not artifact.stat.exists | bool\n    or artifact.stat.checksum != artifact_checksum.content\n  retries: 15\n  delay: 2\n  until: artifact.stat.checksum == artifact_checksum.content\n\n- name: Set downloaded artifact vars\n  set_fact:\n    file_downloaded:\n      dest: '{{ artifact.stat.path }}'\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "89515ddb3434e51055bdc48566a3e104ec15c1ff", "filename": "roles/dns_encryption/tasks/ubuntu.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Add the repository\n  apt_repository:\n    state: present\n    codename: bionic\n    repo: ppa:shevchuk/dnscrypt-proxy\n  register: result\n  until: result is succeeded\n  retries: 10\n  delay: 3\n\n- name: Install dnscrypt-proxy\n  apt:\n    name: dnscrypt-proxy\n    state: latest\n    update_cache: true\n\n- name: Configure unattended-upgrades\n  copy:\n    src: 50-dnscrypt-proxy-unattended-upgrades\n    dest: /etc/apt/apt.conf.d/50-dnscrypt-proxy-unattended-upgrades\n    owner: root\n    group: root\n    mode: 0644\n\n- block:\n  - name: Ubuntu | Unbound profile for apparmor configured\n    copy:\n      src: apparmor.profile.dnscrypt-proxy\n      dest: /etc/apparmor.d/usr.bin.dnscrypt-proxy\n      owner: root\n      group: root\n      mode: 0600\n    notify: restart dnscrypt-proxy\n\n  - name: Ubuntu | Enforce the dnscrypt-proxy AppArmor policy\n    command: aa-enforce usr.bin.dnscrypt-proxy\n    changed_when: false\n  tags: apparmor\n  when: apparmor_enabled|default(false)|bool == true\n\n- name: Ubuntu | Ensure that the dnscrypt-proxy service directory exist\n  file:\n    path: /etc/systemd/system/dnscrypt-proxy.service.d/\n    state: directory\n    mode: 0755\n    owner: root\n    group: root\n\n- name: Ubuntu | Add capabilities to bind ports\n  copy:\n    dest: /etc/systemd/system/dnscrypt-proxy.service.d/99-capabilities.conf\n    content: |\n      [Service]\n      AmbientCapabilities=CAP_NET_BIND_SERVICE\n  notify:\n   - restart dnscrypt-proxy\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "a52502672b59a04ebc5159d84b3e7aabfa0303ea", "filename": "roles/ipareplica/vars/Fedora-26.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# Fedora defaults file for ipareplica\n# vars/Fedora-26.yml\nipareplica_packages: [ \"ipa-server\", \"libselinux-python\" ]\nipareplica_packages_dns: [ \"ipa-server-dns\" ]\nipareplica_packages_adtrust: [ \"ipa-server-trust-ad\" ]"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "a52ff177984ee61c279ed87916aa8818d05322b7", "filename": "ops/playbooks/config_simplivity_backups.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n\n###################################################################################\n#\n# play1: Perform some rudimentary sanity check first\n#\n###################################################################################\n- hosts:  all_vms\n  name: Config Backup - Perform Rudimentary Sanity Check (VMs)\n  connection: local\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  vars:\n    ovc: \"{{ omnistack_ovc | random }}\"\n    sleep_interval: 2\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n  #\n  #   - we accept nodes without a backup policy, which means, we don;t backup the VM\n  #   - but we don't accept nodes specifying a backup policy which is not defined in \"backup_policies\"\n  #\n\n  - fail: msg=\"backup_policies is not definedi, aborting...\"\n    when: backup_policies is not defined\n\n  - set_fact: node_policy_match=\"{{ backup_policies | json_query ( q ) }}\"\n    vars:\n       q: \"[?name=='{{ node_policy }}'].name\"\n    when: node_policy is defined\n\n  - fail: msg=\"Backup policy {{ node_policy }} not found in {{ backup_policies }}\"\n    when: node_policy is defined and node_policy_match | length <= 0\n\n  - debug: msg=\"No backup policy specified for {{ inventory_hostname }} \"\n    when: node_policy is not defined\n\n###################################################################################\n#\n# Play2: Perform sanity check on localhost (docker_volume backups)\n#\n###################################################################################\n- hosts: localhost\n  name: Config Backup - Perform Rudimentary Sanity Check (Docker Volumes)\n  connection: local\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  vars:\n    ovc: \"{{ omnistack_ovc | random }}\"\n    sleep_interval: 2\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n  - fail: msg=\"backup_policies is not definedi, aborting...\"\n    when: backup_policies is not defined\n\n  - fail: msg=\"docker_volumes_policy is not defined, aborting...\"\n    when: docker_volumes_policy is not defined\n\n  - set_fact: node_policy_match=\"{{ backup_policies | json_query ( q ) }}\"\n    vars:\n       q: \"[?name=='{{ docker_volumes_policy }}'].name\"\n\n  - fail: msg=\"Backup policy for Docker volumes {{ docker_volumes_policy  }} not found in {{ backup_policies }}\"\n    when: node_policy_match | length <= 0\n\n###################################################################################\n#\n# Play3: Create all Backup Policies,  populate them with the specified rules\n#\n###################################################################################\n\n- hosts: localhost\n  name: Config Backup - Create Backup policies\n  connection: local\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  vars:\n    ovc: \"{{ omnistack_ovc | random }}\"\n    sleep_interval: 2\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n  - name: Retrieve an auth token \n    include_tasks: includes/svt_get_auth_token.yml\n\n  #\n  # Create backup policies \n  #\n\n  - name: Create backup policy\n    uri:\n      url: \"https://{{ ovc }}/api/policies\"\n      headers:\n        Content-Type: application/vnd.simplivity.v1+json\n        Authorization: Bearer {{ svtapitoken.json.access_token }}\n      method: POST\n      body: {\"name\":\"{{ item.name }}\"}\n      status_code: 202,400\n      body_format: json\n      force_basic_auth: yes\n      validate_certs: no\n    register: res\n    with_items:\n      - \"{{ backup_policies }}\"\n\n  - include_tasks: includes/svt_wait_for_tasks.yml\n    vars:\n      tasks: \"{{ res.results }}\"\n\n  - name: Get policy IDs\n    uri:\n      url: \"https://{{ ovc }}/api/policies?fields=id,name\"\n      headers:\n        Content-Type: application/vnd.simplivity.v1+json\n        Authorization: Bearer {{ svtapitoken.json.access_token }}\n      method: GET\n      body_format: json\n      force_basic_auth: yes\n      validate_certs: no\n    register: policy_ids\n\n  - name: sleep\n    command: sleep '{{ sleep_interval }}'\n\n  - name: Configure backup rules\n    uri:\n      url: \"https://{{ ovc }}/api/policies/{{ policy_ids | json_query(q) }}/rules?replace_all_rules=true\"\n      headers:\n        Content-Type: application/vnd.simplivity.v1.4+json\n        Accept: application/json\n        Authorization: Bearer {{ svtapitoken.json.access_token }}\n      method: POST\n      body: '[{\"days\":\"{{ item.days }}\",\"start_time\":\"{{ item.start_time }}\",\"frequency\":\"{{ item.frequency }}\",\"retention\":\"{{ item.retention }}\"}]'\n      status_code: 202\n      body_format: json\n      force_basic_auth: yes\n      validate_certs: no\n    with_items: \"{{ backup_policies }}\"\n    loop_control:\n      pause: 2\n    vars:\n      q: \"json.policies[?name=='{{ item.name }}'].id | [0]\"\n\n###################################################################################\n#\n# Play4: Assign backup policy to VM (this is done on the VM)\n#\n###################################################################################\n- hosts:  all_vms\n  name: Config Backup - Assign Backup policies\n  connection: local\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  vars:\n    ovc: \"{{ omnistack_ovc | random }}\"\n    sleep_interval: 2\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n  - block: # when node_policy is defined\n\n    - name: Retrieve an auth token\n      include_tasks: includes/svt_get_auth_token.yml\n\n  #\n  # Get the backup policy ID, abort if not found\n  #\n    - name: Get Backup Policy ID\n      uri:\n        url: \"https://{{ ovc }}/api/policies?fields=id,name\"\n        headers:\n          Content-Type: application/vnd.simplivity.v1+json\n          Authorization: Bearer {{ svtapitoken.json.access_token }}\n        method: GET\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      delegate_to: localhost\n      register: policy_ids\n    - set_fact: policy_id=\"{{ policy_ids | json_query(q) }}\"\n      vars:\n        q: \"json.policies[?name=='{{ node_policy }}'].id \"\n    - fail: msg=\"Backup Policy ID for {{ node_policy  }} cannot be determined. Aborting\"\n      when: policy_id | count <= 0\n\n  #\n  # Get the VM ID, abort if not found\n  #\n    - name: Get VM ID\n      uri:\n        url: \"https://{{ ovc }}/api/virtual_machines\"\n        headers:\n          Content-Type: application/vnd.simplivity.v1+json\n          Authorization: Bearer {{ svtapitoken.json.access_token }}\n        method: GET\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      delegate_to: localhost\n      register: vm_ids\n    - set_fact: vm_id=\"{{ vm_ids | json_query(q) }}\"\n      vars:\n        q: \"json.virtual_machines[?name=='{{ inventory_hostname }}' && state=='ALIVE'].id \"\n    - fail: msg=\"VM ID for {{ inventory_hostname }} cannot be determined. Aborting\"\n      when: vm_id | count <= 0\n\n    - name: Assign backup policy to VM\n      uri:\n        url: \"https://{{ ovc }}/api/virtual_machines/{{ vm_id[0] }}/set_policy\"\n        headers:\n          Content-Type: application/vnd.simplivity.v1+json\n          Authorization: Bearer {{ svtapitoken.json.access_token }}\n        method: POST\n        body: {\"policy_id\":\"{{ policy_id[0] }}\"}\n        status_code: 202\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      delegate_to: localhost\n      register: res\n\n    - include_tasks: includes/svt_wait_for_tasks.yml\n      vars:\n        tasks: \"{{ [ res ] }}\"\n\n    when: node_policy is defined\n\n###################################################################################\n#\n# Assign backup policy to Dummy VMs\n#\n###################################################################################\n- hosts: localhost\n  name: Config Backup - Assign Backup Policies (Dummy VMs)\n  connection: local\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  vars:\n    ovc: \"{{ omnistack_ovc | random }}\"\n    sleep_interval: 2\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n    - include_tasks: includes/svt_get_auth_token.yml\n\n    - name: Build the list of Dummy VM names\n      set_fact: dummy_vms=\"{{ dummy_vms | default([]) + [ prefix+'-in-dockvols-'+item  ] }}\"\n      vars: \n        prefix: \"{{ dummy_vm_prefix }}\"\n      with_items:\n        - \"{{ datastores }}\"\n\n    #\n    # Get the backup policy ID, abort if not found\n    #\n    - name: Get Backup Policy ID\n      uri:\n        url: \"https://{{ ovc }}/api/policies?fields=id,name\"\n        headers:\n          Content-Type: application/vnd.simplivity.v1+json\n          Authorization: Bearer {{ svtapitoken.json.access_token }}\n        method: GET\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      register: policy_ids\n    - set_fact: policy_id=\"{{ policy_ids | json_query(q) }}\"\n      vars:\n        q: \"json.policies[?name=='{{ docker_volumes_policy }}'].id \"\n    - fail: msg=\"Backup Policy ID for {{ docker_volumes_policy   }} cannot be determined. Aborting\"\n      when: policy_id | count <= 0\n\n    #\n    # Get All VM IDs\n    #\n    - name: Get Dummy VMs IDs\n      uri:\n        url: \"https://{{ ovc }}/api/virtual_machines\"\n        headers:\n          Content-Type: application/vnd.simplivity.v1+json\n          Authorization: Bearer {{ svtapitoken.json.access_token }}\n        method: GET\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      register: vm_ids\n\n    - set_fact:\n        dummy_vms_ids: \"{{ dummy_vms_ids | default([]) +  vm_ids | json_query (q)  }}\"\n      vars:\n        - q: \"json.virtual_machines[?name=='{{ item }}' && state=='ALIVE'].{id: id, name: name, state: state}\"\n      with_items:\n        - \"{{ dummy_vms }}\"\n\n    #\n    # Assign Backup policy to each dummy VM that we have found\n    #\n    - name: Assign backup policies to Docker volumes\n      uri:\n        url: \"https://{{ ovc }}/api/virtual_machines/{{ item.id }}/set_policy\"\n        headers:\n          Content-Type: application/vnd.simplivity.v1+json\n          Authorization: Bearer {{ svtapitoken.json.access_token }}\n        method: POST\n        body: {\"policy_id\":\"{{ policy_id[0] }}\"}\n        status_code: 202\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      with_items: \"{{ dummy_vms_ids }}\"\n      register: res\n      loop_control:\n        pause: 2\n \n    - include_tasks: includes/svt_wait_for_tasks.yml\n      vars:\n        tasks: \"{{ res.results }}\" \n\n    #\n    # Fail if we could not find all the Dummy VM\n    # \n    - fail:\n        msg: \"Dummy VMs are missing {{ dummy_vms_ids | count }} of {{ dummy_vms | count }} found\"\n      when: dummy_vms_ids | count != dummy_vms | count      \n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ca5b910c203842d389666f50de16757b299c94c6", "filename": "playbooks/add-node-prerequisite.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: new_nodes\n  gather_facts: yes\n  become: yes\n  roles:\n  - rhsm-timeout\n  - role: atomic-update\n    when: openshift.common.is_atomic\n\n- hosts: new_nodes\n  gather_facts: no\n  become: yes\n  serial: 1\n  roles:\n  - role: rhel_subscribe\n    when: deployment_type in [\"enterprise\", \"atomic-enterprise\", \"openshift-enterprise\"] and\n          ansible_distribution == \"RedHat\" and rhel_subscription_user is defined\n  - role: rhsm-subscription\n    when: deployment_type in [\"enterprise\", \"atomic-enterprise\", \"openshift-enterprise\"] and\n          ansible_distribution == \"RedHat\" and rhsm_user is defined\n\n- hosts: new_nodes\n  gather_facts: no\n  become: yes\n  roles:\n  - role: rhsm-repos\n    when: deployment_type in [\"enterprise\", \"atomic-enterprise\", \"openshift-enterprise\"] and\n          ansible_distribution == \"RedHat\" and rhsm_user is defined\n  - prerequisites\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "6a44dbee58786cd6b4467b750aceaf2f92be9c2e", "filename": "roles/dns_adblocking/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- block:\n    - name: Dnsmasq installed\n      package: name=dnsmasq\n\n    - name: The dnsmasq directory created\n      file: dest=/var/lib/dnsmasq state=directory mode=0755 owner=dnsmasq group=nogroup\n\n    - include_tasks: ubuntu.yml\n      when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'\n\n    - include_tasks: freebsd.yml\n      when: ansible_distribution == 'FreeBSD'\n\n    - name: Dnsmasq configured\n      template:\n        src: dnsmasq.conf.j2\n        dest: \"{{ config_prefix|default('/') }}etc/dnsmasq.conf\"\n      notify:\n        - restart dnsmasq\n\n    - name: Adblock script created\n      template:\n        src: adblock.sh.j2\n        dest: /usr/local/sbin/adblock.sh\n        owner: root\n        group: \"{{ root_group|default('root') }}\"\n        mode: 0755\n\n    - name: Adblock script added to cron\n      cron:\n        name: Adblock hosts update\n        minute: \"{{ range(0, 60) | random }}\"\n        hour: \"{{ range(0, 24) | random }}\"\n        job: /usr/local/sbin/adblock.sh\n        user: root\n\n    - name: Update adblock hosts\n      command: /usr/local/sbin/adblock.sh\n\n    - meta: flush_handlers\n\n    - name: Dnsmasq enabled and started\n      service:\n        name: dnsmasq\n        state: started\n        enabled: yes\n  rescue:\n    - debug: var=fail_hint\n      tags: always\n    - fail:\n      tags: always\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ea9dc27d4a727edcdfc07e55c370f20c939997b7", "filename": "roles/osp/packstack-post/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Include 'mariadb' processing\"\n  include_tasks: \"mariadb.yml\"\n  when:\n  - \"'mariadb' in osp_roles.split(',')\"\n\n- name: \"Include 'keystone' processing\"\n  include_tasks: \"keystone.yml\"\n  when:\n  - \"'keystone' in osp_roles.split(',')\"\n\n- name: \"Include 'cinder' processing\"\n  include_tasks: \"cinder.yml\"\n  when:\n  - \"'cinder' in osp_roles.split(',')\"\n\n- name: \"Include 'nova' processing\"\n  include_tasks: \"nova.yml\"\n  when:\n  - \"'nova' in osp_roles.split(',')\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7f105f38bab469a9c22b89f64750b5ba11954a63", "filename": "roles/scm/github.com/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- hosts: localhost\n  remote_user: root\n  tasks:\n    - include_role:\n        name: \"{{ playbook_dir }}/../../github.com\""}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b3d05785c924b490e75f58a40e8f4b6cb1e2574d", "filename": "roles/osp/admin-nova-flavor/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Create the flavor\"\n  os_nova_flavor:\n    cloud: \"{{ item.cloud | default(osp_default_cloud) | default(omit) }}\"\n    vcpus: \"{{ item.vcpus }}\"\n    ram: \"{{ item.ram }}\"\n    disk: \"{{ item.disk }}\"\n    name: \"{{ item.name }}\"\n  with_items:\n  - \"{{ osp_custom_flavors | default([]) }}\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "6f97972f3b6e07a5570ca641cdffa55d571c9c3a", "filename": "ops/playbooks/k8s-install-sysdig.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- name: Install sysdig\n  hosts: docker\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  environment: \"{{ env }}\"\n  tasks:\n    #\n    # Open required firewall ports\n    #\n    - name: Configure required firewall ports\n      firewalld:\n        port: 6666/tcp\n        permanent: true\n        immediate: true\n        state: enabled\n\n    # Verify remote system can open a connection to\n    # collector.sysdigcloud.com port 6666 to send Sysdig\n    # information to Sysdig's SaaS instance.  If the connection\n    # succeeds then set the variable 'sysdig_connection'\n    # to 'true' and continue.  Otherwise print an error\n    # message explaining that connectivity requirements\n    # were not met.\n    \n    - name: Check connectivity to collector.sysdigcloud.com port 6666\n      wait_for:\n        host: collector.sysdigcloud.com\n        port: 6666\n        state: started\n        timeout: 10\n        msg: \"Connectivity to collector.sysdigcloud.com port 6666 failed!  Exiting.\"\n\n    - set_fact:\n        sysdig_connection: true\n\n    #\n    # Install requried kernel headers\n    #\n    - name: Install Sysdig Agent\n\n      block:\n\n      - name: Install Kernel Headers\n        yum:\n          name: kernel-devel-{{ ansible_kernel }}\n          state: latest\n          update_cache: yes\n\n\n- hosts: local\n  connection: local\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  tasks:\n\n    - debug: msg=\"Starting Playbook k8s-install-sysdig\" \n#\n# find a UCP VM that works\n#\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n\n\n#\n# Retrieve and remember a Token for using the UCP API\n#\n    - name: Retrieve a token for the UCP API\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/auth/login\"\n        headers:\n          Content-Type: application/json\n        method: POST\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        body: '{\"username\":\"{{ ucp_username }}\",\"password\":\"{{ ucp_password }}\"}'\n        use_proxy: no\n      register: login\n      until: login.status == 200\n      retries: 20\n      delay: 5\n\n    - name: Remember the token\n      set_fact:\n        auth_token:  \"{{ login.json.auth_token }}\"\n\n#\n# handle the case where the role is already existing\n#\n    - name: List all roles\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/roles\"\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        use_proxy: no\n      register: roles\n\n    - set_fact: \n        restricted_role: \"{{ roles.json | json_query(query) }}\"\n      vars:\n        query: \"[?name == '{{ sysdig_restricted_control_role }}' ].{id: id, name: name }\"\n\n#\n# Create the role if it does not already exists\n#\n    - block:\n\n      - name: Create the role file\n        template:\n          src:  ../templates/k8s/sysdig/sysdig-restricted-role.json\n          dest: /tmp/sysdig-restricted-role.json\n\n      - name: Create the role for the restricted-control\n        uri:\n          url: \"https://{{ ucp_instance }}.{{ domain_name }}/roles\"\n          headers:\n            Content-Type: application/json\n            Authorization: Bearer {{ auth_token }}\n          method: POST\n          status_code: 201\n          body_format: json\n          validate_certs: no\n          body: \"{{ body }}\"\n          use_proxy: no\n        vars:\n          body: \"{{ lookup('file','/tmp/sysdig-restricted-role.json') }}\"\n        register: res\n  \n      - debug: var=res\n\n      when: restricted_role | count == 0\n\n\n    - debug: msg=\"Role for Restricted-Control already exists \"\n      when: restricted_role | count != 0\n\n#\n# \n#\n    - name: List all roles\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/roles\"\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        use_proxy: no\n      register: roles\n\n    - set_fact:\n        restricted_role: \"{{ roles.json | json_query(query) }}\"\n      vars:\n        query: \"[?name == '{{ sysdig_restricted_control_role }}' ].{id: id, name: name }\"\n\n    - name: List all roles\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/roles\"\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        use_proxy: no\n      register: roles\n\n    - set_fact:\n        restricted: \"{{ roles.json | json_query(query) }}\"\n      vars:\n        query: \"[?name == '{{ sysdig_restricted_control_role }}' ].{id: id, name: name }\"\n\n    - fail:\n      when: restricted_role | count ==0\n\n    - debug: var=restricted_role[0].id\n\n#\n# Copy the kubectl client\n#\n    - name: Copy kubectl client\n      copy:\n         src: ../files/k8s/client/kubectl\n         dest: /tmp/kubectl\n         mode: 0744\n\n#\n# Create Sysdig Namespace\n#\n    - name: Create Sysdig Namespace, copy script\n      template:\n         src: ../templates/k8s/sysdig/sysdignamespace.sh\n         dest: /tmp/sysdignamespace.sh\n         mode: 0744\n    - name: Create Sysdig Namespace, run script\n      shell: /tmp/sysdignamespace.sh /tmp/kubectl\n\n#\n# Create the service account\n#\n    - name: Create Service Account, copy script\n      template:\n         src: ../templates/k8s/sysdig/serviceaccount.sh\n         dest: /tmp/serviceaccount.sh\n         mode: 0744\n    - name: Create Service Account, run script\n      shell: /tmp/serviceaccount.sh /tmp/kubectl\n\n    - name: Grant the service account with the role\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/collectionGrants/system%3Aserviceaccount%3Asysdig%3Asysdig/kubernetesnamespaces/{{ restricted_role[0].id }}?type=grantobject\"\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: PUT\n        status_code: 201\n        body_format: json\n        validate_certs: no\n        use_proxy: no\n      register: roles\n\n#\n# deploy the Sysdig Agent\n#\n    - name: Deploy Sysdig Agent, copy script\n      template:\n         src: ../templates/k8s/sysdig/deploy_sysdig.sh\n         dest: /tmp/deploy_sysdig.sh\n         mode: 0744\n    - name: Deploy Sysdig Agent, run script\n      shell: /tmp/deploy_sysdig.sh /tmp/kubectl \n"}, {"commit_sha": "c91b6076e3a957fb0a165131d0ff3b3b208ed419", "sha": "f6b3a84b78e594f199e7f76699f1c3306850dff1", "filename": "tasks/section_08_level2.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n# Change order or the default auditd.conf file will not be created\n  - name: 8.1.2 Install and Enable auditd Service (Scored)\n    apt: name=auditd state=present\n    tags:\n      - section8\n      - section8.1\n      - section8.1.2\n      - section8.1.2\n\n  - name: Check if the file auditd.conf exists\n    stat: >\n        path=/etc/audit/auditd.conf\n    register: auditd_file\n    tags:\n      - section8\n      - section8.1\n      - section8.1.1\n      - section8.1.1.1\n\n  - name: Create the audit directory if it does not exists\n    file: >\n        path=/etc/audit/\n        state=directory\n    when: not auditd_file.stat.exists\n    tags:\n      - section8\n      - section8.1\n      - section8.1.1\n      - section8.1.1.1\n\n  - name: 8.1.1-3 Configure Data Retention\n    lineinfile: >\n        dest=/etc/audit/auditd.conf\n        regexp=\"{{ item.rxp }}\"\n        line=\"{{ item.line }}\"\n        state=present\n        create=yes\n    with_items:\n      - { rxp: '^max_log_file ', line: 'max_log_file = {{ Max_Log_File_Auditd }}' }\n      - { rxp: '^space_left_action', line: 'space_left_action = email' }\n      - { rxp: '^action_mail_acct', line: 'action_mail_acct = root' }\n      - { rxp: '^admin_space_left_action', line: 'admin_space_left_action = halt' }\n      - { rxp: '^max_log_file_action', line: 'max_log_file_action = keep_logs' }\n    notify: restart auditd\n    tags:\n      - section8\n      - section8.1\n      - section8.1.1\n      - section8.1.1.1\n      - section8.1.1.2\n      - section8.1.1.3\n\n  - name: 8.1.3 Enable Auditing for Processes That Start Prior to auditd (Scored)\n    stat: path=/etc/default/grub\n    register: grubcfg_file\n    tags:\n      - section8\n      - section8.1\n      - section8.1.3\n\n  - name: 8.1.3 Enable Auditing for Processes That Start Prior to auditd (Scored)\n    file: >\n        path=/etc/default/grub\n        state=touch\n    when: not grubcfg_file.stat.exists\n    tags:\n      - section8\n      - section8.1\n      - section8.1.3\n\n  - name: 8.1.3 Enable Auditing for Processes That Start Prior to auditd (Scored)\n    lineinfile: >\n        dest=/etc/default/grub\n        line='GRUB_CMDLINE_LINUX=\"audit=1\"'\n    when: not grubcfg_file.stat.exists\n    tags:\n      - section8\n      - section8.1\n      - section8.1.3\n\n  - name: 8.1.4 Record Events That Modify Date and Time Information (Scored)\n    lineinfile: >\n      dest=/etc/audit/audit.rules\n      line='{{ item }}'\n      state=present\n      create=yes\n    with_items:\n      - '-a always,exit -F arch=b64 -S adjtimex -S settimeofday -k time-change'\n      - '-a always,exit -F arch=b32 -S adjtimex -S settimeofday -S stime -k time-change'\n      - '-a always,exit -F arch=b64 -S clock_settime -k time-change'\n      - '-a always,exit -F arch=b32 -S clock_settime -k time-change'\n      - '-w /etc/localtime -p wa -k time-change'\n    notify: restart auditd\n    when: ansible_userspace_bits == \"64\"\n    tags:\n      - section8\n      - section8.1\n      - section8.1.4\n\n  - name: 8.1.4 Record Events That Modify Date and Time Information (Scored)\n    lineinfile: >\n      dest=/etc/audit/audit.rules\n      line='{{ item }}'\n      state=present\n      create=yes\n    with_items:\n      - '-a always,exit -F arch=b32 -S adjtimex -S settimeofday -S stime -k time-change'\n      - '-a always,exit -F arch=b32 -S clock_settime -k time-change'\n      - '-w /etc/localtime -p wa -k time-change'\n    notify: restart auditd\n    when: ansible_userspace_bits == \"32\"\n    tags:\n      - section8\n      - section8.1\n      - section8.1.4\n\n  - name: 8.1.5,7,8,9,15,16,17 Record Events That Modify User/Group Information (Scored)\n    lineinfile: >\n      dest=/etc/audit/audit.rules\n      line='{{ item }}'\n      state=present\n      create=yes\n    with_items:\n      - '-w /etc/group -p wa -k identity'\n      - '-w /etc/passwd -p wa -k identity'\n      - '-w /etc/gshadow -p wa -k identity'\n      - '-w /etc/shadow -p wa -k identity'\n      - '-w /etc/security/opasswd -p wa -k identity'\n      - '-w /var/log/faillog -p wa -k logins'\n      - '-w /var/log/lastlog -p wa -k logins'\n      - '-w /var/log/tallylog -p wa -k logins'\n      - '-w /var/run/utmp -p wa -k session'\n      - '-w /var/log/wtmp -p wa -k session'\n      - '-w /var/log/btmp -p wa -k session'\n      - '-w /etc/selinux/ -p wa -k MAC-policy'\n      - '-w /etc/sudoers -p wa -k scope'\n      - '-w /var/log/sudo.log -p wa -k actions'\n      - '-w /sbin/insmod -p x -k modules'\n      - '-w /sbin/rmmod -px -k modules'\n      - '-w /sbin/modprobe -p x -k modules'\n    notify: restart auditd\n    tags:\n      - section8\n      - section8.1\n      - section8.1.5\n      - section8.1.7\n      - section8.1.8\n      - section8.1.9\n      - section8.1.15\n      - section8.1.16\n      - section8.1.17\n\n  - name: 8.1.6,10,11,13,14,17 Record Events That Modify the System's Network Environment (64b) (Scored)\n    lineinfile: >\n      dest=/etc/audit/audit.rules\n      line='{{ item }}'\n      state=present\n      create=yes\n    with_items:\n      - '-a exit,always -F arch=b64 -S sethostname -S setdomainname -k system-locale'\n      - '-a exit,always -F arch=b32 -S sethostname -S setdomainname -k system-locale'\n      - '-w /etc/issue -p wa -k system-locale'\n      - '-w /etc/issue.net -p wa -k system-locale'\n      - '-w /etc/hosts -p wa -k system-locale'\n      - '-w /etc/network -p wa -k system-locale'\n      - '-a always,exit -F arch=b64 -S chmod -S fchmod -S fchmodat -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S chmod -S fchmod -S fchmodat -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b64 -S chown -S fchown -S fchownat -S lchown -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S chown -S fchown -S fchownat -S lchown -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b64 -S setxattr -S lsetxattr -S fsetxattr -S removexattr -S lremovexattr -S fremovexattr -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S setxattr -S lsetxattr -S fsetxattr -S removexattr -S lremovexattr -S fremovexattr -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b64 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EACCES -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b32 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EACCES -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b64 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EPERM -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b32 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EPERM -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b64 -S mount -F auid>=500 -F auid!=4294967295 -k mounts'\n      - '-a always,exit -F arch=b32 -S mount -F auid>=500 -F auid!=4294967295 -k mounts'\n      - '-a always,exit -F arch=b64 -S unlink -S unlinkat -S rename -S renameat -F auid>=500 -F auid!=4294967295 -k delete'\n      - '-a always,exit -F arch=b32 -S unlink -S unlinkat -S rename -S renameat -F auid>=500 -F auid!=4294967295 -k delete'\n      - '-a always,exit -F arch=b64 -S init_module -S delete_module -k modules'\n    notify: restart auditd\n    when: ansible_userspace_bits == \"64\"\n    tags:\n      - section8\n      - section8.1\n      - section8.1.6\n      - section8.1.10\n      - section8.1.11\n      - section8.1.13\n      - section8.1.14\n      - section8.1.17\n\n  - name: 8.1.6,10,11,13,14,17 Record Events That Modify the System's Network Environment (32b) (Scored)\n    lineinfile: >\n      dest=/etc/audit/audit.rules\n      line='{{ item }}'\n      state=present\n      create=yes\n    with_items:\n      - '-a exit,always -F arch=b32 -S sethostname -S setdomainname -k system-locale'\n      - '-w /etc/issue -p wa -k system-locale'\n      - '-w /etc/issue.net -p wa -k system-locale'\n      - '-w /etc/hosts -p wa -k system-locale'\n      - '-w /etc/network -p wa -k system-locale'\n      - '-a always,exit -F arch=b32 -S chmod -S fchmod -S fchmodat -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S chown -S fchown -S fchownat -S lchown -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S setxattr -S lsetxattr -S fsetxattr -S removexattr -S lremovexattr -S fremovexattr -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EACCES -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b32 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EPERM -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b32 -S mount -F auid>=500 -F auid!=4294967295 -k mounts'\n      - '-a always,exit -F arch=b32 -S unlink -S unlinkat -S rename -S renameat -F auid>=500 -F auid!=4294967295 -k delete'\n      - '-a always,exit -F arch=b32 -S init_module -S delete_module -k modules'\n    notify: restart auditd\n    when: ansible_userspace_bits == \"32\"\n    tags:\n      - section8\n      - section8.1\n      - section8.1.6\n      - section8.1.10\n      - section8.1.11\n      - section8.1.13\n      - section8.1.14\n      - section8.1.17\n\n  - name: 8.1.12 Collect Use of Privileged Commands (Scored)\n    shell: find / -xdev \\( -perm -4000 -o -perm -2000 \\) -type f | awk '{print \"-a always,exit -F path=\" $1 \" -F perm=x -F auid>=500 -F auid!=4294967295 -k privileged\" }'\n    register: audit_lines_for_find\n    changed_when: False\n    tags:\n      - section8\n      - section8.1\n      - section8.1.12\n\n  - name: 8.1.12 Collect Use of Privileged Commands (infos) (Scored)\n    lineinfile: >\n        dest=/etc/audit/audit.rules\n        line='{{ item }}'\n        state=present\n        create=yes\n    with_items: audit_lines_for_find.stdout_lines\n    tags:\n      - section8\n      - section8.1\n      - section8.1.12\n\n  - name: 8.1.18 Make the Audit Configuration Immutable (Scored)\n    lineinfile: >\n        dest='/etc/audit/audit.rules'\n        line='-e 2'\n        insertafter=EOF\n        state=present\n        create=yes\n    tags:\n      - section8\n      - section8.1\n      - section8.1.18\n\n  - name: 8.3.1 Install AIDE (Scored)\n    apt: name=aide state=present\n    register: aide_installed\n    tags:\n      - section8\n      - section8.3\n      - section8.3.1\n\n  - name: 8.3.1 Install AIDE (init) (Scored)\n    command: aideinit\n    when: aide_installed.changed == True\n    tags:\n      - section8\n      - section8.3\n      - section8.3.1\n\n  - name: 8.3.1 Install AIDE (Scored)\n    stat: path=/var/lib/aide/aide.db.new\n    register: aide_db_path\n    when:\n      - aide_installed.changed == True\n    tags:\n      - section8\n      - section8.3\n      - section8.3.1\n\n  - name: 8.3.1 Install AIDE (copy db) (Scored)\n    command: mv /var/lib/aide/aide.db.new /var/lib/aide/aide.db\n    when:\n      - aide_installed.changed == True\n      - aide_db_path.stat.exists == True\n    tags:\n      - section8\n      - section8.3\n      - section8.3.1\n\n  - name: 8.3.2 Implement Periodic Execution of File Integrity (Scored)\n    cron: name=\"Check files integrity\" minute=\"0\" hour=\"5\" job=\"/usr/sbin/aide --check\"\n    tags:\n      - section8\n      - section8.3\n      - section8.3.2\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b65b387be8cb06b33bc8cf87c97bb84d78bb64bb", "filename": "roles/user-management/manage-user-passwd/tasks/idm-set-passwd.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Change IPA password for user: {{ item.user_name }}\"\n  ipa_user:\n    ipa_host: \"{{ ipa_host }}\"\n    ipa_user: \"{{ ipa_admin_user }}\"\n    ipa_pass: \"{{ ipa_admin_password }}\"\n    validate_certs: \"{{ ipa_validate_certs | default(False) }}\"\n    name: \"{{ item.user_name | trim }}\"\n    password: \"{{ item.password }}\"\n  with_items: \n    - \"{{ users }}\" \n  when: \n    - users is defined \n    - item.password|trim != ''\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "3d2e7ec4242379e72030741509d97bee47019876", "filename": "roles/network/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- include_tasks: detected_network.yml\n  when: not installing   #REMOVE THIS LINE IF installing IS ALWAYS false AS SET IN roles/0-init/defaults/main.yml\n  tags:\n    - network            #REMOVE SUCH LINES (BELOW TOO) AS WE'RE IN \"network\" ?\n    - network-discover\n\n- name: IF WIFI IS PRIMARY GATEWAY, PLEASE RUN 'iiab-hotspot-on' MANUALLY\n  set_fact:\n    hostapd_enabled: False    # used in (1) hostapd.yml, (2) rpi_debian.yml +\n                              # (3) its dhcpcd.conf.j2, (4) restart.yml\n    no_net_restart: True      # used below in (1) sysd-netd-debian.yml,\n                              # (2) debian.yml, (3) rpi_debian.yml\n  when: discovered_wireless_iface == iiab_wan_iface and not reboot_to_AP\n# EITHER WAY: hostapd_enabled's state is RECORDED into /etc/iiab/iiab.env\n# in hostapd.yml for later use by...\n# /usr/libexec/iiab-startup.sh, iiab-hotspot-off & iiab-hotspot-on\n#\n# Separate Idea, Not Without Risks: should WiFi-as-gateway detection logic\n# be encapsulated into roles/network/tasks/hostapd.yml in future?  Whereas\n# today \"./runtags hostapd\" doesn't exist & \"./runtags AP\" is UNSUPPORTED!\n\n#- name: RPi - reboot to AP post install - installed via wifi so the services are ready\n#  set_fact:\n#    iiab_lan_iface: br0\n#    iiab_wan_iface: \"{{ discovered_wired_iface }}\"\n#    iiab_wireless_lan_iface: \"{{ discovered_wireless_iface }}\"\n#    iiab_wired_lan_iface: \"\"\n#  when: is_rpi and discovered_wireless_iface is defined and discovered_wireless_iface == iiab_wan_iface and reboot_to_AP\n\n- include_tasks: computed_network.yml\n  when: not installing   #REMOVE THIS LINE IF installing IS ALWAYS false AS SET IN roles/0-init/defaults/main.yml\n  tags:\n    - network\n    - network-discover\n\n- include_tasks: hostapd.yml\n  tags:\n    - network\n    - AP\n\n#- name: RPi - don't reboot to AP post install - installed via wifi - don't blow away current network\n#  set_fact:\n#    no_net_restart: True\n#    hostapd_enabled: False\n#  when: is_rpi and discovered_wireless_iface is defined and discovered_wired_iface != iiab_wan_iface\n\n##### Start static ip address info for first run #####\n#- include_tasks: static.yml\n#  when: 'iiab_wan_iface != \"none\" and wan_ip != \"dhcp\"'\n##### End static ip address info\n\n- include_tasks: hosts.yml\n  tags:\n    - network\n    - hostname\n    - domain\n\n- name: Configuring wondershaper\n  include_tasks: wondershaper.yml\n  when: wondershaper_install\n  tags:\n    - network\n    - wondershaper\n\n- name: (Re)Installing named\n  include_tasks: named.yml\n  when: FQDN_changed and iiab_stage|int == 9\n\n- name: (Re)Installing dhcpd\n  include_tasks: dhcpd.yml\n  when: FQDN_changed and iiab_stage|int == 9\n\n- name: (Re)Installing Squid\n  include_tasks: squid.yml\n  when: FQDN_changed and squid_install and iiab_stage|int == 9\n\n#- name: FOREFULLY ENABLE CAPTIVE PORTAL\n#  set_fact:\n#    py_captive_portal_install: True\n\n- name: (Re)Installing captive portal\n  include_tasks: captive_portal.yml\n  when: py_captive_portal_install\n\n#### start services\n- include_tasks: avahi.yml\n  tags:\n    - network\n\n- include_tasks: computed_services.yml\n  tags:\n    - network\n    - named\n    - dhcpd\n    - dnsmasq\n    - squid\n\n- include_tasks: enable_services.yml\n  tags:\n    - network\n    - named\n    - dhcpd\n    - dnsmasq\n    - squid\n\n#### end services\n#### Start network layout\n- name: Redhat networking\n  include_tasks: ifcfg_mods.yml\n  when: is_redhat\n#and not installing\n  tags:\n    - network\n\n- name: NetworkManager in use\n  include_tasks: NM-debian.yml\n  when: is_ubuntu_18 and network_manager_active\n#and not installing\n  tags:\n    - network\n\n- name: systemd-networkd in use\n  include_tasks: sysd-netd-debian.yml\n  when: is_debuntu and systemd_networkd_active\n#and not installing\n  tags:\n    - network\n\n- name: RPi's have dhcpcd in use\n  include_tasks: rpi_debian.yml\n  when: is_debuntu and is_rpi\n#and not installing\n  tags:\n    - network\n\n- name: Not RPi, Not NetworkManager, Not systemd-networkd in use\n  include_tasks: debian.yml\n  when: (not is_rpi and not network_manager_active and not systemd_networkd_active and is_debuntu) or is_ubuntu_16\n#and not installing\n  tags:\n    - network\n\n#### end network layout\n- include_tasks: restart.yml\n  when: not installing\n  tags:\n    - network\n    - named\n    - dhcpd\n    - dnsmasq\n    - squid\n    - AP\n"}, {"commit_sha": "92dabcd706e72a0dc15ce13086fb9d59f1a8760e", "sha": "d8272a096cc4a69c1dd753fa23a6fb8fc7071948", "filename": "handlers/main.yml", "repository": "RocketChat/Rocket.Chat.Ansible", "decoded_content": "---\n# handlers/main.yml: Handlers for RocketChat.Ansible\n  - name: Reload the Nginx service\n    service:\n      name: nginx\n      state: reloaded\n\n  - name: Restart the MongoDB service\n    service:\n      name: \"{{ rocket_chat_mongodb_service_name }}\"\n      state: restarted\n\n  - name: Upgrade Rocket.Chat\n    import_tasks: upgrade.yml\n    when: ('stat' in rocket_chat_deploy_state)\n           and (rocket_chat_deploy_state.stat.exists | bool)\n    tags: upgrade\n\n  - name: Update the Rocket.Chat service configuration\n    shell: \"{{ rocket_chat_service_update_command }}\"\n    when: (rocket_chat_service_update_command is defined)\n           and (rocket_chat_service_update_command)\n\n  - name: Restart the Rocket.Chat service\n    service:\n      name: rocketchat\n      state: restarted\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c655cc6e9cf26691c9a108e4fcc7fa2042771aa1", "filename": "roles/user-management/manage-local-user-password/test/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nuser_name: user1\nclear_text_password: test1234\n\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "0714a51113c4c57a22f364509fe58098f63f04ce", "filename": "playbooks/roles/docket/tests/test.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- hosts:\n    - docket\n    - stenographer\n  remote_user: root\n  roles:\n    - rocknsm.docket\n"}, {"commit_sha": "a10c5f4577e6e74feb1fadec4bcbab039b8b180a", "sha": "c7a4c6040984cfb50f6c8c5d733e24c0e07a0e5e", "filename": "tasks/remove-pre-docker-ce.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Determine Docker version\n  command: bash -c \"docker version | grep Version | awk '{print $2}'\"\n  ignore_errors: yes\n  changed_when: false\n  register: _cmd_docker_version\n  check_mode: no\n\n- name: Set fact if old Docker installation shall be removed\n  set_fact:\n    _remove_old_docker: \"{{ docker_remove_pre_ce | bool }} and not \\\n      {{ _cmd_docker_version.stdout_lines[0] | search('-ce') }}\"\n  when: _cmd_docker_version.stdout_lines is defined and _cmd_docker_version.stdout_lines[0] is defined\n\n- name: Check if Docker is running\n  become: true\n  systemd:\n    name: docker\n  ignore_errors: yes\n  register: _service_docker_status\n  check_mode: no\n  when: _remove_old_docker | default(False) | bool\n\n- name: Stop Docker service\n  service:\n    name: docker\n    state: stopped\n  when: \"_service_docker_status.rc | default(1) == 0\"\n\n- name: Remove old Docker installation before Docker CE\n  become: true\n  package:\n    name: \"{{ item }}\"\n    state: absent\n  register: _pkg_result\n  until: _pkg_result|succeeded\n  when: _remove_old_docker | default(False) | bool\n  with_items:\n    - \"{{ docker_old_packages[_docker_os_dist] }}\"\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "f1b1197595d6ed21bad08d384d4eb89eed925f94", "filename": "tasks/replication/slave/gtid.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n# Need this hack before:\n# - https://github.com/ansible/ansible/issues/29214\n# - https://mariadb.com/kb/en/mariadb/global-transaction-id/#switching-an-existing-old-style-slave-to-use-gtid\n\n- name: MYSQL_REPLICATION | Stop slave\n  mysql_replication:\n    mode: stopslave\n\n- name: COMMAND | Migrate to MariaDB GTID\n  command: mariadb -e \"CHANGE MASTER TO master_use_gtid=current_pos\";\n\n- name: MYSQL_REPLICATION | Start slave\n  mysql_replication:\n    mode: startslave\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "210ee7e52df9feb04c5c8f671429976ad0ccce48", "filename": "roles/config-quay-enterprise/tasks/complete_setup.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Set Setup Output File Location\n  set_fact:\n    tmp_setup_file_location: /tmp/quay\n\n- name: Flush Handlers\n  meta: flush_handlers\n\n- name: Hit Quay Setup Endpoint\n  uri:\n    url: \"{{ quay_http_protocol }}://{{ quay_hostname }}/setup/\"\n    validate_certs: no\n    method: GET\n    return_content: yes\n  register: uri_setup\n  until: uri_setup.status == 200\n  retries: 30\n  delay: 10\n- name: Write file\n  copy: \n    content: \"{{ uri_setup.content }}\" \n    dest: \"{{ tmp_setup_file_location }}\"\n- name: Extract csrf token\n  shell: cat /tmp/quay | grep \"__token\" | awk -F\\' '{print $(NF-1)}'\n  register: token\n- name: Create Superuser\n  uri:\n    url:  \"{{ quay_http_protocol }}://{{ quay_server_hostname }}/api/v1/superuser/config/createsuperuser?_csrf_token={{ token.stdout | urlencode }}\"\n    validate_certs: no\n    method: POST\n    body_format: json\n    body:\n      username: \"{{ quay_superuser_username }}\"\n      email: \"{{ quay_superuser_email }}\"\n      password: \"{{ quay_superuser_password }}\"\n    headers:\n      Cookie: \"{{ uri_setup.set_cookie }}\"\n\n- name: Delete Temporary Setup File\n  file:\n    state: absent\n    path: \"{{ tmp_setup_file_location }}\"\n\n- name: Set setup_complete Fact\n  set_fact:\n    quay_setup_complete: True\n\n- name: Setup initial quay configuration file\n  template:\n    src: config.yaml.j2\n    dest: \"{{ quay_config_dir }}/config.yaml\"\n    owner: root\n    group: root\n    mode: g+rw\n  notify: Restart quay service"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "806da01147d99bbb8a5e127667ff1e385103e5ed", "filename": "roles/scm/add-webhooks-github/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- hosts: webhooks-server\n  tasks:\n    - include_role:\n        name: \"{{ playbook_dir }}/../../add-webhooks-github\"\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "a242e8fecb26f550ff98e0d5c5cfab9059a0aa6c", "filename": "tasks/main.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n- name: INCLUDE_VARS | Related to OS version\n  include_vars: \"{{ ansible_distribution }}-{{ ansible_distribution_release }}.yml\"\n\n- name: STAT | Check if mysql exists\n  stat:\n    path: /usr/sbin/mysqld\n  register: mariadb_exists\n  changed_when: false\n\n- name: INCLUDE | Install\n  include: install/main.yml\n\n- name: TEMPLATE | Deploy configuration\n  template:\n    src: \"{{ mariadb_config_template }}\"\n    dest: /etc/mysql/my.cnf\n  register: config\n\n- name: TEMPLATE | Deploy extra configuration\n  template:\n    src: etc/mysql/conf.d/10-extra.cnf.j2\n    dest: /etc/mysql/conf.d/10-extra.cnf\n  register: extraconfig\n\n- name: SERVICE | Restart now (prevent bugs)\n  service:\n    name: mysql\n    state: restarted\n  when: >\n    (config.changed or extraconfig.changed) and\n    not mariadb_galera_resetup\n\n- name: TEMPLATE Create .my.cnf for root\n  template:\n    src: root/my.cnf\n    dest: /root/.my.cnf\n    owner: root\n    group: root\n    mode: 0600\n    backup: yes\n\n- name: INCLUDE | Galera\n  include: galera/main.yml\n  when: mariadb_vendor == 'mariadb_galera'\n\n- name: INCLUDE | Replication\n  include: replication/main.yml\n  when: mariadb_replication_master or mariadb_replication_slave\n\n- name: INCLUDE | Secure install\n  include: 'secure.yml'\n\n- name: SERVICE | Ensure service is started\n  service:\n    name: mysql\n    state: started\n\n- name: MYSQL_DB | Create databases\n  mysql_db:\n    name: \"{{ item }}\"\n    state: present\n  with_items: \"{{ mariadb_databases }}\"\n\n- name: MYSQL_USER | Manages users...\n  mysql_user:\n    name: \"{{ item.name }}\"\n    password: \"{{ item.password }}\"\n    priv: \"{{ item.priv }}\"\n    host: \"{{ item.host | default('localhost') }}\"\n    state: present\n  with_items: \"{{ mariadb_users }}\"\n\n- name: TEMPLATE | Deploy logrotate configuration\n  template:\n    src: \"etc/logrotate.d/mysql-server.j2\"\n    dest: \"/etc/logrotate.d/mysql-server\"\n  when: mariadb_manage_logrotate\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "cd5f953cde2b5332a1d25055ea0be42f6ea3ef31", "filename": "playbooks/container-registry/templates/haproxy_backend.cfg.j2", "repository": "redhat-cop/infra-ansible", "decoded_content": "\n\n# Make sure the name matches the frontend name\nbackend quay_http\n    balance roundrobin\n\n    {% for host in groups.quay_enterprise %}\n    server quay_http_{{ hostvars[host]['inventory_hostname'] }} {{ hostvars[host]['ansible_eth0']['ipv4']['address'] }}:80 check\n    {% endfor %}\n\n# Make sure the name matches the frontend name\nbackend quay_https\n    mode tcp\n    balance source\n    option httpchk GET /health/instance\n    http-check expect status 200\n\n    {% for host in groups.quay_enterprise %}\n    server quay_https_{{ hostvars[host]['inventory_hostname'] }} {{ hostvars[host]['ansible_eth0']['ipv4']['address'] }}:443 check check-ssl verify none\n    {% endfor %}\n\n# Make sure the name matches the frontend name\nbackend redis\n    mode tcp\n    option tcp-check\n    tcp-check send PING\\r\\n\n    tcp-check expect string +PONG\n    tcp-check send info\\ replication\\r\\n\n    tcp-check expect string role:master\n    tcp-check send QUIT\\r\\n\n    tcp-check expect string +OK\n    {% for host in groups.redis %}\n    server redis_{{ hostvars[host]['inventory_hostname'] }} {{ hostvars[host]['ansible_eth0']['ipv4']['address'] }}:6379 check inter 1s\n    {% endfor %}\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "2ac9fe3e5a5e1be013a5eac608007bb5f383f478", "filename": "reference-architecture/vmware-ansible/playbooks/roles/keepalived_haproxy/handlers/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: restart keepalived\n  service: name=keepalived state=restarted\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "0ed9d03080073d4ab79d8729959f09c8e27886fa", "filename": "tests/tasks_verify_docker.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "# Instead of \"meta: reset_connection\" which does not work so well before Ansible 2.6\n- name: Reset SSH connection\n  file:\n    path: ~/.ansible/cp\n    state: absent\n  delegate_to: 127.0.0.1\n    \n- name: Docker info\n  become: yes\n  shell: docker info\n  register: _docker_info\n  changed_when: false\n\n- name: Print Docker info\n  debug:\n    var: _docker_info.stdout_lines\n  when: _docker_info.stdout_lines is defined\n\n- name: Run hello-world\n  become: yes\n  shell: docker run --rm hello-world\n  changed_when: false\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "eb054a74bfabcb98e6d56c1356d32bfeac91c9cb", "filename": "reference-architecture/aws-ansible/playbooks/roles/cloudformation-infra/files/user_data_master.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#cloud-config\ncloud_config_modules:\n- disk_setup\n- mounts\n\nfs_setup:\n- label: etcd_storage\n  filesystem: xfs\n  device: /dev/xvdc\n  partition: auto\n- label: emptydir\n  filesystem: xfs\n  device: /dev/xvdd\n  partition: auto\n\nruncmd:\n- mkdir -p /var/lib/etcd\n- mkdir -p /var/lib/origin/openshift.local.volumes\n\nmounts:\n- [ /dev/xvdc, /var/lib/etcd, xfs, \"defaults\" ]\n- [ /dev/xvdd, /var/lib/origin/openshift.local.volumes, xfs, \"defaults,gquota\" ]\n\nwrite_files:\n- content: |\n    DEVS='/dev/xvdb'\n    VG=docker_vol\n    DATA_SIZE=95%VG\n    STORAGE_DRIVER=overlay2\n    CONTAINER_ROOT_LV_NAME=dockerlv\n    CONTAINER_ROOT_LV_MOUNT_PATH=/var/lib/docker\n    CONTAINER_ROOT_LV_SIZE=100%FREE\n    ROOT_SIZE=45G\n    GROWPART=true\n  path: /etc/sysconfig/docker-storage-setup\n  owner: root:root\n\nusers:\n- default\n\nsystem_info:\n  default_user:\n    name: ec2-user\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "6bc13ef4bd5e3e74b43c7e8b4185a87a9ac15884", "filename": "roles/client/tasks/systems/Fedora.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- set_fact:\n    prerequisites:\n      - libselinux-python\n    configs_prefix: /etc/strongswan\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "116ac682a3ef98e24997305a43026d60a68fd5b1", "filename": "roles/openshift-applier/tasks/install-dependencies.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: \"Generate a temp dir to be used for dependencies\"\n  tempfile:\n    state: directory\n  register: tmp_dir\n  notify:\n  - Clean-up temporary dir\n  when:\n  - tmp_dep_dir is undefined\n\n- name: \"Store away the temporary directory path\"\n  set_fact:\n    tmp_dep_dir: \"{{ tmp_dir.path }}/\"\n  when:\n  - tmp_dep_dir is undefined\n\n- name: \"Run ansible-galaxy to pull in dependency roles\"\n  command: >\n    ansible-galaxy install -r \"{{ item }}\" -p \"{{ tmp_dep_dir }}\"\n  with_items:\n  - \"{{ dependencies.galaxy_requirements }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "61c40af88113c05886436ebc751ca9394379832d", "filename": "reference-architecture/azure-ansible/3.5/ansibledeployocp/playbooks/roles/azure-deploy/defaults/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ntemplatelink: \"https://raw.githubusercontent.com/openshift/openshift-ansible-contrib/master/reference-architecture/azure-ansible/3.5/azuredeploy.json\"\nnumberofnodes: 3\nimage: \"rhel\"\nmastervmsize: \"Standard_DS3_v2\"\ninfranodesize: \"Standard_DS3_v2\"\nnodevmsize: \"Standard_DS12_v2\"\nlocation: \"westus\"\nopenshiftsdn: \"redhat/openshift-ovs-multitenant\"\nmetrics: true\nlogging: true\nopslogging: false\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "63fcbad7c72bac5ebc5f7a6f3aeda8c98fd0c15b", "filename": "roles/vnstat/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "---\n- name: Install required packages\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - vnstat\n  tags:\n    - download\n\n- name: Put the config file in place\n  template:\n    src: vnstat.conf.j2\n    dest: /etc/vnstat.conf\n    mode: 0744\n    owner: root\n    group: root\n\n- name: Create database for WAN to collect vnStat data\n  shell: /usr/bin/vnstat -i {{ iiab_wan_iface }}\n\n- name: Create database for LAN to collect vnStat data if not appliance config\n  shell: /usr/bin/vnstat -i {{ iiab_lan_iface }}\n  when: iiab_lan_iface is defined\n\n- name: Start vnStat daemon via systemd\n  service:\n    name: vnstat\n    enabled: yes\n    state: started\n\n- name: Add 'vnstat' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: vnstat\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: vnStat\n    - option: description\n      value: '\"vnStat is a console-based network traffic monitor for Linux and BSD that keeps a log of network traffic for the selected interface(s).\"'\n    - option: installed\n      value: \"{{ vnstat_install }}\"\n    - option: enabled\n      value: \"{{ vnstat_enabled }}\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "a1381d0d9985e2777d0a07cee246ffdfe0b063cc", "filename": "ops/playbooks/install_logspout.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- hosts: docker\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n    - name: Copy the compose file for the global logspout service\n      template: src=../templates/logspout.yml.j2 dest=./logspout.yml\n      when: inventory_hostname in groups.ucp\n\n    - name: Deploy logspout stack\n      command: docker stack deploy --compose-file logspout.yml logspout\n      args:\n        chdir: ./\n      when: inventory_hostname in groups.ucp_main\n\n\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "beb0d7647224c6fbbfc3533764322f448e970fe8", "filename": "playbooks/provider/noop/config.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "# The noop provider should be used when VM provisioning is not needed.\n- name: Skipping hosts provisioning step\n  hosts: localhost\n  gather_facts: False\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "8941da80439573e9e2b2ab0a3f8c25da72c27224", "filename": "playbooks/openshift/delete-aws.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- hosts: localhost\n  roles:\n  - role: manage-aws-infra\n    operation: absent\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "9a5970027bb83bc11f802d5071ca54699e9465a8", "filename": "ops/playbooks/includes/load_certificates.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n\n#\n# validate certificate directory (minimalistic for now)\n#\n  - set_fact:\n      \"{{ item.var }}\": \"{{ lookup('file', item.file) }}\"      \n    with_items: \n      - { var: \"ucp_ca_cert\",     file: '{{ ucp_certs_dir }}/ca.pem' }\n      - { var: \"ucp_server_cert\", file: '{{ ucp_certs_dir }}/cert.pem' }\n      - { var: \"ucp_server_key\",  file: '{{ ucp_certs_dir }}/key.pem' }\n    when: ucp_certs_dir is defined\n\n  - set_fact:\n      \"{{ item.var }}\": \"{{ lookup('file', item.file) }}\"\n    with_items:\n      - { var: \"dtr_ca_cert\",     file: '{{ dtr_certs_dir }}/ca.pem' }\n      - { var: \"dtr_server_cert\", file: '{{ dtr_certs_dir }}/cert.pem' }\n      - { var: \"dtr_server_key\",  file: '{{ dtr_certs_dir }}/key.pem' }\n    when: dtr_certs_dir is defined\n\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "4f867a135244478feb97e2e12501e812a322b063", "filename": "roles/ovirt-common/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# check variables setting ovirt source\n- name: complain if no ovirt source is specified\n  fail:\n    msg: \"At least one of: 'ovirt_repo_file', 'ovirt_rpm_repo' or 'ovirt_repo'\n      must be specified. More information in the README\"\n  when: not (ovirt_repo_file or ovirt_rpm_repo is defined or ovirt_repo)\n\n# install libselinux-python on machine - selinux policy\n- name: install libselinux-python for ansible\n  yum:\n    name: libselinux-python\n    state: \"present\"\n\n# backup repos\n- name: creating directory repo-backup in yum.repos.d\n  file:\n    path: /tmp/repo-backup\n    state: directory\n\n- name: create repository backup\n  shell: 'cp /etc/yum.repos.d/*.repo /tmp/repo-backup'\n  tags:\n    - skip_ansible_lint\n\n## OPTIONS\n# 1) get repository files\n- name: copy repository files\n  get_url:\n    url: \"{{ item.url }}\"\n    dest: \"/etc/yum.repos.d/{{ item.name | default('') }}\"\n    force_basic_auth: yes\n    force: \"{{ item.force | default('no') }}\"\n  with_items: \"{{ ovirt_repo_file }}\"\n\n# 2) install from rpm\n- name: install rpm repository package\n  yum:\n    name: \"{{ ovirt_rpm_repo }}\"\n    state: present\n  when: ovirt_rpm_repo is defined\n\n# 3) create repository files\n- name: create repository files\n  yum_repository:\n    name: \"{{ item.name }}\"\n    description: \"{{ item.name }}\"\n    baseurl: \"{{ item.url }}\"\n    enabled: \"{{ item.enabled | default('yes') }}\"\n  with_items: \"{{ ovirt_repo }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4c5d3a408407ea7a8c1787c296fb8f6777f7df8e", "filename": "playbooks/manage-users/add-users.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Import user data from {{ csv_doc_file_name }} and create users\"\n  hosts: identity-hosts\n  gather_facts: no\n  roles:\n    - user-management/populate-users\n    - user-management/manage-idm-users\n    - user-management/manage-user-passwd\n\n- name: \"Notify users\"\n  import_playbook: ../notifications/email-notify-users.yml\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "7338ccc95a55eb085bf736172a553ac2af1ff4e2", "filename": "archive/playbooks/ose-provision.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n# Provision OpenStack instances\n- hosts: localhost\n  pre_tasks:\n  - include: roles/openstack-create/pre_tasks/pre_tasks.yml\n  - include: roles/common/pre_tasks/pre_tasks.yml\n  - include: roles/subscription-manager/pre_tasks/pre_tasks.yml\n  roles:\n  - role: common\n  - role: openshift-common\n  # Provision Master\n  - role: openstack-create\n    type: \"master\"\n    image_name: \"{{ openshift_openstack_image_name }}\"\n    security_groups: \"{{ openshift_openstack_master_security_groups }}\"\n    key_name: \"{{ openstack_key_name }}\"\n    flavor_name: \"{{ openshift_openstack_flavor_name }}\"\n    register_host_group: \"masters,openshift\"\n    node_count: \"{{ openshift_master_count }}\"\n    disk_volume: \"{{ openshift_storage_disk_volume }}\"\n    volume_size: \"{{ openshift_openstack_master_storage_size }}\"\n  # Provision Nodes\n  - role: openstack-create\n    type: \"node\"\n    image_name: \"{{ openshift_openstack_image_name }}\"\n    security_groups: \"{{ openshift_openstack_node_security_groups }}\"\n    key_name: \"{{ openstack_key_name }}\"\n    flavor_name: \"{{ openshift_openstack_flavor_name }}\"\n    register_host_group: \"nodes,openshift\"\n    node_count: \"{{ openshift_node_count }}\"\n    disk_volume: \"{{ openshift_storage_disk_volume }}\"\n    volume_size: \"{{ openshift_openstack_node_storage_size }}\"\n  # Provision NFS\n  - role: openstack-create\n    type: \"nfs\"\n    image_name: \"{{ openshift_openstack_image_name }}\"\n    security_groups: \"{{ openshift_openstack_nfs_security_groups }}\"\n    key_name: \"{{ openstack_key_name }}\"\n    flavor_name: \"{{ openshift_openstack_flavor_name }}\"\n    register_host_group: \"nfs,openshift\"\n    node_count: \"1\"\n    disk_volume: \"{{ openshift_storage_disk_volume }}\"\n    volume_size: \"{{ openshift_openstack_master_storage_size }}\"\n    # Provision DNS\n\n- include: playbooks/dns-provision.yaml\n\n- hosts: openshift\n  remote_user: \"cloud-user\"\n  vars:\n    ansible_ssh_user: cloud-user\n  tasks:\n  - name: \"Enable direct root access\"\n    shell: \"cat ~/.ssh/authorized_keys | sudo tee /root/.ssh/authorized_keys >/dev/null\"\n\n- hosts: all:!dns:!localhost\n  pre_tasks:\n  - include: roles/common/pre_tasks/pre_tasks.yml\n  roles:\n  - role: hostnames\n\n- hosts: all:!localhost\n  roles:\n    - { role: subscription-manager, when: hostvars.localhost.rhsm_register, tags: 'subscription-manager', ansible_sudo: true }\n\n- hosts: localhost\n  pre_tasks:\n  - include: roles/common/pre_tasks/pre_tasks.yml\n  - name: \"Generate dns-server views\"\n    include: playbooks/dns_dual_view.yaml\n  - name: \"Generate dns records\"\n    include: playbooks/dns_records.yaml\n\n- hosts: dns\n  pre_tasks:\n  - name: \"Include the generated views\"\n    include_vars: /tmp/named_views.yaml\n    delegate_to: localhost\n  - name: \"Include generated dns records\"\n    include_vars: /tmp/records.yaml\n    delegate_to: localhost\n  roles:\n    - role: dns-server\n    - role: dns\n\n# Use newly configured DNS server for this container ...\n- hosts: localhost\n  tasks:\n  - name: \"Edit /etc/resolv.conf in container\"\n    shell: \"sed '0,/.*nameserver.*/s/.*nameserver.*/nameserver {%for host in groups['dns']%}{{ hostvars[host].dns_public_ip }}{% endfor %}\\\\n&/' /etc/resolv.conf > /tmp/resolv.conf && /bin/cp -f /tmp/resolv.conf /etc/resolv.conf\"\n\n\n# Install and configure OpenShift\n\n- hosts: openshift:!dns\n  tasks:\n  - name: \"Edit /etc/resolv.conf on masters/nodes\"\n    lineinfile:\n      state: present\n      dest: /etc/resolv.conf\n      regexp: \"nameserver {%for host in groups['dns']%} {{ hostvars[host].dns_private_ip }} {% endfor %}\"\n      line: \"nameserver {%for host in groups['dns']%} {{ hostvars[host].dns_private_ip }} {% endfor %}\"\n      insertafter: search*\n  - name: \"Include DHCP/DNS workaround for OSE 3.2\"\n    lineinfile:\n      state: present\n      dest: /etc/sysconfig/network\n      regexp: \"IP4_NAMESERVERS={%for host in groups['dns']%}{{ hostvars[host].dns_private_ip }}{% endfor %}\"\n      line: \"IP4_NAMESERVERS={%for host in groups['dns']%}{{ hostvars[host].dns_private_ip }}{% endfor %}\"\n  roles:\n    - { role: docker, tags: 'docker' }\n    - { role: openshift-prep, tags: 'openshift-prep', ansible_sudo: true }\n\n- hosts: localhost\n  roles:\n    - openshift-install\n"}, {"commit_sha": "8d4956fcd97d78caa57ee3e5a36e9c44a23ab2a6", "sha": "f846c3f5c76e9d368274f25e5624f32840cd3c5d", "filename": "tasks/postinstall.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Reset internal variables for additional packages to be installed\n  set_fact:\n    _docker_additional_packages_os: []\n    _docker_additional_packages_pip: []\n\n- name: Set facts to install Docker SDK for Python\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + \\\n      docker_predefined_packages_pip[_docker_os_dist]['sdk'] }}\"\n  when:\n    - docker_sdk | bool\n\n- name: Set facts to install Docker Compose\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + \\\n      docker_predefined_packages_pip[_docker_os_dist]['compose'] }}\"\n  when:\n    - docker_compose | bool and not docker_compose_no_pip | bool\n\n- name: Set facts to install Docker Stack dependencies ('docker_stack')\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + \\\n      docker_predefined_packages_pip[_docker_os_dist]['stack'] }}\"\n  when:\n    - docker_stack | bool\n\n- name: Set facts with additional package to be installed\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + docker_additional_packages_pip }}\"\n    _docker_additional_packages_os: \"{{ _docker_additional_packages_os + docker_additional_packages_os }}\"\n\n- name: Ensure required OS packages will be installed for PiP\n  block:\n    - name: Determine if pip/pip3 exists in path\n      become: true\n      shell: type {{ _docker_python3 | ternary('pip3', 'pip') }}\n      register: _docker_pip_cmd\n      changed_when: false\n      failed_when: false\n      check_mode: no\n      tags:\n        - skip_ansible_lint\n\n    - name: Set fact to install Python 2 PiP\n      set_fact:\n        _docker_additional_packages_os: \"{{ _docker_additional_packages_os + [docker_pip_package] }}\"\n      when:\n        - not _docker_python3 | bool\n        - _docker_pip_cmd.rc != 0\n\n    - name: Set fact to install Python 3 PiP\n      set_fact:\n        _docker_additional_packages_os: \"{{ _docker_additional_packages_os + [docker_pip3_package] }}\"\n      when:\n        - _docker_python3 | bool\n        - _docker_pip_cmd.rc != 0\n\n    - name: Set fact to install build libraries (CentOS/Fedora/RedHat)\n      set_fact:\n        _docker_additional_packages_os: \"{{ _docker_additional_packages_os + ['openssl-devel'] }}\"\n      when:\n        - (_docker_os_dist == \"CentOS\" or _docker_os_dist == \"Fedora\" or _docker_os_dist == \"RedHat\")\n\n    - name: Set fact to install build libraries [General] (Fedora)\n      set_fact:\n        _docker_additional_packages_os: \"{{ _docker_additional_packages_os + ['redhat-rpm-config','make','libffi-devel','gcc'] }}\"\n      when:\n        - _docker_os_dist == \"Fedora\"\n\n    - name: Set fact to install build libraries [Python 3] (Fedora)\n      set_fact:\n        _docker_additional_packages_os: \"{{ _docker_additional_packages_os + ['python3-devel'] }}\"\n      when:\n        - _docker_python3 | bool\n        - _docker_os_dist == \"Fedora\"\n\n    - name: Set fact to install build libraries (Debian/Ubuntu)\n      set_fact:\n        _docker_additional_packages_os: \"{{ _docker_additional_packages_os + ['libffi-dev', 'libssl-dev'] }}\"\n      when:\n        - (_docker_os_dist == \"Debian\" or _docker_os_dist == \"Ubuntu\")\n  when:\n    - _docker_additional_packages_pip | length > 0\n\n- name: Ensure python-pip-whl is present (Debian 8)\n  set_fact:\n    _docker_additional_packages_os: \"{{ _docker_additional_packages_os + ['python-pip-whl'] }}\"\n  when:\n    - _docker_additional_packages_pip | length > 0\n    - _docker_pip_cmd.rc != 0\n    - _docker_os_dist == \"Debian\"\n    - _docker_os_dist_major_version | int == 8\n\n- name: Ensure python-backports.ssl-match-hostname is present (Debian 10)\n  set_fact:\n    _docker_additional_packages_os: \"{{ _docker_additional_packages_os + ['python-backports.ssl-match-hostname'] }}\"\n  when:\n    - not _docker_python3 | bool\n    - _docker_additional_packages_pip | length > 0\n    - _docker_pip_cmd.rc != 0\n    - _docker_os_dist == \"Debian\"\n    - _docker_os_dist_major_version | int == 10\n\n- name: Ensure EPEL release repository is installed\n  become: true\n  package:\n    name: \"epel-release\"\n    state: present\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when:\n    - _docker_os_dist == \"CentOS\"\n    - _docker_additional_packages_os | length > 0\n\n- name: Install additional packages (OS package manager)\n  become: true\n  package:\n    name: \"{{ item }}\"\n    state: present\n  loop: \"{{ _docker_additional_packages_os }}\"\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when: _docker_additional_packages_os | length > 0\n\n- name: Upgrade PiP\n  become: \"{{ docker_pip_sudo | bool }}\"\n  pip:\n    name: pip\n    state: forcereinstall\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when: docker_pip_upgrade | bool\n\n- name: Install additional packages (PiP)\n  become: \"{{ docker_pip_sudo | bool }}\"\n  pip:\n    name: \"{{ item }}\"\n    state: present\n    extra_args: \"{{ docker_pip_extra_args }}\"\n  loop: \"{{ _docker_additional_packages_pip }}\"\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when: _docker_additional_packages_pip | length > 0\n  environment:\n    PYTHONWARNINGS: ignore\n\n# https://github.com/docker/docker-py/issues/1502\n- name: Workaround for issue - No module named ssl_match_hostname (Python 2.7)\n  become: yes\n  command: cp -r /usr/local/lib/python2.7/dist-packages/backports/ssl_match_hostname/ /usr/lib/python2.7/dist-packages/backports\n  when:\n    - docker_x_ssl_match_hostname | bool\n    - _docker_additional_packages_pip | length > 0\n    - not _docker_python3 | bool\n\n- name: Stat /usr/bin/docker-compose\n  stat:\n    path: /usr/bin/docker-compose\n  register: _docker_compose_file\n  check_mode: no\n\n# Official installation of docker-compose (Linux): https://docs.docker.com/compose/install/#install-compose\n- name: Install docker-compose without PiP\n  block:\n    # Not using github_release:  https://github.com/ansible/ansible/issues/45391\n    - name: Get latest release of docker-compose\n      uri:\n        url: https://api.github.com/repos/docker/compose/releases/latest\n        body_format: json\n      register: _github_docker_compose\n      until: _github_docker_compose.status == 200\n      retries: 10\n      check_mode: no\n      when:\n        - docker_compose_no_pip_detect_version | bool\n\n    - name: Set detected docker-compose version\n      set_fact:\n        _docker_compose_version: \"{{ _github_docker_compose.json.tag_name }}\"\n      when:\n        - _github_docker_compose is defined\n        - _github_docker_compose.json is defined\n\n    - name: Set fixed docker-compose version\n      set_fact:\n        _docker_compose_version: \"{{ docker_compose_no_pip_version }}\"\n      when:\n        - not docker_compose_no_pip_detect_version | bool\n\n    - name: Fetch docker-compose SHA265 sum file\n      get_url:\n        url: \"https://github.com/docker/compose/releases/download/\\\n          {{ _docker_compose_version }}/docker-compose-{{ ansible_system }}-{{ ansible_architecture }}.sha256\"\n        dest: \"/tmp/ansible.docker-compose-sha256\"\n      register: _github_docker_compose_shasum_file\n      changed_when: false\n      until: _github_docker_compose_shasum_file.status_code == 200\n      retries: 10\n      check_mode: no\n\n    - name: Dump SHA256 file contents to variable\n      command: cat /tmp/ansible.docker-compose-sha256\n      register: _github_docker_compose_shasum\n      changed_when: false\n      check_mode: no\n\n    - name: Remove temporary file for SHA256 sum\n      file:\n        path: \"/tmp/ansible.docker-compose-sha256\"\n        state: absent\n      changed_when: false\n      check_mode: no\n\n    - name: Set SHA256 facts related to docker-compose\n      set_fact:\n        _docker_compose_checksum: \"sha256:{{ _github_docker_compose_shasum.stdout | \\\n          regex_replace('^([0-9a-zA-Z]*)[\\\\s\\\\t]+.+', '\\\\1') }}\"\n\n    # Use when moving to Ansible 2.7 as minimum version\n    # - name: Set SHA256 facts related to docker-compose (Ansible >= 2.7)\n    #   set_fact:\n    #     _docker_compose_checksum: \"sha256:https://github.com/docker/compose/releases/download/\\\n    #       {{ _github_docker_compose.json.tag_name }}/\\\n    #       docker-compose-{{ ansible_system }}-{{ ansible_architecture }}.sha256\"\n    #   when: ansible_version.full is version_compare('2.7', '>=')\n\n    - name: Install docker-compose {{ _docker_compose_version }} (Linux)\n      become: true\n      get_url:\n        url: \"https://github.com/docker/compose/releases/download/\\\n          {{ _docker_compose_version }}/docker-compose-{{ ansible_system }}-{{ ansible_architecture }}\"\n        checksum: \"{{ _docker_compose_checksum }}\"\n        dest: /usr/local/bin/docker-compose\n        mode: 0755\n\n    - name: Create symlink for docker-compose to work with sudo in some distributions\n      become: true\n      file:\n        src: /usr/local/bin/docker-compose\n        dest: /usr/bin/docker-compose\n        state: link\n  when:\n    - docker_compose | bool\n    - docker_compose_no_pip | bool\n    - not _docker_compose_file.stat.exists\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "1b39744e994ae9bff0ddd700f27ed3a862dd295e", "filename": "playbooks/templates/filebeat.yml.j2", "repository": "rocknsm/rock", "decoded_content": "#=========================== Filebeat prospectors =============================\n\nfilebeat.prospectors:\n- input_type: log\n  paths:\n    - {{ rock_data_dir }}/suricata/eve.json\n  json.keys_under_root: true\n  fields:\n    kafka_topic: suricata-raw\n  fields_under_root: true\n- input_type: log\n  paths:\n    - {{ rock_data_dir }}/fsf/rockout.log\n  json.keys_under_root: true\n  fields:\n    kafka_topic: fsf-raw\n  fields_under_root: true\nprocessors:\n - decode_json_fields:\n     fields: [\"message\",\"Scan Time\", \"Filename\", \"objects\", \"Source\", \"meta\", \"Alert\" ,\"Summary\"]\n     process_array: true\n     max_depth: 10\n\n#================================ General =====================================\n\n# The name of the shipper that publishes the network data. It can be used to group\n# all the transactions sent by a single shipper in the web interface.\n#name:\n\n# The tags of the shipper are included in their own field with each\n# transaction published.\n#tags: [\"service-X\", \"web-tier\"]\n\n# Optional fields that you can specify to add additional information to the\n# output.\n#fields:\n#  env: staging\n\n#================================ Outputs =====================================\n\n# Configure what outputs to use when sending the data collected by the beat.\n# Multiple outputs may be used.\n\noutput.kafka:\n  hosts: [\"localhost:9092\"]\n\n  topic: '%{[kafka_topic]}'\n  required_acks: 1\n  compression: gzip\n  max_message_bytes: 1000000\n\n#================================ Logging =====================================\n\n# Sets log level. The default log level is info.\n# Available log levels are: critical, error, warning, info, debug\n#logging.level: debug\n\n# At debug level, you can selectively enable logging only for some components.\n# To enable all selectors use [\"*\"]. Examples of other selectors are \"beat\",\n# \"publish\", \"service\".\n#logging.selectors: [\"*\"]\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "2e6dbfbcd6edc3cf310e0f901b14ba5490f7681d", "filename": "reference-architecture/gcp/ansible/playbooks/unregister.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: create instance groups\n  hosts: localhost\n  roles:\n  - instance-groups\n\n- include: ../../../../playbooks/unregister.yaml\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "8685624ececfa1263a23f786ac4ec1385036163c", "filename": "playbooks/libvirt/openshift-cluster/tasks/configure_libvirt_storage_pool.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Create libvirt storage directory for openshift\n  file:\n    dest: \"{{ libvirt_storage_pool_path }}\"\n    state: directory\n\n# We need to set permissions on the directory and any items created under the directory, so we need to call the acl module with and without default set.\n- acl:\n    default: '{{ item.default }}'\n    entity: kvm\n    etype: group\n    name: \"{{ libvirt_storage_pool_path }}\"\n    permissions: '{{ item.permissions }}'\n    state: present\n  with_items:\n    - default: no\n      permissions: x\n    - default: yes\n      permissions: rwx\n\n- name: Create the libvirt storage pool for OpenShift\n  virt_pool:\n    name: '{{ libvirt_storage_pool }}'\n    state: '{{ item }}'\n    autostart: 'yes'\n    xml: \"{{ lookup('template', 'storage-pool.xml') }}\"\n    uri: '{{ libvirt_uri }}'\n  with_items:\n    - present\n    - active\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "2cdc6dd4334609446f2ea18a83876ada1594b410", "filename": "roles/ipareplica/tasks/main.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "---\n# tasks file for ipareplica\n\n- name: Import variables specific to distribution\n  include_vars: \"{{ item }}\"\n  with_first_found:\n    - \"vars/{{ ansible_distribution }}-{{ ansible_distribution_version }}.yml\"\n    - \"vars/{{ ansible_distribution }}-{{ ansible_distribution_major_version }}.yml\"\n    - \"vars/{{ ansible_distribution }}.yml\"\n    - \"vars/default.yml\"\n\n- name: Install IPA replica\n  include_tasks: tasks/install.yml\n  when: state|default('present') == 'present'\n\n- name: Uninstall IPA replica\n  include_tasks: tasks/uninstall.yml\n  when: state|default('present') == 'absent'\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "d7631ee0fc1748f301e3f16348765dfa8689eb8b", "filename": "meta/main.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\ngalaxy_info:\n  description: Tools to provision resources, deploy clusters and install KubeVirt\n  issue_tracker_url: https://github.com/kubevirt/kubevirt-ansible/issues\n  license: Apache 2.0\n  min_ansible_version: 2.4.2\n\ndependencies: []\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "18ec9dc7ce3dc0274d29d2cb681c52f819f0a396", "filename": "reference-architecture/aws-ansible/playbooks/roles/cloudformation-infra/files/greenfield.json.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "{\n  \"AWSTemplateFormatVersion\": \"2010-09-09\",\n  \"Parameters\": {\n    \"VpcCidrBlock\": {\n      \"Type\": \"String\"\n    },\n    \"VpcName\": {\n      \"Type\": \"String\",\n      \"Default\": \"ose-on-aws\"\n    },\n    \"S3BucketName\": {\n      \"Type\": \"String\"\n    },\n    \"S3User\": {\n      \"Type\": \"String\"\n    },\n    \"MasterApiPort\": {\n      \"Type\": \"Number\"\n    },\n    \"MasterHealthTarget\": {\n      \"Type\": \"String\"\n    },\n    \"Route53HostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"PublicHostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"Region\": {\n      \"Type\": \"String\"\n    },\n    \"MasterClusterPublicHostname\": {\n      \"Type\": \"String\"\n    },\n    \"MasterClusterHostname\": {\n      \"Type\": \"String\"\n    },\n    \"AppWildcardDomain\": {\n      \"Type\": \"String\"\n    },\n    \"SubnetCidrBlocks\": {\n      \"Type\": \"CommaDelimitedList\"\n    },\n    \"KeyName\": {\n      \"Type\": \"AWS::EC2::KeyPair::KeyName\"\n    },\n    \"MasterInstanceType\": {\n      \"Type\": \"String\"\n    },\n    \"AmiId\": {\n      \"Type\": \"AWS::EC2::Image::Id\"\n    },\n    \"BastionUserData\": {\n      \"Type\": \"String\"\n    },\n    \"MasterDockerVolSize\": {\n      \"Type\": \"String\"\n    },\n    \"MasterEtcdVolSize\": {\n      \"Type\": \"String\"\n    },\n    \"MasterUserData\": {\n      \"Type\": \"String\"\n    },\n    \"MasterEtcdVolType\": {\n      \"Type\": \"String\"\n    },\n    \"MasterDockerVolType\": {\n      \"Type\": \"String\"\n    },\n    \"InfraInstanceType\": {\n      \"Type\": \"String\"\n    },\n    \"InfraDockerVolSize\": {\n      \"Type\": \"String\"\n    },\n    \"InfraDockerVolType\": {\n      \"Type\": \"String\"\n    },\n    \"AppNodeInstanceType\": {\n      \"Type\": \"String\"\n    },\n    \"NodeUserData\": {\n      \"Type\": \"String\"\n    },\n    \"NodeDockerVolSize\": {\n      \"Type\": \"String\"\n    },\n    \"NodeDockerVolType\": {\n      \"Type\": \"String\"\n    }\n  },\n  \"Conditions\": {\n    \"CreateDhcpOpts\": {\"Fn::Equals\" : [{\"Ref\" : \"Region\"}, \"us-east-1\"]}\n  },\n  \"Resources\": {\n    \"Vpc\": {\n      \"Type\": \"AWS::EC2::VPC\",\n      \"Properties\": {\n        \"CidrBlock\": { \"Ref\": \"VpcCidrBlock\" },\n        \"EnableDnsSupport\": \"true\",\n        \"EnableDnsHostnames\": \"true\",\n        \"Tags\": [ { \"Key\": \"Name\", \"Value\": { \"Ref\": \"VpcName\" } } ]\n      }\n    },\n    \"VpcInternetGateway\": {\n      \"Type\": \"AWS::EC2::InternetGateway\",\n      \"Properties\": {}\n    },\n    \"VpcGA\": {\n      \"Type\": \"AWS::EC2::VPCGatewayAttachment\",\n      \"Properties\": {\n        \"InternetGatewayId\": { \"Ref\": \"VpcInternetGateway\" },\n        \"VpcId\": { \"Ref\": \"Vpc\" }\n      }\n    },\n    \"VpcRouteTable\": {\n      \"Type\": \"AWS::EC2::RouteTable\",\n      \"Properties\": {\n        \"VpcId\": { \"Ref\": \"Vpc\" }\n      }\n    },\n    \"PrivateRouteTable\": {\n      \"DependsOn\": [\"Nat\"],\n      \"Type\": \"AWS::EC2::RouteTable\",\n      \"Properties\": {\n        \"VpcId\": { \"Ref\": \"Vpc\" }\n      }\n    },\n    \"VPCRouteInternetGateway\": {\n      \"Type\": \"AWS::EC2::Route\",\n      \"Properties\": {\n        \"GatewayId\": { \"Ref\": \"VpcInternetGateway\" },\n        \"DestinationCidrBlock\": \"0.0.0.0/0\",\n        \"RouteTableId\": { \"Ref\": \"VpcRouteTable\" }\n      }\n    },\n    \"EIP\" : {\n       \"Type\" : \"AWS::EC2::EIP\",\n       \"Properties\" : {\n          \"Domain\" : \"vpc\"\n      }\n    },\n    \"Route\" : {\n      \"DependsOn\": [\"Nat\"],\n      \"Type\" : \"AWS::EC2::Route\",\n      \"Properties\" : {\n         \"RouteTableId\" : { \"Ref\" : \"PrivateRouteTable\" },\n         \"DestinationCidrBlock\" : \"0.0.0.0/0\",\n         \"NatGatewayId\" : { \"Ref\" : \"Nat\" }\n      }\n     },\n    \"PublicSubnet1\": {\n      \"Type\": \"AWS::EC2::Subnet\",\n      \"DependsOn\": [\"Vpc\"],\n      \"Properties\": {\n        \"AvailabilityZone\": {\"Fn::Select\": [\"0\", { \"Fn::GetAZs\" : \"\" }]},\n        \"CidrBlock\": {\"Fn::Select\": [\"0\", {\"Ref\": \"SubnetCidrBlocks\"}]},\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"Public_Subnet_1\"}, {\"Key\": \"KubernetesCluster\", \"Value\": { \"Ref\": \"AWS::StackName\" }}],\n        \"MapPublicIpOnLaunch\": \"true\",\n        \"VpcId\": { \"Ref\": \"Vpc\" }\n      }\n    },\n    \"PublicSubnet1RTA\": {\n      \"Type\": \"AWS::EC2::SubnetRouteTableAssociation\",\n      \"Properties\": {\n        \"RouteTableId\": { \"Ref\": \"VpcRouteTable\" },\n        \"SubnetId\": { \"Ref\": \"PublicSubnet1\" }\n      }\n    },\n    \"PublicSubnet2\": {\n      \"Type\": \"AWS::EC2::Subnet\",\n      \"DependsOn\": [\"Vpc\"],\n      \"Properties\": {\n        \"AvailabilityZone\": {\"Fn::Select\": [\"1\", { \"Fn::GetAZs\" : \"\" }]},\n        \"CidrBlock\": {\"Fn::Select\": [\"1\", {\"Ref\": \"SubnetCidrBlocks\"}]},\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"Public_Subnet_2\"}, {\"Key\": \"KubernetesCluster\", \"Value\": { \"Ref\": \"AWS::StackName\" }}],\n        \"MapPublicIpOnLaunch\": \"true\",\n        \"VpcId\": { \"Ref\": \"Vpc\" }\n      }\n    },\n    \"PublicSubnet2RTA\": {\n      \"Type\": \"AWS::EC2::SubnetRouteTableAssociation\",\n      \"Properties\": {\n        \"RouteTableId\": { \"Ref\": \"VpcRouteTable\" },\n        \"SubnetId\": { \"Ref\": \"PublicSubnet2\" }\n      }\n    },\n    \"PublicSubnet3\": {\n      \"Type\": \"AWS::EC2::Subnet\",\n      \"DependsOn\": [\"Vpc\"],\n      \"Properties\": {\n        \"AvailabilityZone\": {\"Fn::Select\": [\"2\", { \"Fn::GetAZs\" : \"\" }]},\n        \"CidrBlock\": {\"Fn::Select\": [\"2\", {\"Ref\": \"SubnetCidrBlocks\"}]},\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"Public_Subnet_3\"}, {\"Key\": \"KubernetesCluster\", \"Value\": { \"Ref\": \"AWS::StackName\" }}],\n        \"MapPublicIpOnLaunch\": \"true\",\n        \"VpcId\": { \"Ref\": \"Vpc\" }\n      }\n    },\n    \"PublicSubnet3RTA\": {\n      \"Type\": \"AWS::EC2::SubnetRouteTableAssociation\",\n      \"Properties\": {\n        \"RouteTableId\": { \"Ref\": \"VpcRouteTable\" },\n        \"SubnetId\": { \"Ref\": \"PublicSubnet3\" }\n      }\n    },\n    \"PrivateSubnet1\": {\n      \"DependsOn\": [\"Nat\"],\n      \"Type\": \"AWS::EC2::Subnet\",\n      \"Properties\": {\n        \"AvailabilityZone\": {\"Fn::Select\": [\"0\", { \"Fn::GetAZs\" : \"\" }]},\n        \"CidrBlock\": {\"Fn::Select\": [\"3\", {\"Ref\": \"SubnetCidrBlocks\"}]},\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"Private_Subnet_1\"}, {\"Key\": \"KubernetesCluster\", \"Value\": { \"Ref\": \"AWS::StackName\" }}],\n        \"VpcId\": { \"Ref\": \"Vpc\" }\n      }\n    },\n    \"PrivateSubnet1RTA\" : {\n      \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\",\n      \"Properties\" : {\n         \"SubnetId\" : { \"Ref\" : \"PrivateSubnet1\" },\n         \"RouteTableId\" : { \"Ref\" : \"PrivateRouteTable\" }\n      }\n    },\n    \"PrivateSubnet2\": {\n      \"DependsOn\": [\"Nat\"],\n      \"Type\": \"AWS::EC2::Subnet\",\n      \"Properties\": {\n        \"AvailabilityZone\": {\"Fn::Select\": [\"1\", { \"Fn::GetAZs\" : \"\" }]},\n        \"CidrBlock\": {\"Fn::Select\": [\"4\", {\"Ref\": \"SubnetCidrBlocks\"}]},\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"Private_Subnet_2\"}, {\"Key\": \"KubernetesCluster\", \"Value\": { \"Ref\": \"AWS::StackName\" }}],\n        \"VpcId\": { \"Ref\": \"Vpc\" }\n      }\n    },\n    \"PrivateSubnet2RTA\" : {\n      \"DependsOn\": [\"Nat\"],\n      \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\",\n      \"Properties\" : {\n         \"SubnetId\" : { \"Ref\" : \"PrivateSubnet2\" },\n         \"RouteTableId\" : { \"Ref\" : \"PrivateRouteTable\" }\n      }\n    },\n    \"PrivateSubnet3\": {\n      \"DependsOn\": [\"Nat\"],\n      \"Type\": \"AWS::EC2::Subnet\",\n      \"Properties\": {\n        \"AvailabilityZone\": {\"Fn::Select\": [\"2\", { \"Fn::GetAZs\" : \"\" }]},\n        \"CidrBlock\": {\"Fn::Select\": [\"5\", {\"Ref\": \"SubnetCidrBlocks\"}]},\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"Private_Subnet_3\"}, {\"Key\": \"KubernetesCluster\", \"Value\": { \"Ref\": \"AWS::StackName\" }}],\n        \"VpcId\": { \"Ref\": \"Vpc\" }\n      }\n    },\n    \"PrivateSubnet3RTA\" : {\n       \"Type\" : \"AWS::EC2::SubnetRouteTableAssociation\",\n       \"Properties\" : {\n          \"SubnetId\" : { \"Ref\" : \"PrivateSubnet3\" },\n          \"RouteTableId\" : { \"Ref\" : \"PrivateRouteTable\" }\n      }\n    },\n    \"Nat\" : {\n      \"DependsOn\": [\"EIP\"],\n      \"Type\" : \"AWS::EC2::NatGateway\",\n      \"Properties\" : {\n          \"AllocationId\" : { \"Fn::GetAtt\" : [\"EIP\", \"AllocationId\"]},\n          \"SubnetId\" : { \"Ref\" : \"PublicSubnet1\"}\n      }\n    },\n    \"EIP\" : {\n       \"Type\" : \"AWS::EC2::EIP\",\n       \"Properties\" : {\n          \"Domain\" : \"vpc\"\n      }\n    },\n    \"Route\" : {\n      \"DependsOn\": [\"Nat\"],\n      \"Type\" : \"AWS::EC2::Route\",\n      \"Properties\" : {\n         \"RouteTableId\" : { \"Ref\" : \"PrivateRouteTable\" },\n         \"DestinationCidrBlock\" : \"0.0.0.0/0\",\n         \"NatGatewayId\" : { \"Ref\" : \"Nat\" }\n      }\n     },\n    \"DHCPOpts\" : {\n      \"Type\" : \"AWS::EC2::DHCPOptions\",\n      \"Condition\": \"CreateDhcpOpts\",\n      \"Properties\" : {\n        \"DomainName\": \"ec2.internal\",\n        \"DomainNameServers\": [ \"AmazonProvidedDNS\" ]\n      }\n    },\n    \"AssociateOpts\" : {\n      \"Type\" : \"AWS::EC2::VPCDHCPOptionsAssociation\",\n      \"Condition\": \"CreateDhcpOpts\",\n      \"Properties\" : {\n          \"VpcId\": { \"Ref\" : \"Vpc\" },\n          \"DhcpOptionsId\": { \"Ref\" : \"DHCPOpts\" }\n      }\n    },\n    \"BastionSg\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"bastion-sg\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"bastion_sg\"} ],\n        \"SecurityGroupIngress\": [\n          {\n            \"IpProtocol\": \"tcp\",\n            \"FromPort\": \"22\",\n            \"ToPort\": \"22\",\n            \"CidrIp\": \"0.0.0.0/0\"\n          }\n        ]\n      }\n    },\n    \"EtcdSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"etcd\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_etcd_sg\"} ]\n      }\n    },\n    \"InfraElbSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Infra Load Balancer\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_router_sg\"} ],\n        \"SecurityGroupIngress\": [\n          {\n            \"IpProtocol\": \"tcp\",\n            \"FromPort\": \"80\",\n            \"ToPort\": \"80\",\n            \"CidrIp\": \"0.0.0.0/0\"\n          },\n          {\n            \"IpProtocol\": \"tcp\",\n            \"FromPort\": \"443\",\n            \"ToPort\": \"443\",\n            \"CidrIp\": \"0.0.0.0/0\"\n          }\n        ]\n      }\n    },\n    \"MasterExtElbSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Master External Load Balancer\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_elb_master_sg\"} ],\n        \"SecurityGroupIngress\": [\n          {\n            \"IpProtocol\": \"tcp\",\n            \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n            \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n            \"CidrIp\": \"0.0.0.0/0\"\n          }\n        ]\n      }\n    },\n    \"MasterIntElbSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Master Internal Load Balancer\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_internal_elb_master_sg\"} ]\n      }\n    },\n    \"InfraSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Infra\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_infra_node_sg\"} ]\n      }\n    },\n    \"NodeSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Node\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_node_sg\"}, {\"Key\": \"KubernetesCluster\", \"Value\": { \"Ref\": \"AWS::StackName\" }}]\n      }\n    },\n    \"MasterSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Master\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_master_sg\"} ]\n      }\n    },\n    \"InfraElbEgressHTTP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"80\",\n        \"ToPort\": \"80\",\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] }\n      }\n    },\n    \"ElasticApi\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"9200\",\n        \"ToPort\": \"9200\",\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] }\n      }\n    },\n    \"ElasticCluster\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"9300\",\n        \"ToPort\": \"9300\",\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] }\n      }\n    },\n    \"LoggingCluster\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"443\",\n        \"ToPort\": \"443\",\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterExtElbEgressAPI\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterExtElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIntElbEgressAPI\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterIntElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIntElbIngressMasters\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterIntElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIntElbIngressNodes\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterIntElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"InfraIngressHTTP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"80\",\n        \"ToPort\": \"80\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] }\n      }\n    },\n    \"InfraIngressHTTPS\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"443\",\n        \"ToPort\": \"443\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] }\n      }\n    },\n    \"NodeIngressMasterKubelet\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"10250\",\n        \"ToPort\": \"10250\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterDaemon\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24007\",\n        \"ToPort\": \"24007\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterManagement\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24008\",\n        \"ToPort\": \"24008\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterSsh\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"2222\",\n        \"ToPort\": \"2222\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterNfs\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"49152\",\n        \"ToPort\": \"49664\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"NodeIngressNodeKubelet\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"10250\",\n        \"ToPort\": \"10250\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"NodeIngressNodeVXLAN\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"udp\",\n        \"FromPort\": \"4789\",\n        \"ToPort\": \"4789\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"NodeIngressSsh\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"22\",\n        \"ToPort\": \"22\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"BastionSg\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressIntLB\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterIntElbSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressExtLB\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterExtElbSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressNodesDNSUDP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"udp\",\n        \"FromPort\": \"8053\",\n        \"ToPort\": \"8053\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressNodesDNSTCP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"8053\",\n        \"ToPort\": \"8053\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressNodesAPITCP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"LoggingTCP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24224\",\n        \"ToPort\": \"24224\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"LoggingUDP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"udp\",\n        \"FromPort\": \"24224\",\n        \"ToPort\": \"24224\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressMastersAPITCP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"EtcdIngressEtcd\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"EtcdSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"2379\",\n        \"ToPort\": \"2379\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"EtcdSG\", \"GroupId\" ] }\n      }\n    },\n    \"EtcdIngressMasters\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"EtcdSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"2379\",\n        \"ToPort\": \"2379\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"EtcdIngressEtcdPeer\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"EtcdSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"2380\",\n        \"ToPort\": \"2380\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"EtcdSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIntElb\": {\n      \"Type\": \"AWS::ElasticLoadBalancing::LoadBalancer\",\n      \"Properties\": {\n        \"CrossZone\": \"true\",\n        \"ConnectionSettings\": {\"IdleTimeout\" : 300},\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_internal_master_elb\"} ],\n        \"HealthCheck\": {\n          \"HealthyThreshold\" : \"2\",\n          \"Interval\" : \"5\",\n          \"Target\" : { \"Ref\": \"MasterHealthTarget\" },\n          \"Timeout\" : \"2\",\n          \"UnhealthyThreshold\" : \"2\"\n        },\n        \"Listeners\":[\n          {\n            \"InstancePort\": { \"Ref\" : \"MasterApiPort\" },\n            \"InstanceProtocol\": \"TCP\",\n            \"LoadBalancerPort\": { \"Ref\" : \"MasterApiPort\" },\n            \"Protocol\": \"TCP\"\n          }\n        ],\n        \"Scheme\": \"internal\",\n        \"SecurityGroups\": [ { \"Ref\": \"MasterIntElbSG\" } ],\n        \"Subnets\": [\n          {\"Ref\": \"PrivateSubnet1\"},\n          {\"Ref\": \"PrivateSubnet2\"},\n          {\"Ref\": \"PrivateSubnet3\"}\n            ],\n        \"Instances\": [\n          {\"Ref\": \"Master01\"},\n          {\"Ref\": \"Master02\"},\n          {\"Ref\": \"Master03\"}\n            ]\n        }\n    },\n    \"MasterExtElb\": {\n      \"Type\": \"AWS::ElasticLoadBalancing::LoadBalancer\",\n      \"Properties\": {\n        \"CrossZone\": \"true\",\n        \"ConnectionSettings\": {\"IdleTimeout\" : 300},\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_master_elb\"} ],\n        \"HealthCheck\": {\n          \"HealthyThreshold\" : \"2\",\n          \"Interval\" : \"5\",\n          \"Target\" : { \"Ref\": \"MasterHealthTarget\" },\n          \"Timeout\" : \"2\",\n          \"UnhealthyThreshold\" : \"2\"\n        },\n        \"Listeners\":[\n          {\n            \"InstancePort\": { \"Ref\" : \"MasterApiPort\" },\n            \"InstanceProtocol\": \"TCP\",\n            \"LoadBalancerPort\": { \"Ref\" : \"MasterApiPort\" },\n            \"Protocol\": \"TCP\"\n          }\n        ],\n        \"SecurityGroups\": [{\"Ref\": \"MasterExtElbSG\"}],\n        \"Subnets\": [\n          {\"Ref\": \"PublicSubnet1\"},\n          {\"Ref\": \"PublicSubnet2\"},\n          {\"Ref\": \"PublicSubnet3\"}\n            ],\n        \"Instances\": [\n          {\"Ref\": \"Master01\"},\n          {\"Ref\": \"Master02\"},\n          {\"Ref\": \"Master03\"}\n            ]\n      }\n    },\n    \"InfraElb\": {\n      \"Type\": \"AWS::ElasticLoadBalancing::LoadBalancer\",\n      \"Properties\": {\n        \"CrossZone\": \"true\",\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_router_elb\"} ],\n        \"HealthCheck\": {\n          \"HealthyThreshold\" : \"2\",\n          \"Interval\" : \"5\",\n          \"Target\" : \"TCP:443\",\n          \"Timeout\" : \"2\",\n          \"UnhealthyThreshold\" : \"2\"\n        },\n        \"Listeners\":[\n          {\n            \"InstancePort\": \"443\",\n            \"InstanceProtocol\": \"TCP\",\n            \"LoadBalancerPort\": \"443\",\n            \"Protocol\": \"TCP\"\n          },\n          {\n            \"InstancePort\": \"80\",\n            \"InstanceProtocol\": \"TCP\",\n            \"LoadBalancerPort\": \"80\",\n            \"Protocol\": \"TCP\"\n          }\n        ],\n        \"SecurityGroups\": [ { \"Ref\": \"InfraElbSG\" } ],\n        \"Subnets\": [\n          {\"Ref\": \"PublicSubnet1\"},\n          {\"Ref\": \"PublicSubnet2\"},\n          {\"Ref\": \"PublicSubnet3\"}\n        \t],\n        \"Instances\": [\n          {\"Ref\": \"InfraNode01\"},\n          {\"Ref\": \"InfraNode02\"},\n          {\"Ref\": \"InfraNode03\"}\n            ]\n      }\n    },\n    \"NodePolicy\": {\n      \"Type\": \"AWS::IAM::Role\",\n      \"Properties\": {\n        \"AssumeRolePolicyDocument\": {\n          \"Version\": \"2012-10-17\",\n          \"Statement\": [\n            {\n              \"Effect\": \"Allow\",\n              \"Principal\": { \"Service\": [ \"ec2.amazonaws.com\" ] },\n              \"Action\": [ \"sts:AssumeRole\" ]\n            }\n          ]\n        },\n        \"Policies\": [\n          {\n            \"PolicyName\": \"node-describe\",\n            \"PolicyDocument\": {\n              \"Version\" : \"2012-10-17\",\n              \"Statement\": [\n                {\n                  \"Effect\": \"Allow\",\n                  \"Action\": [\n                     \"ec2:DescribeInstance*\"\n                  ],\n                  \"Resource\": \"*\"\n                }\n              ]\n            }\n          }\n        ]\n      }\n    },\n    \"MasterPolicy\": {\n      \"Type\": \"AWS::IAM::Role\",\n      \"Properties\": {\n        \"AssumeRolePolicyDocument\": {\n          \"Version\": \"2012-10-17\",\n          \"Statement\": [\n            {\n              \"Effect\": \"Allow\",\n              \"Principal\": { \"Service\": [ \"ec2.amazonaws.com\" ] },\n              \"Action\": [ \"sts:AssumeRole\" ]\n            }\n          ]\n        },\n        \"Policies\": [\n          {\n            \"PolicyName\": \"master-ec2-all\",\n            \"PolicyDocument\": {\n              \"Version\" : \"2012-10-17\",\n              \"Statement\": [\n                {\n                  \"Effect\": \"Allow\",\n                  \"Action\": [\n                     \"ec2:DescribeVolume*\",\n                     \"ec2:CreateVolume\",\n                     \"ec2:CreateTags\",\n                     \"ec2:DescribeInstance*\",\n                     \"ec2:AttachVolume\",\n                     \"ec2:DetachVolume\",\n                     \"ec2:DeleteVolume\",\n                     \"ec2:DescribeSubnets\",\n                     \"ec2:CreateSecurityGroup\",\n                     \"ec2:DescribeSecurityGroups\",\n                     \"elasticloadbalancing:DescribeTags\",\n                     \"elasticloadbalancing:CreateLoadBalancerListeners\",\n                     \"ec2:DescribeRouteTables\",\n                     \"elasticloadbalancing:ConfigureHealthCheck\",\n                     \"ec2:AuthorizeSecurityGroupIngress\",\n                     \"elasticloadbalancing:DeleteLoadBalancerListeners\",\n                     \"elasticloadbalancing:RegisterInstancesWithLoadBalancer\",\n                     \"elasticloadbalancing:DescribeLoadBalancers\",\n                     \"elasticloadbalancing:CreateLoadBalancer\",\n                     \"elasticloadbalancing:DeleteLoadBalancer\",\n                     \"elasticloadbalancing:ModifyLoadBalancerAttributes\",\n                     \"elasticloadbalancing:DescribeLoadBalancerAttributes\"\n                  ],\n                  \"Resource\": \"*\"\n                }\n              ]\n            }\n          }\n        ]\n      }\n    },\n    \"MasterInstanceProfile\": {\n      \"Type\": \"AWS::IAM::InstanceProfile\",\n      \"DependsOn\": \"MasterPolicy\",\n      \"Properties\": {\n        \"Roles\": [ { \"Ref\": \"MasterPolicy\" } ]\n      }\n    },\n    \"NodeInstanceProfile\": {\n      \"Type\": \"AWS::IAM::InstanceProfile\",\n      \"DependsOn\": \"NodePolicy\",\n      \"Properties\": {\n        \"Roles\": [ { \"Ref\": \"NodePolicy\" } ]\n      }\n    },\n    \"RegistryBucket\": {\n    \"Type\": \"AWS::S3::Bucket\",\n    \"Properties\" : {\n       \"BucketName\": { \"Ref\": \"S3BucketName\"}\n                   }\n     },\n    \"Route53Records\": {\n      \"Type\": \"AWS::Route53::RecordSetGroup\",\n      \"DependsOn\": [\n        \"InfraElb\",\n        \"MasterIntElb\",\n        \"Master01\",\n        \"Master02\",\n        \"Master03\",\n        \"Bastion\",\n        \"MasterExtElb\"\n      ],\n      \"Properties\": {\n        \"HostedZoneName\": { \"Ref\": \"Route53HostedZone\" },\n        \"RecordSets\": [\n          {\n            \"Name\":  { \"Ref\": \"MasterClusterPublicHostname\" },\n            \"Type\": \"A\",\n            \"AliasTarget\": {\n                \"HostedZoneId\": { \"Fn::GetAtt\" : [\"MasterExtElb\", \"CanonicalHostedZoneNameID\"] },\n                \"DNSName\": { \"Fn::GetAtt\" : [\"MasterExtElb\",\"CanonicalHostedZoneName\"] }\n            }\n          },\n          {\n            \"Name\": { \"Ref\": \"MasterClusterHostname\" },\n            \"Type\": \"A\",\n            \"AliasTarget\": {\n                \"HostedZoneId\": { \"Fn::GetAtt\" : [\"MasterIntElb\", \"CanonicalHostedZoneNameID\"] },\n                \"DNSName\": { \"Fn::GetAtt\" : [\"MasterIntElb\",\"DNSName\"] }\n            }\n          },\n          {\n            \"Name\": { \"Ref\": \"AppWildcardDomain\" },\n            \"Type\": \"A\",\n            \"AliasTarget\": {\n                \"HostedZoneId\": { \"Fn::GetAtt\" : [\"InfraElb\", \"CanonicalHostedZoneNameID\"] },\n                \"DNSName\": { \"Fn::GetAtt\" : [\"InfraElb\",\"CanonicalHostedZoneName\"] }\n            }\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-master01\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"Master01\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-master02\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"Master02\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-master03\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"Master03\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-infra-node01\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"InfraNode01\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-infra-node02\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"InfraNode02\", \"PrivateIp\"] }]\n          },\n\t  {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-infra-node03\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"InfraNode03\", \"PrivateIp\"] }]\n          },\n{% for idx in range(1, app_node_count|int + 1) %}\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-app-node{{ '%02d' % idx }}\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n             \"TTL\": \"300\",\n            \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"AppNode{{ '%02d' % idx }}\", \"PrivateIp\"] }]\n          },\n{% endfor %}\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"bastion\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n                        \"TTL\": \"300\",\n                    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"Bastion\", \"PublicIp\"] }]\n          }\n        ]\n      }\n    },\n    \"Bastion\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"BastionUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": \"t2.micro\",\n          \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"BastionSg\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PublicSubnet1\"},\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"bastion\",{\"Ref\": \"PublicHostedZone\"}]]}\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n\t\t\t          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": \"10\",\n              \"VolumeType\": \"gp2\"\n                   }\n            }\n          ]\n       }\n    },\n    \"BastionEip\" : {\n       \"Type\" : \"AWS::EC2::EIP\",\n        \"Properties\" : {\n        \"Domain\" : \"vpc\"\n           }\n       },\n    \"BastionEipAssoc\" : {\n      \"DependsOn\": [\"Bastion\"],\n      \"Type\" : \"AWS::EC2::EIPAssociation\",\n      \"Properties\" : {\n        \"InstanceId\" : {\"Ref\" : \"Bastion\"},\n        \"AllocationId\" : { \"Fn::GetAtt\" : [\"BastionEip\", \"AllocationId\"]}\n        }\n       },\n    \"Master01\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"DependsOn\": [\"MasterInstanceProfile\"],\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"MasterUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t      \"InstanceType\": {\"Ref\": \"MasterInstanceType\"},\n\t\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"MasterSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"EtcdSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet1\"},\n          \"IamInstanceProfile\": { \"Ref\": \"MasterInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-master01\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"master\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": \"50\",\n              \"VolumeType\": \"gp2\"\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"false\",\n              \"VolumeSize\": {\"Ref\": \"MasterEtcdVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterEtcdVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdd\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": \"5\",\n              \"VolumeType\": \"gp2\"\n            }\n          }\n         ]\n     }\n  },\n    \"Master02\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"DependsOn\": [\"MasterInstanceProfile\"],\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"MasterUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n          \"InstanceType\": {\"Ref\": \"MasterInstanceType\"},\n          \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"MasterSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"EtcdSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet2\"},\n          \"IamInstanceProfile\": { \"Ref\": \"MasterInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-master02\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"master\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": \"50\",\n              \"VolumeType\": \"gp2\"\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"false\",\n              \"VolumeSize\": {\"Ref\": \"MasterEtcdVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterEtcdVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdd\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": \"5\",\n              \"VolumeType\": \"gp2\"\n            }\n          }\n         ]\n     }\n   },\n    \"Master03\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"DependsOn\": [\"MasterInstanceProfile\"],\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"MasterUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"MasterInstanceType\"},\n\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"MasterSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"EtcdSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet3\"},\n          \"IamInstanceProfile\": { \"Ref\": \"MasterInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-master03\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"master\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": \"50\",\n              \"VolumeType\": \"gp2\"\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"false\",\n              \"VolumeSize\": {\"Ref\": \"MasterEtcdVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterEtcdVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdd\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": \"5\",\n              \"VolumeType\": \"gp2\"\n            }\n          }\n         ]\n     }\n   },\n    \"InfraNode01\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"DependsOn\": [\"NodeInstanceProfile\"],\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InfraInstanceType\"},\n          \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"InfraSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet1\"},\n          \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-infra-node01\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"infra\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": \"50\",\n              \"VolumeType\": \"gp2\"\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": \"50\",\n              \"VolumeType\": \"gp2\"\n            }\n          }\n         ]\n     }\n    },\n    \"InfraNode02\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"DependsOn\": [\"NodeInstanceProfile\"],\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n          \"InstanceType\": {\"Ref\": \"InfraInstanceType\"},\n          \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"InfraSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet2\"},\n          \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-infra-node02\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"infra\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": \"50\",\n              \"VolumeType\": \"gp2\" \n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": \"50\",\n              \"VolumeType\": \"gp2\"\n            }\n          }\n         ]\n     }\n },\n    \"InfraNode03\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"DependsOn\": [\"NodeInstanceProfile\"],\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n          \"InstanceType\": {\"Ref\": \"InfraInstanceType\"},\n          \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"InfraSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet3\"},\n          \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-infra-node03\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"infra\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": \"50\",\n              \"VolumeType\": \"gp2\"\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": \"50\",\n              \"VolumeType\": \"gp2\"\n            }\n          }\n         ]\n     }\n },\n{% set rotator = 1 %}\n{% for idx in range(1, app_node_count|int + 1) %}\n{% if rotator == 4 %}\n  {% set rotator = 1 %}\n{% endif %}\n    \"AppNode{{ '%02d' % idx }}\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"DependsOn\": [\"NodeInstanceProfile\"],\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n          \"InstanceType\": {\"Ref\": \"AppNodeInstanceType\"},\n\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet{{ rotator }}\"},\n\t  \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n          { \"Key\": \"Name\",\n            \"Value\": {\"Fn::Join\": [\".\", [\"ose-app-node{{ '%02d' % idx }}\",{\"Ref\": \"PublicHostedZone\"}]]}\n          },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"app\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": \"50\",\n              \"VolumeType\": \"gp2\"\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": \"50\",\n              \"VolumeType\": \"gp2\"\n            }\n          }\n         ]\n     }\n   },\n{% set rotator = rotator + 1 %}\n{% endfor %}\n    \"S3UserName\" : {\n      \"Type\" : \"AWS::IAM::User\",\n      \"Properties\" : {\n        \"Path\" : \"/\",\n        \"UserName\": { \"Ref\": \"S3User\" },\n        \"Policies\" : [ {\n          \"PolicyName\" : \"accessalls3\",\n          \"PolicyDocument\" : {\n            \"Version\": \"2012-10-17\",\n            \"Statement\" : [ {\n              \"Effect\" : \"Allow\",\n              \"Action\" : [ \"s3:*\" ],\n              \"Resource\": \"*\"\n            } ]\n          }\n        } ]\n      }\n    },\n    \"CFNKeys\" : {\n      \"Type\" : \"AWS::IAM::AccessKey\",\n      \"Properties\" : {\n        \"UserName\" : { \"Ref\": \"S3UserName\" }\n      }\n    }\n  },\n  \"Outputs\" : {\n    \"StackVpc\" : {\n      \"Value\" : { \"Ref\" : \"Vpc\" },\n      \"Description\" : \"VPC that was created\"\n    },\n    \"InfraLb\" : {\n      \"Value\" : { \"Ref\" : \"InfraElb\" },\n      \"Description\" : \"Infrastructure ELB name\"\n    },\n    \"PrivateSubnet1\" : {\n      \"Value\" : { \"Ref\" : \"PrivateSubnet1\" },\n      \"Description\" : \"Private Subnet 1\"\n    },\n    \"PrivateSubnet2\" : {\n      \"Value\" : { \"Ref\" : \"PrivateSubnet2\" },\n      \"Description\" : \"Private Subnet 2\"\n    },\n    \"PrivateSubnet3\" : {\n      \"Value\" : { \"Ref\" : \"PrivateSubnet3\" },\n      \"Description\" : \"Private Subnet 3\"\n    },\n    \"NodeSGId\" : {\n      \"Value\" : { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ]},\n      \"Description\" : \"Node SG id\"\n    },\n    \"InfraSGId\" : {\n      \"Value\" : { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ]},\n      \"Description\" : \"Infra Node SG id\"\n    },\n    \"BastionSGId\" : {\n      \"Value\" : { \"Fn::GetAtt\": [ \"BastionSg\", \"GroupId\" ]},\n      \"Description\" : \"Bastion SG id\"\n    },\n    \"S3UserAccessId\" : {\n      \"Value\" : { \"Ref\" : \"CFNKeys\" },\n      \"Description\" : \"AWSAccessKeyId of user\"\n    },\n    \"NodeARN\" : {\n      \"Value\" : { \"Ref\": \"NodeInstanceProfile\" },\n      \"Description\": \"ARN for the Node instance profile\"\n    },\n    \"S3UserSecretKey\" : {\n      \"Value\" : { \"Fn::GetAtt\" : [\"CFNKeys\", \"SecretAccessKey\"]},\n      \"Description\" : \"AWSSecretKey of new S3\"\n    },\n    \"S3Bucket\" : {\n      \"Value\" : { \"Ref\" : \"RegistryBucket\"},\n      \"Description\" : \"Name of S3 bucket\"\n    }\n }\n}\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "ca520c591d2662f96693e7ccf4b23048c0e1d324", "filename": "tasks/create_repo_bower_group_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_bower_group\n    args: \"{{ _nexus_repos_bower_defaults|combine(item) }}\""}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "2ab0410296f3d17af58542c0a178f199243f8959", "filename": "roles/dns/config-dns-server/tasks/named/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: prereq.yml\n- import_tasks: named.yml\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "8cdeb6055a76fc77d089ab7b292b88d22adcbc16", "filename": "roles/ovirt-engine-install-packages/meta/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\ngalaxy_info:\n  author: \"Petr Kubica\"\n  description: \"oVirt packages installer\"\n  company: \"Red Hat\"\n  license: \"GPLv3\"\n  min_ansible_version: 1.9\n  platforms:\n  - name: EL\n    versions:\n    - all\n  galaxy_tags:\n    - installer\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "6c1c579e73a438e4ecf19ea23c0e373e8640cb27", "filename": "roles/dhcp/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n# handlers file for dhcp-config\n- name: 'dhcp-config cleanup temp'\n  delegate_to: localhost\n  file:\n    path: '{{ dhcp_config_dir.path }}'\n    state: absent\n\n- name: 'cleanup dhcp tmp file'\n  file:\n    path: '{{ dhcp_config_temp_loc }}'\n    state: absent\n\n- name: 'reload dhcp'\n  service:\n    name: '{{ item }}'\n    state: restarted\n  with_items:\n  - dhcpd\n  when: \n  - dhcp_service_enabled|default(True)\n\n- name: 'enable and start dhcp services'\n  service:\n    name: '{{ item }}'\n    enabled: yes\n    state: started\n  with_items:\n  - dhcpd\n  when: \n  - dhcp_service_enabled|default(True) \n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f42c601b0371afc9eedc0f5825209912f86ab8f0", "filename": "roles/config-redis/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Restart Redis Service\n  systemd:\n    name: \"{{ redis_service }}\"\n    enabled: yes\n    state: restarted\n    daemon_reload: yes\n\n- name: restart firewalld\n  service:\n    name: firewalld\n    state: restarted\n\n- name: restart iptables\n  service:\n    name: iptables\n    state: restarted\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "44c489521654ea39d68b08e2b06628e5ad69cec4", "filename": "roles/config-nagios-target/tasks/nrpe_openshift_node.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Copy in additional Nagios service plugin\n  copy: \n    src: plugins/check_service.sh\n    dest: /usr/lib64/nagios/plugins/check_service.sh\n    owner: root\n    group: root\n    mode: 0755\n\n- name: Copy nrpe.d OpenShift node configuration files\n  copy: \n    src: nrpe.d/check_openshift_node.cfg\n    dest: /etc/nrpe.d/check_openshift_node.cfg\n    owner: root\n    group: root\n    mode: 0644\n\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "b02ac3e7b1633bb6676dde1bcf435a12990b35e2", "filename": "ops/playbooks/config_dummy_vms_for_docker_volumes_backup.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- hosts: ucp_main\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  vars:\n    powercli_script: '/tmp/vols.ps1'\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Create Dummy VMs\n      vmware_guest:\n        hostname: \"{{ vcenter_hostname }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        validate_certs: \"{{ vcenter_validate_certs }}\"\n        esxi_hostname: \"{{ esxi_host }}\"\n        datacenter: \"{{ datacenter }}\"\n        folder: \"{{ datacenter }}/vm{{ folder_name }}\"\n        name: \"{{ dummy_vm_prefix }}-{{ item }}\"\n        guest_id: \"dosGuest\"\n        state: poweredoff\n        disk:\n        - size_gb: 1\n          datastore: \"{{ item }}\"\n          type: thin\n\n        hardware:\n          memory_mb: \"128\"\n          num_cpus: \"1\"\n      with_items: \"{{ datastores }}\"\n      delegate_to: localhost\n\n#\n#\n    - name: Generate powercli script\n      template: src=../templates/powercli_script.j2 dest={{ powercli_script }}\n\n    - name: Run powercli on temporary docker container\n      command: docker run --rm --entrypoint='/usr/bin/pwsh' -v /tmp:/tmp vmware/powerclicore {{ powercli_script }}\n\n    - name: Delete powercli script from docker host\n      file: state=absent path={{ powercli_script }}\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "22f401828e1240e258094da3dfa3f5df1605a25e", "filename": "playbooks/provider/lago/config.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "# Desription: Create a virtual environment using Lago.\n# The specification of the environment can be found in LagoInitFile.yaml\n#\n# PARAMETERS:\n# inventory_file: Path to the inventory file,\n# Lago will populate it with the vms details.\n#\n# prefix: Where to create the lago environment\n\n- hosts: localhost\n  connection: local\n  gather_facts: False\n  tasks:\n    - name: Set prefix\n      set_fact:\n        prefix: \"{{ prefix | default('.lago') }}\"\n    - name: Init Lago env\n      command:\n        lago init \"{{ prefix }}\" \"{{ playbook_dir }}/LagoInitFile.yml\"\n      args:\n        creates: \"{{ prefix }}/default/uuid\"\n    - name: Remove user defined settings from inventory\n      blockinfile:\n        path: \"{{ inventory_file }}\"\n        marker: \"# {mark} CUSTOM SETTINGS\"\n        state: absent\n    - name: Create Ansible Hosts file from Lago env\n      shell: lago --workdir \"{{ prefix }}\" ansible_hosts >> \"{{ inventory_file }}\"\n    - name: Normalize inventory file\n      replace:\n        path: \"{{ inventory_file }}\"\n        regexp: 'groups=(.*)'\n        replace: '\\1'\n    - name: Refresh inventory\n      meta: refresh_inventory\n    - name: Start Lago env\n      command: lago --workdir \"{{ prefix }}\" start\n\n- hosts: all\n  any_errors_fatal: True\n  gather_facts: no\n  tasks:\n    - name: Wait for ssh\n      wait_for:\n        port: 22\n        host: \"{{ ansible_host }}\"\n        search_regex: OpenSSH\n        delay: 10\n      delegate_to: localhost\n      connection: local\n    - name: Mount docker_lib\n      shell: |\n        disk=\"/dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_3\"\n        mount_path=\"/var/lib/docker\"\n        mkdir -p \"$mount_path\"\n        mkfs.ext4 -F \"$disk\"\n        echo -e \"${disk}\\t${mount_path}\\text4\\tdefaults\\t0 0\" >> /etc/fstab\n        mount \"$disk\" \"$mount_path\"\n    - name: Set docker storage disk path\n      set_fact:\n        docker_dev: \"/dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_2\"\n\n- hosts: nfs\n  any_errors_fatal: True\n  tasks:\n    - name: Mount nfs storage\n      shell: |\n        disk=\"/dev/disk/by-id/scsi-0QEMU_QEMU_HARDDISK_5\"\n        mount_path=\"/opt\"\n        mkdir -p \"$mount_path\"\n        mkfs.ext4 -F \"$disk\"\n        echo -e \"${disk}\\t${mount_path}\\text4\\tdefaults\\t0 0\" >> /etc/fstab\n        mount \"$disk\" \"$mount_path\"\n\n- hosts: lago-node0\n  tasks:\n    - name: Mark lago-node0 as infra\n      set_fact:\n        openshift_node_labels:\n          region: infra\n          zone: default\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "d774187f020613a2e67685c91239f46a92c062f5", "filename": "playbooks/aws/openshift-cluster/vars.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ndebug_level: 2\n\ndeployment_rhel7_ent_base:\n  # rhel-7.1, requires cloud access subscription\n  image: \"{{ lookup('oo_option', 'ec2_image') | default('ami-10251c7a', True) }}\"\n  image_name: \"{{ lookup('oo_option', 'ec2_image_name') | default(None, True) }}\"\n  region: \"{{ lookup('oo_option', 'ec2_region') | default('us-east-1', True) }}\"\n  ssh_user: ec2-user\n  become: yes\n  keypair: \"{{ lookup('oo_option', 'ec2_keypair') | default('libra', True) }}\"\n  type: \"{{ lookup('oo_option', 'ec2_instance_type') | default('m4.large', True) }}\"\n  security_groups: \"{{ lookup('oo_option', 'ec2_security_groups') | default([ 'public' ], True) }}\"\n  vpc_subnet: \"{{ lookup('oo_option', 'ec2_vpc_subnet') | default(omit, True) }}\"\n  assign_public_ip: \"{{ lookup('oo_option', 'ec2_assign_public_ip') | default(omit, True) }}\"\n\ndeployment_vars:\n  origin:\n    # centos-7, requires marketplace\n    image: \"{{ lookup('oo_option', 'ec2_image') | default('ami-6d1c2007', True) }}\"\n    image_name: \"{{ lookup('oo_option', 'ec2_image_name') | default(None, True) }}\"\n    region: \"{{ lookup('oo_option', 'ec2_region') | default('us-east-1', True) }}\"\n    ssh_user: centos\n    become: yes\n    keypair: \"{{ lookup('oo_option', 'ec2_keypair') | default('libra', True) }}\"\n    type: \"{{ lookup('oo_option', 'ec2_instance_type') | default('m4.large', True) }}\"\n    security_groups: \"{{ lookup('oo_option', 'ec2_security_groups') | default([ 'public' ], True) }}\"\n    vpc_subnet: \"{{ lookup('oo_option', 'ec2_vpc_subnet') | default(omit, True) }}\"\n    assign_public_ip: \"{{ lookup('oo_option', 'ec2_assign_public_ip') | default(omit, True) }}\"\n\n  enterprise: \"{{ deployment_rhel7_ent_base }}\"\n  openshift-enterprise: \"{{ deployment_rhel7_ent_base }}\"\n  atomic-enterprise: \"{{ deployment_rhel7_ent_base }}\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "da793a6a5f42fd9a6774bb85865e800a3c0486d4", "filename": "ops/playbooks/roles/get-dtr-replica/tasks/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n\n#\n# sets the value of existing_dtr_replica_id to an existing DTR replica ID if there is one, or to the value 0 otherwise\n\n#     existing_dtr_replica_id\n#\n\n    - name: Retrieve a token for the UCP API\n      uri:\n        url: \"https://{{ ARG_UCP_IP }}/auth/login\"\n        headers:\n          Content-Type: application/json\n        method: POST\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        body: '{\"username\":\"{{ ARG_UCP_USER }}\",\"password\":\"{{ ARG_UCP_PASSWORD }}\"}'\n      register: resp\n      until: resp.status == 200\n      retries: 20\n      delay: 5\n\n    - name: Remember the API's token\n      set_fact:\n        auth_token:  \"{{resp.json.auth_token}}\"\n\n\n    - name: Find a replica ID\n      uri:\n        url: 'https://{{ ARG_UCP_IP }}//containers/json?filters={\"name\":[\"dtr-rethinkdb-\"]}' \n        headers:\n          accept: application/json\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        validate_certs: no\n      register: resp\n      until: resp.status == 200\n      retries: 20\n      delay: 5\n\n    - set_fact:\n        dtr_found:  \"{% if resp.json[0] is defined %}true{% else %}false{% endif %}\"\n\n    - set_fact:\n        existing_dtr_replica_id: \"{% if dtr_found == true %}{{ resp.json[0].Names[0].split('-') | last }}{% else %}0{% endif %}\"\n\n    - debug:\n        var: existing_dtr_replica_id\n      when: _debug is defined\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "2fb88344023f1930b72c95ea9ef0fa31ea62c114", "filename": "roles/ajenti/handlers/main.yml", "repository": "iiab/iiab", "decoded_content": "---\n- name: restart ajenti service\n  service: name=ajenti\n           enabled=yes\n           state=restarted\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "6398ca9d6c9e54d5b08fcd12d72baa788f17afeb", "filename": "roles/nodogsplash/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "nodogsplash_install : False\nnodogsplash_enabled : False\nnodogsplash_arm_deb : nodogsplash_2.0.0-1_armhf.deb\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "1fe78a4e5010f57480477e239956a66e43801937", "filename": "ops/playbooks/roles/hpe.haproxy/templates/30-ucp-backends.j2", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "    server {{ inventory_hostname }} {{ hostvars[inventory_hostname].ip_addr | ipaddr('address') }}:443 check\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ce5f2c959a2aae3ba42145646a419ddfaf37870f", "filename": "roles/config-ipa-client/tasks/ipa.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Check if service has already been started - if so skip\"\n  command: \"systemctl status sssd\"\n  register: sssd_status\n  failed_when: False\n  changed_when: False\n\n- name: \"Check if sssd.conf has already been configured - if so skip\"\n  stat:\n    path: /etc/sssd/sssd.conf\n  register: stat_result\n  failed_when: False\n  changed_when: False\n\n- name: \"Check if sssd has already been correctly configured - if so skip\"\n  shell: \"awk /^.domain.{{ ipa_domain }}.$/ /etc/sssd/sssd.conf\"\n  register: check_conf\n  failed_when: False\n  changed_when: False\n\n- name: \"Configure IPA/IdM Integration if not already enabled and correctly configured\"\n  import_tasks: ipa-install.yml\n  when:\n  - (sssd_status.rc != 0 or stat_result.stat.exists == false or check_conf.stdout == \"\")\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "dbccf9375b7b4b08038bdab0d23a102eb111df4d", "filename": "roles/ansible/tower/config-ansible-tower-ldap/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: restart-tower\n  service:\n    name:  supervisord\n    state: restarted\n  become: True\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "c79d37dcb87523bb0a3640ebce131de688f6e72c", "filename": "ops/templates/backup_dtr_metadata.sh.j2", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n#\n# performs a backup of the DTR metadata \n#\nreplica_id=$1\nbackup_name=$2\nucp_instance=$3\nbackup_meta=${backup_name}.tgz \n\n# backup the DTR metadata \n\ndocker run --log-driver none --rm docker/dtr:{{ dtr_version }} backup --ucp-url https://${ucp_instance} --ucp-insecure-tls --ucp-username '{{ucp_username}}' --ucp-password '{{ucp_password}}'  --existing-replica-id $replica_id  | ssh -oStrictHostKeyChecking=no {{ backup_server }} \"cat >{{ backup_dest }}/${backup_meta}\"\n\n#\n# backup the data, compress it and stream it to our SSH server\n#\ndocker run --log-driver none --rm --env UCP_USERNAME --env UCP_PASSWORD docker/dtr:{{ detected_dtr_version }} \\\n    backup \\\n      --ucp-url https://${ucp_instance} \\\n      --ucp-insecure-tls \\\n      --existing-replica-id $replica_id  \\\n | gzip - | ssh -oStrictHostKeyChecking=no {{ backup_server }} \"cat >{{ backup_dest }}/${backup_meta}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "8d6c927cf645a7b98fc9ea21e0451471c000db11", "filename": "reference-architecture/gcp/ansible/playbooks/prereq.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: configure ansible connection to the gcp and some basic stuff\n  hosts: localhost\n  roles:\n  - pre-flight-validation\n  - openshift-ansible-installer\n  - ansible-gcp\n  - dns-zone\n  - gcp-ssh-key\n  - role: rhel-image\n    when: openshift_deployment_type == 'openshift-enterprise'\n  - role: empty-image\n  - role: deployment-create\n    deployment_name: network\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "63e5407abe9c1f95522f1d348832e5bccbc1b21e", "filename": "playbooks/manage-services.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- hosts: all\n  pre_tasks:\n    - name: Include user-override vars\n      include_vars: \"{{ rock_config }}\"\n  tasks:\n    - name: Populate service facts\n      service_facts:\n\n    - name: Flatten ROCK services\n      set_fact:\n        flat_services: \"{{ ansible_facts.services | flatten(levels=1) | replace('.service', '')  }}\"\n\n    - name: collect enabled services\n      set_fact:\n        enabled_services: {{ rock_services | selectattr(\"enabled\", \"equalto\", True) | map(attribute=\"name\") | list }}\n\n    - name: Collect ROCK services from facts\n      set_fact:\n        host_services: \"{{ flat_services | intersect(enabled_services) }}\"\n\n    - name: Perform requested service action\n      service:\n        name: \"{{ item }}\"\n        state: \"{{ service_state }}\"\n      loop: \"{{ host_services }}\"\n      tags:\n        - service_mgmt\n\n    - name: Register service status\n      shell: systemctl status -n 3 {{ item }}*\n      loop: \"{{ host_services }}\"\n      register: services\n      changed_when: false\n      tags:\n        - service_status\n\n    - name: Output service status\n      debug:\n        msg: \"{{ item.stdout }}\"\n      loop: \"{{ services.results }}\"\n      loop_control:\n        label: \"{{ item._ansible_item_label }}\"\n      tags:\n        - service_status\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "6beb2d20cd606cdb832f79056374ca89b1c44087", "filename": "meta/main.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\ngalaxy_info:\n  author: Emilien Mantel\n  description: Install and configure MariaDB (and Galera Cluster) on Debian \n  company: \n  license: GPLv2\n  min_ansible_version: 2.3\n  platforms:\n  - name: Debian\n    versions:\n    - stretch\n  galaxy_tags:\n  - database\n  - database:sql\n  - packaging\n  - mysql\n  - mariadb\n  - replication\n  - debian\n  - galera\n  - cluster\n  - stretch\ndependencies: []\n  \n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "c7b84179a0b12ad8415d21ee1ed973353cf91991", "filename": "ops/playbooks/loadbalancer.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- name: Install Load Balancers\n  hosts: loadbalancer\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - includes/internal_vars.yml\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n    - set_fact: \n        open_ports: \"{{ internal_dtr_lb_ports | union (internal_ucp_lb_ports) | union ( internal_worker_lb_ports ) }}\"\n\n    - name: Open ports  in the firewall\n      firewalld:\n        port: \"{{ item }}\"\n        immediate: true\n        permanent: true\n        state: enabled\n      with_items: \"{{ open_ports }}\"\n\n    - name: Add rule for vrrp protocole\n      firewalld:\n        rich_rule: 'rule protocol value=\"vrrp\" accept'\n        immediate: true\n        permanent: true\n        state: enabled\n\n    - name: Install Required Pkgs for seboolean module\n      yum:\n        name: \"{{ packages }}\"\n        state: latest\n      vars:\n        packages:\n        - libsemanage-python\n        - libselinux-python\n\n    - name: Install haproxy and keepalived\n      yum:\n        name: \"{{ packages }}\"\n        state: latest\n      vars:\n        packages:\n        - psmisc\n        - haproxy\n        - keepalived\n\n    - name: Enable HAPROXY to open non standard ports\n      seboolean:\n        name: haproxy_connect_any\n        state: yes\n        persistent: yes\n\n    - name: Allow binding on remote IPs\n      sysctl:\n        name: net.ipv4.ip_nonlocal_bind\n        value: 1\n        state: present\n\n    - name: Allow routing\n      sysctl:\n        name: net.ipv4.ip_forward\n        value: 1\n        state: present\n\n    - name: Update haproxy.cfg\n      template:\n        src: ../templates/lbs/haproxy.cfg.j2\n        dest: /etc/haproxy/haproxy.cfg\n        owner: root\n        group: root\n        mode: 0644        \n      notify: Enable and start haproxy service\n\n    - name: Enable and start haproxy service\n      systemd:\n        name: haproxy\n        enabled: yes\n        state: started\n\n    - name: Update keepalived.conf\n      template:\n        src: ../templates/lbs/keepalived.conf.j2\n        dest: /etc/keepalived/keepalived.conf\n        owner: root\n        group: root\n        mode: 0644        \n      notify: Enable and start keepalived service\n\n    - name: Enable and start keepalived service\n      systemd:\n        name: keepalived\n        enabled: yes\n        state: started\n\n  handlers:\n\n    - name: Enable and start haproxy service\n      systemd:\n        name: haproxy\n        enabled: yes\n        state: restarted\n\n    - name: Enable and start keepalived service\n      systemd:\n        name: keepalived\n        enabled: yes\n        state: restarted\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "32c5ffa4a22c502a0a1a714e65846029eafdb43e", "filename": "roles/config-openvpn/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: prep.yml\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "0af30dfc4abfb993c73b4768c139b530b8e1ef1b", "filename": "roles/proxy/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: Gather Facts\n  setup:\n\n- name: Privoxy installed\n  apt: name=privoxy state=latest\n\n- name: Privoxy configured\n  template: src=\"{{ item.src }}\" dest=\"{{ item.dest }}\"\n  with_items:\n    - { src: privoxy_config.j2, dest: /etc/privoxy/config }\n    - { src: default.filter.j2, dest: /etc/privoxy/default.filter }\n  notify:\n    - restart privoxy\n\n- name: Privoxy profile for apparmor configured\n  template: src=usr.sbin.privoxy.j2 dest=/etc/apparmor.d/usr.sbin.privoxy owner=root group=root mode=0600\n  when: apparmor_enabled is defined and apparmor_enabled == true\n  notify:\n    - restart privoxy\n\n- name: Enforce the privoxy AppArmor policy\n  shell: aa-enforce usr.sbin.privoxy\n  when: apparmor_enabled is defined and apparmor_enabled == true\n  tags: ['apparmor']\n\n- name: Ensure that the privoxy service directory exist\n  file: path=/etc/systemd/system/privoxy.service.d/ state=directory mode=0755  owner=root group=root\n\n- name: Setup the cgroup limitations for the privoxy daemon\n  template: src=privoxy_100-CustomLimitations.conf.j2 dest=/etc/systemd/system/privoxy.service.d/100-CustomLimitations.conf\n  notify:\n    - daemon-reload\n    - restart privoxy\n\n- meta: flush_handlers\n\n- name: Privoxy enabled and started\n  service: name=privoxy state=started enabled=yes\n\n# PageSpeed\n\n- name: Apache installed\n  apt: name=apache2 state=latest\n\n- name: PageSpeed installed for x86_64\n  apt: deb=https://dl-ssl.google.com/dl/linux/direct/mod-pagespeed-stable_current_amd64.deb\n  when: ansible_architecture == \"x86_64\"\n\n- name: PageSpeed installed for i386\n  apt: deb=https://dl-ssl.google.com/dl/linux/direct/mod-pagespeed-stable_current_i386.deb\n  when: ansible_architecture != \"x86_64\"\n\n- name: PageSpeed configured\n  template: src=pagespeed.conf.j2 dest=/etc/apache2/mods-available/pagespeed.conf\n  notify:\n    - restart apache2\n\n- name: Modules enabled\n  apache2_module: state=present name=\"{{ item }}\"\n  with_items:\n    - proxy_http\n    - pagespeed\n    - cache\n    - proxy_connect\n    - proxy_html\n    - rewrite\n  notify:\n    - restart apache2\n\n- name: VirtualHost configured for the PageSpeed module\n  template: src=000-default.conf.j2 dest=/etc/apache2/sites-enabled/000-default.conf\n  notify:\n    - restart apache2\n\n- name: Apache ports configured\n  template: src=ports.conf.j2 dest=/etc/apache2/ports.conf\n  notify:\n    - restart apache2\n\n- name: Ensure that the apache2 service directory exist\n  file: path=/etc/systemd/system/apache2.service.d/ state=directory mode=0755  owner=root group=root\n\n- name: Setup the cgroup limitations for the apache2 daemon\n  template: src=apache2_100-CustomLimitations.conf.j2 dest=/etc/systemd/system/apache2.service.d/100-CustomLimitations.conf\n  notify:\n    - daemon-reload\n    - restart apache2\n\n- meta: flush_handlers\n\n- name: Set facts for mobileconfigs\n  set_fact:\n    proxy_enabled: true\n\n- name: Register p12 PayloadContent\n  shell: >\n    cat /{{ easyrsa_dir }}/easyrsa3//pki/private/{{ item }}.p12 | base64\n  register:  PayloadContent\n  with_items: \"{{ users }}\"\n\n- name: Register CA PayloadContent\n  shell: >\n    cat /{{ easyrsa_dir }}/easyrsa3/pki/ca.crt | base64\n  register:  PayloadContentCA\n\n- name: Build the mobileconfigs\n  template: src=roles/vpn/templates/mobileconfig.j2 dest=/{{ easyrsa_dir }}/easyrsa3//pki/private/{{ item.0 }}_proxy.mobileconfig mode=0600\n  with_together:\n    - \"{{ users }}\"\n    - \"{{ PayloadContent.results }}\"\n  no_log: True\n\n- name: Fetch users mobileconfig\n  fetch: src=/{{ easyrsa_dir }}/easyrsa3//pki/private/{{ item }}_proxy.mobileconfig dest=configs/{{ IP_subject_alt_name }}_{{ item }}_proxy.mobileconfig flat=yes\n  with_items: \"{{ users }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b8780bdfa288d1b130c6a9a62fdec50cc32d0a99", "filename": "roles/osp/packstack-install/tasks/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - openstack-packstack\n\n- name: 'Ensure necessary directories exists'\n  file:\n    path: \"{{ ssl_cert_directory }}\"\n    state: directory\n\n- name: 'Copy certificates over to the host'\n  copy:\n    src: \"{{ ssl_cert_src_dir }}\"\n    dest: \"{{ ssl_cert_directory }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "24af9e00eb67085c04a15f4e6644a3a0f665e9eb", "filename": "playbooks/post-validation.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: no\n  become: no\n  roles:\n  - validate-public\n\n- hosts: masters\n  gather_facts: no\n  roles:\n  - validate-masters\n\n- hosts: masters\n  gather_facts: yes\n  roles:\n  - validate-etcd\n\n- hosts: single_master\n  gather_facts: yes\n  become: yes\n  roles:\n  - validate-app\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "95a14be163689340acfa0cf0f28b7d4976f01625", "filename": "roles/2-common/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# Common OS-Level Additions & Mods (that only need to be performed once)\n\n- name: ...IS BEGINNING ==========================================\n  command: echo\n\n- include_tasks: centos.yml\n  when: ansible_distribution == \"CentOS\"\n\n- include_tasks: fedora.yml\n  when: ansible_distribution == \"Fedora\"\n\n- include_tasks: prep.yml\n  when: not is_debuntu\n\n- include_tasks: xo.yml\n  when: xo_model != \"none\" or osbuilder is defined\n\n# the following installs common packages for both debian and fedora\n- include_tasks: packages.yml\n\n- include_tasks: iptables.yml\n\n- sysctl:\n    name: net.ipv4.ip_forward\n    value: 1\n    state: present\n\n- sysctl:\n    name: net.ipv4.conf.default.rp_filter\n    value: 1\n    state: present\n\n- sysctl:\n    name: net.ipv4.conf.default.accept_source_route\n    value: 0\n    state: present\n\n- sysctl:\n    name: kernel.sysrq\n    value: 1\n    state: present\n\n- sysctl:\n    name: kernel.core_uses_pid\n    value: 1\n    state: present\n\n- sysctl:\n    name: net.ipv4.tcp_syncookies\n    value: 1\n    state: present\n\n- sysctl:\n    name: kernel.shmmax\n    value: 268435456\n    state: present\n\n# IPv6 disabled\n\n- sysctl:\n    name: net.ipv6.conf.all.disable_ipv6\n    value: 1\n    state: present\n\n- sysctl:\n    name: net.ipv6.conf.default.disable_ipv6\n    value: 1\n    state: present\n\n- sysctl:\n    name: net.ipv6.conf.lo.disable_ipv6\n    value: 1\n    state: present\n\n- name: Install custom profile file\n  template:\n    dest: /etc/profile.d/zzz_iiab.sh\n    src: zzz_iiab.sh\n    owner: root\n    mode: 0644\n    backup: no\n\n- include_tasks: net_mods.yml\n  when: not is_debuntu and not is_F18\n\n- include_tasks: udev.yml\n\n- include_tasks: iiab-startup.yml\n\n- name: Recording STAGE 2 HAS COMPLETED ==========================\n  lineinfile:\n    dest: /etc/iiab/iiab.env\n    regexp: '^STAGE=*'\n    line: 'STAGE=2'\n    state: present\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e8d2e4ec21288a7b8018820416b8e5abbeaafacb", "filename": "roles/config-lvm/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- include_tasks: prep.yml\n\n- include_tasks: lvm.yml\n  with_items:\n  - \"{{ lvm_entries | default([]) }}\"\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "1f5edc2e4d78c943294334f9154ab253293f33dd", "filename": "roles/local/tasks/prompts.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- pause:\n    prompt: |\n      Enter the IP address of your server: (or use localhost for local installation):\n      [localhost]\n  register: _algo_server\n  when: server is undefined\n\n- name: Set the facts\n  set_fact:\n    cloud_instance_ip:  >-\n      {% if server is defined %}{{ server }}\n      {%- elif _algo_server.user_input is defined and _algo_server.user_input != \"\" %}{{ _algo_server.user_input }}\n      {%- else %}localhost{% endif %}\n\n- pause:\n    prompt: |\n      What user should we use to login on the server? (note: passwordless login required, or ignore if you're deploying to localhost)\n      [root]\n  register: _algo_ssh_user\n  when:\n    - ssh_user is undefined\n    - cloud_instance_ip != \"localhost\"\n\n- name: Set the facts\n  set_fact:\n    ansible_ssh_user: >-\n      {% if ssh_user is defined %}{{ ssh_user }}\n      {%- elif _algo_ssh_user.user_input is defined and _algo_ssh_user.user_input != \"\" %}{{ _algo_ssh_user.user_input }}\n      {%- else %}root{% endif %}\n\n- pause:\n    prompt: |\n      Enter the public IP address of your server: (IMPORTANT! This IP is used to verify the certificate)\n      [{{ cloud_instance_ip }}]\n  register: _endpoint\n  when: endpoint is undefined\n\n- name: Set the facts\n  set_fact:\n    IP_subject_alt_name: >-\n      {% if endpoint is defined %}{{ endpoint }}\n      {%- elif _endpoint.user_input is defined and _endpoint.user_input != \"\" %}{{ _endpoint.user_input }}\n      {%- else %}{{ cloud_instance_ip }}{% endif %}\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "8780f25f99d8f951bc6567263a616e8a9bc26ccb", "filename": "ops/files/splunk/linux/SPLUNK_HOME/etc/apps/learned/metadata/default.meta", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "\n### export: eventtypes, savedsearches, transforms and props\n\n[eventtypes]\naccess = read : [ * ], write : [ admin, power ]\nexport = system\n\n[savedsearches]\naccess = read : [ * ], write : [ admin, power ]\nexport = system\n\n[transforms]\naccess = read : [ * ], write : [ admin, power ]\nexport = system\n\n[props]\naccess = read : [ * ], write : [ admin, power ]\nexport = system\n\n### VIEWSTATES: even normal users should be able to create shared viewstates\n\n[viewstates]\naccess = read : [ * ], write : [ * ]\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "6aed9834a5a5cd6808a887d15cbf1992c041f55d", "filename": "roles/mongodb/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "mongodb_install: False\nmongodb_enabled: False\n\nmongodb_db_path: \"{{ content_base }}/dbdata/mongodb\"    # == /library/dbdata/mongodb/\nmongodb_conf: /etc/mongod.conf\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "827bef766f5def98e0734e31da781cc7a3130877", "filename": "roles/vpn/tasks/client_configs.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Register p12 PayloadContent\n  shell: cat private/{{ item }}.p12 | base64\n  register:  PayloadContent\n  args:\n    chdir: \"configs/{{ IP_subject_alt_name }}/pki/\"\n  with_items: \"{{ users }}\"\n\n- name: Set facts for mobileconfigs\n  set_fact:\n    PayloadContentCA: \"{{ lookup('file' , 'configs/{{ IP_subject_alt_name }}/pki/cacert.pem')|b64encode }}\"\n\n- name: Build the mobileconfigs\n  template:\n    src: mobileconfig.j2\n    dest: configs/{{ IP_subject_alt_name }}/{{ item.0 }}.mobileconfig\n    mode: 0600\n  with_together:\n    - \"{{ users }}\"\n    - \"{{ PayloadContent.results }}\"\n  no_log: True\n\n- name: Build the client ipsec config file\n  template:\n    src: client_ipsec.conf.j2\n    dest: configs/{{ IP_subject_alt_name }}/ipsec_{{ item }}.conf\n    mode: 0600\n  with_items:\n    - \"{{ users }}\"\n\n- name: Build the client ipsec secret file\n  template:\n    src: client_ipsec.secrets.j2\n    dest: configs/{{ IP_subject_alt_name }}/ipsec_{{ item }}.secrets\n    mode: 0600\n  with_items:\n    - \"{{ users }}\"\n\n- name: Build the windows client powershell script\n  template:\n    src: client_windows.ps1.j2\n    dest: configs/{{ IP_subject_alt_name }}/windows_{{ item.0 }}.ps1\n    mode: 0600\n  when: algo_windows\n  with_together:\n    - \"{{ users }}\"\n    - \"{{ PayloadContent.results }}\"\n\n- name: Restrict permissions for the local private directories\n  file:\n    path: \"{{ item }}\"\n    state: directory\n    mode: 0700\n  with_items:\n    - configs/{{ IP_subject_alt_name }}\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a6c182b07e6282124e8b6f1c24bcb0abe79100ba", "filename": "roles/config-postgresql/tasks/firewall.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Check if firewalld is installed\n  command: systemctl status firewalld\n  register: firewalld_status\n  failed_when: false\n  changed_when: false\n\n- name: Check if iptables is installed\n  command: systemctl status iptables\n  register: iptables_status\n  failed_when: false\n  changed_when: false\n\n- name: Open port in firewalld\n  firewalld:\n    port: \"{{ postgresql_port }}/tcp\"\n    permanent: true\n    state: enabled\n  when: firewalld_status.rc == 0\n  notify:\n  - restart firewalld\n\n- name: Ensure iptables is correctly configured\n  lineinfile:\n    insertafter: \"^-A INPUT .* --dport {{ postgresql_port }} .* ACCEPT\"\n    state: present\n    dest: /etc/sysconfig/iptables\n    regexp: \"^-A INPUT .* --dport {{ postgresql_port }} .* ACCEPT\"\n    line: \"-A INPUT -p TCP -m state --state NEW -m TCP --dport {{ postgresql_port }} -j ACCEPT\"\n  when: iptables_status.rc == 0 and firewalld_status.rc != 0\n  notify:\n  - restart iptables\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "0f4a71148b2f7248af2f99084a82b43e6e7a893f", "filename": "roles/create_users/tasks/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n - name: 'Install required packages'\n   package:\n     name: '{{ item }}'\n     state: installed\n   with_items:\n   - python-passlib\n\n - name: \"Initialize create_users facts\"\n   set_fact:\n     users: ''\n\n - name: \"Take create_users input when defined\"\n   set_fact:\n     users: \"{{ create_users }}\"\n   when: create_users is defined\n\n - include_tasks: create_users.yml\n   static: no\n   when: create_users is defined\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "2566c0760d4fef6655f2562ddf672d3e7a2f156e", "filename": "roles/client/tasks/systems/Debian.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- set_fact:\n    prerequisites: []\n    configs_prefix: /etc\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "ebee218af6a129fcb1101b4989e4047e78810099", "filename": "roles/notifications/send-email/tests/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nmail:\n  host: \"smtp.example.com\"\n  port: \"465\"\n  username: \"user1@example.com\"\n  password: \"pa55word\"\n  secure: \"always\"\n  header: 'Reply-To=user2@example.com'\n  to: \"person1@example.com, person2@example.com\"\n  subject: \"Test Message 1\"\n  body: \"<html><body><h1>This is a H1 header</h1></body></html>\"\n  subtype: \"html\"\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ea18b7433548d3ce9e57ba5917aee04b8d84ccfd", "filename": "reference-architecture/rhv-ansible/playbooks/output-dns.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Create DNS and host files from dynamic inventory\n  hosts: localhost\n  vars:\n    lb_ip: \"{{hostvars[groups['tag_openshift_lb'].0]['devices']['eth0'].0 }}\"\n    lb_host: \"{{groups['tag_openshift_lb'].0}}\"\n  tasks:\n    - name: Clear existing files\n      file:\n        path: \"../{{ item }}\"\n        state: absent\n      with_items:\n        - inventory.nsupdate\n        - inventory.hosts\n    - name: Recreate files\n      file:\n        path: \"../{{ item }}\"\n        state: touch\n      with_items:\n        - inventory.nsupdate\n        - inventory.hosts\n    ###############################\n    # Create NSUPDATE entries Reverse first\n    - name: Create NSUPDATE reverse pointer entries\n      lineinfile:\n        path: ../inventory.nsupdate\n        line: \"update add {{ '.'.join(hostvars[item]['devices']['eth0'].0.split('.')[::-1]) }}.in-addr.arpa 86400 PTR  {{ item }}.{{public_hosted_zone}}\"\n        state: present\n      with_items:\n        - \"{{ groups['tag_openshift_master'] }}\"\n        - \"{{ groups['tag_openshift_infra'] }}\"\n        - \"{{ groups['tag_openshift_node'] }}\"\n        - \"{{ groups['tag_openshift_lb'] }}\"\n    - name: Create NSUPDATE entries for public hosted zone\n      lineinfile:\n        path: ../inventory.nsupdate\n        line: \"zone {{public_hosted_zone}}\"\n        state: present\n    - name: Create NSUPDATE wildcard A entry\n      lineinfile:\n        path: ../inventory.nsupdate\n        line: \"update add *.{{app_dns_prefix}}.{{public_hosted_zone}} 86400 A {{lb_ip}}\"\n        state: present\n    - name: Create NSUPDATE public hostname entry\n      lineinfile:\n        path: ../inventory.nsupdate\n        line: \"update add {{openshift_master_cluster_public_hostname}} 86400 A {{lb_ip}}\"\n        state: present\n    - name: Create NSUPDATE host A entries\n      lineinfile:\n        path: ../inventory.nsupdate\n        line: \"update add {{ item }}.{{public_hosted_zone}} 86400 A {{hostvars[item]['devices']['eth0'].0}}\"\n        state: present\n      with_items:\n        - \"{{ groups['tag_openshift_master'] }}\"\n        - \"{{ groups['tag_openshift_infra'] }}\"\n        - \"{{ groups['tag_openshift_node'] }}\"\n        - \"{{ groups['tag_openshift_lb'] }}\"\n    ###############################\n    # Create HOST entries\n    - name: Create HOSTS public hostname\n      lineinfile:\n        path: ../inventory.hosts\n        line: \"{{lb_ip}} {{lb_host}}.{{public_hosted_zone}} {{lb_host}} {{openshift_master_cluster_public_hostname}}\"\n        state: present\n    - name: Create HOSTS node entries\n      lineinfile:\n        path: ../inventory.hosts\n        line: \"{{hostvars[item]['devices']['eth0'].0}} {{ item }}.{{ public_hosted_zone }} {{ item }}\"\n      with_items:\n        - \"{{ groups['tag_openshift_master'] }}\"\n        - \"{{ groups['tag_openshift_infra'] }}\"\n        - \"{{ groups['tag_openshift_node'] }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "faba929a430584fd1f952eba0947285dc89e3516", "filename": "playbooks/vm.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Create VM on infra virtual hosts'\n  hosts: infra_virt_hosts\n  roles:\n  - role: virt-install\n  tags:\n  - provision_infra_vms\n  \n- name: 'Check that the VM(s) are alive'\n  hosts: infra_vms\n  gather_facts: no\n  tasks:\n  - name: 'Wait for VM(s) to come alive'\n    local_action: wait_for\n    args: \n      host: \"{{ ansible_host }}\"\n      port: 22\n      delay: 30\n      timeout: 300\n  tags:\n  - vm_health_check\n\n- name: 'Subscribe the VMs to RHSM'\n  hosts: infra_vms\n  vars:\n    rhsm_username: \"{{ hostvars['localhost'].rhsm_username|default(omit) }}\"\n    rhsm_password: \"{{ hostvars['localhost'].rhsm_password|default(omit) }}\"\n  roles:\n  - role: rhsm\n  tags:\n  - configure_rhsm\n\n- name: 'Make sure the VM is running the latest'\n  hosts: infra_vms\n  roles:\n  - role: update-host\n  tags:\n  - update_host\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "bca445081e295fe1a0a93a775828bd028620a88e", "filename": "roles/rhsm/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# Need to use the 'command' module for this task since the \"redhat_subscription\" module\n# won't do a full \"clean\" when using the \"state: absent\" option\n- name: 'Unregister the system if already registered - if this is a force re-registration'\n  command: 'subscription-manager clean'\n  when:\n  - (rhsm_force_register|default('no'))|lower == 'yes'\n\n# Need to use the 'command' module for this task since the \"yum\" module\n# won't honor an \"upgrade\" of the RPM in case where the source server changed. \n# - note the 'warn: False' set because of this situation\n- name: \"Install Satellite certificate (if applicable)\"\n  command: \"rpm -Uh --force http://{{ rhsm_server_hostname }}/pub/katello-ca-consumer-latest.noarch.rpm\"\n  args:\n    warn: False\n  when:\n  - rhsm_server_hostname is defined\n  - rhsm_server_hostname|trim != ''\n\n- name: 'Register system using Red Hat Subscription Manager'\n  redhat_subscription:\n    state: present\n    username: \"{{ rhsm_username | default(omit) }}\"\n    password: \"{{ rhsm_password | default(omit) }}\"\n    pool: \"{{ rhsm_pool | default(omit) }}\"\n    server_hostname: \"{{ rhsm_server_hostname | default(omit) }}\"\n    activationkey: \"{{ rhsm_activationkey | default(omit) }}\"\n    org_id: \"{{ rhsm_org_id | default(omit) }}\"\n    force_register: \"{{ rhsm_force_register | default(omit) }}\"\n\n- name: \"Obtain currently enabled repos\"\n  shell: 'subscription-manager repos --list-enabled | sed -ne \"s/^Repo ID:[^a-zA-Z0-9]*\\(.*\\)/\\1/p\"'\n  register: enabled_repos\n\n- name: \"Disable repositories that should not be enabled\"\n  shell: \"subscription-manager repos --disable={{ item }}\"\n  with_items: \n  - \"{{ enabled_repos.stdout_lines | difference(rhsm_repos) }}\"\n\n- name: \"Enable specified repositories not already enabled\"\n  command: \"subscription-manager repos --enable={{ item }}\"\n  with_items:\n  - \"{{ rhsm_repos | difference(enabled_repos.stdout_lines) }}\"\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "032d50b8d2b4684727f928677cbed04360be9489", "filename": "roles/osp/packstack-post/tasks/nova.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Ensure 'dhcp_domain' is unset for nova\"\n  lineinfile:\n    path: \"/etc/nova/nova.conf\"\n    regexp: '^dhcp_domain='\n    line: 'dhcp_domain='\n  notify: 'restart openstack-nova-compute'\n\n- name: \"Set a 'disk_alocation_ratio' to allow for overcommit on the local disk\"\n  lineinfile:\n    path: \"/etc/nova/nova.conf\"\n    regexp: '^disk_allocation_ratio='\n    line: 'disk_allocation_ratio=2.0'\n  notify: 'restart openstack-nova-compute'\n\n- name: \"Workaround for OSP >12 to allow for Nova Migrations\"\n  replace:\n    path: \"/var/lib/nova/.ssh/config\"\n    regexp: '(.*)'\n    replace: '# \\1'\n\n- name: \"Ensure Nova Migration doesn't get blocked by host key checking\"\n  lineinfile:\n    path: \"/var/lib/nova/.ssh/config\"\n    regexp: '^StrictHostKeyChecking '\n    line: 'StrictHostKeyChecking no'\n    create: yes\n    owner: nova\n    mode: 0644\n\n- name: \"Copy SELinux .te file to remote host\"\n  copy:\n    src: \"nova-ssh.te\"\n    dest: \"/tmp/nova-ssh.te\"\n\n- name: \"Build SELinux module (.mod) to allow Nova Migration\"\n  command: checkmodule -M -m /tmp/nova-ssh.te -o /tmp/nova-ssh.mod\n\n- name: \"Build SELinux module (.pp) to allow Nova Migration\"\n  command: semodule_package -m /tmp/nova-ssh.mod -o /tmp/nova-ssh.pp\n\n- name: \"Enable the 'nova' user's shell\"\n  user:\n    name: nova\n    shell: /bin/bash\n\n- name: \"Load SELinux module to allow Nova Migration\"\n  command: semodule -i /tmp/nova-ssh.pp\n  notify:\n  - 'restart openstack-nova-compute'\n  - 'restart libvirtd'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "d3fbaeeb2efd469552540606cac3cfb12650bf40", "filename": "roles/config-linux-desktop/config-gnome/tasks/gnome-Fedora.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: \"Install additional packages for Gnome\"\n  dnf:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n  - '@gnome'\n\n"}, {"commit_sha": "45971be8249cc4627ef8ddfacf55a661b7fc13ca", "sha": "dcaec52544a89fcdb12393ba13a645caf3541c47", "filename": "tasks/configure-non-systemd.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Combine Docker daemon environment variable configuration\n  set_fact:\n    docker_service_envs: \"{{ docker_service_envs | combine(_docker_service_opts) | combine(docker_daemon_envs) }}\"\n  vars:\n    _docker_service_opts:\n      DOCKER_OPTS: \"{{ docker_daemon_opts }}\"\n\n- name: Setup Docker environment file {{ docker_envs_dir[_docker_os_dist] }}/docker\n  template:\n    src: docker-envs.j2\n    dest: \"{{ docker_envs_dir[_docker_os_dist] }}/docker\"\n  become: yes\n  notify: restart docker\n  vars:\n    docker_envs: \"{{ docker_service_envs }}\""}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "826efe78dc290959d1e7aa0f569d500785083e43", "filename": "roles/static_inventory/tasks/filter_out_new_app_nodes.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Add all new app nodes to new_app_nodes\n  when:\n  - 'oc_old_app_nodes is defined'\n  - 'oc_old_app_nodes | list'\n  - 'node.name not in oc_old_app_nodes'\n  - 'node[\"metadata\"][\"sub-host-type\"] == \"app\"'\n  register: result\n  set_fact:\n    new_app_nodes: '{{ new_app_nodes }} + [ {{ node }} ]'\n\n- name: If the node was added to new_nodes, remove it from registered nodes\n  set_fact:\n    registered_nodes: '{{ registered_nodes | difference([ node ]) }}'\n  when: 'not result | skipped'\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "2b6e59f5ff6883e633f00651eac1c13891a4f28a", "filename": "roles/serverspec/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for serverspec\nserverspec_run_tests: false\nserverspec_upload_folder: false\nserverspec_tests_path: /vagrant\nserverspec_install_bundler: true\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c3c699e18aeab4dbe410ea04b77942763397186c", "filename": "roles/dns/manage-dns-zones/tests/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "dns_data:\n  views:\n    - name: \"private\"\n      zones:\n        - dns_domain: \"roletest1.com\"\n          state: present\n          route53:\n            aws_access_key: \"{{ aws_access_key }}\"\n            aws_secret_key: \"{{ aws_secret_key }}\"\n            vpc_id: vpc-9dcde6f8\n            vpc_region: eu-west-1\n          entries:\n          - type: A\n            record: server_a\n            value: 192.168.1.1\n            ttl: 400\n            state: present\n          - type: A\n            record: server_b\n            value: 192.168.1.2\n            ttl: 500\n            state: present\n        - dns_domain: \"roletest2.com\"\n          state: present\n          route53:\n            aws_access_key: \"{{ aws_access_key }}\"\n            aws_secret_key: \"{{ aws_secret_key }}\"\n            vpc_id: vpc-9dcde6f8\n            vpc_region: eu-west-1\n          entries:\n          - type: A\n            record: server_a\n            value: 192.168.1.1\n            ttl: 400\n            state: present\n          - type: A\n            record: server_b\n            value: 192.168.1.2\n            ttl: 500\n            state: present\n    - name: \"public\"\n      zones:\n        - dns_domain: \"roletest3.com\"\n          state: absent\n          route53:\n            aws_access_key: \"{{ aws_access_key }}\"\n            aws_secret_key: \"{{ aws_secret_key }}\"\n          entries:\n          - type: A\n            record: server_a\n            value: 192.168.1.1\n            ttl: 400\n            state: present\n          - type: A\n            record: server_b\n            value: 192.168.1.2\n            ttl: 500\n            state: present\n        - dns_domain: \"roletest4.com\"\n          state: absent\n          route53:\n            aws_access_key: \"{{ aws_access_key }}\"\n            aws_secret_key: \"{{ aws_secret_key }}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "1ac8adeedd2dd51c7b0063a297fcc10990dd94a2", "filename": "roles/sugarizer/meta/main.yml", "repository": "iiab/iiab", "decoded_content": "dependencies:\n    - { role: mongodb, tags: ['generic','mongodb'], when: sugarizer_install }\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "00940256831fedc93c940cb48f8cee257568c578", "filename": "tasks/galera/nodes.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n- name: COMMAND | GET debian.cnf from primary node\n  command: cat /etc/mysql/debian.cnf\n  register: debiancnf\n  delegate_to: '{{ mariadb_galera_primary_node }}'\n  changed_when: false\n\n- name: COMMAND | Get current debian.cnf\n  command: cat /etc/mysql/debian.cnf\n  register: ondc\n  changed_when: false\n\n- block:\n\n  - name: SERVICE | Stop MariaDB\n    service:\n      name: mysql\n      state: stopped\n\n  - name: COPY | Paste primary node's debian.cnf\n    copy:\n      content: \"{{ debiancnf.stdout }}\"\n      dest: /etc/mysql/debian.cnf\n      mode: 0600\n      owner: root\n      group: root\n    register: paste\n\n  - name: SERVICE | Start MariaDB\n    service:\n      name: mysql\n      state: started\n\n  when: debiancnf.stdout != ondc.stdout\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "34e7be240cdef135d7c2b18e0b42631f1d1783a3", "filename": "reference-architecture/vmware-ansible/playbooks/roles/haproxy-server-config/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- stat: path=/etc/haproxy/haproxy.cfg\n  register: haproxy_cfg\n\n- name: Copy HAProxy configuration in place.\n  template:\n    src: haproxy.cfg.j2\n    dest: /etc/haproxy/haproxy.cfg\n    mode: 0644\n    validate: haproxy -f %s -c -q\n  notify: restart haproxy\n  when: haproxy_cfg.stat.exists == True\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e22e3d80f7ad597ca398e573273556729ca0d2fb", "filename": "roles/ansible/tower/manage-credential-types/tasks/process-credential-type.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Get the credential_type id based on the name\"\n  set_fact:\n    credential_type_id: \"{{ item.id }}\"\n  when:\n  - item.name|trim == credential_type.name|trim\n  with_items:\n  - \"{{ existing_credential_types_output.rest_output }}\"\n\n- name: \"Load up the credential type\"\n  uri:\n    url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/credential_types/\"\n    user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n    password: \"{{ ansible_tower.admin_password }}\"\n    force_basic_auth: yes\n    method: POST\n    body: \"{{ lookup('template', 'credential-type.j2') }}\"\n    body_format: 'json'\n    headers:\n      Content-Type: \"application/json\"\n      Accept: \"application/json\"\n    validate_certs: no\n    status_code: 201\n  when: credential_type_id is not defined\n\n- name: \"Update existing credential type\"\n  uri:\n    url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/credential_types/{{ credential_type_id }}/\"\n    user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n    password: \"{{ ansible_tower.admin_password }}\"\n    force_basic_auth: yes\n    method: PUT\n    body: \"{{ lookup('template', 'credential-type.j2') }}\"\n    body_format: 'json'\n    headers:\n      Content-Type: \"application/json\"\n      Accept: \"application/json\"\n    validate_certs: no\n    status_code: 200\n  when: credential_type_id is defined\n\n- name: \"Clear/Update facts\"\n  set_fact:\n    credential_type_id: ''\n    processed_credential_types: \"{{ processed_credential_types + [ { 'name': credential_type.name.name } ] }}\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "135fdaa4b469df3302c000b1682ae35f7458c3e2", "filename": "roles/manage-aws-infra/tasks/update-cns-nodes.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: \"Set facts needed for CNS deployment\"\n  set_fact:\n    \n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "917cc6317e5dc48ca630cad015c0c717a5253174", "filename": "reference-architecture/aws-ansible/playbooks/infrastructure.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  connection: local\n  gather_facts: no\n  become: no\n  vars_files:\n  - vars/main.yaml\n  vars:\n    vpc_subnet_azs: \"{{ lookup('ec2_zones_by_region', region) }}\"\n  roles:\n  # Upload ssh-key\n  - { role: ssh-key, when: create_key == \"yes\" }\n  - { role: cfn-outputs, when: create_vpc == \"no\" and add_node == \"yes\" or add_node == \"no\" and deploy_crs is defined }\n  # Create VPC and subnets in multiple AZ\n  - pre-install-check\n  - cloudformation-infra\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "f0fdf7bf9ce7825e166df898dc9f710c67cf9ae2", "filename": "roles/weave/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for weave\n- name: include all interfaces.d\n  sudo: yes\n  lineinfile:\n    dest: /etc/network/interfaces\n    state: present\n    line: 'source /etc/network/interfaces.d/*.cfg'\n  tags:\n    - weave\n\n# Start docker as it is a requirement for weave create-bridge.\n- name: Start up docker\n  service:\n    name: docker\n    state: started\n  tags:\n    - weave\n\n- name: configure weave interface\n  sudo: yes\n  template:\n    src: interfaces.j2\n    dest: /etc/network/interfaces.d/weave.cfg\n    owner: root\n    group: root\n    mode: 0644\n  tags:\n    - weave\n\n# Create weave bridge.\n- name: bring up weave bridge\n  command: ifup weave\n  sudo: yes\n\n- name: upload weave template service\n  template:\n    src: weave.conf.j2\n    dest: \"/etc/init/weave.conf\"\n    mode: 0755\n  sudo: yes\n  tags:\n    - weave\n\n# Restart docker with weave bridge available and triggers weave service.\n- name: configure weave bridge for docker\n  sudo: yes\n  lineinfile:\n    dest: /etc/default/docker\n    state: present\n    regexp: ^DOCKER_OPTS=.*--bridge=weave.*\n    line: 'DOCKER_OPTS=\\\"$DOCKER_OPTS {{ weave_docker_opts }}\\\"'\n  notify:\n    - restart docker\n  tags:\n    - weave\n\n- name: download weave scope\n  get_url:\n    url: \"{{ weave_scope_url }}\"\n    dest: \"{{ weave_scope_dest }}\"\n    mode: 0755\n    validate_certs: no\n  environment: proxy_env\n  tags:\n    - weave\n\n- name: upload weave scope template service\n  template:\n    src: scope.conf.j2\n    dest: \"/etc/init/weavescope.conf\"\n    mode: 0755\n  sudo: yes\n  tags:\n    - weave\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "22c3f1aa1d361097234a44aefbd622dcfb34ad59", "filename": "roles/cloud-scaleway/tasks/prompts.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- pause:\n    prompt: |\n      Enter your auth token (https://www.scaleway.com/docs/generate-an-api-token/)\n    echo: false\n  register: _scaleway_token\n  when: scaleway_token is undefined\n\n- pause:\n    prompt: |\n      Enter your organization name (https://cloud.scaleway.com/#/billing)\n  register: _scaleway_org\n  when: scaleway_org is undefined\n\n- pause:\n    prompt: |\n      What region should the server be located in?\n        {% for r in scaleway_regions %}\n        {{ loop.index }}. {{ r['alias'] }}\n        {% endfor %}\n\n      Enter the number of your desired region\n      [{{ scaleway_regions.0.alias }}]\n  register: _algo_region\n  when: region is undefined\n\n- name: Set scaleway facts\n  set_fact:\n    algo_scaleway_token: \"{{ scaleway_token | default(_scaleway_token.user_input) }}\"\n    algo_scaleway_org: \"{{ scaleway_org | default(_scaleway_org.user_input|default(omit)) }}\"\n    algo_region: >-\n      {% if region is defined %}{{ region }}\n      {%- elif _algo_region.user_input is defined and _algo_region.user_input != \"\" %}{{ scaleway_regions[_algo_region.user_input | int -1 ]['alias'] }}\n      {%- else %}{{ scaleway_regions.0.alias }}{% endif %}\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c1b14169ee06efc370b3e1ce8becd46c40d3d25a", "filename": "roles/user-management/manage-idm-users/tasks/configure_user.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "--- \n  - name: Create IPA user\n    ipa_user:\n      ipa_host: \"{{ ipa_host | default(ansible_host)}}\"\n      ipa_user: \"{{ ipa_admin_user }}\"\n      ipa_pass: \"{{ ipa_admin_password }}\"\n      validate_certs: \"{{ ipa_validate_certs | default(False) }}\"\n      givenname: \"{{ item.first_name | trim }}\"\n      sn: \"{{ item.last_name | trim }}\"\n      name: \"{{ item.user_name | trim }}\"\n      mail: \"{{ item.email | default('') }}\"\n      state: \"{{ item.state | default('present') }}\"\n# The addition of expiration date is a request that will be submitted upstream\n#      krbprincipalexpiration: \"{{ item.expiration_date | default('') }}\"\n    with_items: \"{{ users }}\"\n    register: idm_user_list\n\n  - name: \"Clear users before re-building list with additional data\"\n    set_fact: \n       users: []\n\n  - name: \"Create password generation dataset\"\n    set_fact: \n       users: \"{{ users + [idm_data.item | combine(idm_data | set_user_flags) ] }}\"\n    with_items: \"{{ idm_user_list.results }}\"\n    loop_control:\n      loop_var: idm_data\n\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391", "filename": "roles/local/handlers/main.yml", "repository": "trailofbits/algo", "decoded_content": ""}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a5e19beb55bc56f3dfb3b5e75f4805690e2500e1", "filename": "roles/config-nagios-target/tasks/install-nagios.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Installing the Nagios Software Packages\n  package:\n    name=\"{{item}}\"\n    state=present\n  with_items:\n  - nrpe\n  - nagios-plugins*\n  tags: epel\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ce11eb7dc2870e49c52dc7d4ef280d8263f1c7db", "filename": "reference-architecture/rhv-ansible/playbooks/ovirt-vm-uninstall.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Unregister VMs\n  hosts:\n    - tag_openshift_master\n    - tag_openshift_infra\n    - tag_openshift_node\n    - tag_openshift_lb\n  roles:\n    - rhsm-unregister\n\n- name: oVirt infra destroy\n  hosts: localhost\n  connection: local\n  gather_facts: false\n\n  vars_files:\n    - vars/ovirt-infra-vars.yaml\n\n  pre_tasks:\n    - name: Log in to oVirt\n      ovirt_auth:\n        url: \"{{ engine_url }}\"\n        username: \"{{ engine_user }}\"\n        password: \"{{ engine_password }}\"\n        ca_file: \"{{ engine_cafile | default(omit) }}\"\n        insecure: \"{{ engine_insecure | default(true) }}\"\n      tags:\n        - always\n\n  tasks:\n    - name: Erase vms\n      ovirt_vms:\n        auth: \"{{ ovirt_auth }}\"\n        state: absent\n        name: \"{{ item.name }}\"\n      with_items:\n        - \"{{ vms }}\"\n\n  post_tasks:\n    - name: Logout from oVirt\n      ovirt_auth:\n        state: absent\n        ovirt_auth: \"{{ ovirt_auth }}\"\n      tags:\n        - always\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "026edc0f6127b30efe6ddaf6cc3dabe451827819", "filename": "roles/logstash/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n- name: Restart logstash\n  systemd:\n    name: logstash\n    state: restarted\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "2cc5ee007c93e8e58c0fde01696f65ee3e7712c0", "filename": "roles/openshift-management/tasks/prune-builds.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: Prune Builds\n  shell: oc adm prune builds --keep-complete={{ openshift_prune_builds_complete }}  --keep-failed={{ openshift_prune_builds_failed }} --keep-younger-than={{ openshift_prune_builds_keep_younger }} --orphans --confirm\n  environment:\n    KUBECONFIG: \"{{ kubeconfig }}\""}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "9115ef9fb36f874be67a63363240be34825f2fea", "filename": "tasks/section_03.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - include: section_03_level1.yml\n    tags:\n      - section03\n      - level1\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "a74d7360d58fe137df8f0d71179094c0b75b3799", "filename": "reference-architecture/azure-ansible/3.6/ansibledeployocp/playbooks/roles/azure-deploy/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Create Azure Deploy\n  azure_rm_deployment:\n    state: present\n    location: \"{{ location }}\"\n    resource_group_name: \"{{ resourcegroupname }}\"\n    template_link: \"{{ templatelink }}\"\n    parameters:\n      adminUsername:\n        value: \"{{ adminusername }}\"\n      adminPassword:\n        value: \"{{ adminpassword }}\"\n      sshKeyData:\n        value: \"{{ sshkeydata }}\"\n      WildcardZone:\n        value: \"{{ wildcardzone }}\"\n      numberOfNodes:\n        value: \"{{ numberofnodes }}\"\n      image:\n        value: \"{{ image }}\"\n      masterVMSize:\n        value: \"{{ mastervmsize }}\"\n      infranodeVMSize:\n        value: \"{{ infranodesize }}\"\n      nodeVMSize:\n        value: \"{{ nodevmsize }}\"\n      RHNUserName:\n        value: \"{{ rhnusername }}\"\n      RHNPassword:\n        value: \"{{ rhnpassword }}\"\n      SubscriptionPoolId:\n        value: \"{{ subscriptionpoolid }}\"\n      sshPrivateData:\n        value: \"{{ sshprivatedata }}\"\n      aadClientId:\n        value: \"{{ aadclientid }}\"\n      aadClientSecret:\n        value: \"{{ aadclientsecret }}\"\n      rhsmUsernamePasswordOrActivationKey:\n        value: \"{{ rhsmusernamepasswordoractivationkey }}\"\n      OpenShiftSDN:\n        value: \"{{ openshiftsdn }}\"\n      metrics:\n        value: \"{{ metrics }}\"\n      logging:\n        value: \"{{ logging }}\"\n      opslogging:\n        value: \"{{ opslogging }}\"\n\n  register: azuredeploy\n\n- debug:\n    msg: \"Connect to bastion host by: {{ azuredeploy|json_query('deployment.outputs.\\\"bastion ssh\\\".value') }}\"\n- debug:\n    msg: \"OCP console available in {{ azuredeploy|json_query('deployment.outputs.\\\"openshift Webconsole\\\".value') }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "73a934620e53a7efcaf0ea3af10af122cf6cb07f", "filename": "roles/ansible/tower/manage-inventories/tasks/process-group-member.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Associate the host(s) with the group\"\n  uri:\n    url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/groups/{{ group_id }}/hosts/\"\n    user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n    password: \"{{ ansible_tower.admin_password }}\"\n    force_basic_auth: yes\n    method: POST\n    body: \"{{ lookup('template', 'group-member.j2') }}\"\n    body_format: 'json'\n    headers:\n      Content-Type: \"application/json\"\n      Accept: \"application/json\"\n    validate_certs: no\n    status_code: 200,201,204,400\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "0aa3faacaf407c914ae4439ef520ecaa52519f83", "filename": "reference-architecture/aws-ansible/playbooks/roles/cloudformation-infra/files/add-crs-storage.json", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "{\n  \"AWSTemplateFormatVersion\": \"2010-09-09\",\n  \"Parameters\": {\n    \"KeyName\": {\n      \"Type\": \"AWS::EC2::KeyPair::KeyName\"\n    },\n    \"Vpc\": {\n      \"Type\": \"String\"\n    },\n    \"Route53HostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"PublicHostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"AmiId\": {\n      \"Type\": \"AWS::EC2::Image::Id\"\n    },\n    \"InstanceType\": {\n      \"Type\": \"String\",\n      \"Default\": \"m4.2xlarge\"\n    },\n    \"NodeRootVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"50\"\n    },\n    \"NodeDockerVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"NodeDockerVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"GlusterVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"GlusterVolSize\": {\n      \"Type\": \"Number\",\n      \"Default\": \"500\"\n    },\n    \"NodeUserData\": {\n      \"Type\": \"String\"\n    },\n    \"NodeEmptyVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"NodeEmptyVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"NodeRootVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"PublicHostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"NodeType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gluster\"\n    },\n    \"GlusterNodeDns1\": {\n      \"Type\": \"String\"\n    },\n    \"GlusterNodeDns2\": {\n      \"Type\": \"String\"\n    },\n    \"GlusterNodeDns3\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet1\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet2\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet3\": {\n      \"Type\": \"String\"\n    },\n    \"BastionSG\": {\n      \"Type\": \"String\"\n    },\n    \"NodeSG\": {\n      \"Type\": \"String\"\n    }\n  },\n  \"Resources\": {\n    \"Route53Records\": {\n      \"Type\": \"AWS::Route53::RecordSetGroup\",\n      \"DependsOn\": [\n        \"GlusterNode1\",\n        \"GlusterNode2\",\n        \"GlusterNode3\"\n      ],\n      \"Properties\": {\n        \"HostedZoneName\": { \"Ref\": \"Route53HostedZone\" },\n        \"RecordSets\": [\n          {\n            \"Name\":  {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns1\"},{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t            \"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"GlusterNode1\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\":  {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns2\"},{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t            \"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"GlusterNode2\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\":  {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns3\"},{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t            \"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"GlusterNode3\", \"PrivateIp\"] }]\n          }\n        ]\n      }\n    },\n    \"GlusterSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"CRS Gluster SG\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"gluster_crs_sg\"} ]\n      }\n    },\n    \"GlusterDaemon\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24007\",\n        \"ToPort\": \"24007\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] }\n      }\n    },\n    \"ClientDaemon\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24007\",\n        \"ToPort\": \"24007\",\n        \"SourceSecurityGroupId\": { \"Ref\": \"NodeSG\" }\n      }\n    },\n    \"GlusterManagement\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24008\",\n        \"ToPort\": \"24008\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterConnect\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24008\",\n        \"ToPort\": \"24008\",\n        \"SourceSecurityGroupId\": { \"Ref\": \"NodeSG\" }\n      }\n    },\n    \"BastionSsh\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"22\",\n        \"ToPort\": \"22\",\n        \"SourceSecurityGroupId\": {\"Ref\": \"BastionSG\" }\n      }\n    },\n    \"GlusterCommonSsh\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"22\",\n        \"ToPort\": \"22\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterSsh\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"2222\",\n        \"ToPort\": \"2222\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterWeb\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"8080\",\n        \"ToPort\": \"8080\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterWebFromNode\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"8080\",\n        \"ToPort\": \"8080\",\n        \"SourceSecurityGroupId\": { \"Ref\": \"NodeSG\" }\n      }\n    },\n    \"GlusterNfs\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"49152\",\n        \"ToPort\": \"49664\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] }\n      }\n    },\n    \"ClientGlusterNfs\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"49152\",\n        \"ToPort\": \"49664\",\n        \"SourceSecurityGroupId\": { \"Ref\": \"NodeSG\" }\n      }\n    },\n    \"GlusterNode1\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InstanceType\"},\n\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"GlusterSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet1\"},\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns1\"},{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"StorageType\",\n              \"Value\": \"crs\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"GlusterVolSize\"},\n              \"VolumeType\": {\"Ref\": \"GlusterVolType\"}\n            }\n          }\n         ]\n     }\n   },\n    \"GlusterNode2\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InstanceType\"},\n\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"GlusterSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet2\"},\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns2\"},{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"StorageType\",\n              \"Value\": \"crs\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"GlusterVolSize\"},\n              \"VolumeType\": {\"Ref\": \"GlusterVolType\"}\n            }\n          }\n         ]\n     }\n   },\n    \"GlusterNode3\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InstanceType\"},\n\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"GlusterSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet3\"},\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns3\"},{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"StorageType\",\n              \"Value\": \"crs\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"GlusterVolSize\"},\n              \"VolumeType\": {\"Ref\": \"GlusterVolType\"}\n            }\n          }\n         ]\n     }\n   }\n }\n}\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "283ed60a08c98e59432be0ff921719edc99e3ba1", "filename": "playbooks/cloud-post.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Set subjectAltName as afact\n  set_fact:\n    IP_subject_alt_name: \"{% if algo_provider == 'local' %}{{ IP_subject_alt_name }}{% else %}{{ cloud_instance_ip }}{% endif %}\"\n\n- name: Add the server to an inventory group\n  add_host:\n    name: \"{% if cloud_instance_ip == 'localhost' %}localhost{% else %}{{ cloud_instance_ip }}{% endif %}\"\n    groups: vpn-host\n    ansible_connection: \"{% if cloud_instance_ip == 'localhost' %}local{% else %}ssh{% endif %}\"\n    ansible_ssh_user: \"{{ ansible_ssh_user }}\"\n    ansible_python_interpreter: \"/usr/bin/python2.7\"\n    algo_provider: \"{{ algo_provider }}\"\n    algo_server_name: \"{{ algo_server_name }}\"\n    algo_ondemand_cellular: \"{{ algo_ondemand_cellular }}\"\n    algo_ondemand_wifi: \"{{ algo_ondemand_wifi }}\"\n    algo_ondemand_wifi_exclude: \"{{ algo_ondemand_wifi_exclude }}\"\n    algo_local_dns: \"{{ algo_local_dns }}\"\n    algo_ssh_tunneling: \"{{ algo_ssh_tunneling }}\"\n    algo_windows: \"{{ algo_windows }}\"\n    algo_store_cakey: \"{{ algo_store_cakey }}\"\n    IP_subject_alt_name: \"{{ IP_subject_alt_name }}\"\n\n- name: Additional variables for the server\n  add_host:\n    name: \"{% if cloud_instance_ip == 'localhost' %}localhost{% else %}{{ cloud_instance_ip }}{% endif %}\"\n    ansible_ssh_private_key_file: \"{{ SSH_keys.private }}\"\n  when: algo_provider != 'local'\n\n- name: Wait until SSH becomes ready...\n  wait_for:\n    port: 22\n    host: \"{{ cloud_instance_ip }}\"\n    search_regex: \"OpenSSH\"\n    delay: 10\n    timeout: 320\n    state: present\n  when: cloud_instance_ip != \"localhost\"\n\n- debug:\n    var: IP_subject_alt_name\n\n- name: A short pause, in order to be sure the instance is ready\n  pause:\n    seconds: 20\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "cc7c490c126dd4713a509e6f3a4f68951d72a686", "filename": "reference-architecture/gcp/ansible/playbooks/openshift-minor-upgrade.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: create instance groups\n  hosts: localhost\n  roles:\n  - instance-groups\n\n- include: ../../../../../openshift-ansible/playbooks/byo/openshift-cluster/upgrades/{{ openshift_vers | default('v3_6') }}/upgrade.yml\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "1468f93db4b0eaaffc19ba2123bcbb804c1d8252", "filename": "roles/dns/manage-dns-zones/tasks/route53/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: determine-action.yml\n\n- block:\n    - import_tasks: prereq.yml\n    - import_tasks: get-zone-records.yml\n    - import_tasks: process-views.yml\n  when:\n    - route53_processing|bool == True\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "124870f1801fb4025f9627b35d1d9be3460a5bd5", "filename": "tasks/setup-repository-Debian.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Include tasks from setup of repositories for Ubuntu\n  include_tasks: setup-repository-Ubuntu.yml\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "cea8e33d6794f886ef6b24bead44e2919dce4a87", "filename": "meta/main.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\ngalaxy_info:\n  author: ansible-ThoTeam\n  description: Nexus Repository Manager 3.x\n  company: ThoTeam\n\n  license: license (GPLv3)\n\n  min_ansible_version: 2.2\n\n  github_branch: master\n\n  platforms:\n    - name: EL\n      versions:\n        - 7\n    - name: Ubuntu\n      versions:\n        - all\n\n  galaxy_tags:\n    - nexus\n    - nexus3\n    - java\n    - maven\n    - npm\n    - nuget\n    - yum\n    - docker\n    - pypi\n    - web\n\ndependencies: []\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "962b887b19d9682a42e535dff9a1943be1a8d8e5", "filename": "tasks/Linux/fetch/local.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Copy artifact to destination\n  copy:\n    src: '{{ transport_local }}'\n    dest: '{{ java_download_path }}'\n  register: file_downloaded\n  retries: 5\n  delay: 2\n  until: file_downloaded is succeeded\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b45ef7ff2487ef67870ed2daae2fb8d2aa1a3943", "filename": "reference-architecture/gcp/ansible/playbooks/roles/ssh-proxy-delete/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: delete ssh proxy configuration\n  blockinfile:\n    dest: '{{ ssh_config_file }}'\n    mode: 0600\n    marker: '# {mark} OPENSHIFT ON GCP BLOCK'\n    state: absent\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "2993f69462390f6593557cc2f8cc2770798ee725", "filename": "roles/cloud-ec2/tasks/prompts.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- pause:\n    prompt: |\n      Enter your aws_access_key (http://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html)\n      Note: Make sure to use an IAM user with an acceptable policy attached (see https://github.com/trailofbits/algo/blob/master/docs/deploy-from-ansible.md)\n    echo: false\n  register: _aws_access_key\n  when:\n     - aws_access_key is undefined\n     - lookup('env','AWS_ACCESS_KEY_ID')|length <= 0\n\n- pause:\n    prompt: |\n      Enter your aws_secret_key (http://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html)\n    echo: false\n  register: _aws_secret_key\n  when:\n    - aws_secret_key is undefined\n    - lookup('env','AWS_SECRET_ACCESS_KEY')|length <= 0\n\n- set_fact:\n    access_key: \"{{ aws_access_key | default(_aws_access_key.user_input|default(None)) | default(lookup('env','AWS_ACCESS_KEY_ID'), true) }}\"\n    secret_key: \"{{ aws_secret_key | default(_aws_secret_key.user_input|default(None)) | default(lookup('env','AWS_SECRET_ACCESS_KEY'), true) }}\"\n\n- block:\n  - name: Get regions\n    aws_region_facts:\n      aws_access_key: \"{{ access_key }}\"\n      aws_secret_key: \"{{ secret_key }}\"\n      region: us-east-1\n    register: _aws_regions\n\n  - name: Set facts about the regions\n    set_fact:\n      aws_regions: \"{{ _aws_regions.regions | sort(attribute='region_name') }}\"\n\n  - name: Set the default region\n    set_fact:\n      default_region: >-\n        {% for r in aws_regions %}\n        {%- if r['region_name'] == \"us-east-1\" %}{{ loop.index }}{% endif %}\n        {%- endfor %}\n\n  - pause:\n      prompt: |\n        What region should the server be located in?\n        (https://docs.aws.amazon.com/general/latest/gr/rande.html#ec2_region)\n          {% for r in aws_regions %}\n          {{ loop.index }}. {{ r['region_name'] }}\n          {% endfor %}\n\n        Enter the number of your desired region\n        [{{ default_region }}]\n    register: _algo_region\n  when: region is undefined\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b569af4e75ddd44626370273342cae1e0702f2dc", "filename": "roles/dns/manage-dns-zones/tasks/named/process-views.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Ensure the final view directory exists\n  file:\n    path: \"{{ dns_zone_temp_config_dir }}/view\"\n    state: directory\n\n- include_tasks: process-zones.yml\n  with_items:\n    -  \"{{ dns_data.views }}\"\n  loop_control:\n    loop_var: \"view\"\n\n- name: Assemble the final view configuration\n  assemble:\n    src: \"{{ dns_zone_temp_config_dir }}/view\"\n    dest: \"/etc/named/named.conf.view\"\n  notify: restart named\n\n- name: Setup ACLs\n  vars:\n    named_views: \"{{ dns_data.views }}\"\n  template:\n    src: named/acl.j2\n    dest: /etc/named/named.conf.acl\n    owner: named\n    group: named\n    mode: 0660\n  notify: restart named\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "b2f222b6ac2e3fa11aa9f97a93e3ba26f6e0ac8a", "filename": "roles/ovirt-engine-setup/meta/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\ngalaxy_info:\n  author: \"Petr Kubica\"\n  description: \"oVirt setup installer\"\n  company: \"Red Hat\"\n  license: \"GPLv3\"\n  min_ansible_version: 1.9\n  platforms:\n  - name: EL\n    versions:\n    - all\n  galaxy_tags:\n    - installer\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "05b8d56745c11bcd6d4c0ed69fc832db575f8304", "filename": "roles/kalite/tasks/setup.yml", "repository": "iiab/iiab", "decoded_content": "# This is for an OS other than Fedora 18\n\n- name: Create kalite_root directory\n  file:\n    path: \"{{ kalite_root }}/httpsrv/static\"\n    owner: root\n    group: root\n    mode: 0755\n    state: directory\n\n- name: Run the setup using 'kalite manage'\n  command: \"{{ kalite_program }} manage setup --username={{ kalite_admin_user }} --password={{ kalite_admin_password }} --noinput\"\n  environment:\n    KALITE_HOME: \"{{ kalite_root }}\"\n  async: 900\n  poll: 10\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "e9514dae15545a07c17338241093403d164b0647", "filename": "roles/ovirt-engine-install-packages/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# tasks file for ovirt-engine-install-packages\n- name: yum install engine\n  yum:\n    name: \"{{ovirt_engine_type}}\"\n    state: installed\n    update_cache: yes\n  tags:\n    - skip_yum_install_ovirt_engine\n\n- name: yum install dwh\n  yum:\n    name: \"{{ovirt_engine_type}}-dwh\"\n    state: present\n  when: (ovirt_engine_dwh|bool == True) and (ovirt_engine_version|int < 4)\n  tags:\n    - skip_yum_install_ovirt_engine_dwh\n\n- name: yum install ovirt-engine-dwh-setup\n  yum:\n    name: \"{{ovirt_engine_type}}-dwh-setup\"\n    state: present\n  when: ovirt_engine_dwh|bool == True\n  tags:\n    - skip_yum_install_ovirt_engine_dwh_setup\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "333ff805c701bae0dd81623a3c20b5f77e0b5fbb", "filename": "roles/dnsmasq/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for dnsmasq\n- name: remove dnsmasq override\n  file:\n    path: /etc/init/dnsmasq.override\n    state: absent\n  notify:\n    - restart dnsmasq\n  tags:\n    - dnsmasq\n\n- name: ensure dnsmasq is running (and enable it at boot)\n  service:\n    name: dnsmasq\n    state: started\n    enabled: yes\n  tags:\n    - dnsmasq\n\n- name: configure dnsmasq\n  sudo: yes\n  template:\n    src: 10-consul.j2\n    dest: /etc/dnsmasq.d/10-consul\n    owner: root\n    group: root\n    mode: 0644\n  notify:\n    - restart dnsmasq\n  tags:\n    - dnsmasq\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "0488bef0de48847e30fa09847b12d54badc97ceb", "filename": "reference-architecture/gcp/ansible/playbooks/roles/wait-for-instance/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# Ansible bug keeps wait_for_connection module from working through the bastion host\n# https://github.com/ansible/ansible/issues/23774\n- name: wait for the instance {{ instance }} to come up\n  wait_for:\n    host: '{{ instance }}'\n    port: 22\n    state: started\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "c1cca0d1ae48bcc3fcf20735288d4bcda1bb58d2", "filename": "roles/dokuwiki/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install DokuWiki\n  include_tasks: install.yml\n  when: dokuwiki_install\n\n- name: Add 'dokuwiki' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: dokuwiki\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: DokuWiki\n    - option: description\n      value: '\"DokuWiki is a simple to use and highly versatile Open Source wiki software that does not require a database.\"'\n    - option: installed\n      value: \"{{ dokuwiki_install }}\"\n    - option: enabled\n      value: \"{{ dokuwiki_enabled }}\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "4d6985046c7fea2059015b298657c74458823e8b", "filename": "ops/playbooks/config_subscription.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- hosts: vms\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Clean Any Subscription Data\n      shell:\n        cmd: subscription-manager clean\n\n    - name: Register now\n      shell:\n        cmd: subscription-manager register --org=\"{{rhn_orgid}}\" --activationkey=\"{{rhn_key}}\" \n        #cmd: subscription-manager register --username=\"{{redhat_user}}\" --password=\"{{redhat_pass}}\" --auto-attach\n      register: res\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "209062ca788fbcd44be5a9525f20a300d1c629bd", "filename": "reference-architecture/vmware-ansible/playbooks/roles/docker-storage-setup/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- block:\n    - name: create the docker-storage config file\n      template:\n        src: \"{{ role_path }}/templates/docker-storage-setup-overlayfs.j2\"\n        dest: /etc/sysconfig/docker-storage-setup\n        owner: root\n        group: root\n        mode: 0644\n\n  when:\n    - ansible_distribution_version | version_compare('7.4', '>=')\n    - ansible_distribution == \"RedHat\"\n\n- block:\n    - name: create the docker-storage-setup config file\n      template:\n        src: \"{{ role_path }}/templates/docker-storage-setup-dm.j2\"\n        dest: /etc/sysconfig/docker-storage-setup\n        owner: root\n        group: root\n        mode: 0644\n\n  when:\n    - ansible_distribution_version | version_compare('7.4', '<')\n    - ansible_distribution == \"RedHat\"\n\n- name: start docker\n  service: name=docker state=started enabled=true\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "57e665067915b9e9326b0c009f70260da53ee0a6", "filename": "roles/scm/gitlab.com/tests/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "gitlab_api_private_token: !vault |\n          $ANSIBLE_VAULT;1.1;AES256\n          33396164373338656334643933303237616561663430343438643934363635363765626432636466\n          33396164373338656334643933303237616561663430343438643934363635363765626432636466\n          33396164373338656334643933303237616561663430343438643934363635363765626432636466\n          33396164373338656334643933303237616561663430343438643934363635363765626432636466\n          64363839383162313735303333396661303161323962656438373538313431373934\n\ngroup:\n  name: a-gitlab-org\n\nprojects:\n- repo_name: test-ci-cd\n  deploy_key_location: \"{{ lookup('file', './files/test-1.pub') }}\"\n  import_url: https://github.com/rht-labs/labs-ci-cd.git\n- repo_name: test-app\n  import_url: https://github.com/rht-labs/labs-ci-cd.git\n  deploy_key_location: \"{{ lookup('file', './files/test-2.pub') }}\"\n\nusers:\n# ONLY NUMERICAL USER ID VALUES, DO NOT ADD YOURSELF, YOU ALREADY OWN THE GROUP\n- id: 9999999 #someRealGitlabUserId"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "fdda41bb3a1ec38c22b3a91ca1d3dbff7f2116c4", "filename": "roles/static_inventory/meta/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ndependencies:\n  - role: common\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5dc5416da8d484121983d5388da3f4704f658b4d", "filename": "roles/nfs-server/tasks/prep.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Ensure everything is up-to-date' \n  package: \n    name: '*'\n    state: latest\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - nfs-utils\n  - lvm2\n  - firewalld\n  - python-firewall\n\n- name: 'Ensure firewalld is running'\n  service:\n    name: firewalld\n    state: started \n    enabled: yes\n\n- name: 'Ensure nfs-server is running'\n  service:\n    name: nfs-server\n    state: started \n    enabled: yes\n\n- name: 'Open Firewall for NFS use'\n  firewalld: \n    port: \"{{ item }}\"\n    permanent: yes\n    state: enabled\n    immediate: yes\n  with_items:\n  - 111/tcp\n  - 111/udp\n  - 2049/tcp\n  - 2049/udp\n\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "0fbe713516425e5aba4fb7cc752229c0bed7cec9", "filename": "roles/sync-keys/tasks/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: \"Push Administrator's SSH Keys\"\n  authorized_key:\n    user: \"{{ ansible_ssh_user }}\"\n    key: \"{{ key_url }}\"\n  when: key_url is defined and key_url != \"\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "865a70a19b89801c76bfaab898503de077933920", "filename": "reference-architecture/gcp/ansible/playbooks/openshift-post.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- include: ../../../../playbooks/empty-dir-quota.yaml\n\n- name: post ocp deploy tasks for single master node\n  hosts: single_master\n  roles:\n  - role: openshift_default_storage_class\n    openshift_storageclass_default: false\n    openshift_storageclass_name: ssd\n    openshift_storageclass_parameters:\n      type: pd-ssd\n  tasks:\n  - name: set node selector for openshift-infra namespace\n    command: oc annotate --overwrite namespace openshift-infra openshift.io/node-selector='role=infra'\n\n- name: post ocp deploy tasks for master nodes\n  hosts: masters\n  roles:\n  - master-http-proxy\n  - role: os_firewall\n    os_firewall_allow:\n    - service: master http proxy\n      port: 8080/tcp\n\n- name: post ocp deploy tasks for infra nodes\n  hosts: infra_nodes\n  roles:\n  - role: os_firewall\n    os_firewall_allow:\n    - service: router liveness probe\n      port: 1936/tcp\n\n- name: post ocp deploy tasks for app nodes\n  hosts: app_nodes\n  roles:\n  - restrict-gce-metadata\n\n- name: mark instances with deployed openshift\n  hosts: localhost\n  tasks:\n  - name: mark instances\n    command: gcloud --project {{ gcloud_project }} compute instances add-metadata {{ item }} --zone {{ hostvars[item].gce_zone }} --metadata 'openshift_deployed={{ ansible_date_time.iso8601 }}'\n    with_items: '{{ groups[\"tag_\" + prefix] }}'\n\n- include: validation.yaml\n\n- name: print message about the ocp console\n  hosts: localhost\n  tasks:\n  - name: print message about the ocp console url\n    debug:\n      msg: Deployment is complete. OpenShift Console can be found at https://{{ openshift_master_cluster_public_hostname }}/\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "f64c25936853c4fbd6e4d6780ac8ee178414936b", "filename": "roles/2-common/tasks/packages.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install yum packages (redhat)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - yum-utils\n    - createrepo\n    - wpa_supplicant\n    - linux-firmware\n    - syslog\n    - xml-common\n    - nss-mdns\n    - avahi\n    - avahi-tools\n  when: is_redhat\n\n- name: Download usbmount -- not in Debian Stretch (debian-9)\n  get_url:\n    url: \"{{ iiab_download_url }}/usbmount_0.0.14.1_all.deb\"\n    dest: \"{{ downloads_dir }}\"\n    timeout: \"{{ download_timeout }}\"\n  when: internet_available and is_debian_9\n\n- name: Install usbmount (debian-9)\n  command: apt install -y {{ downloads_dir }}/usbmount_0.0.14.1_all.deb\n  when: is_debian_9\n\n- name: Install packages (debuntu)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - inetutils-syslogd\n    - wpasupplicant\n    - libnss-mdns\n    - avahi-daemon\n    - avahi-discover\n    - exfat-fuse\n    - exfat-utils\n  when: is_debuntu\n\n- name: Install common packages\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - acpid\n    - mlocate\n    - rsync\n    - htop\n    - etckeeper\n    - python-passlib\n    - usbmount\n    - net-tools\n    - openssh-server\n    - sudo\n    - logrotate\n    - make\n    - tar\n    - unzip\n    - bzip2\n    - i2c-tools\n    - bridge-utils\n    - netmask\n    - usbutils\n    - hostapd\n    - wget\n    - openssl   #FC 18 does not supply, but pear requires\n    - gawk\n    - curl\n    - pandoc\n    - lynx\n    - ntfs-3g\n\n#- name: Install pip as a commonly required package management system\n#  command: curl https://bootstrap.pypa.io/get-pip.py -o {{ downloads_dir }}/get-pip.py\n\n#- name: Run the install script for pip\n#  command: python {{ downloads_dir }}/get-pip.py\n\n- name: Install common Python packages\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - python-pip\n    - python-setuptools\n    - python-virtualenv\n\n# instructions state to start with a fully updated system before starting, stop using\n# ansible as a crutch for developers not following the directions and taking short-cuts\n\n#- name: Update common packages (not Debian)\n#  package: name={{ item }}\n#           state=latest\n#  with_items:\n#   - NetworkManager\n#   - glibc # CVE-2015-7547\n#   - bash\n#   - iptables\n#  when: is_redhat\n\n# Consensus decision to try to slim down https://github.com/iiab/iiab/issues/518 (per 2017-11-20 community/team call @ http://minutes.iiab.io)\n#- name: Update common packages (debuntu)\n#  package: name={{ item }}\n#           state=latest\n#  with_items:\n#   - libc6\n#   - bash\n#   - iptables\n#  when: is_debuntu\n\n#- name: If version of Network manager has changed, subsequent nmcli commands will fail,restart now\n#  service: name=NetworkManager\n#           state=restarted\n#  when: not installing\n# the above should use a handler - all reboots should wait until all\n# mods are preformed\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b2c195f91e3ef385923eb02ed6c3ed1359d151e0", "filename": "playbooks/provisioning/openstack/custom-actions/add-cas.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: cluster_hosts\n  become: true\n  vars:\n    ca_files: []\n  tasks:\n  - name: Copy CAs to the trusted CAs location\n    with_items: \"{{ ca_files }}\"\n    copy:\n      src: \"{{ item }}\"\n      dest: /etc/pki/ca-trust/source/anchors/\n  - name: Update trusted CAs\n    shell: 'update-ca-trust enable && update-ca-trust extract'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "2c63f89e046da8ccf88d395c252040202f539c30", "filename": "roles/config-hostname/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: my-host\n  roles:\n  - role: config-hostname \n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "1a0336e88e2da74504067dbf293e3d594c1a0373", "filename": "ops/playbooks/restore_swarm.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- name: Restore Swarm\n  hosts: ucp_main\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - ../group_vars/backups\n    - includes/internal_vars.yml\n\n  vars:\n    UCPIP:  \"{{groups.ucp_main[0]}}.{{domain_name}}\"\n    UCPUSER: \"{{ucp_username}}\"\n    UCPPASSWORD: \"{{ucp_password}}\"\n    JOINING_IP:  \"{{ hostvars[groups.ucp_main[0]]['ip_addr'] | ipaddr('address')}}:2377\"\n  \n  pre_tasks:\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n\n  environment:\n    - \"{{ env }}\"\n\n  roles:\n    - role: hpe.openports\n      hpe_openports_ports: \"{{ internal_ucp_ports }}\"\n\n  tasks:\n\n    - name: Check if node already belongs to the swarm\n      shell: docker info | grep \"Swarm{{\":\"}} inactive\" | wc -l\n      register: swarm_inactive\n#\n# configure passwordless ssh to our ansible box\n#\n    - name: Register key\n      stat: path=/root/.ssh/id_rsa\n      register: key\n    - name: Create keypairs\n      shell: ssh-keygen -t rsa -f /root/.ssh/id_rsa -N ''\n      when: key.stat.exists == False\n    - name: Fetch all public ssh keys\n      shell: cat ~/.ssh/id_rsa.pub\n      register: ssh_keys\n    - name: Deploy keys on localhost\n      authorized_key: user=root key=\"{{ item }}\"\n      delegate_to: localhost\n      with_items:\n        - \"{{ ssh_keys.stdout }}\"\n\n    - name: Creates directory\n      file:\n        path: /root/scripts\n        state: directory\n      when: inventory_hostname in groups.ucp_main\n\n    - name: Copy restore script to target\n      template:\n        src: ../templates/restore_swarm.sh.j2\n        dest: /root/scripts/restore_swarm.sh\n        mode: 0744\n      when: inventory_hostname in groups.ucp_main\n\n    - name: restore swarm\n      shell: /root/scripts/restore_swarm.sh \n      register: res\n      when: inventory_hostname in groups.ucp_main and swarm_inactive.stdout == \"1\" and ucp_instance == \".none.\"\n\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "4bed10f14a1d0d8c86f7c1b72f5d3b925c19e98b", "filename": "ops/files/splunk/linux/SPLUNK_HOME/etc/apps/search/metadata/default.meta", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "\n# Application-level permissions\n\n[]\naccess = read : [ * ], write : [ admin, power ]\n\n### MANAGER\n\n[manager]\naccess = read : [ * ], write : [ admin ]\nexport = system\n\n### QUICKSTART RECIPES\n\n[quickstart]\naccess = read : [ * ], write : [ admin ]\nexport = system\n\n\n### VIEWS\n\n[views/search_status]\naccess = read : [ admin ], write : [ admin ]\n\n[views/search_detail_activity]\naccess = read : [ admin ], write : [ admin ]\n\n[views/search_activity_by_user]\naccess = read : [ admin ], write : [ admin ]\n\n[views/scheduler_status]\naccess = read : [ admin ], write : [ admin ]\n\n[views/scheduler_status_errors]\naccess = read : [ admin ], write : [ admin ]\n\n[views/scheduler_savedsearch]\naccess = read : [ admin ], write : [ admin ]\n\n[views/scheduler_user_app]\naccess = read : [ admin ], write : [ admin ]\n\n[views/status_index]\naccess = read : [ admin ], write : [ admin ]\n\n[views/splunkclouduf]\naccess = read : [ admin ], write : [ admin ]\n\n[views/flashtimeline]\nexport = system\n\n[views/basic_charting]\nexport = system\n\n[views/show_source]\nexport = system\n\n[views/show_source.prod_lite]\nexport = system\n\n[views/charting]\nexport = system\n\n[views/report_builder_define_data]\nexport = system\n\n[views/report_builder_display]\nexport = system\n\n[views/report_builder_format_report]\nexport = system\n\n[views/report_builder_print]\nexport = system\n\n[views/search]\nexport = system\n\n[views/dashboards]\nexport = system\n\n[views/alert]\nexport = system\n\n[views/alerts]\nexport = system\n\n[views/reports]\nexport = system\n\n[views/report]\nexport = system\n\n[views/pivot]\nexport = system\n\n[views/data_models]\nexport = system\n\n[views/data_model_explorer]\nexport = system\n\n[views/data_model_editor]\nexport = system\n\n[views/data_model_manager]\nexport = system\n\n[views/field_extractor]\nexport = system\n\n[views/datasets]\nexport = system\n\n[views/dataset]\nexport = system\n\n[views/job_manager]\nexport = system\n\n[views/orphaned_scheduled_searches]\naccess = read : [ admin ], write : [ admin ]\n\n[views/integrity_check_of_installed_files]\naccess = read : [ admin ], write : [ admin ]\n\n[views/mod_setup]\nexport = system\n\n### VIEWSTATES: even normal users should be able to create shared viewstates\n\n[viewstates]\naccess = read : [ * ], write : [ * ]\nexport = system\n\n### EVENT TYPES\n\n[eventtypes]\nexport = system\n\n\n### PROPS\n\n[props]\nexport = system\n\n\n### TRANSFORMS\n\n[transforms]\nexport = system\n\n\n### TAGS\n\n[tags]\nexport = system\n\n### SAVED SEARCHES\n\n[savedsearches/Errors%20in%20the%20last%2024%20hours]\naccess = read : [ * ], write : [ admin ]\n\n[savedsearches/Errors%20in%20the%20last%20hour]\naccess = read : [ * ], write : [ admin ]\n\n[savedsearches/Messages%20by%20minute%20last%203%20hours]\naccess = read : [ admin ], write : [ admin ]\n\n[savedsearches/Splunk%20errors%20last%2024%20hours]\naccess = read : [ admin ], write : [ admin ]\n\n### Alert Actions\n[alert_actions]\naccess = read : [ * ], write : [ admin, power ]\nexport = system\n\n### COMMANDS\n\n[commands]\naccess = read : [ * ], write : [ admin ]\nexport = system\n\n[commands/crawl]\naccess = read : [ admin ], write : [ admin ]\n\n[commands/input]\naccess = read : [ admin ], write : [ admin ]\n\n[commands/mappy]\naccess = read : [ admin ], write : [ admin ]\n\n[commands/reducepy]\naccess = read : [ admin ], write : [ admin ]\n\n\n\n### LOOKUP TABLES\n\n[lookups]\nexport = system\n\n### SEARCH SCRIPTS\n\n[searchscripts]\naccess = read : [ * ], write : [ admin ]\nexport = system\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "f3eb594217290e8974fff1f4399eb48dbfa65fc0", "filename": "roles/network/tasks/squid.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install Squid packages\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - \"{{ proxy }}\"\n    - cadaver\n  tags:\n    - download\n\n- name: Bigger hammer for Ubuntu\n  command: /etc/init.d/squid stop\n  when: is_ubuntu\n\n- name: Stop Squid\n  service:\n    name: \"{{ proxy }}\"\n    state: stopped\n  when: not installing\n\n- name: Create the Squid user\n  user:\n    name: \"{{ proxy_user }}\"\n    createhome: False\n    shell: /bin/false\n\n- name: Copy init script and config file\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: \"{{ item.owner }}\"\n    group: \"{{ item.group }}\"\n    mode: \"{{ item.mode }}\"\n  with_items:\n    - src: 'roles/network/templates/squid/squid.sysconfig'\n      dest: '/etc/sysconfig/squid'\n      owner: 'root'\n      group: 'root'\n      mode: '0755'\n    - src: 'roles/network/templates/squid/sites.whitelist.txt'\n      dest: '/etc/{{ proxy }}/sites.whitelist.txt'\n      owner: '{{ proxy_user }}'\n      group: '{{ proxy_user }}'\n      mode: '0644'\n    - src: 'roles/network/templates/squid/allowregex.rules'\n      dest: '/etc/{{ proxy }}/allowregex.rules'\n      owner: '{{ proxy_user }}'\n      group: '{{ proxy_user }}'\n      mode: '0644'\n    - src: 'roles/network/templates/squid/denyregex.rules'\n      dest: '/etc/{{ proxy }}/denyregex.rules'\n      owner: '{{ proxy_user }}'\n      group: '{{ proxy_user }}'\n      mode: '0644'\n    - src: 'roles/network/templates/squid/dstaddress.rules'\n      dest: '/etc/{{ proxy }}/dstaddress.rules'\n      owner: '{{ proxy_user }}'\n      group: '{{ proxy_user }}'\n      mode: '0644'\n    - src: 'roles/network/templates/squid/iiab-httpcache.j2'\n      dest: '/usr/bin/iiab-httpcache'\n      owner: 'root'\n      group: 'root'\n      mode: '0755'\n\n- name: Create Squid cache directory\n  file:\n    path: /library/cache\n    owner: \"{{ proxy_user }}\"\n    group: \"{{ proxy_user }}\"\n    mode: 0750\n    state: directory\n\n- name: Create Squid log directory\n  file:\n    path: \"/var/log/{{ proxy }}\"\n    owner: \"{{ proxy_user }}\"\n    group: \"{{ proxy_user }}\"\n    mode: 0750\n    state: directory\n\n- include_tasks: roles/network/tasks/dansguardian.yml\n  when: dansguardian_install\n\n# {{ proxy }} is normally \"squid\", but is \"squid3\" on raspbian-8 & debian-8\n- name: Add '{{ proxy }}' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: \"{{ proxy }}\"\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: Squid\n    - option: description\n      value: '\"Squid caches web pages the first time they are accessed, and pulls them from the cache thereafter.\"'\n    - option: enabled\n      value: \"{{ squid_enabled }}\"\n\n- name: Add 'dansguardian' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: dansguardian\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: DansGuardian\n    - option: description\n      value: '\"DansGuardian searches web content for objectionable references and denies access when found.\"'\n    - option: enabled\n      value: \"{{ dansguardian_enabled }}\"\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "f64fc1f20c45f2147f37b8f66203b5e33577077a", "filename": "roles/cluster-login/defaults/main.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "admin_username: \"test_admin\"\nadmin_password: \"123456\"\nmaster_fqdn: \"{{ hostvars[groups['masters'][0]]['ansible_fqdn'] }}\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "f61c19a32793acc2837c5ab776fb378de3c67c8b", "filename": "archive/roles/registry/tasks/openssl.yaml", "repository": "redhat-cop/casl-ansible", "decoded_content": "- name: Creates directory\n  file: path={{ certificate_path }} state=directory\n\n- name: create self-signed SSL cert\n  command: openssl req -new -nodes -x509 -subj \"{{ certificate_subject }}\" -days 3650 -keyout {{ certificate_path }}/server.key -out {{ certificate_path }}/server.crt -extensions v3_ca creates={{ certificate_path }}/server.crt\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7490dce015861ae02952fceb4985090782ccc06f", "filename": "roles/config-mysql/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nmode: containerized\n\n# Name of the database installation and service\n# (different from actual database name)\nmysql_name: mysql\nmysql_service: \"{{ mysql_name }}.service\"\n\n# Systemd\nsystemd_service_dir: /usr/lib/systemd/system\nsystemd_environmentfile_dir: /etc/sysconfig\n\n# MySQL\nmysql_image: registry.access.redhat.com/rhscl/mysql-57-rhel7:latest\nmysql_storage_dir: /var/lib/mysql\nmysql_container_storage_dir: /var/lib/mysql/data\nmysql_database: sampledb\nmysql_host_port: 3306\nmysql_container_port: 3306\nmysql_root_username: root\n\n# These Values will be randomaly generated if not defined\n#mysql_username: mysql\n#mysql_password: mysql\n#mysql_root_password: mysqlroot"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "4a6d1dac461aad0744ee6b9a4de02ff60ae150c5", "filename": "ops/playbooks/collect_facts.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- name: Collect Fact\n  hosts: localhost\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - ../group_vars/backups\n\n  environment:\n    - \"{{ env }}\"\n\n  tasks:\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n    - fail:\n        msg: \"cannot find UCP endpoint at any of {{ groups.ucp }}\"\n      when: ucp_instance == \".none.\"\n    - include_tasks: includes/find_dtr.yml\n      vars:\n        ping_servers: \"{{ groups.dtr }}\"\n    - debug: var=dtr_instance\n    - fail:\n        msg: \"cannot find DTR  endpoint at any of {{ groups.dtr }}\"\n      when: dtr_instance == \".none.\"\n    - include_tasks: includes/get_dtr_version.yml\n    - include_tasks: includes/get_ucp_version.yml\n    - debug: var=detected_dtr_version\n    - debug: var=detected_ucp_version\n\n    - fail:\n        msg: \"Mismatch between detected version of UCP {{ detected_ucp_version }} and documented version {{ ucp_version }}\"\n      when: ucp_version != detected_ucp_version \n\n    - fail:\n        msg: \"Mismatch between detected version of DTR {{ detected_dtr_version }} and documented version {{ dtr_version }}\"\n      when: dtr_version != detected_dtr_version \n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "078efe5b422df3353fecbf3f0302ab56b69a4ae2", "filename": "roles/cloud-gce/tasks/venv.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Clean up the environment\n  file:\n    dest: \"{{ gce_venv }}\"\n    state: absent\n  when: clean_environment\n\n- name: Install requirements\n  pip:\n    name:\n      - apache-libcloud\n      - pycrypto\n    state: latest\n    virtualenv: \"{{ gce_venv }}\"\n    virtualenv_python: python2.7\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "926984841a77f9a9f614416f67e11efe862a4806", "filename": "roles/docker/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for docker\n- name: remove docker override\n  command: /bin/rm -f /etc/init/docker.override\n\n- name: ensure docker is running (and enable it at boot)\n  service: name=docker state=started enabled=yes\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "6adb8394a1a4b4a3ea396e3716ea0cf8d489a5e1", "filename": "roles/0-init/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "# Use these to tag a release at a point in time, for /etc/iiab/iiab.env\niiab_base_ver: 6.6\niiab_revision: 0\n\n# These entries should never be changed in this file.\n# These are defaults for boolean routines.\nfirst_run: False\nrpi_model: none\nis_rpi: False\nxo_model: none\ngw_active: none\ninternet_available: False\ndiscovered_wan_iface: none\n\n# old defs\ngui_port: 80\nexFAT_enabled: False\nis_F18: False\n\n# Set default 1-prep discovered hardware\nrtc_id: ds3231\nNUC6_firmware_needed: False\n\n# used in 2-common/tasks/xo.yml\nwifi_id: none\n\n# used in 2-common, 3-base-server and roles/network\ninstalling: False\n\n# network\nno_net_restart: False\nno_NM_reload: False\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b7371c0074b7d44d01e382640a41dad7bcdbe0a6", "filename": "roles/config-chrony/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: \"prereq.yml\"\n- import_tasks: \"chrony.yml\"\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "71afd3c8e5785e5928510b635fd30a6970ca4f44", "filename": "roles/config-routes/tests/infrahosts.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Configure routes on the host'\n  hosts: infra_hosts\n  roles:\n  - role: config-routes\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "280137e10150a363ae48dedf34d351deede25a63", "filename": "roles/config-vlans/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: 'prereq.yml'\n- import_tasks: 'interfaces.yml'\n\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "7100ac4aa739b6e779e705bab2ef62fa2532f02a", "filename": "reference-architecture/aws-ansible/playbooks/roles/pre-install-check/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Get current Ansible version on local host\n  assert:\n    that: \"ansible_version.full | version_compare('2.2', '>=')\"\n    msg: \"You need Ansible version 2.2.0+\"\n\n- name: Validate that openshift rpms are installed or git repo has been cloned\n  stat:\n    path: /usr/share/ansible/openshift-ansible\n  register: openshift_directory\n\n- name: Fail if directory doesn't exist\n  fail:\n    msg: \"The directory of /usr/share/ansible/ must contain OpenShift playbooks and roles which can be installed by rpm or git clone\"\n  when: openshift_directory.stat.exists == False\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "3f7fa783f303a696b9223faec9bd7fd51a6bf5cc", "filename": "roles/dns-records/defaults/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\nuse_bastion: False\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "776d654e5312f23352cde34b59b9b4e31b312599", "filename": "tasks/delete_repo_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: delete_repo\n    args:\n      name: \"{{ item }}\""}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "692e329819de9126b0e08574bf5121a9fdc12b14", "filename": "roles/config-timezone/test/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: node\n  roles:\n  - role: config-timezone\n\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "470ed8e842485fdfe65f413af67ada1648ee3e00", "filename": "archive/roles/cicd/tasks/prerequisites.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n  \n- name: Installing Prerequisite Software\n  yum: state=present name={{item}}\n  with_items:\n  - http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n  - wget\n  - firewalld\n  - unzip\n  - git\n  - vim\n  tags: prerequisites\n  \n- name: Installing JQ\n  yum:\n    name: jq\n    state: present\n  tags: prerequisites\n  \n- name: Disable iptables\n  service: \n    name: iptables\n    enabled: no\n    state: stopped\n  tags: prerequisites\n  \n- name: Enable firewalld\n  service:\n    name: firewalld\n    enabled: yes\n    state: started\n  tags: prerequisites\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "d25761965bf4e0077b067c3690ff96f157f4ab62", "filename": "roles/config-mysql/tasks/install_containerized.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Generate Random DB Username\n  set_fact:\n    mysql_username: \"{{ lookup('password', '/dev/null length=5 chars=ascii_letters') | lower }}\"\n  when: mysql_username is undefined or mysql_username|trim == \"\"\n  run_once: True\n\n- name: Generate Random DB Admin Username\n  set_fact:\n    mysql_root_username: \"root\"\n  when: mysql_root_username is undefined or mysql_root_username|trim == \"\"\n  run_once: True\n\n- name: Generate Random DB Password\n  set_fact:\n    mysql_password: \"{{ lookup('password', '/dev/null length=10 chars=ascii_letters,digits,hexdigits') }}\"\n  when: mysql_password is undefined or mysql_password|trim == \"\"\n  run_once: True\n\n- name: Generate Random Admin DB Password\n  set_fact:\n    mysql_root_password: \"{{ lookup('password', '/dev/null length=10 chars=ascii_letters,digits,hexdigits') }}\"\n  when: mysql_root_password is undefined or mysql_root_password|trim == \"\"\n  run_once: True\n\n- name: Configure Storage Directory\n  file:\n    state: directory\n    owner: root\n    group: root\n    mode: g+rw\n    path: \"{{ mysql_storage_dir }}\"\n\n- name: Configure systemd environment files\n  template:\n    src: \"{{ mysql_name }}.j2\"\n    dest: \"{{ systemd_environmentfile_dir}}/{{ mysql_name }}\"\n  notify: \"Restart MySQL Service\"\n\n- name: Configure systemd unit files\n  template:\n    src: \"{{ mysql_service }}.j2\"\n    dest: \"{{ systemd_service_dir}}/{{ mysql_service }}\"\n  notify: \"Restart MySQL Service\"\n\n- name: Include firewall tasks\n  include_tasks: firewall.yml"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "05f92bc69ca97bd43a05775eb23251188309b9d2", "filename": "playbooks/openshift/delete-cluster.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- hosts: seed-hosts\n  roles:\n  - openshift-ansible-contrib/roles/openshift-pv-cleanup\n\n- import_playbook: delete-openstack.yml\n  when:\n  - hosting_infrastructure == 'openstack'\n\n- import_playbook: delete-aws.yml\n  when:\n  - hosting_infrastructure == 'aws'\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b1087f9c404402e1798bbd45752c9811e33356cc", "filename": "playbooks/aws/openshift-cluster/templates/user_data.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#cloud-config\n{% if type in ['node', 'master'] and 'docker' in volume_defs[type] %}\nmounts:\n- [ xvdb ]\n- [ ephemeral0 ]\n{% endif %}\n\nwrite_files:\n{% if type in ['node', 'master'] and 'docker' in volume_defs[type] %}\n- content: |\n    DEVS=/dev/xvdb\n    VG=docker_vg\n  path: /etc/sysconfig/docker-storage-setup\n  owner: root:root\n  permissions: '0644'\n{% endif %}\n{% if deployment_vars[deployment_type].become | bool %}\n- path: /etc/sudoers.d/99-{{ deployment_vars[deployment_type].ssh_user }}-cloud-init-requiretty\n  permissions: 440\n  content: |\n    Defaults:{{ deployment_vars[deployment_type].ssh_user }} !requiretty\n{% endif %}\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "2f04e4e7fe33d2c3cdca8edcf5ffe2186659e602", "filename": "roles/config-pxe/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: prep.yml\n- import_tasks: pxe.yml\n- import_tasks: kickstart.yml\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "c6fd6b3534c1972fcd22851c70b8a65fd85259fe", "filename": "roles/ovirt-engine-remote-dwh/install-postgresql/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n- name: check PostgreSQL service\n  service:\n    name: postgresql\n    state: started\n  register: postgresql_status\n  ignore_errors: True\n\n- name: install postgresql\n  yum: \n    name: postgresql-server\n    state: installed\n    update_cache: yes\n  when: postgresql_status|failed\n\n- name: Check if the db is initialized \n  stat:\n    path: /var/lib/pgsql/data\n  register: db_initialised\n\n- name: Initialize the postgresql db\n  command: su -l postgres -c \"/usr/bin/initdb --locale=en_US.UTF8 --auth='ident' --pgdata=/var/lib/pgsql/data/\"\n  when: postgresql_status|failed\n\n- name: Start postgresql.service\n  systemd:\n    state: started\n    name: postgresql\n  when: postgresql_status|failed\n\n- name: creating directory for sql scripts in /tmp/ansible-sql\n  file:\n    path: /tmp/ansible-sql\n    state: directory\n\n- name: copy SQL scripts\n  template:\n    src: \"{{ item }}.j2\"\n    dest: \"/tmp/ansible-sql/{{ item }}\"\n    mode: 0644\n    owner: postgres\n    group: postgres\n  with_items:\n    - \"ovirt-engine-dwh-db-user-create.sql\"\n    - \"ovirt-engine-dwh-db-create.sql\"\n\n- name: create engine DWH DB and user\n  become_user: postgres\n  become: yes\n  command: psql -p {{ ovirt_engine_dwh_db_port }} -a -f /tmp/ansible-sql/'{{ item }}'\n  with_items:\n    - \"ovirt-engine-dwh-db-user-create.sql\"\n    - \"ovirt-engine-dwh-db-create.sql\"      \n\n- name: Adding engine and dwhservice vm IP's in the dwhdb conf to be acessed remotely\n  lineinfile:\n    dest: /var/lib/pgsql/data/pg_hba.conf\n    line: \"{{ item }}\"\n  with_items:\n    - 'host    {{ ovirt_engine_dwh_db_name }}    {{ ovirt_engine_dwh_db_name }}    {{ engine_vm_network_cidr | mandatory }}     md5 # engine'\n    - 'host    {{ ovirt_engine_dwh_db_name }}    {{ ovirt_engine_dwh_db_name }}    {{ dwhservice_vm_network_cidr | mandatory }}     md5 # dwhservice'\n\n- name: Edit the config file /var/lib/pgsql/data/postgresql.conf\n  command: \"{{ item }}\"\n  with_items:\n    - sed -i -- 's/max_connections = 100/max_connections = 150/g' /var/lib/pgsql/data/postgresql.conf\n    - sed -i \"60ilisten_addresses = '*'\" /var/lib/pgsql/data/postgresql.conf\n\n- name: Enable firewalld and open up port 5432\n  command: \"{{ item }}\"\n  with_items:\n    - systemctl start firewalld\n    - firewall-cmd --zone=public --add-port=5432/tcp --permanent\n    - firewall-cmd --reload\n  \n- name: Restart postgresql for the loading the newer configs\n  systemd:\n    state: restarted\n    name: postgresql\n    \n- name: check PostgreSQL service\n  service:\n    name: postgresql\n    state: started\n    enabled: yes\n\n- name: clean tmp files\n  file:\n    path: '/tmp/ansible-sql'\n    state: 'absent'\n\n- name: Enable postgresql.service to start at boot time\n  systemd:\n    enabled: yes\n    name: postgresql\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "ff7be669dd3f79441a587d8f75668cf4e192a8ad", "filename": "roles/ovirt-engine-config/meta/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\ngalaxy_info:\n  author: \"Lukas Bednar\"\n  description: \"oVirt engine config tool\"\n  company: \"Red Hat\"\n  license: \"GPLv3\"\n  min_ansible_version: 1.9\n  platforms:\n    - name: EL\n      versions:\n        - all\n  galaxy_tags:\n    - installer\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "cd31e304b6cd3b76dc151ffe0068d8d81951c3f5", "filename": "ops/playbooks/README.md", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "# Overview of the playbooks\n\n## Create virtual machines\n\nThe playbook [playbooks/create\\_vms.yml][create_vms] will create all the necessary Virtual Machines for the environment from the VM Template defined in the vm_template variable.\n\n## Configure network settings\nThe playbook [config\\_networking.yml][config_networking] will configure the network settings in all the Virtual Machines. \n\n## Distribute public keys\nThe playbook [distribute\\_keys.yml][distribute_keys] distributes public keys between all nodes, to allow each node to password-less login to every other node. As this is not essential and can be regarded as a security risk (a worker node probably should not be able to log in to a UCP node, for instance), this playbook is commented out in site.yml by default and must be explicitly uncommented to enable this functionality.\n\n## Register the VMs with Red Hat\nThe playbook [config\\_subscription.yml][config_subscription] registers and subscribes all virtual machines to the Red Hat Customer Portal. This is only needed if you pull packages from Red Hat.\n\n## Install HAProxy\nThe playbook [install\\_haproxy.yml][install_haproxy] installs and configures the HAProxy package in the load balancer nodes. HAProxy is the chosen tool to implement load balancing between UCP nodes, DTR nodes and worker nodes.\n\n## Install NTP\nThe playbook [install\\_ntp.yml][install_ntp] installs and configures the NTP package in all Virtual Machines in order to have a synchronized clock across the environment. It will use the server or servers specified in the ntp_servers variable in the group variables file.\n\n## Install Docker Enterprise Edition\nThe playbook [install\\_docker.yml][install_docker] installs Docker along with all its dependencies.\n\n\n## Install rsyslog\nThe playbook [install_rsyslog.yml][install_rsyslog] installs and configures rsyslog in the logger node and in all Docker nodes. The logger node will be configured to receive all syslogs on port 514 and the Docker nodes will be configured to send all logs (including container logs) to the logger node.\n\n\n## Configure Docker LVs\nThe playbook [config_docker_lvs.yml][config_docker_lvs] performs a set of operations on the Docker nodes in order to create a partition on the second disk and carry out the LVM configuration, required for a sound Docker installation.\n\n\n## Docker post-install configuration\nThe playbook [docker_post_config.yml][docker_post_config] performs a variety of tasks to complete the installation of the Docker environment.\n\n\n\n## Install NFS server \nThe playbook [install_nfs_server.yml][install_nfs_server] installs and configures an NFS server on the NFS node.\n\n\n\n## Install NFS clients\nThe playbook [install_nfs_clients.yml][install_nfs_clients] installs the required packages on the DTR nodes to be able to mount an NFS share.\n\n\n## Install and configure Docker UCP nodes\nThe playbook [install_ucp_nodes.yml][install_ucp_nodes] installs and configures the Docker UCP nodes defined in the inventory.\n\n\n\n## Install and configure DTR nodes\nThe playbook [install_dtr_nodes.yml][install_dtr_nodes] installs and configures the Docker DTR nodes defined in the inventory. Note that serialization is set to 1 in this playbook as two concurrent installations of DTR may in some cases be assigned the same replica ID.\n\n## Install worker nodes\nThe playbook [install_worker_nodes.yml][install_worker_nodes] installs and configures the Docker Worker nodes defined in the inventory.\n\n\n## Configuring monitoring\nThe playbook [config_monitoring.yml][config_monitoring] configures a monitoring system for the Docker environment by making use of Grafana, Prometheus, cAdvisor and node-exporter Docker containers.\n\n**Note:** If you have your own monitoring solution, you can comment out the corresponding line in the main playbook ```site.yml```\n```\n#- include: playbooks/config_monitoring.yml\n```\n\n\n\n\n\n[create_vms]: </playbooks/create_vms.yml>\n[config_networking]: </playbooks/config_networking.yml>\n[distribute_keys]: </playbooks/distribute_keys.yml>\n[config_subscription]: </playbooks/config_subscription.yml>\n[install_haproxy]: </playbooks/install_haproxy.yml>\n[install_ntp]: </playbooks/install_ntp.yml>\n[install_docker]: </playbooks/install_docker.yml>\n[install_rsyslog]: </playbooks/install_rsyslog.yml>\n[config_docker_lvs]: </playbooks/config_docker_lvs.yml>\n[docker_post_config]: </playbooks/docker_post_config.yml>\n[install_nfs_server]: </playbooks/install_nfs_server.yml>\n[install_nfs_clients]: </playbooks/install_nfs_clients.yml>\n[install_ucp_nodes]: </playbooks/install_ucp_nodes.yml>\n[install_dtr_nodes]: </playbooks/install_dtr_nodes.yml>\n[install_worker_nodes]: </playbooks/install_worker_nodes.yml>\n[config_monitoring]: </playbooks/config_monitoring.yml>\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "d91c095c895ea3df1754538011443d844046aa59", "filename": "roles/openshift-management/tasks/prune-projects.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: Setup variables used in this task\n  set_fact:\n    projects_to_process: []\n\n- name: Set username fact (if this is run by an Ansible Tower job)\n  set_fact:\n    run_as: \"--as='{{ tower_user_name }}'\"\n  when: \n  - tower_user_name is defined\n  - tower_user_name|trim != ''\n\n- name: Get list of projects\n  shell: oc {{ run_as | default('') }} get project --no-headers | awk '{print $1}'\n  environment:\n    KUBECONFIG: \"{{ kubeconfig }}\"\n  register: projects \n\n- name: Remove excluded projects from the list to process\n  set_fact:\n    projects_to_process: \"{{ projects_to_process }} + [ '{{ item }}' ]\"\n  with_items: \"{{ projects.stdout_lines }}\"\n  when:\n  - item not in openshift_prune_projects_system_excludes\n  - item not in openshift_prune_projects_user_excludes\n\n- name: Get OpenShift project details (timestamps)\n  shell: oc {{ run_as | default('') }} get project {{ item }} -o json\n  environment:\n    KUBECONFIG: \"{{ kubeconfig }}\"\n  register: project_details\n  with_items: \"{{ projects_to_process }}\"\n\n- name: Convert timestamps (already in UTC) to seconds\n  shell: date -d '{{ (item.stdout | from_json).metadata.creationTimestamp }}' +%s\n  register: project_timestamp_seconds\n  with_items: \"{{ project_details.results }}\"\n\n- name: Get timestamp for X hrs ago (default 24 hrs) in seconds (UTC) \n  shell: date -u -d '{{ openshift_prune_projects_keep_younger | default(24) }} hour ago' +%s\n  register: cutoff_timestamp_seconds\n\n- name: Prune projects\n  shell: oc {{ run_as | default('') }} delete project {{ item.item.item }}\n  environment:\n    KUBECONFIG: \"{{ kubeconfig }}\"\n  when:\n  - \"{{ item.stdout }} < {{ cutoff_timestamp_seconds.stdout }}\"\n  with_items: \"{{ project_timestamp_seconds.results }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "07cf39f2ba8cce457c5c9295d396213f0d7650ba", "filename": "playbooks/container-registry/README.md", "repository": "redhat-cop/infra-ansible", "decoded_content": "# Quay Enterprise\n\nThis playbook provides support for deploying [Quay Enterprise](https://coreos.com/quay-enterprise/) and its required dependencies.\n\n## Components\n\nThe following sets of resources are provisioned by this playbook:\n\n* [Redis](https://redis.io/)\n* Database Persistence ([PostgreSQL](https://www.postgresql.org/) or [MySQL](https://www.mysql.com/))\n* Load Balancer ([HAProxy](http://www.haproxy.org/))\n* [Quay Enterprise](https://coreos.com/quay-enterprise/)\n* [Clair](https://coreos.com/clair)\n\n## Inventory Options\n\nThe deployment of Quay Enterprise is driven by the inventory found in the [quay-enterprise](../../inventory/quay-enterprise) folder.\n\nFive host groups are available and configured in the [hosts](../../inventory/quay-enterprise/hosts) file:\n\n```\n[quay_enterprise]\n\n[db]\n\n[redis]\n\n[clair]\n\n[lb]\n```\n\nThe following is a list of the most commonly utilized inventory variables used to drive the execution (not comprehensive):\n\n| variable | info |\n|---|---|\n|quay_registry_auth|Base64 encoded value to access a secured registry for Quay|\n|database_type|Database type (`postgresql` or `mysql`)|\n|docker_install|Boolean whether to install and configure docker|\n|quay_hostname|Hostname to configure the optionally generated SSL certificate|\n|quay_superuser_username|Quay superuser username|\n|quay_superuser_password|Quay superuser password|\n|quay_superuser_email|Quay superuser email|\n\nAdditional variables can be utilized by inspecting the variables provided by each role.\n\n## Playbook Execution\n\nExecute the following command to provision the Quay ecosystem:\n\n```\n$ ansible-playbook -i ../../inventory/quay-enterprise  quay-enterprise.yml\n```\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "1d33313c0b01a0d7b9b69416c8b92c9035bee468", "filename": "roles/manage-aws-infra/tasks/create-vpc.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "# Create VPC if requested\n---\n- name: \"Create VPC if required\"\n  ec2_vpc_net:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    name: \"{{ env_id }}-vpc\"\n    cidr_block: \"{{ vpc_cidr_block | default('172.31.0.0/16') }}\"\n    region: \"{{ aws_region }}\"\n    state: \"present\"\n    tags:\n      env_id: \"{{ env_id }}\"\n  register: new_vpc\n\n- name: \"Store away the new VPC id for use later\"\n  set_fact:\n    aws_vpc_id: \"{{ new_vpc.vpc.id }}\"\n\n- name: \"Create an Internet Gateway for the new VPC\"\n  ec2_vpc_igw:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    vpc_id: \"{{ aws_vpc_id }}\"\n    region: \"{{ aws_region }}\"\n    state: present\n    tags:\n      env_id: \"{{ env_id }}\"\n  register: new_gw\n\n- name: \"Store away the new Internet Gateway id for use later\"\n  set_fact:\n    aws_vpc_ig: \"{{ new_gw.gateway_id }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "06f7c613c443c1fbd4e8d6483787108a78ac1bc3", "filename": "roles/config-docker/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: restart docker\n  service:\n    name: docker\n    state: restarted\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "638e98f35a5304e0b5c49a51b5d1257ca35990e3", "filename": "tasks/create_repo_nuget_hosted_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_nuget_hosted\n    args: \"{{ _nexus_repos_nuget_defaults|combine(item) }}\""}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "90febf02a6931d3a86a0c549911772cc0ee890c7", "filename": "roles/ansible/tower/manage-inventories/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- block: # when ansible_tower.inventories is defined\n\n  - name: \"Set default values\"\n    set_fact:\n      processed_inventories: []\n      existing_organizations_output: []\n\n  # Utilize the `rest_get` library routine to ensure REST pagination is handled\n  - name: \"Get the existing organizations\"\n    rest_get:\n      host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n      rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      rest_password: \"{{ ansible_tower.admin_password }}\"\n      api_uri: \"/api/v2/organizations/\"\n    register: existing_organizations_output\n\n  - name: \"Process the inventory entries\"\n    include_tasks: process-inventory.yml\n    with_items:\n    - \"{{ ansible_tower.inventories }}\"\n    loop_control:\n      loop_var: inventory\n\n  - name: \"Elminate the inventories that should not be present\"\n    uri:\n      url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/inventories/{{ item.id }}/\"\n      user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      password: \"{{ ansible_tower.admin_password }}\"\n      force_basic_auth: yes\n      method: DELETE\n      validate_certs: no\n      status_code: 200,202,204\n    with_items:\n    - \"{{ existing_inventories_output.rest_output | get_remaining_items(processed_inventories, 'name', 'name')}}\"\n\n  when:\n  - ansible_tower.inventories is defined\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "ca4fca3c349519467aed490387a8687cb232da62", "filename": "roles/cfme-ocp-provider/defaults/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\ncfme_host:\ncfme_username:\ncfme_password:\nocp_master_host: kubernetes.default.svc.cluster.local\nocp_master_port: 443\nocp_token:\nhawkular_token:\nhawkular_host: hawkular-metrics.openshift-infra.svc.cluster.local\nhawkular_port: 443\nocp_container_provier_name: OCP\ndefault_token_sa_namespace: management-infra\ndefault_token_sa_name: management-admin"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "a378e76dcf275c20b994dc528053028b398f8410", "filename": "roles/manage-confluence-space/tasks/copy_confluence_content.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Get Content from Source\n  uri:\n    url: '{{ confluence.source.url }}/wiki/rest/api/content/{{ confluence_space_content.id }}?expand=body.storage,history,space,container.history,container.version,version,ancestors'\n    method: GET\n    user: '{{ confluence.source.username }}'\n    password: '{{ confluence.source.password }}'\n    force_basic_auth: yes\n    status_code: 200\n    return_content: yes\n  register: content_json\n\n- name: Map ancestor ids if any\n  set_fact: \n    content_ancestors: \"{{ content_json.json.ancestors | map(attribute='id') | list | map('extract', id_mapping) | list }}\"\n\n- name: Pick only the last ancestor\n  set_fact:\n    content_ancestors: \"{{ [-1] | map('extract', content_ancestors) | list }}\"\n  when: content_ancestors|length > 0\n\n- name: Get the current page version\n  uri:\n    url: '{{ confluence.destination.url }}/wiki/rest/api/space/{{ confluence.destination.space_key }}/content?expand=homepage,version'\n    method: GET\n    user: '{{ confluence.destination.username }}'\n    password: '{{ confluence.destination.password }}'\n    force_basic_auth: yes\n    status_code: 200\n    return_content: yes\n  register: homepage_content\n  no_log: false\n\n- name: set homepage at destination\n  block:\n    - set_fact:\n        homepage:\n          body:\n            storage:\n              value: \"{{ content_json.json.body.storage.value }}\"\n              representation: \"storage\"\n          title: \"{{ confluence.destination.space_name }}\"\n          type: \"{{ content_json.json.type }}\"\n          space:\n            key: \"{{ confluence.destination.space_key }}\"\n          version:\n            number: \"{{ homepage_content.json['page']['results'][0]['version']['number'] + 1 }}\"\n    - uri:\n        url: '{{ confluence.destination.url }}/wiki/rest/api/content/{{ destination_homepage_id }}'\n        method: PUT\n        user: '{{ confluence.destination.username }}'\n        password: '{{ confluence.destination.password }}'\n        force_basic_auth: yes\n        status_code: 200\n        body_format: json\n        body: \"{{  homepage | to_json }}\"\n        return_content: yes\n      register: homepage_content_json\n\n    - set_fact:\n       id_mapping: \"{{ id_mapping|combine({ content_json.json.id : { 'id' : homepage_content_json.json.id }}) }}\"\n  when: content_json.json.id == source_homepage_id\n\n- name: set payload for create contect\n  set_fact:\n    payload:\n      id: \"{{ content_json.json.id }}\"\n      title: \"{{ content_json.json.title }}\"\n      type: \"{{ content_json.json.type }}\"\n      space:\n        key: \"{{ confluence_space_key }}\"\n      body:\n        storage:\n          value: \"{{ content_json.json.body.storage.value }}\"\n          representation: \"storage\"\n\n- name: if page is child add ancestors\n  set_fact:\n    payload: \"{{ payload | combine( {'ancestors': content_ancestors} )}}\"\n  when: content_json.json.ancestors|length > 1\n\n- name: Create Content at Destination Site\n  uri:\n    url: '{{ confluence.destination.url }}/wiki/rest/api/content'\n    method: POST\n    user: '{{ confluence.destination.username }}'\n    password: '{{ confluence.destination.password }}'\n    force_basic_auth: yes\n    status_code: 200\n    body_format: json\n    body: \"{{  payload |  to_json }}\"\n    return_content: yes\n  register: new_content_json\n\n- name: Append New id to old id mapping\n  set_fact:\n    id_mapping: \"{{ id_mapping|combine({ content_json.json.id : { 'id' : new_content_json.json.id }}) }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a7e97f542ba5fb877267225d00bcab9798aa0174", "filename": "roles/osp/admin-user/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Create the User Accounts\"\n  os_user:\n    cloud: \"{{ item.cloud | default(osp_default_cloud) | default(omit) }}\"\n    domain: \"{{ item.domain }}\"\n    password: \"{{ item.password }}\"\n    name: \"{{ item.name }}\"\n  with_items:\n  - \"{{ osp_users | default([]) }}\"\n  \n- name: \"Grant access to accounts\"\n  include_tasks: \"roles.yml\"\n  with_items:\n  - \"{{ osp_users | default([]) }}\"\n  loop_control:\n    loop_var: role\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "288ca2e6d5e5e91b05c6642fc580d3933b8a09e7", "filename": "tasks/checks.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- include_tasks: checks/distribution-checks.yml\n  when:\n    _docker_os_dist_check | bool\n\n- include_tasks: checks/compatibility-checks.yml\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "0873c8c6b87152c21f4d57b6c024b193252a1555", "filename": "roles/haproxy/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for haproxy\nhaproxy_image: asteris/haproxy-consul\nhaproxy_image_tag: latest\n\n# Set the domain that haproxy uses to match URLs to internal apps.\n# For example, if all your apps will be\n#    app1.example.com, app2.example.com, etc.  set this to 'example.com'\nhaproxy_domain: example.com\n\nconsul_template_dir: /mnt/consul-template.d\nconsul_template_loglevel: debug\nconsul_backend: consul.service.consul:8500\nconsul_template_version: 0.8.0\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "78e514d018fac7cc08276636ebc38ab564ea5153", "filename": "roles/cloud-vultr/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- block:\n  - name: Include prompts\n    import_tasks: prompts.yml\n\n  - name: Upload the SSH key\n    vr_ssh_key:\n      name: \"{{ SSH_keys.comment }}\"\n      ssh_key: \"{{ lookup('file', '{{ SSH_keys.public }}') }}\"\n    register: ssh_key\n\n  - name: Creating a server\n    vr_server:\n      name: \"{{ algo_server_name }}\"\n      hostname: \"{{ algo_server_name }}\"\n      os: \"{{ cloud_providers.vultr.os }}\"\n      plan: \"{{ cloud_providers.vultr.size }}\"\n      region: \"{{ algo_vultr_region }}\"\n      state: started\n      tag: Environment:Algo\n      ssh_key: \"{{ ssh_key.vultr_ssh_key.name }}\"\n      ipv6_enabled: true\n      auto_backup_enabled: false\n      notify_activate: false\n    register: vultr_server\n\n  - set_fact:\n      cloud_instance_ip: \"{{ vultr_server.vultr_server.v4_main_ip }}\"\n      ansible_ssh_user: root\n\n  environment:\n    VULTR_API_CONFIG: \"{{ algo_vultr_config }}\"\n  rescue:\n    - debug: var=fail_hint\n      tags: always\n    - fail:\n      tags: always\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b83188980e56a26ff82dd7465b97106e58b4e721", "filename": "reference-architecture/gcp/ansible/playbooks/roles/dns-records/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: get master ssl lb ip\n  command: gcloud --project {{ gcloud_project }} compute addresses describe {{ prefix }}-master-https-lb-ip --global --format='value(address)'\n  register: master_ssl_lb_ip\n  changed_when: false\n\n- name: get master network lb ip\n  command: gcloud --project {{ gcloud_project }} compute addresses describe {{ prefix }}-master-network-lb-ip --region {{ gcloud_region }} --format='value(address)'\n  register: master_network_lb_ip\n  changed_when: false\n\n- name: get router network lb ip\n  command: gcloud --project {{ gcloud_project }} compute addresses describe {{ prefix }}-router-network-lb-ip --region {{ gcloud_region }} --format='value(address)'\n  register: router_network_lb_ip\n  changed_when: false\n\n- name: set ns records\n  gcdns_record:\n    record: '{{ item.record }}'\n    zone: '{{ public_hosted_zone }}'\n    type: '{{ item.type }}'\n    record_data: '{{ item.record_data }}'\n    ttl: 3600\n    overwrite: true\n    service_account_email: '{{ service_account_id }}'\n    credentials_file: '{{ credentials_file }}'\n    project_id: '{{ gcloud_project }}'\n    state: present\n  with_items:\n  - record: '{{ openshift_master_cluster_public_hostname }}'\n    record_data: '{{ master_ssl_lb_ip.stdout }}'\n    type: A\n  - record: '{{ openshift_master_cluster_hostname }}'\n    record_data: '{{ master_network_lb_ip.stdout }}'\n    type: A\n  - record: '{{ wildcard_zone }}'\n    record_data: '{{ router_network_lb_ip.stdout }}'\n    type: A\n  - record: '*.{{ wildcard_zone }}'\n    record_data: '{{ wildcard_zone }}.'\n    type: CNAME\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4221a456423eafeade8450a9bff17c522c43d81f", "filename": "playbooks/osp/README.md", "repository": "redhat-cop/infra-ansible", "decoded_content": "# OpenStack Platform (OSP) related playbooks\n\nThese playbooks are mean to assist with creating OpenStack Platform objects in an automated way. Please checkout the `osp/admin-*` roles for management of individual features of the platform, while playbooks with various combinations of these roles are managed in this director.\n\n\n## Prerequisites\n\n1. A valid OpenStack RC file, with the proper access rights to an OpenStack tenant, needs to be sourced before using this role.\n1. The `openstack` python shade packages to allow for interactions with the platform.\n\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "46f4538a7a6707fcf94ad3d470f8fd196136d5a8", "filename": "roles/cdi/tasks/provision.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n# CDI Deployment\n- name: Determine Environment\n  shell: \"oc version> /dev/null 2>&1; if [ \\\"$?\\\" -eq 127 ]; then echo 'kubectl'; else echo 'oc'; fi\"\n  register: cli\n\n- name: Check if namespace {{ cdi_namespace }} exists\n  shell: kubectl get ns | grep -w {{ cdi_namespace }} | awk '{ print $1 }'\n  register: ns\n\n- name: Create {{ cdi_namespace }} namespace using kubectl\n  shell: kubectl create namespace {{ cdi_namespace }}\n  when: ns.stdout != cdi_namespace\n        and cli.stdout == \"kubectl\"\n\n- name: Create {{ cdi_namespace }} namespace using oc\n  shell: oc new-project {{ cdi_namespace }}\n  when: ns.stdout != cdi_namespace\n        and cli.stdout == \"oc\"\n\n- name: Create RBAC for CDI\n  shell: kubectl create clusterrolebinding c-{{ cdi_namespace }}-default --clusterrole=cluster-admin  --serviceaccount={{ cdi_namespace }}:default\n\n- name: Render {{ cdi_namespace }} ResourceQuota deployment yaml\n  template:\n    src:  cdi-resourcequota.yml\n    dest: /tmp/cdi-provision-resourcequota.yml\n\n- name: Create {{ cdi_namespace }} ResourceQuota\n  command: \"kubectl apply -f /tmp/cdi-provision-resourcequota.yml -n {{ cdi_namespace }}\"\n\n- name: Render CDI deployment yaml\n  template:\n    src:  cdi-controller-deployment.yml\n    dest: /tmp/cdi-provision.yml\n\n- name: Create CDI deployment\n  command: \"kubectl apply -f /tmp/cdi-provision.yml -n {{ cdi_namespace }}\"\n\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "ece601e232065573055026b661ca1fa973589887", "filename": "playbooks/roles/sensor-common/files/RPM-GPG-KEY-RockNSM-2", "repository": "rocknsm/rock", "decoded_content": "-----BEGIN PGP PUBLIC KEY BLOCK-----\n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\n=vLlV\n-----END PGP PUBLIC KEY BLOCK-----\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "67e4417576c3c4b6cf93a8bb956525d5c78f1c5e", "filename": "roles/rhsm/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# Use a 'block' to ensure \"become: True\" for all tasks since\n# all of these tasks require elevated privileges\n- block:\n\n  # Need to use the 'command' module for this task since the \"redhat_subscription\" module\n  # won't do a full \"clean\" when using the \"state: absent\" option\n  # - note the 'warn: False' set because of this situation\n  - name: 'Unregister the system if already registered - if this is a force re-registration'\n    command: \"{{ item }}\"\n    args:\n      warn: False\n    with_items:\n    - 'subscription-manager clean'\n    - 'subscription-manager remove --all'\n    - 'yum remove -y \"katello-ca-consumer-*\"'\n    when:\n    - (rhsm_force_register|default('no'))|lower == 'yes'\n\n  # Need to use the 'command' module for this task since the \"yum\" module\n  # won't honor an \"upgrade\" of the RPM in case where the source server changed. \n  # - note the 'warn: False' set because of this situation\n  - name: \"Install Satellite certificate (if applicable)\"\n    command: \"rpm -Uh --force http://{{ rhsm_server_hostname }}/pub/katello-ca-consumer-latest.noarch.rpm\"\n    args:\n      warn: False\n    when:\n    - rhsm_server_hostname is defined\n    - rhsm_server_hostname|trim != ''\n\n  - name: 'Register system using Red Hat Subscription Manager'\n    redhat_subscription:\n      state: present\n      username: \"{{ rhsm_username | default(omit) }}\"\n      password: \"{{ rhsm_password | default(omit) }}\"\n      pool_ids: \"{{ rhsm_pool_ids | default(omit) }}\"\n      pool: \"{{ rhsm_pool | default(omit) }}\"\n      autosubscribe: \"{{ ((rhsm_pool is defined or rhsm_pool_ids is defined or rhsm_activationkey is defined) | ternary(omit, true)) }}\"\n      server_hostname: \"{{ rhsm_server_hostname | default(omit) }}\"\n      activationkey: \"{{ rhsm_activationkey | default(omit) }}\"\n      org_id: \"{{ rhsm_org_id | default(omit) }}\"\n      force_register: \"{{ rhsm_force_register | default(omit) }}\"\n\n  - name: \"Obtain currently enabled repos\"\n    shell: 'subscription-manager repos --list-enabled | sed -ne \"s/^Repo ID:[^a-zA-Z0-9]*\\(.*\\)/\\1/p\"'\n    register: enabled_repos\n\n  # Build the list of repos to disable/enable before calling 'subscription-manager' as it's a very \n  # expensive command to run and hence better to call just once (especially with a long list of repos)\n  - name: \"Build command line for repos to disable\"\n    set_fact:\n      repos_params: \"{{ repos_params|default('') }} --disable={{ item }}\"\n    with_items: \n    - \"{{ enabled_repos.stdout_lines | difference(rhsm_repos) }}\"\n\n  - name: \"Build command line for repos to enable\"\n    set_fact:\n      repos_params: \"{{ repos_params|default('') }} --enable={{ item }}\"\n    with_items:\n    - \"{{ rhsm_repos | difference(enabled_repos.stdout_lines) }}\"\n\n  - name: \"Run 'subscription-manager to disable/enable repos\"\n    command: \"subscription-manager repos {{ repos_params }}\"\n    when:\n    - repos_params is defined \n    - repos_params|trim != ''\n\n  # End of outer block for \"become: True\"\n  become: True\n\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "6e32b56c520785a37bc7d05bd1eb2f0f936d8e77", "filename": "roles/openshift-login/tasks/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n# This role requires the following facts to be set before execution:\n# - openshift_login_url\n# - openshift_user/openshift_password OR openshift_token\n# - oc_login (optional - only set to False if a valid session / token already exists)\n\n- name: \"Set Default Login Facts\"\n  set_fact:\n    auth_token: False\n    auth_user_pass: False\n\n- name: \"Fail for Missing OpenShift Hosting Env\"\n  fail: msg=\"This role requires openshift_login_url to be set and non empty\"\n  when: \n  - openshift_login_url|trim == \"\"\n\n- name: \"Set variable openshift_user when not set\"\n  set_fact:\n    openshift_user: ''\n  when: openshift_user is not defined\n\n- name: \"Set variable openshift_password when not set\"\n  set_fact:\n    openshift_password: ''\n  when: openshift_password is not defined\n\n- name: \"Set variable openshift_token when not set\"\n  set_fact:\n    openshift_token: ''\n  when: openshift_token is not defined\n\n- name: \"Default oc_login to True when not set\"\n  set_fact:\n    oc_login: True\n  when: oc_login is not defined\n\n- name: \"Check whether we will authenticate with username & password\"\n  set_fact:\n    auth_user_pass: True\n  when:\n  - openshift_user|trim != ''\n  - openshift_password|trim != ''\n  - oc_login|bool == True\n\n- name: \"Check whether we will authenticate with a token\"\n  set_fact:\n    auth_token: True\n  when:\n  - openshift_token|trim != ''\n  - oc_login|bool == True\n\n- name: \"Fail for Missing Authentication method\"\n  fail: msg=\"This role requires either openshift_token, or openshift_user and openshift_password to be set\"\n  when:\n  - auth_token|bool == False\n  - auth_user_pass|bool == False\n  - oc_login|bool == True\n\n- name: \"Log in to OpenShift Client (username/password)\"\n  command: >\n    {{ openshift.common.client_binary }} login {{ openshift_login_url }}\n    --insecure-skip-tls-verify=true --username={{ openshift_user }} --password={{ openshift_password }}\n  changed_when: False\n  when:\n  - auth_user_pass|bool == True\n  - auth_token|bool == False\n  - oc_login|bool == True\n\n- name: \"Log in to OpenShift Client (token)\"\n  command: >\n    {{ openshift.common.client_binary }} login {{ openshift_login_url }}\n    --insecure-skip-tls-verify=true --token={{ openshift_token }}\n  changed_when: False\n  when:\n  - auth_user_pass|bool == False\n  - auth_token|bool == True\n  - oc_login|bool == True\n\n- name: \"Add token to all openshift client comands when token is set\"\n  set_fact:\n    openshift:\n      common:\n        client_binary: \"{{ openshift.common.client_binary }} --token={{ openshift_token }}\"\n        admin_binary: \"{{ openshift.common.admin_binary }} --token={{ openshift_token }}\"\n  when: auth_token|bool == True\n\n- name: \"Gather the openshift token, if not already set\"\n  command: >\n    {{ openshift.common.client_binary }} whoami -t\n  register: token_output\n  when:\n  - openshift_token|trim == ''\n\n- name: \"Set openshift_token fact when not already set\"\n  set_fact:\n    openshift_token: \"{{ token_output.stdout }}\"\n  when: \n  - token_output.stdout is defined\n  - token_output|trim != ''\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "26468133f3d02c6773a07bc3c7cc0331f3532c56", "filename": "playbooks/infra-virt-hosts.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Configure libvirt on the infrastructure hosts'\n  hosts: infra_virt_hosts\n  roles:\n  - role: config-libvirt\n  tags:\n  - configure_infra_hosts_libvirt\n\n- name: 'Configure the software source to ensure it is available for use'\n  hosts: infra_virt_hosts\n  roles:\n  - role: config-software-src\n  tags:\n  - configure_software_src\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "c0bc12f557e6cd0353011398ad38d638452e0a7c", "filename": "playbooks/openstack/openshift-cluster/launch.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Launch instance(s)\n  hosts: localhost\n  become: no\n  connection: local\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  tasks:\n  # TODO: Write an Ansible module for dealing with HEAT stacks\n  #       Dealing with the outputs is currently terrible\n\n  - name: Check OpenStack stack\n    command: 'heat stack-show openshift-ansible-{{ cluster_id }}-stack'\n    register: stack_show_result\n    changed_when: false\n    failed_when: stack_show_result.rc != 0 and 'Stack not found' not in stack_show_result.stderr\n\n  - set_fact:\n      heat_stack_action: 'stack-create'\n    when: stack_show_result.rc == 1\n  - set_fact:\n      heat_stack_action: 'stack-update'\n    when: stack_show_result.rc == 0\n\n  - name: Create or Update OpenStack Stack\n    command: 'heat {{ heat_stack_action }} -f {{ openstack_infra_heat_stack }}\n             --timeout {{ openstack_heat_timeout }}\n             -P cluster_env={{ cluster_env }}\n             -P cluster_id={{ cluster_id }}\n             -P subnet_24_prefix={{ openstack_subnet_24_prefix }}\n             -P dns_nameservers={{ openstack_network_dns | join(\",\") }}\n             -P external_net={{ openstack_network_external_net }}\n             -P ssh_public_key=\"{{ openstack_ssh_public_key }}\"\n             -P ssh_incoming={{ openstack_ssh_access_from }}\n             -P node_port_incoming={{ openstack_node_port_access_from }}\n             -P num_etcd={{ num_etcd }}\n             -P num_masters={{ num_masters }}\n             -P num_nodes={{ num_nodes }}\n             -P num_infra={{ num_infra }}\n             -P etcd_image={{ deployment_vars[deployment_type].image }}\n             -P master_image={{ deployment_vars[deployment_type].image }}\n             -P node_image={{ deployment_vars[deployment_type].image }}\n             -P infra_image={{ deployment_vars[deployment_type].image }}\n             -P etcd_flavor={{ openstack_flavor[\"etcd\"] }}\n             -P master_flavor={{ openstack_flavor[\"master\"] }}\n             -P node_flavor={{ openstack_flavor[\"node\"] }}\n             -P infra_flavor={{ openstack_flavor[\"infra\"] }}\n             openshift-ansible-{{ cluster_id }}-stack'\n    args:\n      chdir: '{{ playbook_dir }}'\n\n  - name: Wait for OpenStack Stack readiness\n    shell: 'heat stack-show openshift-ansible-{{ cluster_id }}-stack | awk ''$2 == \"stack_status\" {print $4}'''\n    register: stack_show_status_result\n    until: stack_show_status_result.stdout not in ['CREATE_IN_PROGRESS', 'UPDATE_IN_PROGRESS']\n    retries: 30\n    delay: 5\n\n  - name: Display the stack resources\n    command: 'heat resource-list openshift-ansible-{{ cluster_id }}-stack'\n    register: stack_resource_list_result\n    when: stack_show_status_result.stdout not in ['CREATE_COMPLETE', 'UPDATE_COMPLETE']\n\n  - name: Display the stack status\n    command: 'heat stack-show openshift-ansible-{{ cluster_id }}-stack'\n    register: stack_show_result\n    when: stack_show_status_result.stdout not in ['CREATE_COMPLETE', 'UPDATE_COMPLETE']\n\n  - name: Delete the stack\n    command: 'heat stack-delete openshift-ansible-{{ cluster_id }}-stack'\n    when: stack_show_status_result.stdout not in ['CREATE_COMPLETE', 'UPDATE_COMPLETE']\n\n  - fail:\n      msg: |\n\n        +--------------------------------------+\n        |   ^                                  |\n        |  /!\\ Failed to create the heat stack |\n        | /___\\                                |\n        +--------------------------------------+\n\n        Here is the list of stack resources and their status:\n        {{ stack_resource_list_result.stdout }}\n\n        Here is the status of the stack:\n        {{ stack_show_result.stdout }}\n\n          ^   Failed to create the heat stack\n         /!\\\n        /___\\ Please check the `stack_status_reason` line in the above array to know why.\n    when: stack_show_status_result.stdout not in ['CREATE_COMPLETE', 'UPDATE_COMPLETE']\n\n  - name: Read OpenStack Stack outputs\n    command: 'heat stack-show openshift-ansible-{{ cluster_id }}-stack'\n    register: stack_show_result\n\n  - set_fact:\n      parsed_outputs: \"{{ stack_show_result | oo_parse_heat_stack_outputs }}\"\n\n  - name: Add new etcd instances groups and variables\n    add_host:\n      hostname: '{{ item[0] }}'\n      ansible_ssh_host: '{{ item[2] }}'\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n      groups: 'meta-environment_{{ cluster_env }}, meta-host-type_etcd, meta-sub-host-type_default, meta-clusterid_{{ cluster_id }}'\n      openshift_node_labels:\n        type: \"etcd\"\n      openstack:\n        public_v4: '{{ item[2] }}'\n        private_v4: '{{ item[1] }}'\n    with_together:\n    - '{{ parsed_outputs.etcd_names }}'\n    - '{{ parsed_outputs.etcd_ips }}'\n    - '{{ parsed_outputs.etcd_floating_ips }}'\n\n  - name: Add new master instances groups and variables\n    add_host:\n      hostname: '{{ item[0] }}'\n      ansible_ssh_host: '{{ item[2] }}'\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n      groups: 'meta-environment_{{ cluster_env }}, meta-host-type_master, meta-sub-host-type_default, meta-clusterid_{{ cluster_id }}'\n      openshift_node_labels:\n        type: \"master\"\n      openstack:\n        public_v4: '{{ item[2] }}'\n        private_v4: '{{ item[1] }}'\n    with_together:\n    - '{{ parsed_outputs.master_names }}'\n    - '{{ parsed_outputs.master_ips }}'\n    - '{{ parsed_outputs.master_floating_ips }}'\n\n  - name: Add new node instances groups and variables\n    add_host:\n      hostname: '{{ item[0] }}'\n      ansible_ssh_host: '{{ item[2] }}'\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n      groups: 'meta-environment_{{ cluster_env }}, meta-host-type_node, meta-sub-host-type_compute, meta-clusterid_{{ cluster_id }}'\n      openshift_node_labels:\n        type: \"compute\"\n      openstack:\n        public_v4: '{{ item[2] }}'\n        private_v4: '{{ item[1] }}'\n    with_together:\n    - '{{ parsed_outputs.node_names }}'\n    - '{{ parsed_outputs.node_ips }}'\n    - '{{ parsed_outputs.node_floating_ips }}'\n\n  - name: Add new infra instances groups and variables\n    add_host:\n      hostname: '{{ item[0] }}'\n      ansible_ssh_host: '{{ item[2] }}'\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n      groups: 'meta-environment_{{ cluster_env }}, meta-host-type_node, meta-sub-host-type_infra, meta-clusterid_{{ cluster_id }}'\n      openshift_node_labels:\n        type: \"infra\"\n      openstack:\n        public_v4: '{{ item[2] }}'\n        private_v4: '{{ item[1] }}'\n    with_together:\n    - '{{ parsed_outputs.infra_names }}'\n    - '{{ parsed_outputs.infra_ips }}'\n    - '{{ parsed_outputs.infra_floating_ips }}'\n\n  - name: Wait for ssh\n    wait_for:\n      host: '{{ item }}'\n      port: 22\n    with_flattened:\n    - '{{ parsed_outputs.master_floating_ips }}'\n    - '{{ parsed_outputs.node_floating_ips }}'\n    - '{{ parsed_outputs.infra_floating_ips }}'\n\n  - name: Wait for user setup\n    command: 'ssh -o StrictHostKeyChecking=no -o PasswordAuthentication=no -o ConnectTimeout=10 -o UserKnownHostsFile=/dev/null {{ deployment_vars[deployment_type].ssh_user }}@{{ item }} echo {{ deployment_vars[deployment_type].ssh_user }} user is setup'\n    register: result\n    until: result.rc == 0\n    retries: 30\n    delay: 1\n    with_flattened:\n    - '{{ parsed_outputs.master_floating_ips }}'\n    - '{{ parsed_outputs.node_floating_ips }}'\n    - '{{ parsed_outputs.infra_floating_ips }}'\n\n- include: update.yml\n\n- include: list.yml\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a00d2a82699da1f10fe1ea2a569937e0666d5db2", "filename": "roles/ansible/tower/manage-inventories/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: tower\n  roles:\n  - role: ansible/tower/manage-inventories\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "eda7d133fcbc791fb9664af112fbd73206f1fed7", "filename": "tasks/setup_user_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: setup_user\n    args: \"{{ item }}\""}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c6dd75ebf1696169bb6f44d81fc218f47167e526", "filename": "playbooks/manage-jira/manage-jira.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- hosts: jira\n  roles:\n    - manage-jira\n"}, {"commit_sha": "45971be8249cc4627ef8ddfacf55a661b7fc13ca", "sha": "3f09e68f137afe4fab5d79ea6166e3c17bce20c9", "filename": "tasks/install-docker.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Set docker-ce package state to latest\n  set_fact:\n    docker_pkg_state: 'latest'\n  when: docker_latest_version|bool\n\n- name: Ensure docker-ce is installed\n  package:\n    name: \"{{ docker_pkg_name }}\"\n    state: \"{{ docker_pkg_state|default('present') }}\"\n  become: true\n  notify: restart docker\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "8a47c4673c67eb244be2e478d1bd72e6ddd0c316", "filename": "roles/ovirt-collect-logs/tasks/hypervisor.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n\n- name: Prepare directory structure\n  file:\n    src: \"{{ item }}\"\n    dest: \"{{ ovirt_collect_logs_tmp_dir }}/{{ item }}\"\n    state: directory\n  with_items:\n    - \"etc\"\n\n- name: Register coredump path\n  shell: \"cat /proc/sys/kernel/core_pattern | xargs dirname | head -n 1\"\n  register: coredumpdir\n\n- name: Link vdsm and libvirt logs\n  file:\n    src: \"{{ item.src }}\"\n    dest: \"{{ ovirt_collect_logs_tmp_dir }}/{{ item.dest }}\"\n    state: link\n  with_items:\n    -\n      src: \"/var/log/vdsm\"\n      dest: \"vdsm-logs\"\n    -\n      src: \"/var/log/libvirt\"\n      dest: \"libvirt-logs\"\n    -\n      src: \"/etc/ovirt-vmconsole\"\n      dest: \"etc/ovirt-vmconsole\"\n    -\n      src: \"/etc/vdsm\"\n      dest: \"etc/vdsm\"\n    -\n      src: \"/etc/libvirt\"\n      dest: \"etc/libvirt\"\n    -\n      src: \"/var/log/ovirt-hosted-engine-setup\"\n      dest: \"hosted-engine-setup-logs\"\n    -\n      src: \"/var/log/ovirt-hosted-engine-ha\"\n      dest: \"hosted-engine-logs\"\n    -\n      src: \"{{ coredumpdir }}\"\n      dest: \"coredumps\"\n  ignore_errors: true\n\n- name: Dump mount\n  shell: \"mount &> {{ ovirt_collect_logs_tmp_dir }}/mount.txt\"\n  ignore_errors: true\n\n- name: Dump multipath\n  shell: \"multipath -ll &> {{ ovirt_collect_logs_tmp_dir }}/multipath.txt\"\n  ignore_errors: true\n\n- name: Dump lvm\n  shell: \"(vgs; pvs; lvs) &> {{ ovirt_collect_logs_tmp_dir }}/lvm.txt\"\n  ignore_errors: true\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "996f0c953a254c85f080d8a516c0ee698627f43f", "filename": "reference-architecture/vmware-ansible/playbooks/cleanup-crs.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  user: root\n  become: false\n  ignore_errors: yes\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - instance-groups\n\n- hosts: crs\n  user: root\n  become: false\n  ignore_errors: yes\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - rhsm-unregister\n\n- hosts: single_master\n  user: root\n  become: false\n  ignore_errors: yes\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - heketi-ocp-clean\n\n- hosts: localhost\n  user: root\n  become: false\n  ignore_errors: yes\n  vars_files:\n    - vars/main.yaml\n  tasks:\n    - name: Delete crs VMs\n      vmware_guest:\n        hostname: \"{{ vcenter_host }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        datacenter: \"{{ vcenter_datacenter }}\"\n        folder: \"/{{ vcenter_folder }}\"\n        name: \"{{ item.value.guestname }}\"\n        state: absent\n        force: true\n      with_dict: \"{{host_inventory}}\"\n      when: \"'crs' in item.value.guestname\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "5b2b60ec1714415ead654ca41588e9395181df1a", "filename": "playbooks/roles/zookeeper/tasks/all.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- import_tasks: install.yml method=install\n- import_tasks: configure.yml method=configure\n...\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "79cbd2976f4ccdbe58989e676fdfbc40d82a3d87", "filename": "roles/schooltool/meta/main.yml", "repository": "iiab/iiab", "decoded_content": "---\ndependencies:\n    - { role: docker }\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "58bd861cdff5294a1251c974f772ed0b7027b359", "filename": "roles/dns-server-detect/defaults/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n\nexternal_nsupdate_keys: {}\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ad8d65d4c9516beeda348bdaa3e85e251f4cb169", "filename": "roles/notifications/send-email/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n# This test covers the full feature set provided by the role\n\n- name: Test email/send role\n  hosts: localhost\n  roles:\n  - notifications/send-email\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "6e95ac7dcc047c0ecfbce844db06c23ad924f5fb", "filename": "roles/manage-aws-infra/tasks/start_stop_instances.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "# Start and Stop AWS OCP Cluster based on given state\n---\n- name: Ensure ec2 instances are in {{ operation }} state\n  ec2:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    instance_tags:\n      env_id: \"{{ env_id }}\"\n    state: \"{{ operation }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6f87a4a655162d34e9a7e089c395ebd15fd18ebb", "filename": "reference-architecture/gcp/ansible/playbooks/roles/deployment-create/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: create deployment {{ deployment_name_with_prefix }} config file\n  template:\n    src: '{{ deployment_config_template }}'\n    dest: '{{ deployment_config }}'\n  register: deployment_config_from_template\n\n- name: check if deployment {{ deployment_name_with_prefix }} exists\n  command: gcloud --project {{ gcloud_project }} deployment-manager deployments describe {{ deployment_name_with_prefix }} --format 'yaml(deployment.operation)'\n  register: deployment_exists\n  changed_when: false\n  ignore_errors: true\n\n- name: delete deployment if it exists in failed state\n  include_role:\n    name: deployment-delete\n  when:\n  - deployment_exists | succeeded\n  - (deployment_exists.stdout | from_yaml).deployment.operation.error is defined\n\n- name: create deployment {{ deployment_name_with_prefix }}\n  command: gcloud --project {{ gcloud_project }} deployment-manager deployments create {{ deployment_name_with_prefix }} --config '{{ deployment_config }}'\n  when: deployment_exists | failed or (deployment_exists.stdout | from_yaml).deployment.operation.error is defined\n\n- name: update deployment {{ deployment_name_with_prefix }}\n  command: gcloud --project {{ gcloud_project }} deployment-manager deployments update {{ deployment_name_with_prefix }} --config '{{ deployment_config }}'\n  when:\n  - deployment_exists | succeeded\n  - deployment_config_from_template | changed\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "2589dd416d683a99794a2c21ba93a2239dacb5ca", "filename": "roles/ovirt-engine-cleanup/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# copy answer file\n- name: copy answerfile\n  template:\n    # TODO generate this varialbe from ovirt_engine_version and ovirt_engine_dwh\n    src: answerfile_{{ ovirt_engine_version|int }}.x.txt.j2\n    dest: /tmp/answerfile.txt\n    mode: 0644\n    owner: root\n    group: root\n\n- name: run engine-cleanup with answerfile\n  shell: 'engine-cleanup --config-append=/tmp/answerfile.txt'\n  tags:\n    - skip_ansible_lint\n\n- name: clean tmp files\n  file:\n    path: '/tmp/answerfile.txt'\n    state: 'absent'\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "6b91d0723f94ac7a8422222227c4d47ff78cfe84", "filename": "playbooks/templates/nginx-rock.conf.j2", "repository": "rocknsm/rock", "decoded_content": "server {\n    listen 443 ssl;\n\n    server_name {{ rock_hostname }};\n    server_name _;\n\n    ssl on;\n    ssl_certificate     {{ http_tls_crt }};\n    ssl_certificate_key {{ http_tls_key }};\n    ssl_protocols       TLSv1 TLSv1.1 TLSv1.2;\n    #ssl_ciphers         HIGH:!aNULL:!MD5;\n    ssl_ciphers 'ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA';\n    ssl_prefer_server_ciphers on;\n    ssl_dhparam\t\t{{http_tls_dhparams}};\n\n    auth_basic \"Restricted Access\";\n    auth_basic_user_file /etc/nginx/htpasswd.users;\n\n    location / {\n        proxy_pass http://localhost:5601;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection 'upgrade';\n        proxy_set_header Host $host;\n        proxy_cache_bypass $http_upgrade;\n    }\n}\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "568b5ab6c1b262fb582af225aade814d45d25043", "filename": "roles/xovis/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "---\n# The values here are defaults.\n# To override them edit the main var definitions in iiab/vars\n\nxovis_target_host: \"127.0.0.1:5984\"\nxovis_deployment_name: olpc\n\nxovis_db_name: xovis\nxovis_db_user: admin\nxovis_db_password: admin\nxovis_db_login: \"{{ xovis_db_user }}:{{ xovis_db_password }}\"\nxovis_db_url: \"http://{{ xovis_db_login }}@{{ xovis_target_host }}/{{ xovis_db_name }}\"\n\nxovis_root: \"/opt/xovis\"\nxovis_backup_dir: \"/library/users\"\nxovis_repo_url: \"https://github.com/XSCE/xovis.git\"\nxovis_chart_heading: \"My School: Usage Data Visualization\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "426a767f83ff2985ff81676d36affc906c9ec865", "filename": "roles/config-chrony/tasks/chrony.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Ensure chrony service is enabled and started\" \n  service:\n    name: chronyd\n    enabled: yes\n    state: started\n\n- name: \"Ensure chrony is listening on the correct subnet\"\n  lineinfile: \n    path: /etc/chrony.conf\n    regexp: '#?allow .+'\n    line: \"allow {{ chrony_allow_subnet }}\"\n  notify: 'Reload chrony' \n\n- name: \"Ensure chrony has a fall-back in case of failed external sync\"\n  lineinfile: \n    path: /etc/chrony.conf\n    regexp: '#?local stratum .+'\n    line: \"local stratum 10\"\n  notify: 'Reload chrony' \n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "1a20ef5ee5b6db65fb2f935fe650f8e24a6a86d8", "filename": "roles/manage-confluence-space/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- import_tasks: prepare_confluence_vars.yml\n\n- import_tasks: prepare_confluence_source.yml\n\n- import_tasks: create_confluence_space.yml\n\n- include_tasks: copy_confluence_content.yml\n  with_items: '{{ processed_contents.stdout }}'\n  loop_control:\n    loop_var: confluence_space_content\n\n- include_tasks: copy_confluence_attachments.yml\n  with_dict: '{{ id_mapping }}'\n  loop_control:\n    loop_var: confluence_content_ids\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "aeb19aff80ab35b2f96c988c40576875d3981797", "filename": "tasks/Linux/fetch/security-fetch/security-fetch-s3.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Download security policy artifact from s3\n  aws_s3:\n    bucket: '{{ java_unlimited_policy_transport_s3_bucket }}'\n    object: '{{ java_unlimited_policy_transport_s3_path }}'\n    dest: \"{{ java_download_path }}/\\\n      {{ java_unlimited_policy_transport_s3_path|basename }}\"\n    aws_access_key: '{{ transport_s3_aws_access_key }}'\n    aws_secret_key: '{{ transport_s3_aws_secret_key }}'\n    mode: get\n    overwrite: different\n  retries: 5\n  delay: 2\n\n- name: Downloaded security policy artifact\n  set_fact:\n    security_policy_java_artifact: >\n      {{ java_download_path }}/{{ java_unlimited_policy_transport_s3_path\n                             | basename }}\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "8f87887866a388d7afe56de0048c90978aed02c2", "filename": "roles/ovirt-iso-uploader-conf/meta/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\ngalaxy_info:\n  author: \"Meni Yakove\"\n  description: \"Set parameters for ovirt-iso-uploader in isouploader.conf\"\n  company: \"Red Hat\"\n  license: \"GPLv3\"\n  min_ansible_version: 1.9\n  platforms:\n    - name: EL\n      versions:\n        - all\n  galaxy_tags:\n    - installer\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "bdcf5faf549a33044bd7ad252941426c665b2d01", "filename": "roles/notifications/send-email/tests/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nmail:\n  host: \"smtp.example.com\"\n  port: \"465\"\n  username: \"user1@example.com\"\n  password: \"pa55word\"\n  secure: \"always\"\n  headers:\n  - 'Reply-To=user2@example.com'\n  to:\n  - \"person1@example.com\"\n  - \"person2@example.com\"\n  subject: \"Test Message 1\"\n  body: \"<html><body><h1>This is a H1 header</h1></body></html>\"\n  subtype: \"html\"\n\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "c2bb257c62a476205e07b80b0da0ea7bef898cf7", "filename": "ops/playbooks/roles/hpe.haproxy/tasks/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n\n  - name: Install Required Pkgs for seboolean module\n    yum:\n      name: \"{{ item }}\"\n      state: latest\n    with_items:\n      - libsemanage-python\n      - libselinux-python\n    environment: \"{{ hpe_haproxy_env }}\"\n    delegate_to: \"{{ hpe_haproxy_server }}\"\n\n  - name: Enable HAPROXY to open non standard ports\n    seboolean:\n      name: haproxy_connect_any\n      state: yes\n      persistent: yes\n    delegate_to: \"{{ hpe_haproxy_server }}\"\n\n  - name: Install haproxy\n    yum:\n      name: haproxy\n      state: latest\n    environment: \"{{ hpe_haproxy_env }}\"\n    delegate_to: \"{{ hpe_haproxy_server }}\"\n\n  - name: Copy HA proxy file fragments\n    copy:\n      src: files/\n      dest: /etc/haproxy/fragments\n    notify: Rebuild haproxy.cfg\n    delegate_to: \"{{ hpe_haproxy_server }}\"\n\n  - name: update file fragments\n    template:\n      src: templates/30-ucp-backends.j2\n      dest: \"/etc/haproxy/fragments/30-ucp-backends.{{ inventory_hostname }}\"\n    notify: Rebuild haproxy.cfg\n    delegate_to: \"{{ hpe_haproxy_server }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "6c9787a162ebd802d7272c5a0d1c66f83594ea79", "filename": "roles/config-redis/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nmode: containerized\n\nredis_name: redis\nredis_service: \"{{ redis_name }}.service\"\n\n# Redis\nredis_image: quay.io/quay/redis:latest\nredis_storage_dir: /var/lib/redis\nredis_container_storage_dir: /var/lib/redis\nredis_container_port: 6379\nredis_host_port: 6379\n\n#Systemd\nsystemd_service_dir: /usr/lib/systemd/system\nsystemd_environmentfile_dir: /etc/sysconfig\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "6c77d3e7c98a1ad05096649f1d14e9197eb1199d", "filename": "roles/mediawiki/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Include the install playbook\n  include_tasks: install.yml\n  when: mediawiki_install\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a01c196db81219812b7e6a57c0b26f24cfe70b50", "filename": "roles/config-openvpn/tasks/prep.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - \"{{ openvpn_rpm | default(default_openvpn_rpm) }}\"\n  - firewalld\n  - python-firewall\n\n- name: 'Ensure firewalld is running'\n  service:\n    name: firewalld\n    state: started \n    enabled: yes\n\n- name: 'Open Firewall for OpenVPN use'\n  firewalld: \n    port: \"{{ item }}\"\n    permanent: yes\n    state: enabled\n    immediate: yes\n  with_items:\n  - 443/tcp\n  - 943/tcp\n  - 1194/udp\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "99d0f44f0338ff9bb95def86e13650722b0c6d14", "filename": "playbooks/aws/openshift-cluster/library/ec2_ami_find.py", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#!/usr/bin/python\n#pylint: skip-file\n# flake8: noqa\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\nDOCUMENTATION = '''\n---\nmodule: ec2_ami_find\nversion_added: 2.0\nshort_description: Searches for AMIs to obtain the AMI ID and other information\ndescription:\n  - Returns list of matching AMIs with AMI ID, along with other useful information\n  - Can search AMIs with different owners\n  - Can search by matching tag(s), by AMI name and/or other criteria\n  - Results can be sorted and sliced\nauthor: Tom Bamford\nnotes:\n  - This module is not backwards compatible with the previous version of the ec2_search_ami module which worked only for Ubuntu AMIs listed on cloud-images.ubuntu.com.\n  - See the example below for a suggestion of how to search by distro/release.\noptions:\n  region:\n    description:\n      - The AWS region to use.\n    required: true\n    aliases: [ 'aws_region', 'ec2_region' ]\n  owner:\n    description:\n      - Search AMIs owned by the specified owner\n      - Can specify an AWS account ID, or one of the special IDs 'self', 'amazon' or 'aws-marketplace'\n      - If not specified, all EC2 AMIs in the specified region will be searched.\n      - You can include wildcards in many of the search options. An asterisk (*) matches zero or more characters, and a question mark (?) matches exactly one character. You can escape special characters using a backslash (\\) before the character. For example, a value of \\*amazon\\?\\\\ searches for the literal string *amazon?\\.\n    required: false\n    default: null\n  ami_id:\n    description:\n      - An AMI ID to match.\n    default: null\n    required: false\n  ami_tags:\n    description:\n      - A hash/dictionary of tags to match for the AMI.\n    default: null\n    required: false\n  architecture:\n    description:\n      - An architecture type to match (e.g. x86_64).\n    default: null\n    required: false\n  hypervisor:\n    description:\n      - A hypervisor type type to match (e.g. xen).\n    default: null\n    required: false\n  is_public:\n    description:\n      - Whether or not the image(s) are public.\n    choices: ['yes', 'no']\n    default: null\n    required: false\n  name:\n    description:\n      - An AMI name to match.\n    default: null\n    required: false\n  platform:\n    description:\n      - Platform type to match.\n    default: null\n    required: false\n  sort:\n    description:\n      - Optional attribute which with to sort the results.\n      - If specifying 'tag', the 'tag_name' parameter is required.\n    choices: ['name', 'description', 'tag']\n    default: null\n    required: false\n  sort_tag:\n    description:\n      - Tag name with which to sort results.\n      - Required when specifying 'sort=tag'.\n    default: null\n    required: false\n  sort_order:\n    description:\n      - Order in which to sort results.\n      - Only used when the 'sort' parameter is specified.\n    choices: ['ascending', 'descending']\n    default: 'ascending'\n    required: false\n  sort_start:\n    description:\n      - Which result to start with (when sorting).\n      - Corresponds to Python slice notation.\n    default: null\n    required: false\n  sort_end:\n    description:\n      - Which result to end with (when sorting).\n      - Corresponds to Python slice notation.\n    default: null\n    required: false\n  state:\n    description:\n      - AMI state to match.\n    default: 'available'\n    required: false\n  virtualization_type:\n    description:\n      - Virtualization type to match (e.g. hvm).\n    default: null\n    required: false\n  no_result_action:\n    description:\n      - What to do when no results are found.\n      - \"'success' reports success and returns an empty array\"\n      - \"'fail' causes the module to report failure\"\n    choices: ['success', 'fail']\n    default: 'success'\n    required: false\nrequirements:\n  - boto\n\n'''\n\nEXAMPLES = '''\n# Note: These examples do not set authentication details, see the AWS Guide for details.\n\n# Search for the AMI tagged \"project:website\"\n- ec2_ami_find:\n    owner: self\n    tags:\n      project: website\n    no_result_action: fail\n  register: ami_find\n\n# Search for the latest Ubuntu 14.04 AMI\n- ec2_ami_find:\n    name: \"ubuntu/images/ebs/ubuntu-trusty-14.04-amd64-server-*\"\n    owner: 099720109477\n    sort: name\n    sort_order: descending\n    sort_end: 1\n  register: ami_find\n\n# Launch an EC2 instance\n- ec2:\n    image: \"{{ ami_search.results[0].ami_id }}\"\n    instance_type: m4.medium\n    key_name: mykey\n    wait: yes\n'''\n\ntry:\n    import boto.ec2\n    HAS_BOTO=True\nexcept ImportError:\n    HAS_BOTO=False\n\nimport json\n\ndef main():\n    argument_spec = ec2_argument_spec()\n    argument_spec.update(dict(\n            region = dict(required=True,\n                aliases = ['aws_region', 'ec2_region']),\n            owner = dict(required=False, default=None),\n            ami_id = dict(required=False),\n            ami_tags = dict(required=False, type='dict',\n                aliases = ['search_tags', 'image_tags']),\n            architecture = dict(required=False),\n            hypervisor = dict(required=False),\n            is_public = dict(required=False),\n            name = dict(required=False),\n            platform = dict(required=False),\n            sort = dict(required=False, default=None,\n                choices=['name', 'description', 'tag']),\n            sort_tag = dict(required=False),\n            sort_order = dict(required=False, default='ascending',\n                choices=['ascending', 'descending']),\n            sort_start = dict(required=False),\n            sort_end = dict(required=False),\n            state = dict(required=False, default='available'),\n            virtualization_type = dict(required=False),\n            no_result_action = dict(required=False, default='success',\n                choices = ['success', 'fail']),\n        )\n    )\n\n    module = AnsibleModule(\n        argument_spec=argument_spec,\n    )\n\n    if not HAS_BOTO:\n        module.fail_json(msg='boto required for this module, install via pip or your package manager')\n\n    ami_id = module.params.get('ami_id')\n    ami_tags = module.params.get('ami_tags')\n    architecture = module.params.get('architecture')\n    hypervisor = module.params.get('hypervisor')\n    is_public = module.params.get('is_public')\n    name = module.params.get('name')\n    owner = module.params.get('owner')\n    platform = module.params.get('platform')\n    sort = module.params.get('sort')\n    sort_tag = module.params.get('sort_tag')\n    sort_order = module.params.get('sort_order')\n    sort_start = module.params.get('sort_start')\n    sort_end = module.params.get('sort_end')\n    state = module.params.get('state')\n    virtualization_type = module.params.get('virtualization_type')\n    no_result_action = module.params.get('no_result_action')\n\n    filter = {'state': state}\n\n    if ami_id:\n        filter['image_id'] = ami_id\n    if ami_tags:\n        for tag in ami_tags:\n            filter['tag:'+tag] = ami_tags[tag]\n    if architecture:\n        filter['architecture'] = architecture\n    if hypervisor:\n        filter['hypervisor'] = hypervisor\n    if is_public:\n        filter['is_public'] = is_public\n    if name:\n        filter['name'] = name\n    if platform:\n        filter['platform'] = platform\n    if virtualization_type:\n        filter['virtualization_type'] = virtualization_type\n\n    ec2 = ec2_connect(module)\n\n    images_result = ec2.get_all_images(owners=owner, filters=filter)\n\n    if no_result_action == 'fail' and len(images_result) == 0:\n        module.fail_json(msg=\"No AMIs matched the attributes: %s\" % json.dumps(filter))\n\n    results = []\n    for image in images_result:\n        data = {\n            'ami_id': image.id,\n            'architecture': image.architecture,\n            'description': image.description,\n            'is_public': image.is_public,\n            'name': image.name,\n            'owner_id': image.owner_id,\n            'platform': image.platform,\n            'root_device_name': image.root_device_name,\n            'root_device_type': image.root_device_type,\n            'state': image.state,\n            'tags': image.tags,\n            'virtualization_type': image.virtualization_type,\n        }\n\n        if image.kernel_id:\n            data['kernel_id'] = image.kernel_id\n        if image.ramdisk_id:\n            data['ramdisk_id'] = image.ramdisk_id\n\n        results.append(data)\n\n    if sort == 'tag':\n        if not sort_tag:\n            module.fail_json(msg=\"'sort_tag' option must be given with 'sort=tag'\")\n        results.sort(key=lambda e: e['tags'][sort_tag], reverse=(sort_order=='descending'))\n    elif sort:\n        results.sort(key=lambda e: e[sort], reverse=(sort_order=='descending'))\n\n    try:\n        if sort and sort_start and sort_end:\n            results = results[int(sort_start):int(sort_end)]\n        elif sort and sort_start:\n            results = results[int(sort_start):]\n        elif sort and sort_end:\n            results = results[:int(sort_end)]\n    except TypeError:\n        module.fail_json(msg=\"Please supply numeric values for sort_start and/or sort_end\")\n\n    module.exit_json(results=results)\n\n# import module snippets\nfrom ansible.module_utils.basic import *\nfrom ansible.module_utils.ec2 import *\n\nif __name__ == '__main__':\n    main()\n\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "a8b9e2288ee34cd0afa723fe22b123d12d071af2", "filename": "roles/kafka/defaults/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# defaults file for kafka\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "f19dcf2129fbeeb68dc6e01f2b1ad9b20cf442b7", "filename": "dev/playbooks/config_simplivity_backups.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts:  localhost vms\n  serial: 3\n  connection: local\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  vars:\n    ovc: \"{{ omnistack_ovc | random }}\"\n    sleep_interval: 5\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    # This task should use the uri module but not sure how to use the parameters in it\n    - name: Get Simplivity token\n      shell: curl -k https://simplivity@{{ ovc }}/api/oauth/token -d grant_type=password -d username='{{ simplivity_username }}' -d password='{{ simplivity_password }}' | python -mjson.tool | grep access_token | awk -F'\"' '{ print $4 }'\n      register: token\n      when: inventory_hostname in groups.local\n\n    # This one doesnt work\n    #- name: Get Simplivity token\n    #  uri:\n    #    url: \"https://simplivity@{{ ovc }}/api/oauth/token\"\n    #    headers:\n    #      Content-Type: application/vnd.simplivity.v1+json\n    #    method: GET\n    #    body: {\"grant_type\": \"password\",\"username\": \"{{ simplivity_username }}\",\"password\": \"{{ simplivity_password }}\"}\n    #    status_code: 200\n    #    body_format: json\n    #    force_basic_auth: yes\n    #    validate_certs: no\n    #  when: inventory_hostname in groups.local\n\n    - name: sleep\n      command: sleep '{{ sleep_interval }}'\n\n    - name: Retrieve current backup policies\n      uri:\n        url: \"https://{{ ovc }}/api/policies\"\n        headers:\n          Content-Type: application/vnd.simplivity.v1+json\n          Authorization: Bearer {{ token.stdout }}\n        method: GET\n        status_code: 200\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      when: inventory_hostname in groups.local\n      register: current_policies\n\n    - name: sleep\n      command: sleep '{{ sleep_interval }}'\n\n    - name: Extract existing policies names\n      set_fact: current_policies_names=\"{{ current_policies | json_query(q) }}\"\n      vars:\n        q: \"json.policies[].name\"\n      when: inventory_hostname in groups.local\n\n    - name: Extract policies names to be added\n      set_fact: backup_policies_names='{{ backup_policies_names|default([])+[item.name] }}'\n      with_items: \"{{ backup_policies }}\"\n      when: inventory_hostname in groups.local\n\n    - name: Set list of nonexistent policies to be added\n      set_fact: new_policies_names=\"{{ backup_policies_names | difference(current_policies_names) }}\"\n      when: inventory_hostname in groups.local\n\n    - name: Create backup policies\n      uri:\n        url: \"https://{{ ovc }}/api/policies\"\n        headers:\n          Content-Type: application/vnd.simplivity.v1+json\n          Authorization: Bearer {{ token.stdout }}\n        method: POST\n        body: {\"name\":\"{{ item }}\"}\n        status_code: 202\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      with_items: \"{{ new_policies_names }}\"\n      loop_control:\n        pause: 10\n      when: inventory_hostname in groups.local\n      register: create_output\n\n    - name: Get policy IDs\n      uri:\n        url: \"https://{{ ovc }}/api/policies\"\n        headers:\n          Content-Type: application/vnd.simplivity.v1+json\n          Authorization: Bearer {{ token.stdout }}\n        method: GET\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      register: policy_ids\n      when: inventory_hostname in groups.local\n\n    - name: sleep\n      command: sleep '{{ sleep_interval }}'\n\n    - name: Create backup rules\n      uri:\n        url: \"https://{{ ovc }}/api/policies/{{ policy_ids | json_query(q) }}/rules\"\n        headers:\n          Content-Type: application/vnd.simplivity.v1+json\n          Authorization: Bearer {{ token.stdout }}\n        method: POST\n        body: {\"days\":\"{{ item.days }}\",\"start_time\":\"{{ item.start_time }}\",\"frequency\":\"{{ item.frequency }}\",\"retention\":\"{{ item.retention }}\"}\n        status_code: 202\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      with_items: \"{{ backup_policies }}\"\n      loop_control:\n        pause: 10\n      vars: \n        q: \"json.policies[?name=='{{ item.name }}'].id | [0]\"\n      when: inventory_hostname in groups.local and new_policies_names | length > 0\n\n    - name: sleep\n      command: sleep '{{ sleep_interval }}'\n\n    - name: Get VMs information\n      uri:\n        url: \"https://{{ ovc }}/api/virtual_machines\"\n        headers:\n          Content-Type: application/vnd.simplivity.v1+json\n          Authorization: Bearer {{ token.stdout }}\n        method: GET\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      register: vm_ids\n      when: inventory_hostname in groups.local\n\n    - name: sleep\n      command: sleep '{{ sleep_interval }}'\n\n    - name: Assign backup policies to VMs\n      uri:\n        url: \"https://{{ ovc }}/api/virtual_machines/{{ hostvars[groups['local'][0]]['vm_ids'] | json_query(q_vms) }}/set_policy\"\n        headers:\n          Content-Type: application/vnd.simplivity.v1+json\n          Authorization: Bearer {{ hostvars[groups['local'][0]]['token']['stdout'] }}\n        method: POST\n        body: {\"policy_id\":\"{{ hostvars[groups['local'][0]]['policy_ids'] | json_query(q_policies) }}\"}\n        status_code: 202\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      vars:\n        q_policies: \"json.policies[?name=='{{ node_policy }}'].id | [0]\"\n        q_vms: \"json.virtual_machines[?name=='{{ inventory_hostname }}' && state=='ALIVE'].id | [0]\"\n      delegate_to: localhost\n      when: inventory_hostname not in groups.local\n\n    - name: sleep\n      command: sleep '{{ sleep_interval }}'\n\n    - name: Set dummy VM names in one string\n      set_fact: dummy_vms_string=\"{{ dummy_vm_prefix }}-in-dockvols-{{ datastores | join(','+dummy_vm_prefix+'-in-dockvols-') }}\"\n      #set_fact: dummy_vms_string=\"{{ dummy_vm_prefix }}-{{ datastores | join(','+dummy_vm_prefix+'-in-dockvols-') }}\"\n      when: inventory_hostname in groups.local\n\n    - name: Convert to list \n      set_fact: dummy_vms=\"{{ dummy_vms_string.split(',') }}\"\n      when: inventory_hostname in groups.local\n\n    - name: Assign backup policies to Docker volumes\n      uri:\n        url: \"https://{{ ovc }}/api/virtual_machines/{{ hostvars[groups['local'][0]]['vm_ids'] | json_query(q_vms) }}/set_policy\"\n        headers:\n          Content-Type: application/vnd.simplivity.v1+json\n          Authorization: Bearer {{ hostvars[groups['local'][0]]['token']['stdout'] }}\n        method: POST\n        body: {\"policy_id\":\"{{ hostvars[groups['local'][0]]['policy_ids'] | json_query(q_policies) }}\"}\n        status_code: 202\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      vars:\n        q_policies: \"json.policies[?name=='{{ docker_volumes_policy }}'].id | [0]\"\n        q_vms: \"json.virtual_machines[?name=='{{ item }}' && state=='ALIVE'].id | [0]\"\n      delegate_to: localhost\n      when: inventory_hostname in groups.local\n      with_items: \"{{ dummy_vms }}\"\n      loop_control:\n        pause: 10\n\n    - name: sleep\n      command: sleep '{{ sleep_interval }}'\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "ff90fb6fa64c19cec07db856135218683a71eca3", "filename": "ops/playbooks/includes/svt_get_auth_token.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n\n- name: Retrieve a token for SimpliVity API\n  uri:\n    url: \"https://simplivity@{{ ovc }}/api/oauth/token\"\n    headers:\n      Content-Type: application/x-www-form-urlencoded\n    method: POST\n    status_code: 200\n    validate_certs: no\n    body: 'grant_type=password&username={{ simplivity_username }}&password={{ simplivity_password }}'\n    force_basic_auth: yes\n  delegate_to: localhost\n  register: svtapitoken\n"}, {"commit_sha": "92dabcd706e72a0dc15ce13086fb9d59f1a8760e", "sha": "a32514dc89626a83717ad1af80e627c9492c21a8", "filename": "tasks/nginx.yml", "repository": "RocketChat/Rocket.Chat.Ansible", "decoded_content": "---\n# tasks/nginx.yml: Nginx management tasks for RocketChat.Ansible\n\n  - name: Grant Nginx permissions to proxy requests to an upstream [SELinux]\n    shell: setsebool httpd_can_network_connect on -P\n    changed_when: false\n    when:\n      - ('status' in ansible_selinux)\n      - (ansible_selinux.status == \"enabled\")\n\n  - name: Ensure Nginx is present\n    package:\n      name: nginx\n      state: present\n\n  - name: Check if Nginx was compiled with the HTTP/2 module\n    shell: nginx -V 2>&1 | grep -q 'with-http_v2_module'\n    register: nginx_http2_module\n    changed_when: false\n    failed_when: false\n\n  - name: Gather the current Nginx version string\n    shell: nginx -v 2>&1 | awk 'BEGIN{ FS=\"/\" } { print $2 }'\n    register: nginx_version_string\n    changed_when: false\n    failed_when: false\n\n  - name: Deploy Nginx configuration\n    template:\n      src: \"{{ item.src }}\"\n      dest: \"{{ item.dest }}\"\n    with_items:\n      - src: nginx.conf.j2\n        dest: /etc/nginx/nginx.conf\n      - src: ssl.inc.j2\n        dest: /etc/nginx/conf.d/ssl.inc\n      - src: rocket_chat.conf.j2\n        dest: /etc/nginx/conf.d/rocket_chat.conf\n    notify: Reload the Nginx service\n\n  - name: Ensure provided SSL certs have been deployed\n    copy:\n      src: \"{{ item.src }}\"\n      dest: \"{{ item.dest }}\"\n    when:\n      - not (rocket_chat_ssl_generate_certs | bool)\n      - (rocket_chat_ssl_deploy_data | bool)\n      - (rocket_chat_ssl_key_file is defined)\n      - (rocket_chat_ssl_cert_file is defined)\n      - (rocket_chat_ssl_key_file)\n      - (rocket_chat_ssl_cert_file)\n    with_items:\n      - src: \"{{ rocket_chat_ssl_key_file }}\"\n        dest: \"{{ rocket_chat_ssl_key_path }}\"\n      - src: \"{{ rocket_chat_ssl_cert_file }}\"\n        dest: \"{{ rocket_chat_ssl_cert_path }}\"\n    notify: Reload the Nginx service\n\n  - name: Ensure SSL certs have been generated\n    shell: >-\n      openssl req -x509 -newkey rsa:4096 -nodes\n      -subj \"/CN={{ rocket_chat_service_host }}/\n      /C=NA/ST=NA/L=NA/O=NA/OU=NA\"\n      -keyout {{ rocket_chat_ssl_key_path }}\n      -out {{ rocket_chat_ssl_cert_path }}\n      -days 3650\n    when:\n      - (rocket_chat_include_letsencrypt | bool)\n        or (rocket_chat_ssl_generate_certs | bool)\n    args:\n      creates: \"{{ rocket_chat_ssl_key_path }}\"\n    notify: Reload the Nginx service\n    register: key_gen_result\n\n  - name: Ensure provided PFS key has been deployed\n    copy:\n      src: \"{{ rocket_chat_nginx_pfs_file }}\"\n      dest: \"{{ rocket_chat_nginx_pfs_key_path }}\"\n    when:\n      - (rocket_chat_nginx_pfs_file is defined)\n      - (rocket_chat_nginx_pfs_file)\n      - (rocket_chat_nginx_pfs_file | exists)\n    notify: Reload the Nginx service\n    tags: pfs\n\n  - name: Ensure the PFS key has been generated (this can take a while!)\n    shell: >-\n      openssl dhparam -out {{ rocket_chat_nginx_pfs_key_path }}\n      {{ rocket_chat_nginx_pfs_key_numbits }}\n    when: (rocket_chat_nginx_generate_pfs_key | bool)\n    args:\n      creates: \"{{ rocket_chat_nginx_pfs_key_path }}\"\n    notify: Reload the Nginx service\n    tags: pfs\n\n  - name: Ensure the Nginx service is running/enabled\n    service:\n      name: nginx\n      state: started\n      enabled: true\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "e24095209e9ddbee86615cca9e2683a1f8c6009e", "filename": "reference-architecture/aws-ansible/playbooks/roles/cfn-outputs/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Get cfn stack outputs\n  cloudformation_facts:\n    stack_name: \"{{ stack_name }}\"\n    region: \"{{ region }}\"\n  register: stack\n\n- name: Set s3 facts\n  set_fact:\n    s3user_id: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['S3UserAccessId'] }}\"\n    s3user_secret: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['S3UserSecretKey'] }}\"\n    s3_bucket_name: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['S3Bucket'] }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9bfa345ddf55829c174d613c07cbc18f7cc4f32c", "filename": "roles/config-postgresql/tasks/install_containerized.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Generate Random DB Username\n  set_fact:\n    postgresql_username: \"{{ lookup('password', '/dev/null length=5 chars=ascii_letters') | lower }}\"\n  when: postgresql_username is undefined and postgresql_username|trim == \"\"\n\n- name: Generate Random DB Password\n  set_fact:\n    postgresql_password: \"{{ lookup('password', '/dev/null length=10 chars=ascii_letters,digits,hexdigits') }}\"\n  when: postgresql_password is undefined and postgresql_password|trim == \"\"\n\n- name: Generate Random DB Admin Username\n  set_fact:\n    postgresql_admin_user: \"{{ lookup('password', '/dev/null length=5 chars=ascii_letters') | lower }}\"\n  when: postgresql_admin_user is undefined and postgresql_admin_user|trim == \"\"\n\n- name: Generate Random Admin DB Password\n  set_fact:\n    postgresql_admin_password: \"{{ lookup('password', '/dev/null length=10 chars=ascii_letters,digits,hexdigits') }}\"\n  when: postgresql_admin_password is undefined and postgresql_admin_password|trim == \"\"\n\n- name: Configure Storage Directory\n  file:\n    state: directory\n    owner: root\n    group: root\n    mode: g+rw\n    path: \"{{ postgresql_storage_dir }}\"\n  notify: \"Restart PostgreSQL Service\"\n\n- name: Configure systemd environment files\n  template:\n    src: \"postgresql.j2\"\n    dest: \"{{ systemd_environmentfile_dir}}/{{ postgresql_name }}\"\n  notify: \"Restart PostgreSQL Service\"\n\n- name: Configure systemd unit files\n  template:\n    src: \"postgresql.service.j2\"\n    dest: \"{{ systemd_service_dir}}/{{ postgresql_service }}\"\n  notify: \"Restart PostgreSQL Service\"\n\n- name: Include firewall tasks\n  include_tasks: firewall.yml\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "3f7cdcca12bda693066f03531c98c333e24c4efd", "filename": "tasks/create_blobstore_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- name: Create directory for blob store.\n  file:\n    path: \"{{ item['path'] }}\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n    state: directory\n    recurse: true\n\n- include: call_script.yml\n  vars:\n    script_name: create_blobstore\n    args: \"{{ item }}\"\n"}, {"commit_sha": "e9fb46dc84b9c815a69f6de1347c9ece5db01cc8", "sha": "1b3dd6d9a77679bc53cc3dbc5b9a285ac101bb89", "filename": "tasks/nvm.yml", "repository": "fubarhouse/ansible-role-nodejs", "decoded_content": "---\n# Tasks file for NVM\n\n- name: \"NVM | Clean-up\"\n  become: yes\n  become_user: \"root\"\n  file:\n    path: \"{{ fubarhouse_npm.nvm_install_dir }}\"\n    state: absent\n  when: fubarhouse_npm.clean_install\n\n- name: NVM | Clean-up default version from shell profiles\n  become: yes\n  become_user: \"root\"\n  lineinfile:\n    dest: \"{{ fubarhouse_npm.user_dir }}/{{ item.filename }}\"\n    regexp: '.nvm/v{{ node_version }}/bin'\n    line:  'export PATH=$PATH:{{ fubarhouse_npm.user_dir }}/.nvm/v{{ node_version }}/bin;'\n    state: absent\n  with_items:\n    - \"{{ fubarhouse_npm.shell_profiles }}\"\n  ignore_errors: yes\n  when: fubarhouse_npm.clean_install\n\n- name: NVM | Clean-up other versions from shell profiles\n  become: yes\n  become_user: \"root\"\n  lineinfile:\n    dest: \"{{ fubarhouse_npm.user_dir }}/{{ item[0].filename }}\"\n    regexp: '.nvm/v{{ item[1] }}/bin'\n    line:  'export PATH=$PATH:{{ fubarhouse_npm.user_dir }}/.nvm/v{{ item[1] }}/bin;'\n    state: absent\n  with_nested:\n    - \"{{ fubarhouse_npm.shell_profiles }}\"\n    - \"{{ node_versions }}\"\n  ignore_errors: yes\n  when: fubarhouse_npm.clean_install\n\n- name: \"NodeJS | Remove imported exports not associated to specific versions\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  lineinfile:\n    dest: \"{{ fubarhouse_npm.user_dir }}/{{ item.filename }}\"\n    line: \"export PATH=$PATH:$(npm config --global get prefix)/bin\"\n    state: absent\n  with_items: \"{{ fubarhouse_npm.shell_profiles }}\"\n  when: fubarhouse_npm.clean_install\n\n- name: \"NVM | Check\"\n  stat:\n    path: \"{{ fubarhouse_npm.nvm_install_dir }}\"\n  register: fubarhouse_npm_nvm_installed\n\n- name: \"NVM | Ensure permissions are set\"\n  become: yes\n  become_user: \"root\"\n  file:\n    path: \"{{ item.path }}\"\n    state: directory\n    mode: 0777\n    owner: \"{{ fubarhouse_user }}\"\n    recurse: yes\n  with_items: \"{{ fubarhouse_npm.folder_paths }}\"\n  changed_when: false\n\n- name: \"NVM | Clone/Update\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  git:\n    repo: \"{{ nvm_repo }}\"\n    dest: \"{{ fubarhouse_npm.nvm_install_dir }}\"\n    clone: yes\n    update: yes\n    force: yes\n    version: master\n    recursive: false\n  changed_when: false\n\n- name: \"NVM | Install\"\n  shell: \"{{ fubarhouse_npm.nvm_install_dir }}/install.sh\"\n  when: fubarhouse_npm_nvm_installed.stat.exists == false\n\n- name: \"NVM | Create an executable\"\n  template:\n    src: \"nvm.sh\"\n    dest: \"{{ fubarhouse_npm.nvm_symlink_exec }}\"\n    owner: \"{{ fubarhouse_user }}\"\n    mode: 0755\n  when: fubarhouse_npm_nvm_installed.stat.exists == false\n\n- name: \"NVM | Get versions\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell: \"{{ fubarhouse_npm.nvm_symlink_exec }} ls-remote\"\n  register: nodejs_available_versions\n  changed_when: false\n\n- name: NVM | Ensure shell profiles are available\n  become: yes\n  become_user: \"root\"\n  file:\n    path: \"{{ fubarhouse_npm.user_dir }}/{{ item.filename }}\"\n    state: touch\n  with_items: \"{{ fubarhouse_npm.shell_profiles }}\"\n  ignore_errors: yes\n  changed_when: false\n\n- name: NVM | Ensure shell profiles are configured for default version\n  become: yes\n  become_user: \"root\"\n  lineinfile:\n    dest: \"{{ fubarhouse_npm.user_dir }}/{{ item.filename }}\"\n    regexp: '.nvm/v{{ node_version }}/bin'\n    line:  'export PATH=$PATH:{{ fubarhouse_npm.user_dir }}/.nvm/v{{ node_version }}/bin;'\n    state: present\n  with_items:\n    - \"{{ fubarhouse_npm.shell_profiles }}\"\n  when: '\"{{ node_version }}\" in nodejs_available_versions.stdout'\n  ignore_errors: yes\n\n- name: NVM | Ensure shell profiles are configured for other versions\n  become: yes\n  become_user: \"root\"\n  lineinfile:\n    dest: \"{{ fubarhouse_npm.user_dir }}/{{ item[0].filename }}\"\n    regexp: '.nvm/v{{ item[1] }}/bin'\n    line:  'export PATH=$PATH:{{ fubarhouse_npm.user_dir }}/.nvm/v{{ item[1] }}/bin;'\n    state: present\n  when: '\"{{ item[1] }}\" in nodejs_available_versions.stdout'\n  with_nested:\n    - \"{{ fubarhouse_npm.shell_profiles }}\"\n    - \"{{ node_versions }}\"\n  ignore_errors: yes"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "b7b2cad328fdc9492e800438081be90d758a66bd", "filename": "tasks/section_10.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - include: section_10_level1.yml\n    tags:\n      - section10\n      - level1\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "d6ea61dc3c822c0091934bbbdd2d088b1d072a4e", "filename": "roles/httpd/tasks/html.yml", "repository": "iiab/iiab", "decoded_content": "- name: Copy css files\n  copy:\n    src: \"{{ item }}\"\n    dest: \"{{ doc_root }}/common/css\"\n    mode: 0644\n    owner: root\n    group: root\n  with_fileglob:\n    - html/css/*.css\n\n- name: Copy js files\n  copy:\n    src: \"{{ item }}\"\n    dest: \"{{ doc_root }}/common/js\"\n    mode: 0644\n    owner: root\n    group: root\n  with_fileglob:\n    - html/js/*.js\n\n- name: Copy fonts files\n  copy:\n    src: \"{{ item }}\"\n    dest: \"{{ doc_root }}/common/fonts\"\n    mode: 0644\n    owner: root\n    group: root\n  with_fileglob:\n    - html/fonts/*\n\n- name: Copy html files\n  copy:\n    src: \"{{ item }}\"\n    dest: \"{{ doc_root }}/common/html\"\n    mode: 0644\n    owner: root\n    group: root\n  with_fileglob:\n    - html/html/*\n\n- name: Copy assets files\n  copy:\n    src: \"{{ item }}\"\n    dest: \"{{ doc_root }}/common/assets\"\n    mode: 0644\n    owner: root\n    group: root\n  with_fileglob:\n    - html/assets/*\n\n# copy all services, even if not permissioned elsewhere\n- name: Copy services files\n  copy:\n    src: \"{{ item }}\"\n    dest: \"{{ doc_root }}/common/services\"\n    mode: 0644\n    owner: root\n    group: root\n  with_fileglob:\n    - html/services/*\n\n- name: Create symlink from assets to iiab.ini\n  file:\n    src: \"/etc/iiab/iiab.ini\"\n    dest: \"{{ doc_root }}/common/assets/iiab.ini\"\n    owner: root\n    group: root\n    state: link\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "e80d7d773b52f666072d924217350e6d9d62b7a7", "filename": "playbooks/generate-defaults.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- hosts: all\n  tasks:\n  - name: Apply override settings, if available\n    include_vars: /etc/rocknsm/config.yml\n    ignore_errors: true\n    failed_when: false\n\n  - name: Create config directory\n    file:\n      state: directory\n      owner: root\n      group: root\n      mode:  0755\n      path: /etc/rocknsm\n\n  - name: Render template\n    template:\n      backup: yes\n      src: rock_config.yml.j2\n      dest: /etc/rocknsm/config.yml\n      owner: root\n      group: root\n      mode: 0644\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "747963efcab2faf78bdc0b621c4442fbae761939", "filename": "playbooks/empty-dir-quota.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: schedulable_nodes\n  gather_facts: yes\n  become: yes\n  roles:\n  - openshift-emptydir-quota\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "5bfb4fcbc8fbf8f080435031e34ca8b7d5126e5e", "filename": "tasks/create_repo_docker_group_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_docker_group\n    args: \"{{ _nexus_repos_docker_defaults|combine(item) }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "215ba70c3276cd50f5d8e91c85e16608a6afe761", "filename": "roles/user-management/populate-users/test/playbook.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Import identities information from CSV file \n  hosts: identity\n\n  roles: \n    - populate-users\n"}, {"commit_sha": "6d10af54bdbf8e81c3d90a70ffea87b4d2c20eb2", "sha": "b89d14acaeea14e2701e1023dc14edda789a7462", "filename": "tasks/chown.yml", "repository": "Oefenweb/ansible-wordpress", "decoded_content": "# tasks file for wordpress, chown\n---\n- name: change owner (and group)\n  file:\n    path: \"{{ item.path }}\"\n    owner: \"{{ item.owner | default('www-data') }}\"\n    group: \"{{ item.group | default(item.owner) | default('www-data') }}\"\n    recurse: true\n  with_items: wordpress_installs\n  tags: [configuration, wordpress, wordpress-chown]\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "1b7889a88c3f31022d0b988d38c5438e5b9a0a60", "filename": "playbooks/osp/provision-osp-instance.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Provision Instance(s)\n  hosts: osp-provisioner\n  roles:\n  - role: osp/admin-volume\n  - role: osp/admin-sec-group\n  - role: osp/admin-instance\n\n- name: Refresh Server inventory\n  hosts: osp-provisioner\n  gather_facts: False\n  tasks:\n  - meta: refresh_inventory\n\n- name: Print Instance info + Wait for Instances to come alive\n  hosts: osp_instances\n  gather_facts: false\n  tasks:\n  - name: Debug hostvar\n    debug:\n      msg: \"{{ hostvars[inventory_hostname] }}\"\n      verbosity: 2\n  - name: waiting for server to come back\n    local_action:\n      module: wait_for\n      host: \"{{ hostvars[inventory_hostname]['ansible_ssh_host'] }}\"\n      port: 22\n      delay: 10\n      timeout: 300\n\n- name: \"Ensure the host is ready for Ansible\"\n  hosts: osp_instances\n  gather_facts: no\n  roles:\n  - role: ansible/prep-for-ansible\n\n- name: \"Workaround for .novalocal\"\n  hosts: osp_instances\n  tasks:\n  - name: \"Eliminate the .novalocal at the end of the FQDN\"\n    hostname:\n      name: \"{{ ansible_fqdn | regex_replace('(.*).novalocal$', '\\\\1') }}\"\n  - name: \"Ensure the local host can resolve by its own IP\"\n    lineinfile:\n      path: /etc/hosts\n      regexp: \"^{{ ansible_default_ipv4.address }}.*\"\n      line: \"{{ ansible_default_ipv4.address }} {{ ansible_fqdn | regex_replace('(.*).novalocal$', '\\\\1') }}\"\n  - name: \"Ensure the changes stick during reboot\"\n    stat: path=/etc/cloud/cloud.cfg\n    register: cloud_cfg\n  - lineinfile:\n      dest: /etc/cloud/cloud.cfg\n      state: present\n      regexp: \"{{ item.regexp }}\"\n      line: \"{{ item.line }}\"\n    with_items:\n    - { regexp: '^ - set_hostname', line: '# - set_hostname' }\n    - { regexp: '^ - update_hostname', line: '# - update_hostname' }\n    when: cloud_cfg.stat.exists == True\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "192c3372e5b5ae00d3c77eec0ff7f284e931e048", "filename": "roles/scm/gitlab.com/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Create new Gitlab Group in {{ gitlab_group_name }}\n  uri:\n    url: '{{ gitlab_api_groups }}'\n    headers:\n      Accept: application/json\n      Private-Token: '{{ gitlab_api_private_token }}'\n    method: POST\n    status_code: 201\n    body_format: json\n    body: '{\n             \"name\": \"{{ group.name }}\",\n             \"path\": \"{{ group.path | default(group.name) }}\",\n             \"description\": \"{{ group.description | default(group.name) }}\",\n             \"visibility\": \"{{ group.visibility | default(\"private\") }}\",\n             \"lfs_enabled\": {{ group.lfs_enabled | default(\"false\") }},\n             \"request_access_enabled\": {{ group.request_access_enabled | default(\"false\") }},\n           }'\n  register: groups_json_response\n\n- debug: msg={{ groups_json_response.json.id }}\n\n- name: Add Users to Gitlab Group\n  uri:\n    url: '{{ gitlab_api_groups }}/{{ group.name }}/members'\n    headers:\n      Accept: application/json\n      Private-Token: '{{ gitlab_api_private_token }}'\n    method: POST\n    status_code: 201\n    body_format: json\n    body: '{\n             \"user_id\": \"{{ item.id }}\",\n             \"access_level\": \"{{ item.access_level | default(40) }}\"\n           }'\n  register: users_json_response\n  with_items: \"{{ users }}\"\n\n- debug: msg={{ users_json_response }}\n\n- name: Add Project to existing Gitlab Group\n  uri:\n    url: '{{ gitlab_api_projects }}'\n    headers:\n      Accept: application/json\n      Private-Token: '{{ gitlab_api_private_token }}'\n    method: POST\n    status_code: 201\n    body_format: json\n    body: '{\n             \"name\": \"{{ item.repo_name }}\",\n             \"path\": \"{{ item.path | default(item.repo_name) }}\",\n             \"namespace_id\": {{ groups_json_response.json.id }},\n             \"description\": \"{{ item.description | default(item.repo_name) }}\",\n             \"issues_enabled\": {{ item.issues_enabled | default(\"true\") }},\n             \"merge_requests_enabled\": {{ item.merge_requests_enabled | default(\"true\") }},\n             \"wiki_enabled\": {{ item.wiki_enabled | default(\"true\") }},\n             \"visibility\": \"{{ item.visibility | default(\"private\") }}\",\n             \"import_url\": \"{{ item.import_url }}\",\n             \"lfs_enabled\": {{ item.lfs_enabled | default(\"false\") }},\n             \"printing_merge_request_link_enabled\": {{ item.printing_merge_request_link_enabled | default(\"true\") }}\n           }'\n  register: projects_json_response\n  with_items: \"{{ projects }}\"\n\n- debug: msg={{ projects_json_response }}\n\n- name: Add a new deploy key to the gitlab repositories\n  uri:\n    url: '{{ gitlab_api_projects }}/{{ item.1.json.id }}/deploy_keys'\n    headers:\n      Accept: application/json\n      Private-Token: '{{ gitlab_api_private_token }}'\n    method: POST\n    status_code: 201\n    body_format: json\n    body: '{\n             \"title\": \"deploy-key-{{ item.0.repo_name }}\",\n             \"key\": \"{{ item.0.deploy_key_location }}\",\n             \"can_push\": {{ item.0.can_push | default(\"true\") }}\n           }'\n  register: deploy_keys_json_response\n  when: item.0.repo_name == item.1.json.name\n  with_together:\n    - \"{{ projects }}\"\n    - \"{{ projects_json_response.results }}\"\n\n- debug: msg={{ deploy_keys_json_response }}"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "becd77e081a6968dd96e6a9047f42f2d368fe768", "filename": "roles/nfs-server/tests/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nnfs_shares:\n- name: registry\n- name: metrics\n  nfs_owner: \"root\" \n  nfs_group: \"root\"\n  nfs_mode: \"0755\"     \n  nfs_share_options: \"ro\"\n- name: logging\n- name: data\n\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "1cf236847821c73259c645dd0b6b82b20d2c1b07", "filename": "roles/ssh_tunneling/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- set_fact:\n    IP_subject_alt_name: \"{{ IP_subject_alt_name }}\"\n\n- name: Ensure that the sshd_config file has desired options\n  blockinfile:\n    dest: /etc/ssh/sshd_config\n    marker: '# {mark} ANSIBLE MANAGED BLOCK ssh_tunneling_role'\n    block: |\n      Match Group algo\n          AllowTcpForwarding local\n          AllowAgentForwarding no\n          AllowStreamLocalForwarding no\n          PermitTunnel no\n          X11Forwarding no\n  notify:\n    - restart ssh\n\n- name: Ensure that the algo group exist\n  group: name=algo state=present\n\n- name: Ensure that the jail directory exist\n  file: path=/var/jail/ state=directory mode=0755 owner=root group=\"{{ root_group|default('root') }}\"\n\n- name: Ensure that the SSH users exist\n  user:\n    name: \"{{ item }}\"\n    groups: algo\n    home: '/var/jail/{{ item }}'\n    createhome: yes\n    generate_ssh_key: yes\n    shell: /bin/false\n    ssh_key_type: ecdsa\n    ssh_key_bits: 256\n    ssh_key_comment: '{{ item }}@{{ IP_subject_alt_name }}'\n    ssh_key_passphrase: \"{{ easyrsa_p12_export_password }}\"\n    state: present\n    append: yes\n  with_items: \"{{ users }}\"\n\n- name: The authorized keys file created\n  file:\n    src: '/var/jail/{{ item }}/.ssh/id_ecdsa.pub'\n    dest: '/var/jail/{{ item }}/.ssh/authorized_keys'\n    owner: \"{{ item }}\"\n    group: \"{{ item }}\"\n    state: link\n  with_items: \"{{ users }}\"\n\n- name: Generate SSH fingerprints\n  shell: >\n    ssh-keyscan {{ IP_subject_alt_name }} 2>/dev/null\n  register: ssh_fingerprints\n\n- name: The known_hosts file created\n  template: src=known_hosts.j2 dest=/root/.ssh/{{ IP_subject_alt_name }}_known_hosts\n\n- name: Fetch users SSH private keys\n  fetch: src='/var/jail/{{ item }}/.ssh/id_ecdsa' dest=configs/{{ IP_subject_alt_name }}/{{ item }}.ssh.pem flat=yes\n  with_items: \"{{ users }}\"\n\n- name: Change mode for SSH private keys\n  local_action: file path=configs/{{ IP_subject_alt_name }}/{{ item }}.ssh.pem mode=0600\n  with_items: \"{{ users }}\"\n  become: false\n\n- name: Fetch the known_hosts file\n  fetch: src='/root/.ssh/{{ IP_subject_alt_name }}_known_hosts' dest=configs/{{ IP_subject_alt_name }}/{{ IP_subject_alt_name }}_known_hosts flat=yes\n\n- name: Build the client ssh config\n  local_action:\n    module: template\n    src: ssh_config.j2\n    dest: configs/{{ IP_subject_alt_name }}/{{ item }}.ssh_config\n    mode: 0600\n  become: no\n  with_items:\n    - \"{{ users }}\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "42d5f46b28e9129fadcccbb558e264d6ca84b736", "filename": "archive/roles/expose-registry/tasks/main.yaml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: \"Check that Openshift Docker Registry exists\"\n  command: >\n    {{ openshift.common.client_binary }} get deploymentConfig {{ registry_dc }}\n     -n {{ openshift_registry_project }}\n  register: registry_exists\n\n- fail:\n    msg: \"No docker registry found\"\n  when: registry_exists.rc != 0\n\n- name: \"Check that Openshift router exists\"\n  command: >\n    {{ openshift.common.client_binary }} get deploymentConfig {{ router_dc }}\n     -n {{ openshift_router_project }}\n  register: router_exists\n\n- fail:\n    msg: \"No router found\"\n  when: router_exists.rc != 0\n\n- name: \"Collect registry service info\"\n  command: >\n    {{ openshift.common.client_binary}} get service {{ registry_dc }}\n     -n {{ openshift_registry_project }} -o yaml\n  register: registry_svc_info\n\n- set_fact:\n    registry_svc_ip: \"{{ (registry_svc_info.stdout | from_yaml).spec.clusterIP }}\"\n\n\n- name: \"Check if registry is already secured\"\n  uri:\n    url: \"https://{{ registry_svc_ip }}:5000\"\n    method: \"GET\"\n    status_code: '200'\n    validate_certs: no\n  register: secured\n  failed_when: false\n\n- fail:\n    msg: \"The registry has not been secured.\"\n  when: secured.status != 200\n\n- name: \"Check if registry is already exposed\"\n  command: >\n    {{openshift.common.client_binary}} get routes {{registry_dc}} -n {{openshift_registry_project}}\n  register: already_exposed\n  failed_when: false\n\n- fail:\n    msg: \"Docker registry is already exposed\"\n  when: already_exposed.rc == 0\n\n- fail:\n    msg: \"Variable 'registry_hostname' has not been defined.\"\n  when: registry_hostname == '' or registry_hostname is not defined\n\n- name: \"Create route\"\n  command: >\n   {{ openshift.common.client_binary }} create route passthrough\n   --service={{registry_dc}}\n   --hostname={{registry_hostname}}\n   -n {{ openshift_registry_project }}\n  when: \"{{openshift.common.version_gte_3_2_or_1_2 | bool}}\"\n\n- name: \"Create route\"\n  command: >\n   {{ openshift.common.client_binary }} expose service/{{registry_dc}}\n   --hostname={{registry_hostname}}\n   -n {{ openshift_registry_project }}\n  when: \"{{not openshift.common.version_gte_3_2_or_1_2 | bool}}\"\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "75b3db6dae9ba61dd530c0fdab55a6d6bdf5c7dc", "filename": "roles/cloud-openstack/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- fail:\n    msg: \"OpenStack credentials are not set. Download it from the OpenStack dashboard->Compute->API Access and source it in the shell (eg: source /tmp/dhc-openrc.sh)\"\n  when: lookup('env', 'OS_AUTH_URL') == \"\"\n\n- block:\n  - name: Build python virtual environment\n    import_tasks: venv.yml\n\n  - block:\n    - name: Security group created\n      os_security_group:\n        state: \"{{ state|default('present') }}\"\n        name: \"{{ algo_server_name }}-security_group\"\n        description: AlgoVPN security group\n      register: os_security_group\n\n    - name: Security rules created\n      os_security_group_rule:\n        state: \"{{ state|default('present') }}\"\n        security_group: \"{{ os_security_group.id }}\"\n        protocol: \"{{ item.proto }}\"\n        port_range_min: \"{{ item.port_min }}\"\n        port_range_max: \"{{ item.port_max }}\"\n        remote_ip_prefix: \"{{ item.range }}\"\n      with_items:\n        - { proto: tcp, port_min: 22, port_max: 22, range: 0.0.0.0/0 }\n        - { proto: icmp, port_min: -1, port_max: -1, range: 0.0.0.0/0 }\n        - { proto: udp, port_min: 4500, port_max: 4500, range: 0.0.0.0/0 }\n        - { proto: udp, port_min: 500, port_max: 500, range: 0.0.0.0/0 }\n        - { proto: udp, port_min: \"{{ wireguard_port }}\", port_max: \"{{ wireguard_port }}\", range: 0.0.0.0/0 }\n\n    - name: Keypair created\n      os_keypair:\n        state: \"{{ state|default('present') }}\"\n        name: \"{{ SSH_keys.comment|regex_replace('@', '_') }}\"\n        public_key_file: \"{{ SSH_keys.public }}\"\n      register: os_keypair\n\n    - name: Gather facts about flavors\n      os_flavor_facts:\n        ram: \"{{ cloud_providers.openstack.flavor_ram }}\"\n\n    - name: Gather facts about images\n      os_image_facts:\n        image: \"{{ cloud_providers.openstack.image }}\"\n\n    - name: Gather facts about public networks\n      os_networks_facts:\n\n    - name: Set the network as a fact\n      set_fact:\n        public_network_id: \"{{ item.id }}\"\n      when:\n        - item['router:external']|default(omit)\n        - item['admin_state_up']|default(omit)\n        - item['status'] == 'ACTIVE'\n      with_items: \"{{ openstack_networks }}\"\n\n    - name: Set facts\n      set_fact:\n        flavor_id: \"{{ (openstack_flavors | sort(attribute='ram'))[0]['id'] }}\"\n        image_id: \"{{ openstack_image['id'] }}\"\n        keypair_name: \"{{ os_keypair.key.name }}\"\n        security_group_name: \"{{ os_security_group['secgroup']['name'] }}\"\n\n    - name: Server created\n      os_server:\n        state: \"{{ state|default('present') }}\"\n        name: \"{{ algo_server_name }}\"\n        image: \"{{ image_id }}\"\n        flavor: \"{{ flavor_id }}\"\n        key_name: \"{{ keypair_name }}\"\n        security_groups: \"{{ security_group_name }}\"\n        nics:\n          - net-id: \"{{ public_network_id }}\"\n      register: os_server\n\n    - set_fact:\n        cloud_instance_ip: \"{{ os_server['openstack']['public_v4'] }}\"\n        ansible_ssh_user: ubuntu\n    environment:\n      PYTHONPATH: \"{{ openstack_venv }}/lib/python2.7/site-packages/\"\n\n  rescue:\n  - debug: var=fail_hint\n    tags: always\n  - fail:\n    tags: always\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "e3e9ece788104302f230b5ed14d78646b10b6419", "filename": "tasks/checks/compatibility-checks.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# https://github.com/moby/moby/issues/35873\n# https://access.redhat.com/solutions/2991041\n- name: Compatibility check - Fail if both MountFlags=slave and live-restore are set\n  fail:\n    msg: >\n      Setting both `MountFlags=slave` (docker_enable_mount_flag_fix: true)\n      and `live-restore=true` (docker_daemon_config['live-restore']: true)\n      triggers a bug (https://github.com/moby/moby/issues/35873). For now,\n      don't use both.\n  when:\n    - docker_enable_mount_flag_fix | bool\n    - docker_daemon_config['live-restore'] is defined\n    - docker_daemon_config['live-restore']\n\n\n# Issues related to specified URL http://pypi.python.org/simple/ which now must be https.\n- name: Compatibility check - Fail if PiP is required due to issues getting it to work smooth in Debian Wheezy\n  fail:\n    msg: >\n      Make sure Docker SDK, docker-compose etc is already in place before using\n      this role on a host running Debian Wheezy.\n  when:\n    - _docker_os_dist == \"Debian\"\n    - _docker_os_dist_major_version | int == 7\n    - docker_stack | bool or\n      docker_sdk | bool or\n      docker_pip_upgrade | bool or\n      (docker_compose | bool and not docker_compose_no_pip | bool) or\n      docker_additional_packages_pip|length > 0"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "11ef97a1933ed38dd8e28dbf69dd6f1fef1911b9", "filename": "playbooks/templates/bro-broctl.cfg.j2", "repository": "rocknsm/rock", "decoded_content": "## Global BroControl configuration file.\n\n###############################################\n# Mail Options\n\n# Recipient address for all emails sent out by Bro and BroControl.\nMailTo = root@localhost\n\n# Mail connection summary reports each log rotation interval.  A value of 1\n# means mail connection summaries, and a value of 0 means do not mail\n# connection summaries.  This option has no effect if the trace-summary\n# script is not available.\nMailConnectionSummary = 1\n\n# Lower threshold (in percentage of disk space) for space available on the\n# disk that holds SpoolDir. If less space is available, \"broctl cron\" starts\n# sending out warning emails.  A value of 0 disables this feature.\nMinDiskSpace = 5\n\n# Send mail when \"broctl cron\" notices the availability of a host in the\n# cluster to have changed.  A value of 1 means send mail when a host status\n# changes, and a value of 0 means do not send mail.\nMailHostUpDown = 1\n\n###############################################\n# Logging Options\n\n# Rotation interval in seconds for log files on manager (or standalone) node.\n# A value of 0 disables log rotation.\nLogRotationInterval = 3600\n\n# Expiration interval for archived log files in LogDir.  Files older than this\n# will be deleted by \"broctl cron\".  The interval is an integer followed by\n# one of these time units:  day, hr, min.  A value of 0 means that logs\n# never expire.\nLogExpireInterval = {{ bro_log_retention }}\n\n# Enable BroControl to write statistics to the stats.log file.  A value of 1\n# means write to stats.log, and a value of 0 means do not write to stats.log.\nStatsLogEnable = 1\n\n# Number of days that entries in the stats.log file are kept.  Entries older\n# than this many days will be removed upon running \"broctl cron\".  A value of 0\n# means that entries never expire.\nStatsLogExpireInterval = {{ bro_stats_retention }}\n\n###############################################\n# Other Options\n\n# Show all output of the broctl status command.  If set to 1, then all output\n# is shown.  If set to 0, then broctl status will not collect or show the peer\n# information (and the command will run faster).\nStatusCmdShowAll = 0\n\n# Site-specific policy script to load. Bro will look for this in\n# $PREFIX/share/bro/site. A default local.bro comes preinstalled\n# and can be customized as desired.\nSitePolicyScripts = local.bro\n\n# Location of the log directory where log files will be archived each rotation\n# interval.\nLogDir = {{ bro_data_dir }}/logs\n\n# Location of the spool directory where files and data that are currently being\n# written are stored.\nSpoolDir = {{ bro_data_dir }}/spool\n\n# Location of other configuration files that can be used to customize\n# BroControl operation (e.g. local networks, nodes).\nCfgDir = {{ bro_sysconfig_dir }}\n"}, {"commit_sha": "01c4359d8ad17ba10149ac898663e598069b9055", "sha": "784e2c6825f4463077d228a75bb5fd36e31449ae", "filename": "tasks/autoupdate-Debian.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\n- name: Install unattended upgrades package.\n  apt: name=unattended-upgrades state=present\n\n- name: Copy unattended-upgrades configuration files in place.\n  template:\n    src: \"../templates/{{ item }}.j2\"\n    dest: \"/etc/apt/apt.conf.d/{{ item }}\"\n    owner: root\n    group: root\n    mode: 0644\n  with_items:\n    - 10periodic\n    - 50unattended-upgrades\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "8dbb9b83d1b1f54cdf2d1967c5d73c53f34df6a1", "filename": "reference-architecture/vmware-ansible/playbooks/prod-ose-cns.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  connection: local\n  gather_facts: yes\n  become: no\n  vars_files:\n  - vars/main.yaml\n  roles:\n  # Group systems\n  - create-vm-cns-prod-ose\n  - instance-groups\n"}, {"commit_sha": "a10c5f4577e6e74feb1fadec4bcbab039b8b180a", "sha": "61283a1c3a865b5124e97a4773f86862fa888c42", "filename": "tasks/checks/compatibility-checks.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# https://github.com/moby/moby/issues/35873\n# https://access.redhat.com/solutions/2991041\n- name: Compatibility check - Fail if both MountFlags=slave and live-restore are set\n  fail:\n    msg: >\n      Setting both `MountFlags=slave` (docker_enable_mount_flag_fix: true)\n      and `live-restore=true` (docker_daemon_config['live-restore']: true)\n      triggers a bug (https://github.com/moby/moby/issues/35873). For now,\n      don't use both.\n  when:\n    - docker_enable_mount_flag_fix\n    - docker_daemon_config['live-restore'] is defined\n    - docker_daemon_config['live-restore']\n\n\n# Issues related to specified URL http://pypi.python.org/simple/ which now must be https.\n- name: Compatibility check - Fail if PiP is required due to issues getting it to work smooth in Debian Wheezy\n  fail:\n    msg: >\n      Make sure Docker SDK, docker-compose etc is already in place before using\n      this role on a host running Debian Wheezy.\n  when:\n    - _docker_os_dist == \"Debian\"\n    - _docker_os_dist_major_version == '7'\n    - docker_stack|bool or\n      docker_sdk|bool or\n      docker_pip_upgrade|bool or\n      (docker_compose|bool and not docker_compose_no_pip|bool) or\n      docker_additional_packages_pip|length > 0"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "1e4c4769ec7001fb357f41ff04ec0e8ecee11532", "filename": "roles/idm-host-cert/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# 'host_force_add' is used to add the host even if no DNS record(s) exists for the host\nhost_force_add: true\n\n# Description of the host entry being added/processed\nhost_description: ''\n\n# Default API version to be passed\napi_version: '2.213'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "bdf1f22cbb4aed61ff052fe3125279420de41907", "filename": "roles/dns/manage-dns-zones/tasks/route53/process-zones.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- include_tasks: process-one-zone.yml\n  with_items:\n    - \"{{ view.zones }}\"\n  loop_control:\n    loop_var: \"zone\"\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "5ba1b00ffddf308d5b1b22565e89f06b46bffacc", "filename": "roles/haproxy/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for haproxy\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "af779d1e0835ca6e5e2e098bbb3dd8806b05e775", "filename": "roles/consul/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for consul\n- name: remove consul override\n  file:\n    path: /etc/init/consul.override\n    state: absent\n\n- name: configure consul\n  sudo: yes\n  template:\n    src: consul.json.j2\n    dest: /etc/consul.d/consul.json\n    owner: root\n    group: root\n    mode: 0644\n  notify:\n    - restart consul\n  tags:\n    - consul\n\n- name: configure atlas for consul\n  sudo: yes\n  template:\n    src: atlas.json.j2\n    dest: /etc/consul.d/atlas.json\n    owner: root\n    group: root\n    mode: 0644\n  when: consul_atlas_join\n  notify:\n    - restart consul\n  tags:\n    - consul\n\n- name: enable consul\n  sudo: yes\n  service:\n    name: consul\n    enabled: yes\n    state: started\n  tags:\n    - consul\n\n# Give some time for leader election to occur\n- name: wait for leader\n  wait_for:\n    host: \"{{ consul_bind_addr }}\"\n    port: 8301\n    delay: 10\n  tags:\n    - consul\n\n- name: remove consul-join override\n  file:\n    path: /etc/init/consul-join.override\n    state: absent\n  when: consul_join is defined\n  tags:\n    - consul\n\n- name: configure consul-join\n  sudo: yes\n  template:\n    src: consul-join.j2\n    dest: /etc/service/consul-join\n    owner: root\n    group: root\n    mode: 0644\n  notify:\n    - restart consul\n  when: consul_join is defined\n  tags:\n    - consul\n\n# We need to force reload here because sometimes Consul gets in a weird\n# state where it cannot elect a cluster leader. Simply restarting the service\n# seems to allow it to recover automatically.\n- name: force reload consul\n  sudo: yes\n  command: /sbin/restart consul\n  tags:\n    - consul\n\n- name: force wait for leader\n  wait_for:\n    host: \"{{ consul_bind_addr }}\"\n    port: 8301\n    delay: 10\n  tags:\n    - consul\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "d3e57133c2caf2a52fa7a97a6924a73344e10a63", "filename": "ops/playbooks/distribute_keys.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- hosts: vms\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Register key\n      stat: path=/root/.ssh/id_rsa\n      register: key\n\n    - name: Create keypairs\n      shell: ssh-keygen -t rsa -f /root/.ssh/id_rsa -N ''\n      when: key.stat.exists == False\n\n    - name: Fetch all public ssh keys\n      shell: cat ~/.ssh/id_rsa.pub\n      register: ssh_keys\n\n    - name: Deploy keys on all servers\n      authorized_key: user=root key=\"{{ item[0] }}\"\n      delegate_to: \"{{ item[1] }}\"\n      with_nested: \n        - \"{{ ssh_keys.stdout }}\"\n        - \"{{ groups.all }}\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "89ef04747b14eab88c3aab5fb8a4645575c50bc0", "filename": "playbooks/templates/ifup-local.j2", "repository": "rocknsm/rock", "decoded_content": "#!/bin/bash\n# File: /sbin/ifup-local\n#\n#\n# This script is run after normal sysconfig network-script configuration\n# is performed on RHEL/CentOS-based systems.\n#\n# Parameters:\n# $1: network interface name\n#\n# Post ifup configuration for tuning capture interfaces\n# This is compatible with the ixgbe driver, YMMV\n\n# Change this to something like /tmp/ifup-local.log for troubleshooting\nLOG=/dev/null\n#LOG=/tmp/ifup-local.log\n\ncase $1 in\n{{ rock_monifs | list | join('|') }})\n\n  for i in rx tx sg tso ufo gso gro lro rxvlan txvlan\n  do\n    ethtool -K $1 $i off &>$LOG\n  done\n\n  ethtool -N $1 rx-flow-hash udp4 sdfn  &>$LOG\n  ethtool -N $1 rx-flow-hash udp6 sdfn &>$LOG\n  ethtool -n $1 rx-flow-hash udp6 &>$LOG\n  ethtool -n $1 rx-flow-hash udp4 &>$LOG\n  ethtool -C $1 rx-usecs 10 &>$LOG\n  ethtool -C $1 adaptive-rx off &>$LOG\n  ethtool -G $1 rx 4096 &>$LOG\n\n  # Disable ipv6\n  echo 1 > /proc/sys/net/ipv6/conf/$1/disable_ipv6 &>$LOG\n  echo 0 > /proc/sys/net/ipv6/conf/$1/autoconf &>$LOG\n\n  # Set promiscuous mode\n  ip link set $1 promisc on &>$LOG\n\n  # Just in case ipv6 is already on this interfaces, let's kill it\n  ip addr show dev $1 | grep --silent inet6\n\n  if [ $? -eq 0 ]\n  then\n    ADDR=$(ip addr show dev $1 | grep inet6 | awk '{ print $2 }')\n    ip addr del $ADDR dev $1 &>$LOG\n  fi\n\n;;\n\n*)\n# No post commands needed for this interface\n;;\n\nesac\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "7374ae31e98e686602d540424103ac7621dd55ab", "filename": "roles/moodle/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install Moodle required packages (OS's other than debuntu)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - python-psycopg2\n    - php-pgsql\n  when: not is_debuntu\n\n- name: Install Moodle required packages (debuntu)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - python-psycopg2\n    - php{{ php_version }}-pgsql\n    - php{{ php_version }}-curl\n    #- php{{ php_version }}-zip\n    - php{{ php_version }}-gd\n    #- php{{ php_version }}-mbstring\n    # mbstring is now included in php-cli\n    - php{{ php_version }}-cli\n  when: is_debuntu\n\n- name: php-zip name (debian-9 or ubuntu)\n  package:\n    name: \"php{{ php_version }}-zip\"\n  when: is_debian_9 or is_ubuntu\n\n- name: php-zip name for (debian-8)\n  package:\n    name: php-pclzip\n  when: is_debian_8\n\n- name: Determine if Moodle is already downloaded\n  stat:\n    path: \"{{ moodle_base }}/config-dist.php\"\n  register: moodle\n\n- name: Download the latest Moodle repo\n  git:\n    repo: \"{{ moodle_repo_url }}\"\n    dest: \"{{ moodle_base }}\"\n    depth: 1\n    force: yes\n    version: \"MOODLE_{{ moodle_version }}_STABLE\"\n    #version: master   # TEMPORARY DURING MAY 2018 TESTING, installed 3.5beta+ = https://download.moodle.org/releases/development/\n  #ignore_errors: yes\n  when: internet_available and moodle.stat.exists is defined and not moodle.stat.exists\n\n- name: Prepare the downloaded directory so Apache can install config file\n  file:\n    path: \"{{ moodle_base }}\"\n    owner: \"{{ apache_user }}\"\n    recurse: yes\n    state: directory\n\n- name: Give Apache permission to write Moodle data directory\n  file:\n    path: \"{{ content_base }}/dbdata/moodle\"\n    owner: \"{{ apache_user }}\"\n    mode: 0755\n    state: directory\n\n- name: Create a Moodle data dir with Apache permission to write\n  file:\n    path: \"{{ moodle_data }}\"\n    owner: \"{{ apache_user }}\"\n    group: \"{{ apache_user }}\"\n    mode: 0770\n    state: directory\n\n- name: Remove stock Moodle config file\n  file:\n    path: \"/etc/{{ apache_config_dir }}/moodle.conf\"\n    state: absent\n\n- name: Put Moodle config file in place\n  template:\n    src: 022-moodle.j2\n    dest: \"/etc/{{ apache_config_dir }}/022-moodle.conf\"\n    owner: root\n    group: root\n    mode: 0644\n  when: moodle_enabled\n\n- name: Enable Moodle (debuntu)\n  file:\n    src: /etc/apache2/sites-available/022-moodle.conf\n    dest: /etc/apache2/sites-enabled/022-moodle.conf\n    state: link\n  when: moodle_enabled and is_debuntu\n\n- name: Disable Moodle (debuntu)\n  file:\n    path: /etc/apache2/sites-enabled/022-moodle.conf\n    state: absent\n  when: not moodle_enabled and is_debuntu\n\n- name: Start postgresql-iiab\n  service:\n    name: postgresql-iiab\n    state: restarted\n\n- name: Create db user\n  postgresql_user:\n    name: Admin\n    password: changeme\n    encrypted: yes   # Required by PostgreSQL 10+ e.g. Ubuntu 18.04's PostgreSQL 10.3+, see https://github.com/iiab/iiab/issues/759\n    role_attr_flags: NOSUPERUSER,NOCREATEROLE,NOCREATEDB\n    state: present\n  become: yes\n  become_user: postgres\n\n- name: Create database\n  postgresql_db:\n    name: \"{{ moodle_database_name }}\"\n    encoding: utf8\n    owner: Admin\n    template: template1\n    state: present\n  become: yes\n  become_user: postgres\n\n- name: Put a startup install script in place\n  template:\n    dest: \"{{ moodle_base }}\"\n    src: moodle_installer\n    mode: 0755\n\n- name: Restart postgresql-iiab\n  service:\n    name: postgresql-iiab\n    state: restarted\n    enabled: yes\n  when: moodle_enabled\n\n- name: Restart Apache\n  service:\n    name: \"{{ apache_service }}\"\n    state: restarted\n\n- name: See if config.php exists\n  stat:\n    path: \"{{ moodle_base }}/config.php\"\n  register: config\n\n- name: Execute Moodle startup script\n  shell: '{{ moodle_base }}/moodle_installer'\n  when: config.stat.exists is defined and not config.stat.exists\n\n- name: Give Apache permission to read config file\n  #command: chown -R {{ apache_user }} {{ moodle_base }}\n  file:\n    path: \"{{ moodle_base }}/config.php\"\n    mode: 0644\n\n- name: Add 'moodle' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: moodle\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: Moodle\n    - option: description\n      value: '\"Access the Moodle learning management system.\"'\n    - option: \"moodle_base\"\n      value: \"{{ moodle_base }}\"\n    - option: moodle_enabled\n      value: \"{{ moodle_enabled }}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "38f4469db509d3b637c5c00ed0aa44e3ec83b787", "filename": "roles/xovis/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install Couchdb and other necessary packages\n  package: name={{ item }}\n           state=present\n  with_items:\n    - couchdb\n    - curl\n    - python-pip\n    - nodejs\n    - npm\n  when: internet_available\n\n- name: Determine if xovis is already downloaded\n  stat: path={{ downloadds_dir }}/xovis/xxx\n  register: xovis\n\n- name: download the latest xovis repo\n  git: repo={{ xovis_repo_url }}\n       dest={{ downloads_dir }}/xovis\n       depth=1\n  when: internet_available  and xovis.stat.exists is defined and not xovis.stat.exists\n\n- name: Install node.js package kanso to maintain couchdb\n  npm: name=kanso\n       global=yes\n       path={{ downloads_dir }}\n  when: internet_available\n\n- name: move the xovis repo into place\n  shell: \"cp -rp {{ downloads_dir }}/xovis {{ xovis_root }}\"\n\n- name: Make sure the XO users directory exists\n  file: state=directory\n        dest=/library/users\n        owner=root\n        group=root\n        mode=0755\n\n- name: Install the xovis python dependencies\n  pip: requirements={{ xovis_root }}/process_stats/requirements.txt\n  when: internet_available\n\n- name: Update xovis repo with Chart Heading\n  lineinfile: dest=\"{{ xovis_root }}/index.html\" regexp='(.+)<h1>(.*)</h1>' line='\\1<h1>{{ xovis_chart_heading }}</h1>' backrefs=yes\n\n- name: Allow access to Couchdb from other hosts\n  command: sed -i 's/^\\(bind_address\\s*=\\s*\\).*$/\\10\\.0\\.0\\.0/' /etc/couchdb/default.ini\n\n- name: Enable Couchdb service\n  service: name=couchdb\n           enabled=yes\n           state=started\n  when: xovis_enabled\n\n- name: Wait for couchdb to become ready\n  wait_for: port=5984\n            delay=1\n            timeout=5\n  when: xovis_enabled\n\n- name: Add admin user\n  command: curl -X PUT {{ xovis_target_host }}/_config/admins/{{ xovis_db_user }} -d \"\\\"{{ xovis_db_password }}\\\"\"\n  when: xovis_enabled\n\n- name: Check if db exists\n  shell: \"kanso listdb | grep {{ xovis_db_name }}\"\n  register: found_db\n  ignore_errors: yes\n\n- name: Create Couchdb database if does not already exist\n  command: kanso createdb {{ xovis_db_url }}\n  when: xovis_enabled and found_db.stdout != xovis_db_name\n\n- name: Load initial xovis database\n  command: kanso push {{ xovis_root }} {{ xovis_db_url }}\n  when: xovis_enabled and found_db.stdout != xovis_db_name\n\n- name: Insert Sugar Journal metadata into Couchdb\n  command: \"{{ xovis_root }}/process_stats/process_journal_stats.py dbinsert {{ xovis_db_name }}\n  -d {{ xovis_backup_dir }}\n  --deployment {{ xovis_deployment_name }}\n  --server http://{{ xovis_db_login }}@{{ xovis_target_host }}\"\n  when: xovis_enabled\n\n- name: Add xovis to service list\n  ini_file: dest='{{ service_filelist }}'\n            section=xovis\n            option='{{ item.option }}'\n            value='{{ item.value }}'\n  with_items:\n    - option: name\n      value: xovis\n    - option: description\n      value: '\"XOVis - Analytics and Visualization for Sugar and OLPC\"'\n    - option: installed\n      value: \"{{ xovis_install }}\"\n    - option: enabled\n      value: \"{{ xovis_enabled }}\"\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "a0f834043fe1c982619e1e5b924ca5b0153b3db5", "filename": "roles/filebeat/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n- name: Install filebeat package\n  yum:\n    name: filebeat\n    state: present\n\n- name: Create filebeat config directory\n  file:\n    path: /etc/filebeat/configs\n    mode: 0755\n    owner: root\n    group: root\n    state: directory\n\n- name: Configure filebeat\n  template:\n    src: filebeat.yml.j2\n    dest: /etc/filebeat/filebeat.yml\n  notify: Restart filebeat\n\n- name: Add filebeat configs\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"/etc/filebeat/configs/{{ item.dest }}\"\n  notify: Restart filebeat\n  when: filebeat_configs is defined\n  with_items: \"{{ filebeat_configs }}\"\n\n- name: Enable and start filebeat\n  service:\n    name: filebeat\n    state: \"{{ 'started' if rock_services | selectattr('name', 'equalto', 'filebeat') | map(attribute='enabled') | bool else 'stopped' }}\"\n    enabled: \"{{ rock_services | selectattr('name', 'equalto', 'filebeat') | map(attribute='enabled') | bool }}\"\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "ff34748a4e40a07c413f4b4b3f845bd1c62aa4f5", "filename": "roles/ipaclient/vars/default.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# defaults file for ipaclient\n# vars/default.yml\nipaclient_packages: [ \"ipa-client\", \"python3-libselinux\" ]\n#ansible_python_interpreter: '/usr/bin/python3'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e060cb91ad0c91b21a1d53566a2cdc8dfce0daa3", "filename": "playbooks/provision-idm-server/setup-idm-dns.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: idm-server\n  tasks:\n  - name: Set required ip address for forward dns\n    set_fact:\n      dns_records: \"{{ dns_records|combine({\n        'view': inventory_hostname.split('.')[1:-1],\n        'forward': { 'ip': openstack.private_v4,\n                      'hostname': inventory_hostname.split('.')[0]  },\n        'reverse': { 'hostname': openstack.private_v4.split('.')[-1],\n                      'target': inventory_hostname + '.'  }\n      }, recursive=True) }}\"\n    when:\n    - hosting_infrastructure == 'openstack'\n\n- name: 'Copying dns_records from idm-server hosts'\n  hosts: dns-server\n  tasks:\n   - set_fact:\n       dns_records: \"{{ hostvars[groups['idm-server'][0]]['dns_records'] }}\"\n\n- name: Add DNS records for IdM'\n  import_playbook: ../update-dns-records.yml\n  vars:\n    dns_records_add:\n    - view: \"{{ dns_records.view }}\"\n      zone: \"{{ dns_records.reverse.zone }}\"\n      server: \"{{ dns_records.server }}\"\n      key_name: \"{{ dns_records.reverse.key_name }}\"\n      key_secret: \"{{ dns_records.reverse.key_secret }}\"\n      key_algorithm: \"{{ dns_records.reverse.key_algorithm }}\"\n      entries:\n      - type: ptr\n        hostname: \"{{ dns_records.reverse.hostname }}\"\n        ip: \"{{ dns_records.reverse.target }}\"\n    - view: \"{{ dns_records.view }}\"\n      zone: \"{{ dns_records.forward.zone }}\"\n      server: \"{{ dns_records.server }}\"\n      key_name: \"{{ dns_records.forward.key_name }}\"\n      key_secret: \"{{ dns_records.forward.key_secret }}\"\n      key_algorithm: \"{{ dns_records.forward.key_algorithm }}\"\n      entries:\n      - type: A\n        hostname: \"{{ dns_records.forward.hostname }}\"\n        ip: \"{{ dns_records.forward.ip }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "12abca623301819d612960f5949807ab4c998df9", "filename": "roles/keepalived/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: prereq.yml\n- import_tasks: keepalived-config.yml\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "e85dab5097c90aeb8a3346c4f1dc3631a1d5dd62", "filename": "vagrant/tasks/install_bootstrap_enterprise.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- package:\n    name: atomic-openshift-utils\n    state: present\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "64f98d33b6959aa52bf4efa0873653f7b1bed98d", "filename": "roles/config-versionlock/tasks/versionlock.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Add version locks for specified packages\"\n  shell: >\n    {{ ansible_pkg_mgr }} versionlock {{ item }}\n  with_items:\n    - \"{{ versionlock_packages }}\"\n\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "515126350b528c4800d5df21cf36bcb53e857b74", "filename": "ops/playbooks/install_sysdig.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\r\n# Copyright (2017) Hewlett Packard Enterprise Development LP\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n# You may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n# http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n###\r\n---\r\n- hosts: docker\r\n  gather_facts: true\r\n  become_user: root\r\n  become: true\r\n\r\n  vars_files:\r\n    - ../group_vars/vars\r\n    - ../group_vars/vault\r\n\r\n  environment: \"{{ env }}\"\r\n\r\n  tasks:\r\n    #\r\n    # Determine if firewalld is enabled and set \r\n    # 'firewalld_enabled' to true if enabled\r\n    #\r\n    - name: Determine if firewalld is enabled\r\n      command: systemctl status firewalld\r\n      ignore_errors: yes\r\n      register: firewalld\r\n\r\n    - set_fact:\r\n        firewalld_not_enabled: true\r\n      when: \r\n        - firewalld.rc != 0\r\n\r\n    #\r\n    # Open required firewall ports\r\n    #\r\n    - name: Configure required firewall ports\r\n      firewalld:\r\n        port: 6666/tcp\r\n        permanent: true\r\n        immediate: true\r\n        state: enabled\r\n      when:\r\n        - firewalld_not_enabled is undefined\r\n\r\n    #\r\n    # Verify remote system can open a connection to \r\n    # collector.sysdigcloud.com port 6666 to send Sysdig\r\n    # information to Sysdig's SaaS instance.  If the connection\r\n    # succeeds then set the variable 'sysdig_connection'\r\n    # to 'true' and continue.  Otherwise print an error\r\n    # message explaining that connectivity requirements \r\n    # were not met.\r\n    #\r\n    - name: Check connectivity to collector.sysdigcloud.com port 6666\r\n      wait_for:\r\n        host: collector.sysdigcloud.com\r\n        port: 6666\r\n        state: started\r\n        timeout: 10\r\n        msg: \"Connectivity to collector.sysdigcloud.com port 6666 failed!  Exiting.\"\r\n\r\n    - set_fact:\r\n        sysdig_connection: true\r\n\r\n    #\r\n    # Install requried kernel headers\r\n    #    \r\n    - name: Install Sysdig Agent\r\n\r\n      block:\r\n\r\n      - name: Install Kernel Headers\r\n        yum:\r\n          name: kernel-devel-{{ ansible_kernel }}\r\n          state: latest\r\n          update_cache: yes\r\n\r\n      - name: Install easy_install on RedHat-family Systems\r\n        yum:\r\n          name: python-setuptools\r\n          state: latest\r\n\r\n      when:\r\n        - sysdig_connection is defined\r\n\r\n    #\r\n    # Install PIP\r\n    #\r\n    - name: Install PIP\r\n      easy_install:\r\n        name: pip\r\n        state: latest\r\n\r\n    #\r\n    # Install docker-py\r\n    #\r\n    - name: Install docker-py\r\n      pip:\r\n        name: docker-py\r\n        state: present\r\n\r\n    #\r\n    # Pull latest Sysdig Docker image\r\n    #\r\n    - name: Pull latest Sysdig docker image\r\n      docker_image:\r\n        name: sysdig/agent\r\n\r\n    #\r\n    # Remove any existing Sysdig container\r\n    #\r\n    - name: Remove existing Sysdig docker container\r\n      docker_container:\r\n        name: sysdig-agent\r\n        state: absent\r\n \r\n    #\r\n    # Start Sysdig Docker Container\r\n    #\r\n    - name: Start Sysdig docker container\r\n      docker_container:\r\n        name: sysdig-agent\r\n        image: sysdig/agent\r\n        detach: true\r\n        restart_policy: always\r\n        privileged: true\r\n        network_mode: host\r\n        pid_mode: host\r\n        env:\r\n            ACCESS_KEY: '{{ sysdig_access_key }}'\r\n            TAGS: '{{ sysdig_tags }}'\r\n            ADDITIONAL_CONF: \"security: {enabled: true}\\ncommandlines_capture: {enabled: true}\\nmemdump: {enabled: true}\"\r\n        volumes:\r\n            - /var/run/docker.sock:/host/var/run/docker.sock\r\n            - /dev:/host/dev\r\n            - /proc:/host/proc:ro\r\n            - /boot:/host/boot:ro\r\n            - /lib/modules:/host/lib/modules:ro\r\n            - /usr:/host/usr:ro\r\n        shm_size: 512m\r\n   \r\n\r\n- hosts: nfs,logger,lbs\r\n  vars_files:\r\n    - ../group_vars/vars\r\n    - ../group_vars/vault\r\n\r\n  environment: \"{{ env }}\"\r\n  tasks:\r\n    #\r\n    # Determine if firewalld is enabled and set \r\n    # 'firewalld_enabled' to true if enabled\r\n    #\r\n    - name: Determine if firewalld is enabled\r\n      command: systemctl status firewalld\r\n      ignore_errors: yes\r\n      register: firewalld\r\n\r\n    - set_fact:\r\n        firewalld_not_enabled: true\r\n      when: \r\n        - firewalld.rc != 0\r\n\r\n    #\r\n    # Open required firewall ports\r\n    #\r\n    - name: Configure required firewall ports\r\n      firewalld:\r\n        port: 6666/tcp\r\n        permanent: true\r\n        immediate: true\r\n        state: enabled\r\n      when:\r\n        - firewalld_not_enabled is undefined\r\n\r\n    #\r\n    # Verify remote system can open a connection to \r\n    # collector.sysdigcloud.com port 6666 to send Sysdig\r\n    # information to Sysdig's SaaS instance.  If the connection\r\n    # succeeds then set the variable 'sysdig_connection'\r\n    # to 'true' and continue.  Otherwise print an error\r\n    # message explaining that connectivity requirements \r\n    # were not met.\r\n    #\r\n    - name: Check connectivity to collector.sysdigcloud.com port 6666\r\n      wait_for:\r\n        host: collector.sysdigcloud.com\r\n        port: 6666\r\n        state: started\r\n        timeout: 10\r\n        msg: \"Connectivity to collector.sysdigcloud.com port 6666 failed!  Exiting.\"\r\n\r\n    - set_fact:\r\n        sysdig_connection: true\r\n\r\n    - name: Download Sysdig native Linux agent\r\n      get_url: \r\n       url: \"{{ sysdig_agent }}\"\r\n       dest: /root/install-agent.sh\r\n       mode: 0755\r\n      \r\n    - name: execute install-agent.sh\r\n      command: /root/install-agent.sh --access_key '{{ sysdig_access_key }}' --tags '{{ sysdig_tags }}'\r\n...\r\n     \r\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "ae8048ed7e2c1861e4df8c9e75d876fd1885ff51", "filename": "roles/openshift-applier/tasks/copy-inventory-to-remote.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: \"Create a temporary directory to use\" \n  tempfile:\n    state: directory\n  register: tmp_dir\n  notify:\n  - Clean-up temporary dir\n\n- name: \"Store away the temporary directory path\"\n  set_fact:\n    tmp_inv_dir: \"{{ tmp_dir.path }}/\"\n\n- name: \"Copy local inventory files to target host(s)\"\n  include_tasks: copy-inventory-content.yml\n  loop_control:\n    loop_var: content_entry\n  with_items:\n  - \"{{ openshift_cluster_content | filter_applier_items(filter_tags) | default([]) }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "29929f487e4d26786aac1b39c50fa126cf554971", "filename": "reference-architecture/vmware-ansible/playbooks/add-node.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  connection: local\n  gather_facts: no\n  become: no\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - create-vm-add-prod-ose\n\n- hosts: new_nodes\n  gather_facts: yes\n  become: no\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - instance-groups\n  - rhsm\n  - vmware-guest-setup\n  - docker-storage-setup\n  - openshift-volume-quota\n- include: add-node-prerequisite.yaml\n\n- include: node-setup.yaml\n\n- hosts: loadbalancer, master, infra\n  gather_facts: yes\n  become: no\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - haproxy-server-config\n\n- hosts: single_master\n  gather_facts: yes\n  become: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - ../../../roles/router-scaleup\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "848b8e9464c602b897542f1c4150aa84a513051f", "filename": "roles/dns/manage-dns-zones/tasks/named/determine-action.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Determine if named processing is required\n  set_fact:\n    named_processing: True\n  when:\n    - item.0.named is defined or item.1.named is defined\n  with_subelements:\n    - \"{{ dns_data.views | default({}) }}\"\n    - zones\n"}, {"commit_sha": "6d10af54bdbf8e81c3d90a70ffea87b4d2c20eb2", "sha": "6f18349e26ae5ac124f876032ff8579d0f50325e", "filename": "tasks/main.yml", "repository": "Oefenweb/ansible-wordpress", "decoded_content": "# tasks file for wordpress\n---\n- include: wp-cli.yml\n- include: core.yml\n- include: themes.yml\n- include: plugins.yml\n- include: users.yml\n- include: options.yml\n- include: chown.yml\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "71e32966bd974f4a8632a870e10e27f9f4761676", "filename": "roles/osp/admin-sec-group/test/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "--- \n\nosp_security_groups:\n- name: \"ingress-sec-group\"\n  description: \"My ingress Sec Group\"\n  rules:\n  - protocol: tcp\n    port_range_min: 1022\n    port_range_max: 1022\n    direction: ingress\n    remote_ip_prefix: 0.0.0.0/0\n- name: \"egress-sec-group\"\n  description: \"My egress Sec Group\"\n  rules:\n  - protocol: tcp\n    port_range_min: 1122\n    port_range_max: 1122\n    direction: egress\n    remote_ip_prefix: 0.0.0.0/0\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6be59472041df6efb5b7ef1bcd67946e78c44c3d", "filename": "reference-architecture/vmware-ansible/playbooks/roles/heketi-ocp/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Copy heketi secret config file to master\n  copy:\n    src: ~/heketi-secret.yaml\n    dest: ~/heketi-secret.yaml\n\n- name: Copy heketi secret config file to master\n  copy:\n    src: ~/storage-crs.json\n    dest: ~/storage-crs.json\n\n- name: Switch to default project\n  command: oc project default\n\n- name: Check to see if heketi secret is already created\n  command: \"oc get secrets\"\n  register: oc_secrets\n\n- name: Check to see if storage class is already created\n  command: \"oc get storageclass\"\n  register: storage_class\n\n- name: Add heketi secret\n  command: \"oc create -f ~/heketi-secret.yaml\"\n  when: \"'heketi-secret' not in oc_secrets.stdout\"\n\n- name: Create storage class\n  command: \"oc create -f ~/storage-crs.json\"\n  when: \"'crs-gluster' not in storage_class.stdout\"\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "dcee602670e4f635ba08fc68a4e761a6ae95d6e4", "filename": "roles/ovirt-engine-setup/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# health page\n- name: check if ovirt-engine running (health page)\n  uri:\n    url: \"http://{{ ansible_fqdn }}/ovirt-engine/services/health\"\n    status_code: 200\n  register: ovirt_engine_status\n  retries: 2\n  delay: 5\n  until: ovirt_engine_status|success\n  ignore_errors: True\n\n# copy default answer file\n- name: copy default answerfile\n  template:\n    src: answerfile_{{ ovirt_engine_version }}_basic.txt.j2\n    dest: /tmp/answerfile.txt\n    mode: 0644\n    owner: root\n    group: root\n  when: \"{{ ovirt_engine_answer_file_path is undefined }}\"\n\n# copy custom answer file\n- name: copy custom answer file\n  template:\n    src: \"{{ ovirt_engine_answer_file_path }}\"\n    dest: /tmp/answerfile.txt\n    mode: 0644\n    owner: root\n    group: root\n  when: \"{{ ovirt_engine_answer_file_path is defined }}\"\n\n- name: run engine-setup with answerfile\n  shell: 'engine-setup --config-append=/tmp/answerfile.txt'\n  when: ovirt_engine_status|failed\n  tags:\n    - skip_ansible_lint\n\n- name: check state of database\n  service:\n    name: postgresql\n    state: running\n  when: (ovirt_engine_dwh_db_host == 'localhost' and ovirt_engine_dwh == True) or ovirt_engine_db_host == 'localhost'\n\n- name: check state of engine\n  service:\n    name: ovirt-engine\n    state: running\n\n- name: restart of ovirt-engine service\n  service:\n    name: ovirt-engine\n    state: restarted\n\n- name: check health status of page\n  uri:\n    url: \"http://{{ ansible_fqdn }}/ovirt-engine/services/health\"\n    status_code: 200\n  register: health_page\n  retries: 12\n  delay: 10\n  until: health_page|success\n\n- name: clean tmp files\n  file:\n    path: '/tmp/answerfile.txt'\n    state: 'absent'\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "739bac75be9c32dc7a74198385c7c309947f949c", "filename": "reference-architecture/gcp/ansible/playbooks/roles/dns-zone/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: create dns zone\n  gcdns_zone:\n    zone: '{{ public_hosted_zone }}'\n    description: '{{ public_hosted_zone }} domain'\n    service_account_email: '{{ service_account_id }}'\n    credentials_file: '{{ credentials_file }}'\n    project_id: '{{ gcloud_project }}'\n    state: present\n  register: dns_created\n\n- block:\n  - name: get ns servers\n    command: gcloud --project {{ gcloud_project }} dns managed-zones describe {{ public_hosted_zone | replace('.', '-') }} --format='value(nameServers)'\n    register: ns_servers\n\n  - name: print info about ns servers and fail\n    fail:\n      msg: \"DNS zone '{{ public_hosted_zone }}' didn't exist. It was created, but before the deployment can continue, you have to configure the following NS servers in your domain provider, then run the deployment again: {{ ns_servers.stdout }}\"\n  when: dns_created | changed\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "989e9f1d82560b60b04f027ba26ec010a501c5e6", "filename": "roles/debian_schooltool/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "schooltool_version: schooltool-2.8.5\nschooltool_src: '{{ schooltool_version }}.tar.gz'\ndebian_schooltool_install: True\ndebian_schooltool_enabled: False\n\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "2baa874fc8f1f759ddf2e462aa0341356b15be86", "filename": "roles/ipareplica/vars/CentOS-7.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# defaults file for ipareplica\n# vars/RedHat-7.yml\nipareplica_packages: [ \"ipa-server\", \"libselinux-python\" ]\nipareplica_packages_dns: [ \"ipa-server-dns\" ]\nipareplica_packages_adtrust: [ \"ipa-server-trust-ad\" ]"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "ebdb619f76975495ed19cb89eb1442449fa95ffc", "filename": "ops/playbooks/config_ntp.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- hosts: vms\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n    - name: Update chrony.conf\n      template:\n        src: ../templates/chrony.conf.j2\n        dest: /etc/chrony.conf\n        owner: root\n        group: root\n        mode: 0644        \n      notify: Enable and restart chrony service\n\n    - name: use timedatectl\n      command: timedatectl set-ntp true\n\n  handlers:\n    - name: Enable and restart chrony service\n      systemd:\n        name: chronyd\n        enabled: yes\n        state: restarted\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e5d36a050b7258fd3c26af2b0864428debbf524a", "filename": "roles/virt-install/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Unmount install ISO'\n  mount:\n    path: \"{{ mounted_iso[item] }}\"\n    state: absent\n  with_items:\n  - \"{{ mounted_iso.keys() }}\"\n\n- name: 'Remove authorized_keys'\n  file: \n    path: \"{{ default_http_dir }}/{{ virtinstall_authorized_keys | basename }}\"\n    state: absent\n\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "066d9600ce992851f9be8eba5a45db07e496e98d", "filename": "roles/ssh_tunneling/handlers/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: restart ssh\n  service: name=\"{{ ssh_service_name|default('ssh') }}\" state=restarted\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c048fee9b9bbc87461ac44c1089380639e55b9c7", "filename": "roles/ansible/tower/config-ansible-tower/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# ansible_tower_download_url: http://releases.ansible.com/ansible-tower/setup/ansible-tower-setup-latest.tar.gz\nansible_tower_download_url: https://releases.ansible.com/ansible-tower/setup/ansible-tower-setup-3.3.0-1.tar.gz\n\n# oc clients found at 'https://mirror.openshift.com/pub/openshift-v3/clients/'\nansible_tower_oc_download_url: https://mirror.openshift.com/pub/openshift-v3/clients/3.10.47/linux/oc.tar.gz\n\n# EPEL release can be changed, but default to '-latest'\nansible_tower_epel_download_url: https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n\ndefault_ansible_tower_url: 'https://localhost'\ndefault_ansible_tower_admin_username: 'admin'\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "6123eac9b5c7110e969b10898f9e72f3afddcc06", "filename": "ops/playbooks/roles/ucp/tasks/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n\n#    - name: Open required ports for swarm managers\n#      firewalld:\n#        port: \"{{ item }}\"\n#        immediate: true\n#        permanent: true\n#        state: enabled\n#      with_items: \"{{ role_ports }}\"\n\n    - name: Retrieve a token for the UCP API\n      uri:\n        url: \"https://{{ ARG_UCP_IP }}/auth/login\"\n        headers:\n          Content-Type: application/json\n        method: POST\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        body: '{\"username\":\"{{ ARG_UCP_USER }}\",\"password\":\"{{ ARG_UCP_PASSWORD }}\"}'\n      register: resp\n      until: resp.status == 200\n      retries: 20\n      delay: 5\n\n    - name: Remember the API's token\n      set_fact:\n        auth_token:  \"{{resp.json.auth_token}}\"\n\n    - name: Is the node already in the swarm\n      uri:\n        url: 'https://{{ ARG_UCP_IP }}/nodes?filters={\"name\":{\"{{ inventory_hostname }}.{{ domain_name }}\":true}}'\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        validate_certs: no\n      register: resp\n\n    - set_fact:\n        swarm_member: \"{% if resp.json[0] is defined %}true{% else %}false{% endif %}\"  \n\n    - debug:\n        var: swarm_member\n      when: _debug is defined\n\n    - name: Retrieve a manager token\n      uri:\n        url: \"https://{{ ARG_UCP_IP }}/swarm\"\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      register: resp\n\n    - name: Memorize the swarm's token\n      set_fact:\n        token:  \"{{ resp.json.JoinTokens.Manager }}\"\n\n    - name: Add node to the swarm\n      command: \"docker swarm join --token {{ token }} {{ ARG_ADVERTIZE_IP }}\"\n      retries: 20\n      delay: 10\n      failed_when: false\n      when: swarm_member == false\n\n#    - name: Join the swarm\n#      uri:\n#        url: 'https://{{ ARG_UCP_IP }}/swarm/join'\n#        headers:\n#          Content-Type: application/json\n#          Authorization: Bearer {{ auth_token }}\n#        method: POST\n#        status_code: 200\n#        body_format: json\n#        body: |\n#              '{\n#                \"AdvertiseAddr\": \"\",\n#                \"Availability\": \"\",\n#                \"DataPathAddr\": \"\",\n#                \"JoinToken\": \"{{ token }}\",\n#                \"ListenAddr\": \"0.0.0.0:2377\",\n#                \"RemoteAddrs\": [\"{{ ARG_ADVERTIZE_IP }}\"]\n#               }'\n#        validate_certs: no\n#      vars:\n#         advaddr:  \"{{ ip_addr | ipaddr('address') }}\"\n#      register: resp\n#      when: swarm_member == false\n\n    - name: Is the node already in the swarm\n      uri:\n        url: 'https://{{ ARG_UCP_IP }}/nodes/{{ inventory_hostname }}.{{ domain_name }}'\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200,404\n        body_format: json\n        validate_certs: no\n      delegate_to: localhost\n      register: resp\n      until: resp.status == 200 and resp.json.Spec.Role == \"manager\" and resp.json.Status.State == \"ready\" \n      delay: 10\n      retries:  \"{{ 1 + ( ucp_role_join_delay  / 10 ) | int }}\"\n\n    - debug: msg=\"Availability={{resp.json.Spec.Availability}} Role={{resp.json.Spec.Role}} State={{resp.json.Status.State}}\"\n      when: _debug is defined\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "98278cef15c017b9026ed5b58ff1b1ba2dfa3b06", "filename": "roles/dns_adblocking/handlers/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: restart dnsmasq\n  service: name=dnsmasq state=restarted\n\n- name: restart apparmor\n  service: name=apparmor state=restarted\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "a9420601f248a200ad9261db79816d15be25030e", "filename": "roles/serverspec/meta/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\ngalaxy_info:\n  author: your name\n  description:\n  company: Capgemini\n  # If the issue tracker for your role is not on github, uncomment the\n  # next line and provide a value\n  # issue_tracker_url: http://example.com/issue/tracker\n  # Some suggested licenses:\n  # - BSD (default)\n  # - MIT\n  # - GPLv2\n  # - GPLv3\n  # - Apache\n  # - CC-BY\n  license: license (GPLv2, CC-BY, etc)\n  min_ansible_version: 1.2\n  #\n  # Below are all platforms currently available. Just uncomment\n  # the ones that apply to your role. If you don't see your\n  # platform on this list, let us know and we'll get it added!\n  #\n  #platforms:\n  #- name: EL\n  #  versions:\n  #  - all\n  #  - 5\n  #  - 6\n  #  - 7\n  #- name: GenericUNIX\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Fedora\n  #  versions:\n  #  - all\n  #  - 16\n  #  - 17\n  #  - 18\n  #  - 19\n  #  - 20\n  #- name: SmartOS\n  #  versions:\n  #  - all\n  #  - any\n  #- name: opensuse\n  #  versions:\n  #  - all\n  #  - 12.1\n  #  - 12.2\n  #  - 12.3\n  #  - 13.1\n  #  - 13.2\n  #- name: Amazon\n  #  versions:\n  #  - all\n  #  - 2013.03\n  #  - 2013.09\n  #- name: GenericBSD\n  #  versions:\n  #  - all\n  #  - any\n  #- name: FreeBSD\n  #  versions:\n  #  - all\n  #  - 8.0\n  #  - 8.1\n  #  - 8.2\n  #  - 8.3\n  #  - 8.4\n  #  - 9.0\n  #  - 9.1\n  #  - 9.1\n  #  - 9.2\n  #- name: Ubuntu\n  #  versions:\n  #  - all\n  #  - lucid\n  #  - maverick\n  #  - natty\n  #  - oneiric\n  #  - precise\n  #  - quantal\n  #  - raring\n  #  - saucy\n  #  - trusty\n  #- name: SLES\n  #  versions:\n  #  - all\n  #  - 10SP3\n  #  - 10SP4\n  #  - 11\n  #  - 11SP1\n  #  - 11SP2\n  #  - 11SP3\n  #- name: GenericLinux\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Debian\n  #  versions:\n  #  - all\n  #  - etch\n  #  - lenny\n  #  - squeeze\n  #  - wheezy\n  #\n  # Below are all categories currently available. Just as with\n  # the platforms above, uncomment those that apply to your role.\n  #\n  #categories:\n  #- cloud\n  #- cloud:ec2\n  #- cloud:gce\n  #- cloud:rax\n  #- clustering\n  #- database\n  #- database:nosql\n  #- database:sql\n  #- development\n  #- monitoring\n  #- networking\n  #- packaging\n  #- system\n  #- web\ndependencies: []\n  # List your role dependencies here, one per line.\n  # Be sure to remove the '[]' above if you add dependencies\n  # to this list.\n\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "60fafed2c06bc936bb92049d1efafca72d6eca5f", "filename": "roles/client/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: Gather Facts\n  setup:\n\n- name: Include system based facts and tasks\n  import_tasks: systems/main.yml\n\n- name: Install prerequisites\n  package: name=\"{{ item }}\" state=present\n  with_items:\n    - \"{{ prerequisites }}\"\n\n- name: Install strongSwan\n  package: name=strongswan state=present\n\n- name: Setup the ipsec config\n  template:\n    src: \"roles/vpn/templates/client_ipsec.conf.j2\"\n    dest: \"{{ configs_prefix }}/ipsec.{{ IP_subject_alt_name }}.conf\"\n    mode: '0644'\n  with_items:\n    - \"{{ vpn_user }}\"\n  notify:\n    - restart strongswan\n\n- name: Setup the ipsec secrets\n  template:\n    src: \"roles/vpn/templates/client_ipsec.secrets.j2\"\n    dest: \"{{ configs_prefix }}/ipsec.{{ IP_subject_alt_name }}.secrets\"\n    mode: '0600'\n  with_items:\n    - \"{{ vpn_user }}\"\n  notify:\n    - restart strongswan\n\n- name: Include additional ipsec config\n  lineinfile:\n    dest: \"{{ item.dest }}\"\n    line: \"{{ item.line }}\"\n    create: yes\n  with_items:\n    - dest: \"{{ configs_prefix }}/ipsec.conf\"\n      line: \"include ipsec.{{ IP_subject_alt_name }}.conf\"\n    - dest: \"{{ configs_prefix }}/ipsec.secrets\"\n      line: \"include ipsec.{{ IP_subject_alt_name }}.secrets\"\n  notify:\n    - restart strongswan\n\n- name: Setup the certificates and keys\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n  with_items:\n    - src: \"configs/{{ IP_subject_alt_name }}/pki/certs/{{ vpn_user }}.crt\"\n      dest: \"{{ configs_prefix }}/ipsec.d/certs/{{ vpn_user }}.crt\"\n    - src: \"configs/{{ IP_subject_alt_name }}/pki/cacert.pem\"\n      dest: \"{{ configs_prefix }}/ipsec.d/cacerts/{{ IP_subject_alt_name }}.pem\"\n    - src: \"configs/{{ IP_subject_alt_name }}/pki/private/{{ vpn_user }}.key\"\n      dest: \"{{ configs_prefix }}/ipsec.d/private/{{ vpn_user }}.key\"\n  notify:\n    - restart strongswan\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "5cd7c1baf014c9e985589f90a79b1256cad522d6", "filename": "playbooks/roles/docket/tasks/docket_config.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: docker | check existing secret_key\n  shell: >\n    cat /etc/docket/prod.yaml | awk '/^SECRET_KEY/ {print $2}'\n  register: docket_prod\n  changed_when: false\n\n- debug: msg=\"{{ docket_prod }}\"\n\n- name: docket | keep existing secret_key\n  set_fact: docket_secret=\"{{ docket_prod.stdout }}\"\n  when: '\"CHANGE_THIS\" not in docket_prod.stdout'\n\n- name: docket | set production docket config\n  template:\n    src:  docket_prod.yaml.j2\n    dest: /etc/docket/prod.yaml\n  notify:\n    - docket | restart docket uwsgi\n    - docket | restart docket celery services\n\n- name: docket | enable redis\n  service:\n    name: redis\n    enabled: yes\n  notify: docket | restart redis\n  when: docket_enable\n\n- name: docket | enable docket celery services\n  service:\n    name: \"{{ item }}\"\n    enabled: \"{{ docket_enable | bool }}\"\n  notify: docket | restart docket celery services\n  with_items:\n    - docket-celery-io\n    - docket-celery-query\n\n- name: docket | enable docket uwsgi service\n  service:\n    name: docket\n    enabled: \"{{ docket_enable | bool }}\"\n  notify: docket | restart docket uwsgi\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f4b01db517cd9a0cf7cbcb02412be52910ac6ce2", "filename": "roles/dns/manage-dns-records/tasks/route53/process-records.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Print WARNING for views other than 'public' and 'private'\"\n  fail:\n    msg: \"WARNING: only 'public' and 'private' views are processed for Route53\"\n  ignore_errors: True\n  when:\n    - dns.0.name != 'private'\n    - dns.0.name != 'public'\n\n- name: \"Manage Route53 DNS records for view: {{ dns.0.name }}, zone: {{ dns.1.dns_domain }}\"\n  route53:\n    zone: \"{{ dns.1.dns_domain }}\"\n    record: \"{{ item.record }}\"\n    value: \"{{ item.value | default(omit) }}\"\n    type: \"{{ item.type }}\"\n    ttl: \"{{ item.ttl | default(omit) }}\"\n    state: \"{{ item.state | default(present) }}\"\n  with_items:\n    - \"{{ dns.1.entries }}\"\n  when:\n    - dns.0.name == 'private' or dns.0.name == 'public'\n    - dns.1.entries is defined\n    - dns.1.route53 is defined\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "9339113c788c19e69895e392c76c28e7596f443f", "filename": "roles/cloud-gce/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- set_fact:\n    credentials_file_path: \"{{ credentials_file | default(lookup('env','GCE_CREDENTIALS_FILE_PATH')) }}\"\n    ssh_public_key_lookup: \"{{ lookup('file', '{{ SSH_keys.public }}') }}\"\n\n- set_fact:\n    credentials_file_lookup: \"{{ lookup('file', '{{ credentials_file_path }}') }}\"\n\n- set_fact:\n    service_account_email: \"{{ credentials_file_lookup.client_email | default(lookup('env','GCE_EMAIL')) }}\"\n    project_id: \"{{ credentials_file_lookup.project_id | default(lookup('env','GCE_PROJECT')) }}\"\n\n- name: \"Creating a new instance...\"\n  gce:\n    instance_names: \"{{ server_name }}\"\n    zone: \"{{ zone }}\"\n    machine_type: f1-micro\n    image: ubuntu-1604\n    service_account_email: \"{{ service_account_email  }}\"\n    credentials_file: \"{{ credentials_file_path  }}\"\n    project_id: \"{{ project_id  }}\"\n    metadata: '{\"sshKeys\":\"root:{{ ssh_public_key_lookup }}\"}'\n    tags:\n      - \"environment-algo\"\n  register: google_vm\n\n- name: Add the instance to an inventory group\n  add_host:\n    name: \"{{ google_vm.instance_data[0].public_ip }}\"\n    groups: vpn-host\n    ansible_ssh_user: ubuntu\n    ansible_python_interpreter: \"/usr/bin/python2.7\"\n    ansible_ssh_private_key_file: \"{{ SSH_keys.private }}\"\n    cloud_provider: gce\n    ipv6_support: no\n\n- name: Firewall configured\n  local_action:\n    module: gce_net\n    name: \"{{ google_vm.instance_data[0].network }}\"\n    fwname: \"algo-ikev2\"\n    allowed: \"udp:500,4500;tcp:22\"\n    state: \"present\"\n    src_range: 0.0.0.0/0\n    service_account_email: \"{{ credentials_file_lookup.client_email }}\"\n    credentials_file: \"{{ credentials_file  }}\"\n    project_id: \"{{ credentials_file_lookup.project_id }}\"\n\n- set_fact:\n    cloud_instance_ip: \"{{ google_vm.instance_data[0].public_ip }}\"\n\n- name: Ensure the group gce exists in the dynamic inventory file\n  lineinfile:\n    state: present\n    dest: configs/inventory.dynamic\n    line: '[gce]'\n\n- name: Populate the dynamic inventory\n  lineinfile:\n    state: present\n    dest: configs/inventory.dynamic\n    insertafter: '\\[gce\\]'\n    regexp: \"^{{ google_vm.instance_data[0].public_ip }}.*\"\n    line: \"{{ google_vm.instance_data[0].public_ip }}\"\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "5b1eef96698f2936f1da6c44e5b76ee879109bfe", "filename": "roles/dockerbench/meta/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\ngalaxy_info:\n  author: Cam Parry\n  description:\n  company: Capgemini\n  # Some suggested licenses:\n  # - BSD (default)\n  # - MIT\n  # - GPLv2\n  # - GPLv3\n  # - Apache\n  # - CC-BY\n  license: license (MIT)\n  min_ansible_version: 1.2\n  min_docker_version: 1.6\n  #\n  # Below are all platforms currently available. Just uncomment\n  # the ones that apply to your role. If you don't see your\n  # platform on this list, let us know and we'll get it added!\n  #\n  platforms:\n  #- name: EL\n  #  versions:\n  #  - all\n  #  - 5\n  #  - 6\n  #  - 7\n  #- name: GenericUNIX\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Fedora\n  #  versions:\n  #  - all\n  #  - 16\n  #  - 17\n  #  - 18\n  #  - 19\n  #  - 20\n  #- name: SmartOS\n  #  versions:\n  #  - all\n  #  - any\n  #- name: opensuse\n  #  versions:\n  #  - all\n  #  - 12.1\n  #  - 12.2\n  #  - 12.3\n  #  - 13.1\n  #  - 13.2\n  #- name: Amazon\n  #  versions:\n  #  - all\n  #  - 2013.03\n  #  - 2013.09\n  #- name: GenericBSD\n  #  versions:\n  #  - all\n  #  - any\n  #- name: FreeBSD\n  #  versions:\n  #  - all\n  #  - 8.0\n  #  - 8.1\n  #  - 8.2\n  #  - 8.3\n  #  - 8.4\n  #  - 9.0\n  #  - 9.1\n  #  - 9.1\n  #  - 9.2\n  - name: Ubuntu\n    versions:\n  #  - all\n  #  - lucid\n  #  - maverick\n  #  - natty\n  #  - oneiric\n  #  - precise\n  #  - quantal\n  #  - raring\n  #  - saucy\n     - trusty\n  #- name: SLES\n  #  versions:\n  #  - all\n  #  - 10SP3\n  #  - 10SP4\n  #  - 11\n  #  - 11SP1\n  #  - 11SP2\n  #  - 11SP3\n  #- name: GenericLinux\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Debian\n  #  versions:\n  #  - all\n  #  - etch\n  #  - lenny\n  #  - squeeze\n  #  - wheezy\n  #\n  # Below are all categories currently available. Just as with\n  # the platforms above, uncomment those that apply to your role.\n  #\n  categories:\n  - cloud\n  #- cloud:ec2\n  #- cloud:gce\n  #- cloud:rax\n  #- clustering\n  #- database\n  #- database:nosql\n  #- database:sql\n  #- development\n  #- monitoring\n  #- networking\n  #- packaging\n  - system\n  #- web\ndependencies: []\n  # List your role dependencies here, one per line. Only\n  # dependencies available via galaxy should be listed here.\n  # Be sure to remove the '[]' above if you add dependencies\n  # to this list.\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "bff7770809b5a41883b955b6dc9cd983091254ae", "filename": "reference-architecture/azure-ansible/3.6/ansibledeployocp/playbooks/roles/azure-delete/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Destroy Azure Deploy\n  azure_rm_deployment:\n    state: absent\n    location: \"{{ location }}\"\n    resource_group_name: \"{{ resourcegroupname }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "f85bba167509c3f211b86ed1e5aae71852548757", "filename": "reference-architecture/gcp/ansible/playbooks/roles/temp-instance-disk-delete/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: delete temp instance disk\n  gce_pd:\n    name: '{{ prefix }}-tmp-instance'\n    zone: '{{ gcloud_zone }}'\n    service_account_email: '{{ service_account_id }}'\n    credentials_file: '{{ credentials_file }}'\n    project_id: '{{ gcloud_project }}'\n    state: absent\n  ignore_errors: true\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "bdada6fea1ed62d17569396de901b5a27b862733", "filename": "roles/dns_encryption/tasks/freebsd.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Install dnscrypt-proxy\n  package:\n    name: dnscrypt-proxy2\n\n- name: Enable mac_portacl\n  lineinfile:\n    path: /etc/rc.conf\n    line: 'dnscrypt_proxy_mac_portacl_enable=\"YES\"'\n  when: listen_port|int == 53\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "8db59137457761bf1c9d0b2e9f9fe89907715410", "filename": "roles/common/defaults/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\nopenshift_cluster_node_labels:\n  app:\n    region: primary\n  infra:\n    region: infra\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "aec575e95497df55956ccbbc51c9453d4dd20172", "filename": "playbooks/roles/stenographer/tasks/deploy.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- import_tasks: prechecks.yml\n- import_tasks: install.yml\n- import_tasks: config.yml\n...\n"}, {"commit_sha": "a10c5f4577e6e74feb1fadec4bcbab039b8b180a", "sha": "c9889d345c7dcaee0b6042d0198a10454e804adb", "filename": "tasks/configure-docker/configure-docker-plugins.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Fetch Docker daemon status\n  become: true\n  service:\n    name: docker\n  register: _docker_status\n\n- name: Start Docker daemon\n  become: yes\n  service:\n    name: docker\n    state: started\n  when:\n    - _docker_status.status is defined\n    - _docker_status.status.SubState is defined\n    - _docker_status.status.SubState != \"running\"\n\n- name: Wait for Docker daemon to started\n  become: true\n  shell: docker info\n  register: _docker_info\n  until: _docker_info.rc == 0\n  retries: 10\n  changed_when: false\n  tags:\n    - skip_ansible_lint\n\n- name: Install Docker plugins\n  become: true\n  shell: \"(docker plugin install --grant-all-permissions --alias {{ item.alias | default(item.name) }} {{ item.name }} \\\n    && echo 'installed') || echo 'nop'\"\n  with_items: \"{{ docker_plugins }}\"\n  register: _docker_plugin_install\n  changed_when: _docker_plugin_install.stdout_lines | last == 'installed'\n\n- name: Reset list of authorization plugins\n  set_fact:\n    _authz_plugins: []\n\n- name: Create list of authorization plugins\n  set_fact:\n    _authz_plugins: \"{{ _authz_plugins + [item.alias | default(item.name)] }}\"\n  with_items: \"{{ docker_plugins }}\"\n  when:\n    - item.type == 'authz'\n\n- name: Update Docker daemon configuration with authorization plugins\n  set_fact:\n    docker_daemon_config: \"{{ docker_daemon_config | combine(_updated_item, recursive=true) }}\"\n  vars:\n    _updated_item: \"{ 'authorization-plugins': {{ _authz_plugins | list }} }\"\n\n- name: Update Docker daemon (variables)\n  become: true\n  copy:\n    content: \"{{ docker_daemon_config | to_nice_json }}\"\n    dest: /etc/docker/daemon.json\n  notify: restart docker\n  when:\n    - docker_daemon_config_file is not defined\n    - docker_daemon_config is defined\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "fa1afe2715a8a1dd512028695ad8458505437b67", "filename": "roles/docket/tasks/prereqs.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# prepreqs checks for rocknsm.docket\n\n# Validate hosts exist in stenographer group\n- name: Check for docket and stenographer hosts\n  assert:\n    that:\n      - \"{{ ('docket' in groups) and (groups['docket'] | length) > 0 }}\"\n      - \"{{ ('stenographer' in groups) and (groups['stenographer'] | length) > 0 }}\"\n    msg: \"The [docket] and [stenographer] inventory groups must each have at least one host.\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "0a9b5156677b47deb5e1d06ddb35e01a7322439b", "filename": "roles/nfs-server/tasks/lvm.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Check if LV has been created and mounted\"\n  shell: \"lsblk {{ nfs_storage_device }} | egrep 'lvm.*/exports'\"\n  register: lvm_check\n  ignore_errors: yes\n\n- include_tasks: configure_lvm.yml\n  when: lvm_check.rc != 0\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "1cef975ec390987689da84a8de85fc979515cd80", "filename": "playbooks/openshift/delete-openstack.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- hosts: localhost\n  pre_tasks:\n  - import_tasks: pre-tasks.yml\n  roles:\n  - role: openshift-ansible-contrib/roles/openstack-stack\n    stack_name: \"{{ env_id }}.{{ dns_domain }}\"\n    stack_state: 'absent'\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "45461aa09a5b5838dedd0f5fbdede713a7526bb7", "filename": "roles/ovirt-engine-remote-db/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# main file for remote DB task\n# based on https://fedoraproject.org/wiki/PostgreSQL\n\n- name: check PostgreSQL service\n  service:\n    name: postgresql\n    state: running\n  register: postgresql_status\n  ignore_errors: True\n\n# install libselinux-python on machine - selinux policy\n- name: install libselinux-python for ansible\n  yum:\n    name: libselinux-python\n    state: \"present\"\n  when: postgresql_status|failed\n\n# for semanage utility\n- name: install policycoreutils-python for changing selinux port\n  yum:\n    name: policycoreutils-python\n    state: \"present\"\n  when: postgresql_status|failed\n\n- name: yum install PostgreSQL\n  yum:\n    name: \"postgresql-server\"\n    state: installed\n    update_cache: yes\n  when: postgresql_status|failed\n\n- name: run PostgreSQL initdb\n  become_user: postgres\n  become: yes\n  shell: '/usr/bin/initdb -D /var/lib/pgsql/data'\n  when: postgresql_status|failed\n  tags:\n    - skip_ansible_lint\n\n- name: start PostgreSQL service\n  service:\n    name: postgresql\n    state: started\n    enabled: yes\n\n# allow access engine database access from outside\n- name: update pg_hba.conf -> host ovirt_engine_db_name ovirt_engine_db_user 0.0.0.0/0 md5\n  lineinfile:\n    dest: '/var/lib/pgsql/data/pg_hba.conf'\n    insertafter: EOF\n    line: \"host {{ovirt_engine_db_name}} {{ovirt_engine_db_user}} 0.0.0.0/0 md5\"\n  when: ovirt_engine_remote_db == True\n\n# allow access dwh database access from outside\n- name: update pg_hba.conf -> host ovirt_engine_db_dwh_name ovirt_engine_db_dwh_user 0.0.0.0/0 md5\n  lineinfile:\n    dest: '/var/lib/pgsql/data/pg_hba.conf'\n    insertafter: EOF\n    line: \"host {{ovirt_engine_dwh_db_name}} {{ovirt_engine_dwh_db_user}} 0.0.0.0/0 md5\"\n  when: ovirt_engine_dwh_remote_db == True\n\n# listen on specific address\n- name: update postgresql.conf -> listen_addresses='*'\n  lineinfile:\n    dest: '/var/lib/pgsql/data/postgresql.conf'\n    insertafter: EOF\n    line: \"listen_addresses='{{ovirt_engine_remote_db_listen_address}}'\"\n  when: postgresql_status|failed\n\n# listen on specific port\n- name: update postgresql.conf -> port number\n  lineinfile:\n    dest: '/var/lib/pgsql/data/postgresql.conf'\n    insertafter: EOF\n    line: \"port={{ovirt_engine_remote_db_port}}\"\n  when: postgresql_status|failed and ovirt_engine_remote_db_port != 5432\n\n# postgresql.conf: (el7)\n# Note: In RHEL/Fedora installations, you can't set the port number here;\n#   adjust it in the service file instead.\n#   /usr/lib/systemd/system/postgresql.service\n#    - Environment=PGPORT=5432\n- name: update postgresql.conf -> port number in service file (Fedora & RHEL)\n  lineinfile:\n    dest: '/usr/lib/systemd/system/postgresql.service'\n    backrefs: yes\n    regexp: \"Environment=PGPORT=5432\"\n    line: \"Environment=PGPORT={{ovirt_engine_remote_db_port}}\"\n  register: port_update\n  when: postgresql_status|failed and ovirt_engine_remote_db_port != 5432\n  ignore_errors: True\n\n# daemon reload - service file was changed\n- name: systemctl daemon-reload (el7)\n  shell: 'systemctl daemon-reload'\n  when: postgresql_status|failed and ovirt_engine_remote_db_port != 5432 and port_update|success\n  tags:\n    - skip_ansible_lint\n\n# el6 use only service (systemctl not present)\n- name: update postgresql.conf -> port number in service file (el6)\n  lineinfile:\n    dest: '/etc/init.d/postgresql'\n    backrefs: yes\n    regexp: \"PGPORT=5432\"\n    line: \"PGPORT={{ovirt_engine_remote_db_port}}\"\n  when: postgresql_status|failed and ovirt_engine_remote_db_port != 5432 and port_update|failed\n  ignore_errors: True\n\n# allow selinux for postgresql non-standard port\n- name: allow selinux for non-standard port\n  shell: 'semanage port -a -t postgresql_port_t -p tcp {{ovirt_engine_remote_db_port}}'\n  when: postgresql_status|failed and ovirt_engine_remote_db_port != 5432\n  ignore_errors: True\n  tags:\n    - skip_ansible_lint\n\n# first check of PostgreSQL - if fail, setup\n- name: PostgreSQL reload configuration\n  service:\n    name: postgresql\n    state: restarted\n\n- name: check iptables service\n  service:\n    name: iptables\n    state: running\n  register: iptables_status\n  when: postgresql_status|failed\n  ignore_errors: True\n\n- name: open port for PostgreSQL in iptables\n  shell: \"iptables -I INPUT -p tcp -m state --state NEW -m tcp --dport {{ovirt_engine_remote_db_port}} -j ACCEPT\"\n  when: postgresql_status|failed and not iptables_status|failed\n  tags:\n    - skip_ansible_lint\n\n- name: save iptables rules\n  shell: \"/sbin/iptables-save\"\n  when: postgresql_status|failed and not iptables_status|failed\n  tags:\n    - skip_ansible_lint\n\n- name: check firewalld service\n  service:\n    name: firewalld\n    state: running\n  register: firewalld_status\n  when: postgresql_status|failed\n  ignore_errors: True\n\n- name: open port for PostgreSQL in firewalld\n  firewalld:\n    port: \"{{ovirt_engine_remote_db_port|int}}/tcp\"\n    permanent: True\n    state: enabled\n  when: postgresql_status|failed and not firewalld_status|failed\n\n- name: reload firewalld\n  shell: \"firewall-cmd --reload\"\n  when: postgresql_status|failed and not firewalld_status|failed\n  tags:\n    - skip_ansible_lint\n\n- name: creating directory for sql scripts in /tmp/ansible-sql\n  file:\n    path: /tmp/ansible-sql\n    state: directory\n\n- name: copy SQL scripts\n  template:\n    src: \"{{item}}.j2\"\n    dest: \"/tmp/ansible-sql/{{item}}\"\n    mode: 0644\n    owner: postgres\n    group: postgres\n  with_items:\n    - \"ovirt-engine-db-create.sql\"\n    - \"ovirt-engine-db-user-create.sql\"\n    - \"ovirt-engine-dwh-db-create.sql\"\n    - \"ovirt-engine-dwh-db-user-create.sql\"\n\n- name: create engine DB and user\n  become_user: postgres\n  become: yes\n  command: psql -p {{ovirt_engine_remote_db_port}} -a -f /tmp/ansible-sql/'{{item}}'\n  with_items:\n    - \"ovirt-engine-db-user-create.sql\"\n    - \"ovirt-engine-db-create.sql\"\n  when: ovirt_engine_remote_db == True\n\n- name: create engine DWH DB and user\n  become_user: postgres\n  become: yes\n  command: psql -p {{ovirt_engine_remote_db_port}} -a -f /tmp/ansible-sql/'{{item}}'\n  with_items:\n    - \"ovirt-engine-dwh-db-user-create.sql\"\n    - \"ovirt-engine-dwh-db-create.sql\"\n  when: ovirt_engine_dwh_remote_db == True\n\n- name: check PostgreSQL service\n  service:\n    name: postgresql\n    state: started\n    enabled: yes\n\n- name: clean tmp files\n  file:\n    path: '/tmp/ansible-sql'\n    state: 'absent'\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "bf25230c6c8279c150360b50ed487f06a3391abc", "filename": "tasks/section_02_level2.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 2.18 Disable Mounting of cramfs Filesystems (Not Scored)\n    lineinfile: >\n        dest=/etc/modprobe.d/CIS.conf\n        line='install cramfs /bin/true'\n        state=present\n        create=yes\n    tags:\n      - section2\n      - section2.18\n\n  - name: 2.19 Disable Mounting of freevxfs Filesystems (Not Scored)\n    lineinfile: >\n        dest=/etc/modprobe.d/CIS.conf\n        line='install freevxfs /bin/true'\n        state=present\n        create=yes\n    tags:\n      - section2\n      - section2.19\n\n  - name: 2.20 Disable Mounting of jffs2 Filesystems (Not Scored)\n    lineinfile: >\n        dest=/etc/modprobe.d/CIS.conf\n        line='install jffs2 /bin/true'\n        state=present\n        create=yes\n    tags:\n      - section2\n      - section2.20\n\n  - name: 2.21 Disable Mounting of hfs Filesystems (Not Scored)\n    lineinfile: >\n        dest=/etc/modprobe.d/CIS.conf\n        line='install hfs /bin/true'\n        state=present\n        create=yes\n    tags:\n      - section2\n      - section2.21\n\n  - name: 2.22 Disable Mounting of hfsplus Filesystems (Not Scored)\n    lineinfile: >\n        dest=/etc/modprobe.d/CIS.conf\n        line='install hfsplus /bin/true'\n        state=present\n        create=yes\n    tags:\n      - section2\n      - section2.22\n\n  - name: 2.23 Disable Mounting of squashfs Filesystems (Not Scored)\n    lineinfile: >\n        dest=/etc/modprobe.d/CIS.conf\n        line='install squashfs /bin/true'\n        state=present\n        create=yes\n    tags:\n      - section2\n      - section2.23\n\n  - name: 2.24 Disable Mounting of udf Filesystems (Not Scored)\n    lineinfile: >\n        dest=/etc/modprobe.d/CIS.conf\n        line='install udf /bin/true'\n        state=present\n        create=yes\n    tags:\n      - section2\n      - section2.24\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "f421a24af4ec2621d2856842ec477aab5fe5453f", "filename": "tasks/Linux/fetch/openjdk-fallback.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: 'Fetch root page {{ openjdk_root_page }}'\n  uri:\n    url: '{{ openjdk_root_page }}'\n    return_content: true\n  register: root_page\n\n- name: Find GA release version\n  set_fact:\n    java_major_version: >-\n      {{ root_page['content']\n        | regex_findall('Ready for use:.*>JDK ([\\d]+)<')\n        | first\n      }}\n\n- name: Out java_major_version\n  debug:\n    var: java_major_version\n\n- name: Fetch GA release page\n  uri:\n    url: '{{ openjdk_root_page }}/{{ java_major_version }}/'\n    return_content: true\n  register: ga_release_page\n\n- name: Find release url\n  set_fact:\n    release_url: >-\n      {{ ga_release_page['content']\n        | regex_findall('(https://download[\\.\\w]+/java/GA/jdk'\n          + java_major_version|string + '[.\\d]*/[\\d\\w]+/'\n          + '[.\\d]+/GPL/openjdk-'\n          + java_major_version|string + '[\\d._]+linux-x64_bin[\\w\\d.]+)')\n      }}\n\n- name: Exit if OpenJDK version is not General-Availability Release\n  fail:\n    msg: 'OpenJDK version {{ java_major_version }} not GA Release'\n  when: release_url[1] is not defined\n\n- name: 'Get artifact checksum {{ release_url[1] }}'\n  uri:\n    url: '{{ release_url[1] }}'\n    return_content: true\n  register: artifact_checksum\n\n- name: 'Download artifact from {{ release_url[0] }}'\n  get_url:\n    url: '{{ release_url[0] }}'\n    dest: '{{ java_download_path }}'\n    checksum: 'sha256:{{ artifact_checksum.content }}'\n  register: file_downloaded\n  retries: 20\n  delay: 5\n  until: file_downloaded is succeeded\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "0c3b763f9a15bdb3ac7616723ca44561b145e60a", "filename": "reference-architecture/gcp/ansible/playbooks/gold-image-include.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: create temp instance for gold image creation\n  hosts: localhost\n  roles:\n  - role: deployment-create\n    deployment_name: tmp-instance\n  post_tasks:\n  - name: refresh gce inventory\n    command: '{{ inventory_dir }}/gce/hosts/gce.py --refresh-cache'\n    changed_when: false\n  - meta: refresh_inventory\n  - name: configure ssh for temp instance\n    include_role:\n      name: ssh-config-tmp-instance\n  - name: wait for the temp instance to come up\n    wait_for:\n      host: '{{ hostvars[prefix + \"-tmp-instance\"][\"gce_public_ip\"] }}'\n      port: 22\n      state: started\n\n- name: modify temp instance\n  hosts: '{{ prefix }}-tmp-instance'\n  roles:\n  - gold-image-instance\n\n- name: create gold image from temp instance disk\n  hosts: localhost\n  roles:\n  - role: deployment-delete\n    deployment_name: tmp-instance\n  - ssh-config-tmp-instance-delete\n  - gold-image\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "467de88b469145b5d3d274942716008bbc8eb46a", "filename": "roles/logging/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "# Auditd\n\n- name: Auditd installed\n  apt: name=auditd state=latest\n\n- name: Auditd rules configured\n  template: src=audit.rules.j2 dest=/etc/audit/audit.rules\n  notify:\n    - restart auditd\n\n- name: Auditd configured\n  template: src=auditd.conf.j2 dest=/etc/audit/auditd.conf\n  notify:\n    - restart auditd\n\n- name: Enable services\n  service: name=auditd enabled=yes\n\n# Rsyslog\n\n- name: Rsyslog installed\n  apt: name=rsyslog state=latest\n\n- name: Rsyslog configured\n  template: src=rsyslog.conf.j2 dest=/etc/rsyslog.conf\n  notify:\n    - restart rsyslog\n\n- name: Rsyslog CIS configured\n  template: src=CIS.conf.j2 dest=/etc/rsyslog.d/CIS.conf owner=root group=root mode=0644\n  notify:\n    - restart rsyslog\n\n- name: Enable services\n  service: name=rsyslog enabled=yes\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "84c893a11d70fb2beae588f9032d80c787cf6576", "filename": "roles/client/handlers/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n  \n- name: restart strongswan\n  service: name=strongswan state=restarted\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "8aa6802108c1039416a2540999ac5bb0ec501ee7", "filename": "dev/playbooks/install_cloudbees.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: cloudbees\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Install easy_install\n      yum:\n        name: python-setuptools\n        state: latest\n      changed_when: false \n    \n    - name: Install PIP\n      easy_install:\n        name: pip\n        state: latest\n\n    - name: Ensure latest docker.py is installed\n      pip:\n        name: docker.py\n        state: latest\n      changed_when: false\n\n    - name: Pull Cloudbees Docker Image\n      docker_image:\n        name: cloudbees/cloudbees-jenkins-team\n\n    - name: Start Cloudbees Docker Container\n      docker_container:\n        name: cloudbees\n        image: cloudbees/cloudbees-jenkins-team\n        state: started\n        ports:\n          - 8080:8080\n\n    #\n    # Sleep for 1 minute to allow Jenkins to initialize before retrieving password\n    #\n    - name: Sleep one minute\n      pause: minutes=1\n\n    - name: Retrieve Initial Jenkins Password\n      shell: docker exec cloudbees cat /var/jenkins_home/secrets/initialAdminPassword\n      register: initial_admin_password\n      changed_when: false\n\n    - debug:\n        msg: \"Initial Jenkins Admin Password is: {{ initial_admin_password.stdout }}\"\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "f21bca607ab7f23203ce4f0bb68ae1167222e8bf", "filename": "roles/2-common/tasks/centos.yml", "repository": "iiab/iiab", "decoded_content": "- name: Centos Server specific tasks\n  command: echo Starting centos.yml\n\n- name: Keep yum cache\n  ini_file: dest=/etc/yum.conf\n            section=main\n            option=keepcache\n            value=1\n\n- name: Install IIAB repo for CentOS\n  template: src={{ item }} dest=/etc/yum.repos.d/ owner=root group=root mode=0644\n  with_items:\n   - iiab-centos.repo\n   - li.nux.ro.repo\n   - ansible.repo\n\n- name: Install optional exFAT packages for CentOS\n  shell: yum --enablerepo=li-nux-ro install exfat-utils fuse-exfat\n  when: exFAT_enabled == \"True\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "55dedee1a57e31b2ae1c9069266efa98ac5c8f93", "filename": "roles/moodle-1.9/moodle/meta/main.yml", "repository": "iiab/iiab", "decoded_content": "---\ndependencies:\n    - { role: postgresql }\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "db625270140b73d90f2da1d7be4a9cadabbd34c1", "filename": "roles/config-quay-enterprise/tasks/firewall.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Check if firewalld is installed\n  command: systemctl status firewalld\n  register: firewalld_status\n  failed_when: false\n  changed_when: false\n\n- name: Check if iptables is installed\n  command: systemctl status iptables\n  register: iptables_status\n  failed_when: false\n  changed_when: false\n\n- name: Open port in firewalld\n  firewalld:\n    port: \"{{ item }}/tcp\"\n    permanent: true\n    state: enabled\n  when: firewalld_status.rc == 0\n  with_items:\n    - \"{{ quay_host_http_port }}\"\n    - \"{{ quay_host_https_port }}\"\n  notify:\n  - restart firewalld\n\n- name: Ensure iptables is correctly configured\n  lineinfile:\n    insertafter: \"^-A INPUT .* --dport {{ item }} .* ACCEPT\"\n    state: present\n    dest: /etc/sysconfig/iptables\n    regexp: \"^-A INPUT .* --dport {{ item }} .* ACCEPT\"\n    line: \"-A INPUT -p TCP -m state --state NEW -m TCP --dport {{ item }} -j ACCEPT\"\n  with_items:\n    - \"{{ quay_host_http_port }}\"\n    - \"{{ quay_host_https_port }}\"\n  when: iptables_status.rc == 0 and firewalld_status.rc != 0\n  notify:\n  - restart iptables\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "1cf1dd11e54a641c170bb7ce970078b4b40ddfad", "filename": "roles/ipareplica/meta/main.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# dependencies:\n#   - role: t_woerner.ipaclient\n\ngalaxy_info:\n  author: Thomas Woerner\n  description: A role to setup an IPA domain replica\n  company: Red Hat, Inc\n  license: GPLv3\n  min_ansible_version: 2.5\n  platforms:\n  - name: Fedora\n    versions:\n    - all\n  - name: EL\n    versions:\n    - 7\n    # - 8\n  galaxy_tags:\n    - identity\n    - ipa\n    - freeipa\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "91fe5c902f4b1022e0a136d8b4ce352a76448d0f", "filename": "roles/dockerbench/vars/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# vars file for dockerbench\n"}, {"commit_sha": "4a9aaf0951e383c57077cf651b93e78eeea1b5ac", "sha": "cd9481c9c7c57a6446cd5b9f377a5ad31f0283a8", "filename": "tasks/cores.yml", "repository": "geerlingguy/ansible-role-solr", "decoded_content": "---\n- name: Check current list of Solr cores.\n  uri:\n    url: http://{{ solr_connect_host }}:{{ solr_port }}/solr/admin/cores\n    return_content: true\n  register: solr_cores_current\n  check_mode: false\n\n- name: Ensure Solr conf directories exist.\n  file:\n    path: \"{{ solr_home }}/data/{{ item }}/conf\"\n    state: directory\n    owner: \"{{ solr_user }}\"\n    group: \"{{ solr_user }}\"\n    recurse: true\n  when: \"item not in solr_cores_current.content\"\n  with_items: \"{{ solr_cores }}\"\n\n- name: Ensure core configuration directories exist.\n  command: \"cp -r {{ solr_install_path }}/example/files/conf/ {{ solr_home }}/data/{{ item }}/\"\n  when: \"item not in solr_cores_current.content\"\n  with_items: \"{{ solr_cores }}\"\n  become: true\n  become_user: \"{{ solr_user }}\"\n\n- name: Create configured cores.\n  command: \"{{ solr_install_path }}/bin/solr create -c {{ item }} -p {{ solr_port }}\"\n  when: \"item not in solr_cores_current.content\"\n  with_items: \"{{ solr_cores }}\"\n  become: true\n  become_user: \"{{ solr_user }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "30e094459449479aba7c4eb098c582b87c8c0b6e", "filename": "playbooks/provisioning/openstack/prepare-and-format-cinder-volume.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: False\n  become: False\n  tasks:\n  - set_fact:\n      cinder_volume: \"{{ hostvars[groups.masters[0]].openshift_hosted_registry_storage_openstack_volumeID }}\"\n      cinder_fs: \"{{ hostvars[groups.masters[0]].openshift_hosted_registry_storage_openstack_filesystem }}\"\n\n  - name: Attach the volume to the VM\n    os_server_volume:\n      state: present\n      server: \"{{ groups['masters'][0] }}\"\n      volume: \"{{ cinder_volume }}\"\n    register: volume_attachment\n\n  - set_fact:\n      attached_device: >-\n        {{ volume_attachment['attachments']|json_query(\"[?volume_id=='\" + cinder_volume + \"'].device | [0]\") }}\n\n  - delegate_to: \"{{ groups['masters'][0] }}\"\n    block:\n    - name: Wait for the device to appear\n      wait_for: path={{ attached_device }}\n\n    - name: Create a temp directory for mounting the volume\n      tempfile:\n        prefix: cinder-volume\n        state: directory\n      register: cinder_mount_dir\n\n    - name: Format the device\n      filesystem:\n        fstype: \"{{ cinder_fs }}\"\n        dev: \"{{ attached_device }}\"\n\n    - name: Mount the device\n      mount:\n        name: \"{{ cinder_mount_dir.path }}\"\n        src: \"{{ attached_device }}\"\n        state: mounted\n        fstype: \"{{ cinder_fs }}\"\n\n    - name: Change mode on the filesystem\n      file:\n        path: \"{{ cinder_mount_dir.path }}\"\n        state: directory\n        recurse: true\n        mode: 0777\n\n    - name: Unmount the device\n      mount:\n        name: \"{{ cinder_mount_dir.path }}\"\n        src: \"{{ attached_device }}\"\n        state: absent\n        fstype: \"{{ cinder_fs }}\"\n\n    - name: Delete the temp directory\n      file:\n        name: \"{{ cinder_mount_dir.path }}\"\n        state: absent\n\n  - name: Detach the volume from the VM\n    os_server_volume:\n      state: absent\n      server: \"{{ groups['masters'][0] }}\"\n      volume: \"{{ cinder_volume }}\"\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "b51d0ca414e16ea71dd47c32d48f9033ba59db08", "filename": "playbooks/post.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Wait until SSH becomes ready...\n  local_action:\n    module: wait_for\n    port: 22\n    host: \"{{ cloud_instance_ip }}\"\n    search_regex: \"OpenSSH\"\n    delay: 10\n    timeout: 320\n    state: present\n\n- name: A short pause, in order to be sure the instance is ready\n  pause:\n    seconds: 10\n\n- include: local_ssh.yml\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "7844bd2937142407b5e7a9a5e744eb2c6c261828", "filename": "tasks/create_repo_nuget_group_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_nuget_group\n    args: \"{{ _nexus_repos_nuget_defaults|combine(item) }}\""}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "6d1989bc85bcb7ccf514ea417d20867f0500f63c", "filename": "ops/playbooks/k8s-nfs-provisioner.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- name: K8S Install NFS provisioner \n  hosts: local\n  connection: local\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  tasks:\n\n    - debug: msg=\"Starting Playbook k8s-nfs-provisioner\" \n#\n# find a UCP VM that works\n#\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n\n\n#\n# Retrieve and remember a Token for using the UCP API\n#\n    - name: Retrieve a token for the UCP API\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/auth/login\"\n        headers:\n          Content-Type: application/json\n        method: POST\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        body: '{\"username\":\"{{ ucp_username }}\",\"password\":\"{{ ucp_password }}\"}'\n        use_proxy: no\n      register: login\n      until: login.status == 200\n      retries: 20\n      delay: 5\n\n    - name: Remember the token\n      set_fact:\n        auth_token:  \"{{ login.json.auth_token }}\"\n\n#\n# handle the case where the role is already existing\n#\n    - name: List all roles\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/roles\"\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        use_proxy: no\n      register: roles\n\n    - set_fact: \n        nfs_role: \"{{ roles.json | json_query(query) }}\"\n      vars:\n        query: \"[?name == '{{ nfs_provisioner_role }}' ].{id: id, name: name }\"\n\n#\n# Create the role if it does not already exists\n#\n    - block:\n\n      - name: Create the role file\n        template:\n          src:  ../templates/k8s/nfs-provisioner/nfs-provisioner-role.json\n          dest: /tmp/nfs-provisioner-role.json\n\n      - name: Create the role for the nfs-provisioner\n        uri:\n          url: \"https://{{ ucp_instance }}.{{ domain_name }}/roles\"\n          headers:\n            Content-Type: application/json\n            Authorization: Bearer {{ auth_token }}\n          method: POST\n          status_code: 201\n          body_format: json\n          validate_certs: no\n          body: \"{{ body }}\"\n          use_proxy: no\n        vars:\n          body: \"{{ lookup('file','/tmp/nfs-provisioner-role.json') }}\"\n        register: res\n  \n      - debug: var=res\n\n      when: nfs_role | count == 0\n\n\n    - debug: msg=\"Role for nfs-provisioner already exists \"\n      when: nfs_role | count != 0\n\n#\n# \n#\n    - name: List all roles\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/roles\"\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        use_proxy: no\n      register: roles\n\n    - set_fact:\n        nfs_role: \"{{ roles.json | json_query(query) }}\"\n      vars:\n        query: \"[?name == '{{ nfs_provisioner_role }}' ].{id: id, name: name }\"\n\n    - name: List all roles\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/roles\"\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        use_proxy: no\n      register: roles\n\n    - set_fact:\n        nfs_role: \"{{ roles.json | json_query(query) }}\"\n      vars:\n        query: \"[?name == '{{ nfs_provisioner_role }}' ].{id: id, name: name }\"\n\n    - fail:\n      when: nfs_role | count ==0\n\n    - debug: var=nfs_role[0].id\n\n#\n# Copy the kubectl client\n#\n    - name: Copy kubectl client\n      copy:\n         src: ../files/k8s/client/kubectl\n         dest: /tmp/kubectl\n         mode: 0744\n\n#\n# Create the service account\n#\n    - name: Create Service Account, copy script\n      template:\n         src: ../templates/k8s/nfs-provisioner/serviceaccount.sh\n         dest: /tmp/serviceaccount.sh\n         mode: 0744\n    - name: Create Service Account, run script\n      shell: /tmp/serviceaccount.sh /tmp/kubectl\n\n    - name: Grant the service account with the role\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/collectionGrants/system%3Aserviceaccount%3A{{ nfs_provisioner_namespace}}%3Anfs-provisioner/kubernetesnamespaces/{{ nfs_role[0].id }}?type=grantobject\"\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: PUT\n        status_code: 201\n        body_format: json\n        validate_certs: no\n        use_proxy: no\n      register: roles\n\n#\n# deploy the nfs provisioner\n#\n    - name: Deploy NFS Provisioner, copy script\n      template:\n         src: ../templates/k8s/nfs-provisioner/deployment-sa.sh\n         dest: /tmp/deployment-sa.sh\n         mode: 0744\n    - name: Deploy NFS  Provisioner, run script\n      shell: /tmp/deployment-sa.sh /tmp/kubectl\n\n#\n# Create the storage class\n#\n    - name: Create Storage Class, Copy Script\n      template:\n         src: ../templates/k8s/nfs-provisioner/storageclass.sh\n         dest: /tmp/storageclass.sh\n         mode: 0744\n    - name: Create Storage Class, run script\n      shell: /tmp/storageclass.sh /tmp/kubectl\n\n\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "d50ecfa4b0e473acbaa4e435ceadfaa0c40ae28f", "filename": "roles/vpn/tasks/distribute_keys.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Copy the keys to the strongswan directory\n  copy:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: \"{{ item.owner }}\"\n    group: \"{{ item.group }}\"\n    mode: \"{{ item.mode }}\"\n  with_items:\n    - src: \"configs/{{ IP_subject_alt_name }}/pki/cacert.pem\"\n      dest: \"{{ config_prefix|default('/') }}etc/ipsec.d/cacerts/ca.crt\"\n      owner: strongswan\n      group: \"{{ root_group|default('root') }}\"\n      mode: \"0600\"\n    - src: \"configs/{{ IP_subject_alt_name }}/pki/certs/{{ IP_subject_alt_name }}.crt\"\n      dest: \"{{ config_prefix|default('/') }}etc/ipsec.d/certs/{{ IP_subject_alt_name }}.crt\"\n      owner: strongswan\n      group: \"{{ root_group|default('root') }}\"\n      mode: \"0600\"\n    - src: \"configs/{{ IP_subject_alt_name }}/pki/private/{{ IP_subject_alt_name }}.key\"\n      dest: \"{{ config_prefix|default('/') }}etc/ipsec.d/private/{{ IP_subject_alt_name }}.key\"\n      owner: strongswan\n      group: \"{{ root_group|default('root') }}\"\n      mode: \"0600\"\n  notify:\n    - restart strongswan\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "c341d795fe7472dc4291021ed417cf15dfcb5458", "filename": "roles/ovirt-engine-remote-dwh/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# health page\n- name: check if ovirt-engine running (health page)\n  uri:\n    url: \"http://{{ ovirt_engine_db_host }}/ovirt-engine/services/health\"\n    status_code: 200\n  register: ovirt_engine_status\n  retries: 2\n  delay: 5\n  until: ovirt_engine_status|success\n  ignore_errors: True\n\n# copy dwh on dwh answer file\n# this answerfile results in a 2 VM setup where\n# VM 1: holds engine and it's `engine` DB instance\n# and VM 2: which holds dwh service and it's `ovirt_engine_history` DB\n- name: copy answerfile for `ovirt_engine_history` db to be on dwhservice VM\n  template:\n    src: answerfile_{{ ovirt_engine_version }}_ovirt_engine_history_db_on_dwhservice_host.txt.j2\n    dest: /tmp/answerfile.txt\n    mode: 0644\n    owner: root\n    group: root\n  when: ovirt_engine_history_db_on_dwhservice_host == True\n\n# - case where the database of dwh on 3rd machine\n- name: copy answerfile for `ovirt_engine_history` db to be on remote VM and not on dwhservice VM\n  template:\n    src: answerfile_{{ ovirt_engine_version }}_ovirt_engine_history_db_not_on_dwhservice_host.txt.j2\n    dest: /tmp/answerfile.txt\n    mode: 0644\n    owner: root\n    group: root\n  when: ovirt_engine_history_db_on_dwhservice_host == False\n\n- name: run engine-setup with answerfile\n  shell: 'engine-setup --config-append=/tmp/answerfile.txt'\n  tags:\n    - skip_ansible_lint\n\n- name: check state of database\n  service:\n    name: postgresql\n    state: started\n  when: (ovirt_engine_dwh_db_host == 'localhost' and ovirt_engine_dwh == True) or ovirt_engine_db_host == 'localhost'\n\n# health page\n- name: check if ovirt-engine running (health page)\n  uri:\n    url: \"http://{{ ovirt_engine_db_host }}/ovirt-engine/services/health\"\n    status_code: 200\n  register: ovirt_engine_status\n  retries: 12\n  delay: 5\n  until: ovirt_engine_status|success\n  ignore_errors: True\n\n- name: Restart `ovirt-engine-dwhd` service\n  service:\n    name: ovirt-engine-dwhd\n    state: restarted\n\n- debug: msg='Restart ovirt-engine on the remote host by doing a `service ovirt-engine restart`'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "bc2d346811b98873ee014bc2f62859293c6779e6", "filename": "roles/config-ipa-client/tasks/ipa-install.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Include prereqs per the type of OS\"\n  include_tasks: \"{{ distro_file }}\"\n  with_first_found:\n  - files:\n    - prereq-{{ ansible_distribution }}.yml\n    skip: true\n  loop_control:\n    loop_var: distro_file\n\n- import_tasks: move-local-user-home.yml\n\n- name: \"Ensure SUDO access for main user (if any)\"\n  lineinfile:\n    path: /etc/sudoers.d/10-idm-user\n    regexp: \"^{{ main_user }}\"\n    line: \"{{ main_user }} ALL=(ALL) NOPASSWD:ALL\"\n    create: yes\n  when:\n  - main_user is defined\n  - main_user|trim != \"\"\n\n- name: \"Ensure SUDO access for group (if any)\"\n  lineinfile:\n    path: /etc/sudoers.d/11-idm-admin-group\n    regexp: \"^%{{ admin_group }}\"\n    line: \"%{{ admin_group }} ALL=(ALL) NOPASSWD:ALL\"\n    create: yes\n  when:\n  - admin_group is defined\n  - admin_group|trim != \"\"\n\n- name: \"Set up the IPA/IdM client integration\"\n  command: 'ipa-client-install -U --automount-location={{ ipa_automount_location }} -p \"{{ ipa_username }}\" -w \"{{ ipa_password }}\" --domain=\"{{ ipa_domain }}\" --force-join'\n\n- name: \"Workaround for missing sss in nsswitch.conf\"\n  lineinfile:\n    path: /etc/nsswitch.conf\n    regexp: '^automount:.*files'\n    line: 'automount: files sss'\n    state: present\n  register: automount_line\n\n- name: \"Restart sssd if nsswitch.conf was updated\"\n  service:\n    name: sssd\n    state: restarted\n  when:\n  - automount_line.changed\n\n- name: \"Restart autofs if nsswitch.conf was updated\"\n  service:\n    name: autofs\n    state: restarted\n  when:\n  - automount_line.changed\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "0118bc466f818ce5d4e47b8620fd827b653745fb", "filename": "archive/roles/cicd/defaults/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\ncicd_block_device: \"/dev/vdb\"\njava_keystore_password: \"changeit\"\njava_certs: \"\"\ncicd_temp_dir: \"/tmp/cicd-setup\"\n\ngroovy_version: \"2.4.4\"\ngroovy_url: \"http://dl.bintray.com/groovy/maven/apache-groovy-binary-{{groovy_version}}.zip\"\ngroovy_local_archive: \"{{cicd_temp_dir}}/apache-groovy-binary-{{groovy_version}}.zip\"\ngroovy_base_dir: \"/usr/local\"\ngroovy_install_dir: \"{{groovy_base_dir}}/groovy-{{groovy_version}}\"\n\nnexus_version: \"2.12.0-01\"\nnexus_url: \"https://sonatype-download.global.ssl.fastly.net/nexus/oss/nexus-{{nexus_version}}-bundle.tar.gz\"\nnexus_local_archive: \"{{cicd_temp_dir}}/nexus-{{                                                               nexus_version}}-bundle.tar.gz\"\nnexus_base_dir: \"/usr/local\"\nnexus_install_dir: \"{{nexus_base_dir}}/nexus-{{nexus_version}}\"\nnexus_home_dir: \"{{ nexus_base_dir }}/nexus\"\nnexus_pid_dir: \"/var/run/nexus\"\nnexus_sonatype_work_dir: \"{{nexus_base_dir}}/sonatype-work\"\nnexus_user: \"nexus\"\nnexus_group: \"nexus\"\n\njenkins_repo_url: \"http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo\"\njenkins_repo_key_url: \"https://jenkins-ci.org/redhat/jenkins-ci.org.key\"\njenkins_home_dir: \"/var/lib/jenkins\"\njenkins_authz_xml: '<authorizationStrategy class=\"hudson.security.AuthorizationStrategy$Unsecured\"/><securityRealm class=\"hudson.security.SecurityRealm$None\"/>'\njenkins_plugins_base_url: \"https://updates.jenkins-ci.org/download/plugins\"\njenkins_plugins_home_dir: \"{{ jenkins_home_dir }}/plugins\"\njenkins_user: \"jenkins\"\njenkins_group: \"jenkins\""}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "eaf02e06b21bdf1d7f34c8de82a9b2381a238869", "filename": "playbooks/notifications/email-notify-group-of-users.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Obtain list of users to e-mail\"\n  hosts: mail-host\n  gather_facts: no\n  tasks:\n  - include_role:\n      name: roles/user-management/list-users-by-group\n\n- import_playbook: email-notify-list-of-users.yml\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "e07227f0ce924d5868349b36957e916c6a00711a", "filename": "ops/playbooks/roles/hpe.haproxy/templates/20-ucp-intro", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "frontend ucp\n    option tcplog\n    log global\n    bind *:80\n    redirect scheme https code 301 if !{ ssl_fc }\n    bind *:443\n    default_backend ucp\n\nbackend ucp\n    balance roundrobin\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7f867d73b11e9dfb6365ae9e42f28b80c8d15d0c", "filename": "meta/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\ndependencies:\n"}, {"commit_sha": "c91b6076e3a957fb0a165131d0ff3b3b208ed419", "sha": "9fcc07fae0acf7b39eef728b3f9208f6f113e5d2", "filename": "tasks/section_12_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 12.1 Verify Permissions on /etc/passwd (Scored)\n    file: path=/etc/passwd mode=0644\n    tags:\n      - section12\n      - section12.1\n\n  - name: 12.2 Verify Permissions on /etc/shadow (Scored)\n    file: path=/etc/shadow mode='o-rwx,g-rw'\n    tags:\n      - section12\n      - section12.2\n\n  - name: 12.3 Verify Permissions on /etc/group (Scored)\n    file: path=/etc/group mode=0644\n    tags:\n      - section12\n      - section12.3\n\n  - name: 12.4 Verify User/Group Ownership on /etc/passwd (Scored)\n    file: path=/etc/passwd owner=root group=root\n    tags:\n      - section12\n      - section12.4\n\n  - name: 12.5 Verify User/Group Ownership on /etc/shadow (Scored)\n    file: path=/etc/shadow owner=root group=shadow\n    tags:\n      - section12\n      - section12.5\n\n  - name: 12.6 Verify User/Group Ownership on /etc/group (Scored)\n    file: path=/etc/group owner=root group=root\n    tags:\n      - section12\n      - section12.6\n\n  - name: 12.7 Find World Writable Files (check) (Not Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -type f -perm -0002 -print\n    changed_when: False\n    failed_when: False\n    register: world_files\n    tags:\n      - section12\n      - section12.7\n\n  - name: 12.7 Find World Writable Files (Not Scored)\n    debug: msg=\"{{ item }}\"\n    with_items:\n        world_files.stdout_lines\n    tags:\n      - section12\n      - section12.7\n\n  - name: 12.8 Find Un-owned Files and Directories (check) (Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -nogroup -ls\n    changed_when: False\n    failed_when: False\n    register: unowned_files\n    tags:\n      - section12\n      - section12.8\n\n  - name: 12.8 Find Un-owned Files and Directories (Scored)\n    debug: msg=\"{{ item }}\"\n    with_items:\n        unowned_files.stdout_lines\n    tags:\n      - section12\n      - section12.9\n\n  - name: 12.9 Find Un-grouped Files and Directories (check) (Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -nogroup -ls\n    changed_when: False\n    failed_when: False\n    register: ungrouped_files\n    tags:\n      - section12\n      - section12.9\n\n  - name: 12.9 Find Un-grouped Files and Directories (Scored)\n    debug: >\n        msg=\"{{ item }}\"\n    with_items:\n        ungrouped_files.stdout_lines\n    tags:\n      - section12\n      - section12.9\n\n  - name: 12.10 Find SUID System Executables (check) (Not Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -type f -perm -4000 -print\n    changed_when: False\n    failed_when: False\n    register: suid_files\n    tags:\n      - section12\n      - section12.10\n\n  - name: 12.10 Find SUID System Executables (Not Scored)\n    debug: msg=\"{{ item }}\"\n    with_items:\n        suid_files.stdout_lines\n    tags:\n      - section12\n      - section12.10\n\n  - name: 12.11 Find SGID System Executables (check) (Not Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -type f -perm -2000 -print\n    changed_when: False\n    failed_when: False\n    register: gsuid_files\n    tags:\n      - section12\n      - section12.11\n\n  - name: 12.11 Find SGID System Executables (Not Scored)\n    debug: msg=\"{{ item }}\"\n    with_items:\n        gsuid_files.stdout_lines\n    tags:\n      - section12\n      - section12.10\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "d8f6ec3ea740e0daeb27abf58131662972afce53", "filename": "roles/common/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Gather Facts\n  setup:\n  tags:\n    - always\n\n- include: ubuntu.yml\n  when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'\n\n- include: freebsd.yml\n  when: ansible_distribution == 'FreeBSD'\n\n- name: Install tools\n  package: name=\"{{ item }}\" state=present\n  with_items:\n    - \"{{ tools }}\"\n  tags:\n    - always\n\n- name: Enable packet forwarding for IPv4\n  sysctl: name=\"{{ item }}\" value=1\n  with_items:\n    - \"{{ sysctl.forwarding }}\"\n  tags:\n    - always\n\n- meta: flush_handlers\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "737b5a1e7856cab4a465c774027d0b35bcbafd4b", "filename": "roles/setup-slack/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\nslack_channels: {}\nslack_users: {}\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e323d0b6c82aa96a999181a03f6b662ef33c041a", "filename": "roles/config-httpd/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: prep.yml\n- import_tasks: seed.yml\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "c208d11b188b8c0ade72e246541c0eb7f433ebf6", "filename": "roles/cfme-ocp-provider/tasks/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: Fail if Required Parameters Not Provided\n  fail:\n    msg: \"Required Parameters have Not Been Provided\"\n  when: (cfme_host | trim == \"\") or (cfme_host is none) or (cfme_host is undefined) or (cfme_username | trim == \"\") or (cfme_username is none) or (cfme_username is undefined) or (cfme_password | trim == \"\") or (cfme_password is none) or (cfme_password is undefined)\n\n- name: Retrieve Default Service Account Token\n  command: >\n    oc serviceaccounts get-token -n {{ default_token_sa_namespace }} {{ default_token_sa_name }}\n  ignore_errors: True\n  register: default_management_token\n  \n- name: Set Default OpenShift Token if Not Provided\n  set_fact:\n    ocp_token: \"{{ default_management_token.stdout }}\"\n  when: default_management_token.rc == 0 and ((ocp_token | trim == \"\") or (ocp_token is none) or (ocp_token is undefined))\n    \n- name: Set Default Hawkular Token if Not Provided\n  set_fact:\n    hawkular_token: \"{{ default_management_token.stdout }}\"\n  when: default_management_token.rc == 0 and ((hawkular_token | trim == \"\") or (hawkular_token is none) or (hawkular_token is undefined))\n\n- name: Query Existing Providers\n  uri:\n    url: https://{{ cfme_host }}/api/providers\n    method: GET\n    user: \"{{ cfme_username }}\"\n    password: \"{{ cfme_password }}\"\n    force_basic_auth: true\n    validate_certs: false\n    status_code: 200\n  register: providers_result\n\n- name: Query Details From Each Provider\n  include_tasks: query_provider.yml\n  vars:    \n    provider_url: \"{{ item }}\"\n  with_items : \"{{ providers_result.json.resources | default([]) }}\"\n  when: providers_result is defined\n\n- name: Create Container Provider\n  uri:\n    url: https://{{ cfme_host }}/api/providers\n    method: POST\n    body: \"{{ lookup('template', '{{role_path}}/templates/container_provider_rest.j2') }}\"\n    user: \"{{ cfme_username }}\"\n    password: \"{{ cfme_password }}\"\n    force_basic_auth: true\n    validate_certs: false\n    status_code: 200\n    body_format: json\n  register: container_provider_result\n  when: ocp_provider_found is not defined or ocp_provider_found != True\n\n- name: Update Container Provider\n  uri:\n    url: https://{{ cfme_host }}/api/providers/{{ ocp_provider_id }}\n    method: POST\n    body: \"{{ lookup('template', '{{role_path}}/templates/container_provider_rest.j2') }}\"\n    user: \"{{ cfme_username }}\"\n    password: \"{{ cfme_password }}\"\n    force_basic_auth: true\n    validate_certs: false\n    status_code: 200\n    body_format: json\n  when: ocp_provider_found is defined and ocp_provider_found == True\n\n- name: Set Provider ID for New Provider\n  set_fact:\n    ocp_provider_id: \"{{ container_provider_result.json.results[0].id }}\"\n  when: ocp_provider_id is not defined\n\n- name: Refresh Container Provider\n  uri:\n    url: https://{{ cfme_host }}/api/providers/{{ ocp_provider_id }}\n    method: POST\n    body: \"{\\\"action\\\" : \\\"refresh\\\"}\"\n    user: \"{{ cfme_username }}\"\n    password: \"{{ cfme_password }}\"\n    force_basic_auth: true\n    validate_certs: false\n    status_code: 200\n    body_format: json\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "0d30f42ceafa621ac64b381283133bdc58c8a08f", "filename": "reference-architecture/vmware-ansible/playbooks/roles/create-vm-cns-prod-ose/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Create CNS production VMs on vCenter\n  vmware_guest:\n    hostname: \"{{ vcenter_host }}\"\n    username: \"{{ vcenter_username }}\"\n    password: \"{{ vcenter_password }}\"\n    validate_certs: False\n    name: \"{{ item.value.guestname }}\"\n    cluster: \"{{ vcenter_cluster}}\"\n    datacenter: \"{{ vcenter_datacenter }}\"\n    resource_pool: \"{{ vcenter_resource_pool }}\"\n    template: \"{{vcenter_template_name}}\"\n    state: poweredon\n    wait_for_ip_address: true\n    folder: \"/{{ vcenter_folder }}\"\n    annotation: \"{{ item.value.tag }}\"\n    disk:\n    - size_gb: 60\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 40\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 40\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 300\n      datastore: \"{{ vcenter_datastore }}\"\n      type: eagerZeroedThick\n    hardware:\n      memory_mb: 32768\n    networks:\n    - name: \"{{ vm_network }}\"\n      ip: \"{{ item.value.ip4addr }}\"\n      netmask: \"{{ vm_netmask }}\"\n      gateway: \"{{ vm_gw }}\"\n    customization:\n      domain: \"{{dns_zone}}\"\n      dns_servers:\n      - \"{{ vm_dns }}\"\n      dns_suffix: \"{{dns_zone}}\"\n      hostname: \"{{ item.value.guestname}}\"\n  with_dict: \"{{host_inventory}}\"\n  when: \"'cns' in item.value.guestname\"\n\n- name: Add CNS production VMs to inventory\n  add_host: hostname=\"{{ item.value.guestname }}\" ansible_ssh_host=\"{{ item.value.ip4addr }}\" groups=\"{{ item.value.tag }}, cns, new_nodes\"\n  with_dict: \"{{host_inventory}}\"\n  when: \"'cns' in item.value.guestname\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ef8037f32986a4a3e5a077561ff146cab0f07b29", "filename": "reference-architecture/ansible-tower-integration/tower_config_aws/tower_config_aws/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n\n- name: Install pip\n  yum:\n    name: python2-pip\n    state: present\n  become: true\n\n- name: Install git\n  yum:\n    name: git\n    state: present\n  become: true\n\n- name: Install Tower CLI\n  pip:\n    name: ansible-tower-cli\n  become: true\n\n- name: Install ManageIQ module\n  pip:\n    name: manageiq-client\n  become: true\n\n- name: Install python2-boto\n  yum:\n    name: python2-boto\n    state: present\n  become: true\n\n- name: Install pyOpenSSL\n  yum:\n    name: pyOpenSSL\n    state: present\n  become: true\n\n- name: Install python-netaddr\n  yum:\n    name: python-netaddr\n    state: present\n  become: true\n\n- name: Install python-six\n  yum:\n    name: python-six\n    state: present\n  become: true\n\n- name: Install python2-boto3\n  yum:\n    name: python2-boto3\n    state: present\n  become: true\n\n- name: Install python-click\n  yum:\n    name: python-click\n    state: present\n  become: true\n\n- name: Install python-httplib2\n  yum:\n    name: python-httplib2\n    state: present\n  become: true\n\n- name: Create tower organization\n  tower_organization:\n    name: \"Default\"\n    description: \"Set to Default since the trial license only allows one organization. You can change it if you have deep pockets\"\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Add tower project for openshift-ansible-contrib\n  tower_project:\n    name: \"openshift-ansible-contrib\"\n    description: \"sync openshift-ansible-contrib\"\n    organization: \"Default\"\n    scm_url: https://github.com/strategicdesignteam/openshift-ansible-contrib.git\n    scm_type: git\n    scm_branch: master\n    scm_update_on_launch: true\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Add tower project for ansible-redhat-access-insights-client\n  tower_project:\n    name: \"ansible-redhat-access-insights-client\"\n    description: \"sync ansible-redhat-access-insights-client\"\n    organization: \"Default\"\n    scm_url: https://github.com/strategicdesignteam/ansible-redhat-access-insights-client.git\n    scm_type: git\n    scm_branch: master\n    scm_update_on_launch: true\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Add tower project for cfme-ansible\n  tower_project:\n    name: \"cfme-ansible\"\n    description: \"sync cfme-ansible\"\n    organization: \"Default\"\n    scm_url: https://github.com/strategicdesignteam/cfme-ansible.git\n    scm_type: git\n    scm_branch: master\n    scm_update_on_launch: true\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Sync project openshift-ansible-contrib\n  command: tower-cli project update -n openshift-ansible-contrib\n\n- name: Sync project ansible-redhat-access-insights-client\n  command: tower-cli project update -n ansible-redhat-access-insights-client\n\n- name: Sync project cfme-ansible\n  command: tower-cli project update -n cfme-ansible\n\n- name: Add tower credential for machine\n  tower_credential:\n    name: aws-privkey\n    kind: ssh\n    become_method: sudo\n    description: aws-privkey\n    organization: \"Default\"\n    state: present\n    ssh_key_data: \"{{ AWS_MACHINE_SSH_KEY }}\"\n    username: ec2-user\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Add tower credential for aws\n  tower_credential:\n    name: ec2\n    kind: aws\n    description: ec2\n    organization: \"Default\"\n    state: present\n    username: \"{{ AWS_KEY }}\"\n    password: \"{{ AWS_SECRET }}\"\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Add tower inventory\n  tower_inventory:\n    name: \"aws-inventory\"\n    description: \"Tower inventory for AWS\"\n    organization: \"Default\"\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Add tower group\n  tower_group:\n    name: aws\n    description: \"Tower Group for AWS\"\n    source: ec2\n    credential: ec2\n    inventory: \"aws-inventory\"\n    source_vars:\n      regions: all\n      regions_exclude: us-gov-west-1,cn-north-1\n      vpc_destination_variable: private_ip_address\n      route53: True\n      rds: False\n      elasticache: False\n      hostname_variable: tag_Name\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Get Inventory Source ID\n  shell: curl -s -k -u {{ TOWER_USER }}:{{ TOWER_PASSWORD }} https://{{ TOWER_HOSTNAME }}/api/v1/inventory_sources/ | python -m json.tool | grep -m 1 id |awk -F\":\" '{print $2}' |awk -F\",\" '{print $1}' |sed 's/^[ \\t]*//;s/[ \\t]*$//'\n  register: myoutput\n\n- name: Create aws-infrastructure job template\n  become: true\n  tower_job_template:\n    name: aws-infrastructure\n    job_type: run\n    inventory: aws-inventory\n    project: openshift-ansible-contrib\n    playbook: \"reference-architecture/aws-ansible/playbooks/infrastructure.yaml\"\n    machine_credential: aws-privkey\n    cloud_credential: ec2\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Create openshift_create_httpd_file job template\n  tower_job_template:\n    name: create_httpd_file\n    job_type: run\n    inventory: aws-inventory\n    project: openshift-ansible-contrib\n    playbook: \"reference-architecture/ansible-tower-integration/create_httpd_file/create_httpd_file.yaml\"\n    machine_credential: aws-privkey\n    cloud_credential: ec2\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Create redhat-access-insights client job template\n  tower_job_template:\n    name: redhat-access-insights-client\n    job_type: run\n    inventory: aws-inventory\n    become_enabled: yes\n    project: ansible-redhat-access-insights-client\n    playbook: \"redhat-access-insights-client.yaml\"\n    machine_credential: aws-privkey\n    cloud_credential: ec2\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Create aws-openshift-install job template\n  tower_job_template:\n    name: aws-openshift-install\n    job_type: run\n    inventory: aws-inventory\n    become_enabled: yes\n    project: openshift-ansible-contrib\n    playbook: \"reference-architecture/aws-ansible/playbooks/openshift-install.yaml\"\n    machine_credential: aws-privkey\n    cloud_credential: ec2\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Create aws-openshift-cfme-install job template\n  tower_job_template:\n    name: aws-openshift-cfme-install\n    job_type: run\n    inventory: aws-inventory\n    become_enabled: no\n    project: cfme-ansible\n    playbook: \"cfme_ose_install.yaml\"\n    machine_credential: aws-privkey\n    cloud_credential: ec2\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Create aws-openshift-cfme-ocp-provider job template\n  tower_job_template:\n    name: aws-openshift-cfme-ocp-provider\n    job_type: run\n    inventory: aws-inventory\n    become_enabled: no\n    project: cfme-ansible\n    playbook: \"cfme_ose_aws_provision.yaml\"\n    machine_credential: aws-privkey\n    cloud_credential: ec2\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Create workflow-ocp-aws-install\n  command: tower-cli workflow create --name=\"workflow-ocp-aws-install\" --organization=\"Default\" --description=\"A workflow for deploying OCP on AWS\" -e @workflow-ocp-aws-install-extravars.yaml\n\n- name: Replace inventory source\n  lineinfile:\n    path: schema.yml\n    regexp: 'REPLACEME'\n    line: 'aws (aws-inventory - {{ myoutput.stdout }})'\n\n- name: Replace inventory source second time\n  lineinfile:\n    path: schema.yml\n    regexp: 'ONEMORETIME'\n    line: 'aws (aws-inventory - {{ myoutput.stdout }})'\n\n- name: Create a schema for workflow-ocp-aws-install\n  command: tower-cli workflow schema workflow-ocp-aws-install @schema.yml\n\n- name: Reset schema file in case we run again, again\n  lineinfile:\n    path: schema.yml\n    regexp: '^(.*)inventory_source'\n    line: '            - inventory_source: ONEMORETIME'\n\n- name: Reset schema file in case we run again\n  lineinfile:\n    path: schema.yml\n    regexp: '^(.*)inventory_source.*(?<!\\ONEMORETIME)$'\n    line: '  - inventory_source: REPLACEME'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ea6bb28e04d30bab9c71cb03e62b90845c7f3093", "filename": "playbooks/manage-users/manage-local-user-access.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Update local access for account:{{ user_name }}'\n  hosts: all\n  roles:\n    - role: user-management/manage-local-user-password\n    - role: user-management/manage-local-user-ssh-authkeys\n    - role: manage-sshd-config\n  tags:\n    - manage-local-user-access\n\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "85da1ebd57d4185afd213539ef3c629652e78a94", "filename": "roles/client/tasks/systems/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- include: Debian.yml\n  when: ansible_distribution == 'Debian'\n\n- include: Ubuntu.yml\n  when: ansible_distribution == 'Ubuntu'\n\n- include: CentOS.yml\n  when: ansible_distribution == 'CentOS'\n\n- include: Fedora.yml\n  when: ansible_distribution == 'Fedora'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "53bc5212449d571ba9e4642e5690c6e2a59dc6c4", "filename": "roles/setup-slack/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Test slack setup\n  hosts: localhost\n  roles:\n    - setup-slack"}, {"commit_sha": "bf6e08dcb2440421477b6536ff6a8d11adc2be17", "sha": "07694112839bfa048124eddb4ececefefac20b2f", "filename": "roles/handlers/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "- name: restart consul\n  service:\n    name: consul\n    state: restarted\n  sudo: yes\n  notify:\n    - wait for consul to listen\n\n- name: wait for consul to listen\n  wait_for:\n    host: localhost\n    port: 8500\n\n- name: restart docker\n  service:\n   name: docker\n   state: restarted\n  sudo: yes\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "541bd0cadbc5eecdd5ad6096b3be8d8c7209dda7", "filename": "roles/ipaclient/tasks/main.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "---\n# tasks file for ipaclient\n\n- name: Import variables specific to distribution\n  include_vars: \"{{ item }}\"\n  with_first_found:\n    - \"{{ role_path }}/vars/{{ ansible_distribution }}-{{ ansible_distribution_version }}.yml\"\n    - \"{{ role_path }}/vars/{{ ansible_distribution }}-{{ ansible_distribution_major_version }}.yml\"\n    - \"{{ role_path }}/vars/{{ ansible_distribution }}.yml\"\n    - \"{{ role_path }}/vars/default.yml\"\n\n- name: Install IPA client\n  include_tasks: tasks/install.yml\n  when: state|default('present') == 'present'\n\n- name: Uninstall IPA client\n  include_tasks: tasks/uninstall.yml\n  when: state|default('present') == 'absent'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "20c4c58cfade12582ee1658a23dfc5239fd094fc", "filename": "playbooks/ansible/tower/roles", "repository": "redhat-cop/infra-ansible", "decoded_content": "../../../roles"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9f3cc1f8a19b50a9945d679bbe21de4fa24aae2c", "filename": "roles/config-openvpn/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# Default OpenVPN RPM to be installed\n# NOTE: this is the CentOS flavor \ndefault_openvpn_rpm: http://swupdate.openvpn.org/as/openvpn-as-2.1.9-CentOS7.x86_64.rpm\n\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "5b8597a807edd52677f449e5efc5de2e4179f6a4", "filename": "roles/kubevirt/tasks/release.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "- name: Download KubeVirt Template\n  get_url:\n    url: \"{{ kubevirt_manifest_url }}/{{ manifest_version }}/kubevirt.yaml.in\"\n    dest: \"{{ kubevirt_template_dir }}/kubevirt.yml\"\n  when: byo_template.stat.exists == False\n\n- name: Render KubeVirt Yml\n  template:\n    src: \"{{ kubevirt_template_dir }}/kubevirt.yml\"\n    dest: /tmp/kubevirt.yml\n\n- name: Remove downloaded templates\n  file:\n    path: \"{{ kubevirt_template_dir }}/kubevirt.yml\"\n    state: absent\n  when: byo_template.stat.exists == False\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "0974b1defb8cb3b6c44a247802e2e448d24c64eb", "filename": "roles/config-linux-desktop/config-mate/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install, configure and enable MATE Desktop\"\n  include_tasks: \"{{ distro_file }}\"\n  with_first_found:\n  - files:\n    - mate-{{ ansible_distribution }}.yml\n    skip: true\n  loop_control:\n    loop_var: distro_file\n  when:\n  - mate_install|default(False)\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "92ecae8c4dcba2212060b8f8aec1e23d5a49f3c9", "filename": "reference-architecture/vmware-ansible/playbooks/roles/heketi-ocp/templates/storage-crs.json.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "apiVersion: storage.k8s.io/v1beta1\nkind: StorageClass\nmetadata:\n  name: crs-gluster\nprovisioner: kubernetes.io/glusterfs\nparameters:\n  resturl: \"http://{{ansible_default_ipv4.address }}:8080\"\n  restauthenabled: \"true\"\n  restuser: \"admin\"\n  secretNamespace: \"default\"\n  secretName: \"heketi-secret\"\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "eefb0252fa3ec9126fc00e8643fcd17b12de0395", "filename": "tasks/create_repo_raw_proxy_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_raw_proxy\n    args: \"{{ _nexus_repos_raw_defaults|combine(item) }}\""}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "72bb95254084c562b53acca6ab3b92a4b6ee3bfd", "filename": "playbooks/provisioning/openstack/advanced-configuration.md", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "## Dependencies for localhost (ansible control/admin node)\n\n* [Ansible 2.3](https://pypi.python.org/pypi/ansible)\n* [Ansible-galaxy](https://pypi.python.org/pypi/ansible-galaxy-local-deps)\n* [jinja2](http://jinja.pocoo.org/docs/2.9/)\n* [shade](https://pypi.python.org/pypi/shade)\n* python-jmespath / [jmespath](https://pypi.python.org/pypi/jmespath)\n* python-dns / [dnspython](https://pypi.python.org/pypi/dnspython)\n* Become (sudo) is not required.\n\n**NOTE**: You can use a Docker image with all dependencies set up.\nFind more in the [Deployment section](#deployment).\n\n### Optional Dependencies for localhost\n**Note**: When using rhel images, `rhel-7-server-openstack-10-rpms` repository is required in order to install these packages.\n\n* `python-openstackclient`\n* `python-heatclient`\n\n## Dependencies for OpenStack hosted cluster nodes (servers)\n\nThere are no additional dependencies for the cluster nodes. Required\nconfiguration steps are done by Heat given a specific user data config\nthat normally should not be changed.\n\n## Required galaxy modules\n\nIn order to pull in external dependencies for DNS configuration steps,\nthe following commads need to be executed:\n\n    ansible-galaxy install \\\n      -r openshift-ansible-contrib/playbooks/provisioning/openstack/galaxy-requirements.yaml \\\n      -p openshift-ansible-contrib/roles\n\nAlternatively you can install directly from github:\n\n    ansible-galaxy install git+https://github.com/redhat-cop/infra-ansible,master \\\n      -p openshift-ansible-contrib/roles\n\nNotes:\n* This assumes we're in the directory that contains the clonned\nopenshift-ansible-contrib repo in its root path.\n* When trying to install a different version, the previous one must be removed first\n(`infra-ansible` directory from [roles](https://github.com/openshift/openshift-ansible-contrib/tree/master/roles)).\nOtherwise, even if there are differences between the two versions, installation of the newer version is skipped.\n\n\n## Accessing the OpenShift Cluster\n\n### Use the Cluster DNS\n\nIn addition to the OpenShift nodes, we created a DNS server with all\nthe necessary entries. We will configure your *Ansible host* to use\nthis new DNS and talk to the deployed OpenShift.\n\nFirst, get the DNS IP address:\n\n```bash\n$ openstack server show dns-0.openshift.example.com --format value --column addresses\nopenshift-ansible-openshift.example.com-net=192.168.99.11, 10.40.128.129\n```\n\nNote the floating IP address (it's `10.40.128.129` in this case) -- if\nyou're not sure, try pinging them both -- it's the one that responds\nto pings.\n\nNext, edit your `/etc/resolv.conf` as root and put `nameserver DNS_IP` as your\n**first entry**.\n\nIf your `/etc/resolv.conf` currently looks like this:\n\n```\n; generated by /usr/sbin/dhclient-script\nsearch openstacklocal\nnameserver 192.168.0.3\nnameserver 192.168.0.2\n```\n\nChange it to this:\n\n```\n; generated by /usr/sbin/dhclient-script\nsearch openstacklocal\nnameserver 10.40.128.129\nnameserver 192.168.0.3\nnameserver 192.168.0.2\n```\n\n### Get the `oc` Client\n\n**NOTE**: You can skip this section if you're using the Docker image\n-- it already has the `oc` binary.\n\nYou need to download the OpenShift command line client (called `oc`).\nYou can download and extract `openshift-origin-client-tools` from the\nOpenShift release page:\n\nhttps://github.com/openshift/origin/releases/latest/\n\nOr you can now copy it from the master node:\n\n    $ ansible -i inventory masters[0] -m fetch -a \"src=/bin/oc dest=oc\"\n\nEither way, find the `oc` binary and put it in your `PATH`.\n\n\n### Logging in Using the Command Line\n\n\n```\noc login --insecure-skip-tls-verify=true https://master-0.openshift.example.com:8443 -u user -p password\noc new-project test\noc new-app --template=cakephp-mysql-example\noc status -v\ncurl http://cakephp-mysql-example-test.apps.openshift.example.com\n```\n\nThis will trigger an image build. You can run `oc logs -f\nbc/cakephp-mysql-example` to follow its progress.\n\nWait until the build has finished and both pods are deployed and running:\n\n```\n$ oc status -v\nIn project test on server https://master-0.openshift.example.com:8443\n\nhttp://cakephp-mysql-example-test.apps.openshift.example.com (svc/cakephp-mysql-example)\n  dc/cakephp-mysql-example deploys istag/cakephp-mysql-example:latest <-\n    bc/cakephp-mysql-example source builds https://github.com/openshift/cakephp-ex.git on openshift/php:7.0\n    deployment #1 deployed about a minute ago - 1 pod\n\nsvc/mysql - 172.30.144.36:3306\n  dc/mysql deploys openshift/mysql:5.7\n    deployment #1 deployed 3 minutes ago - 1 pod\n\nInfo:\n  * pod/cakephp-mysql-example-1-build has no liveness probe to verify pods are still running.\n    try: oc set probe pod/cakephp-mysql-example-1-build --liveness ...\nView details with 'oc describe <resource>/<name>' or list everything with 'oc get all'.\n\n```\n\nYou can now look at the deployed app using its route:\n\n```\n$ curl http://cakephp-mysql-example-test.apps.openshift.example.com\n```\n\nIts `title` should say: \"Welcome to OpenShift\".\n\n\n### Accessing the UI\n\nYou can also access the OpenShift cluster with a web browser by going to:\n\nhttps://master-0.openshift.example.com:8443\n\nNote that for this to work, the OpenShift nodes must be accessible\nfrom your computer and it's DNS configuration must use the cruster's\nDNS.\n\n\n## Removing the OpenShift Cluster\n\nEverything in the cluster is contained within a Heat stack. To\ncompletely remove the cluster and all the related OpenStack resources,\nrun this command:\n\n```bash\nopenstack stack delete --wait --yes openshift.example.com\n```\n\n\n## DNS configuration variables\n\nPay special attention to the values in the first paragraph -- these\nwill depend on your OpenStack environment.\n\nNote that the provsisioning playbooks update the original Neutron subnet\ncreated with the Heat stack to point to the configured DNS servers.\nSo the provisioned cluster nodes will start using those natively as\ndefault nameservers. Technically, this allows to deploy OpenShift clusters\nwithout dnsmasq proxies.\n\nThe `env_id` and `public_dns_domain` will form the cluster's DNS domain all\nyour servers will be under. With the default values, this will be\n`openshift.example.com`. For workloads, the default subdomain is 'apps'.\nThat sudomain can be set as well by the `openshift_app_domain` variable in\nthe inventory.\n\nThe `openstack_<role name>_hostname` is a set of variables used for customising\nhostnames of servers with a given role. When such a variable stays commented,\ndefault hostname (usually the role name) is used.\n\nThe `public_dns_nameservers` is a list of DNS servers accessible from all\nthe created Nova servers. These will be serving as your DNS forwarders for\nexternal FQDNs that do not belong to the cluster's DNS domain and its subdomains.\nIf you're unsure what to put in here, you can try the google or opendns servers,\nbut note that some organizations may be blocking them.\n\nThe `openshift_use_dnsmasq` controls either dnsmasq is deployed or not.\nBy default, dnsmasq is deployed and comes as the hosts' /etc/resolv.conf file\nfirst nameserver entry that points to the local host instance of the dnsmasq\ndaemon that in turn proxies DNS requests to the authoritative DNS server.\nWhen Network Manager is enabled for provisioned cluster nodes, which is\nnormally the case, you should not change the defaults and always deploy dnsmasq.\n\n`external_nsupdate_keys` describes an external authoritative DNS server(s)\nprocessing dynamic records updates in the public and private cluster views:\n\n    external_nsupdate_keys:\n      public:\n        key_secret: <some nsupdate key>\n        key_algorithm: 'hmac-md5'\n        key_name: 'update-key'\n        server: <public DNS server IP>\n      private:\n        key_secret: <some nsupdate key 2>\n        key_algorithm: 'hmac-sha256'\n        server: <public or private DNS server IP>\n\nHere, for the public view section, we specified another key algorithm and\noptional `key_name`, which normally defaults to the cluster's DNS domain.\nThis just illustrates a compatibility mode with a DNS service deployed\nby OpenShift on OSP10 reference architecture, and used in a mixed mode with\nanother external DNS server.\n\nAnother example defines an external DNS server for the public view\nadditionally to the in-stack DNS server used for the private view only:\n\n    external_nsupdate_keys:\n      public:\n        key_secret: <some nsupdate key>\n        key_algorithm: 'hmac-sha256'\n        server: <public DNS server IP>\n\nHere, updates matching the public view will be hitting the given public\nserver IP. While updates matching the private view will be sent to the\nauto evaluated in-stack DNS server's **public** IP.\n\nNote, for the in-stack DNS server, private view updates may be sent only\nvia the public IP of the server. You can not send updates via the private\nIP yet. This forces the in-stack private server to have a floating IP.\nSee also the [security notes](#security-notes)\n\n## Flannel networking\n\nIn order to configure the\n[flannel networking](https://docs.openshift.com/container-platform/3.6/install_config/configuring_sdn.html#using-flannel),\nuncomment and adjust the appropriate `inventory/group_vars/OSEv3.yml` group vars.\nNote that the `osm_cluster_network_cidr` must not overlap with the default\nDocker bridge subnet of 172.17.0.0/16. Or you should change the docker0 default\nCIDR range otherwise. For example, by adding `--bip=192.168.2.1/24` to\n`DOCKER_NETWORK_OPTIONS` located in `/etc/sysconfig/docker-network`.\n\nAlso note that the flannel network will be provisioned on a separate isolated Neutron\nsubnet defined from `osm_cluster_network_cidr` and having ports security disabled.\nUse the `openstack_private_data_network_name` variable to define the network\nname for the heat stack resource.\n\nAfter the cluster deployment done, you should run an additional post installation\nstep for flannel and docker iptables configuration:\n\n    ansible-playbook openshift-ansible-contrib/playbooks/provisioning/openstack/post-install.yml\n\n## Other configuration variables\n\n`openstack_ssh_public_key` is a Nova keypair - you can see your\nkeypairs with `openstack keypair list`. It must correspond to the\nprivate SSH key Ansible will use to log into the created VMs. This is\n`~/.ssh/id_rsa` by default, but you can use a different key by passing\n`--private-key` to `ansible-playbook`.\n\n`openstack_default_image_name` is the default name of the Glance image the\nservers will use. You can see your images with `openstack image list`.\nIn order to set a different image for a role, uncomment the line with the\ncorresponding variable (e.g. `openstack_lb_image_name` for load balancer) and\nset its value to another available image name. `openstack_default_image_name`\nmust stay defined as it is used as a default value for the rest of the roles.\n\n`openstack_default_flavor` is the default Nova flavor the servers will use.\nYou can see your flavors with `openstack flavor list`.\nIn order to set a different flavor for a role, uncomment the line with the\ncorresponding variable (e.g. `openstack_lb_flavor` for load balancer) and\nset its value to another available flavor. `openstack_default_flavor` must\nstay defined as it is used as a default value for the rest of the roles.\n\n`openstack_external_network_name` is the name of the Neutron network\nproviding external connectivity. It is often called `public`,\n`external` or `ext-net`. You can see your networks with `openstack\nnetwork list`.\n\n`openstack_private_network_name` is the name of the private Neutron network\nproviding admin/control access for ansible. It can be merged with other\ncluster networks, there are no special requirements for networking.\n\nThe `openstack_num_masters`, `openstack_num_infra` and\n`openstack_num_nodes` values specify the number of Master, Infra and\nApp nodes to create.\n\nThe `openshift_cluster_node_labels` defines custom labels for your openshift\ncluster node groups. It currently supports app and infra node groups.\nThe default value of this variable sets `region: primary` to app nodes and\n`region: infra` to infra nodes.\nAn example of setting a customised label:\n```\nopenshift_cluster_node_labels:\n  app:\n    mylabel: myvalue\n```\n\nThe `openstack_nodes_to_remove` allows you to specify the numerical indexes\nof App nodes that should be removed; for example, ['0', '2'],\n\nThe `docker_volume_size` is the default Docker volume size the servers will use.\nIn order to set a different volume size for a role,\nuncomment the line with the corresponding variable (e. g. `docker_master_volume_size`\nfor master) and change its value. `docker_volume_size` must stay defined as it is\nused as a default value for some of the servers (master, infra, app node).\nThe rest of the roles (etcd, load balancer, dns) have their defaults hard-coded.\n\n**Note**: If the `ephemeral_volumes` is set to `true`, the `*_volume_size` variables\nwill be ignored and the deployment will not create any cinder volumes.\n\nThe `openstack_flat_secgrp`, controls Neutron security groups creation for Heat\nstacks. Set it to true, if you experience issues with sec group rules\nquotas. It trades security for number of rules, by sharing the same set\nof firewall rules for master, node, etcd and infra nodes.\n\nThe `required_packages` variable also provides a list of the additional\nprerequisite packages to be installed before to deploy an OpenShift cluster.\nThose are ignored though, if the `manage_packages: False`.\n\nThe `openstack_inventory` controls either a static inventory will be created after the\ncluster nodes provisioned on OpenStack cloud. Note, the fully dynamic inventory\nis yet to be supported, so the static inventory will be created anyway.\n\nThe `openstack_inventory_path` points the directory to host the generated static inventory.\nIt should point to the copied example inventory directory, otherwise ti creates\na new one for you.\n\n## Multi-master configuration\n\nPlease refer to the official documentation for the\n[multi-master setup](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#multiple-masters)\nand define the corresponding [inventory\nvariables](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#configuring-cluster-variables)\nin `inventory/group_vars/OSEv3.yml`. For example, given a load balancer node\nunder the ansible group named `ext_lb`:\n\n    openshift_master_cluster_method: native\n    openshift_master_cluster_hostname: \"{{ groups.ext_lb.0 }}\"\n    openshift_master_cluster_public_hostname: \"{{ groups.ext_lb.0 }}\"\n\n## Provider Network\n\nNormally, the playbooks create a new Neutron network and subnet and attach\nfloating IP addresses to each node. If you have a provider network set up, this\nis all unnecessary as you can just access servers that are placed in the\nprovider network directly.\n\nTo use a provider network, set its name in `openstack_provider_network_name` in\n`inventory/group_vars/all.yml`.\n\nIf you set the provider network name, the `openstack_external_network_name` and\n`openstack_private_network_name` fields will be ignored.\n\n**NOTE**: this will not update the nodes' DNS, so running openshift-ansible\nright after provisioning will fail (unless you're using an external DNS server\nyour provider network knows about). You must make sure your nodes are able to\nresolve each other by name.\n\n## Security notes\n\nConfigure required `*_ingress_cidr` variables to restrict public access\nto provisioned servers from your laptop (a /32 notation should be used)\nor your trusted network. The most important is the `node_ingress_cidr`\nthat restricts public access to the deployed DNS server and cluster\nnodes' ephemeral ports range.\n\nNote, the command ``curl https://api.ipify.org`` helps fiding an external\nIP address of your box (the ansible admin node).\n\nThere is also the `manage_packages` variable (defaults to True) you\nmay want to turn off in order to speed up the provisioning tasks. This may\nbe the case for development environments. When turned off, the servers will\nbe provisioned omitting the ``yum update`` command. This brings security\nimplications though, and is not recommended for production deployments.\n\n### DNS servers security options\n\nAside from `node_ingress_cidr` restricting public access to in-stack DNS\nservers, there are following (bind/named specific) DNS security\noptions available:\n\n    named_public_recursion: 'no'\n    named_private_recursion: 'yes'\n\nExternal DNS servers, which is not included in the 'dns' hosts group,\nare not managed. It is up to you to configure such ones.\n\n## Configure the OpenShift parameters\n\nFinally, you need to update the DNS entry in\n`inventory/group_vars/OSEv3.yml` (look at\n`openshift_master_default_subdomain`).\n\nIn addition, this is the place where you can customise your OpenShift\ninstallation for example by specifying the authentication.\n\nThe full list of options is available in this sample inventory:\n\nhttps://github.com/openshift/openshift-ansible/blob/master/inventory/byo/hosts.ose.example\n\nNote, that in order to deploy OpenShift origin, you should update the following\nvariables for the `inventory/group_vars/OSEv3.yml`, `all.yml`:\n\n    deployment_type: origin\n    openshift_deployment_type: \"{{ deployment_type }}\"\n\n\n## Setting a custom entrypoint\n\nIn order to set a custom entrypoint, update `openshift_master_cluster_public_hostname`\n\n    openshift_master_cluster_public_hostname: api.openshift.example.com\n\nNote than an empty hostname does not work, so if your domain is `openshift.example.com`,\nyou cannot set this value to simply `openshift.example.com`.\n\n## Creating and using a Cinder volume for the OpenShift registry\n\nYou can optionally have the playbooks create a Cinder volume and set\nit up as the OpenShift hosted registry.\n\nTo do that you need specify the desired Cinder volume name and size in\nGigabytes in `inventory/group_vars/all.yml`:\n\n    cinder_hosted_registry_name: cinder-registry\n    cinder_hosted_registry_size_gb: 10\n\nWith this, the playbooks will create the volume and set up its\nfilesystem. If there is an existing volume of the same name, we will\nuse it but keep the existing data on it.\n\nTo use the volume for the registry, you must first configure it with\nthe OpenStack credentials by putting the following to `OSEv3.yml`:\n\n    openshift_cloudprovider_openstack_username: \"{{ lookup('env','OS_USERNAME') }}\"\n    openshift_cloudprovider_openstack_password: \"{{ lookup('env','OS_PASSWORD') }}\"\n    openshift_cloudprovider_openstack_auth_url: \"{{ lookup('env','OS_AUTH_URL') }}\"\n    openshift_cloudprovider_openstack_tenant_name: \"{{ lookup('env','OS_TENANT_NAME') }}\"\n\nThis will use the credentials from your shell environment. If you want\nto enter them explicitly, you can. You can also use credentials\ndifferent from the provisioning ones (say for quota or access control\nreasons).\n\n**NOTE**: If you're testing this on (DevStack)[devstack], you must\nexplicitly set your Keystone API version to v2 (e.g.\n`OS_AUTH_URL=http://10.34.37.47/identity/v2.0`) instead of the default\nvalue provided by `openrc`. You may also encounter the following issue\nwith Cinder:\n\nhttps://github.com/kubernetes/kubernetes/issues/50461\n\nYou can read the (OpenShift documentation on configuring\nOpenStack)[openstack] for more information.\n\n[devstack]: https://docs.openstack.org/devstack/latest/\n[openstack]: https://docs.openshift.org/latest/install_config/configuring_openstack.html\n\n\nNext, we need to instruct OpenShift to use the Cinder volume for it's\nregistry. Again in `OSEv3.yml`:\n\n    #openshift_hosted_registry_storage_kind: openstack\n    #openshift_hosted_registry_storage_access_modes: ['ReadWriteOnce']\n    #openshift_hosted_registry_storage_openstack_filesystem: xfs\n\nThe filesystem value here will be used in the initial formatting of\nthe volume.\n\nIf you're using the dynamic inventory, you must uncomment these two values as\nwell:\n\n    #openshift_hosted_registry_storage_openstack_volumeID: \"{{ lookup('os_cinder', cinder_hosted_registry_name).id }}\"\n    #openshift_hosted_registry_storage_volume_size: \"{{ cinder_hosted_registry_size_gb }}Gi\"\n\nBut note that they use the `os_cinder` lookup plugin we provide, so you must\ntell Ansible where to find it either in `ansible.cfg` (the one we provide is\nconfigured properly) or by exporting the\n`ANSIBLE_LOOKUP_PLUGINS=openshift-ansible-contrib/lookup_plugins` environment\nvariable.\n\n\n\n## Use an existing Cinder volume for the OpenShift registry\n\nYou can also use a pre-existing Cinder volume for the storage of your\nOpenShift registry.\n\nTo do that, you need to have a Cinder volume. You can create one by\nrunning:\n\n    openstack volume create --size <volume size in gb> <volume name>\n\nThe volume needs to have a file system created before you put it to\nuse.\n\nAs with the automatically-created volume, you have to set up the\nOpenStack credentials in `inventory/group_vars/OSEv3.yml` as well as\nregistry values:\n\n    #openshift_hosted_registry_storage_kind: openstack\n    #openshift_hosted_registry_storage_access_modes: ['ReadWriteOnce']\n    #openshift_hosted_registry_storage_openstack_filesystem: xfs\n    #openshift_hosted_registry_storage_openstack_volumeID: e0ba2d73-d2f9-4514-a3b2-a0ced507fa05\n    #openshift_hosted_registry_storage_volume_size: 10Gi\n\nNote the `openshift_hosted_registry_storage_openstack_volumeID` and\n`openshift_hosted_registry_storage_volume_size` values: these need to\nbe added in addition to the previous variables.\n\nThe **Cinder volume ID**, **filesystem** and **volume size** variables\nmust correspond to the values in your volume. The volume ID must be\nthe **UUID** of the Cinder volume, *not its name*.\n\nWe can do formate the volume for you if you ask for it in\n`inventory/group_vars/all.yml`:\n\n    prepare_and_format_registry_volume: true\n\n**NOTE:** doing so **will destroy any data that's currently on the volume**!\n\nYou can also run the registry setup playbook directly:\n\n   ansible-playbook -i inventory playbooks/provisioning/openstack/prepare-and-format-cinder-volume.yaml\n\n(the provisioning phase must be completed, first)\n\n\n\n## Configure static inventory and access via a bastion node\n\nExample inventory variables:\n\n    openstack_use_bastion: true\n    bastion_ingress_cidr: \"{{openstack_subnet_prefix}}.0/24\"\n    openstack_private_ssh_key: ~/.ssh/id_rsa\n    openstack_inventory: static\n    openstack_inventory_path: ../../../../inventory\n    openstack_ssh_config_path: /tmp/ssh.config.openshift.ansible.openshift.example.com\n\nThe `openstack_subnet_prefix` is the openstack private network for your cluster.\nAnd the `bastion_ingress_cidr` defines accepted range for SSH connections to nodes\nadditionally to the `ssh_ingress_cidr`` (see the security notes above).\n\nThe SSH config will be stored on the ansible control node by the\ngitven path. Ansible uses it automatically. To access the cluster nodes with\nthat ssh config, use the `-F` prefix, f.e.:\n\n    ssh -F /tmp/ssh.config.openshift.ansible.openshift.example.com master-0.openshift.example.com echo OK\n\nNote, relative paths will not work for the `openstack_ssh_config_path`, but it\nworks for the `openstack_private_ssh_key` and `openstack_inventory_path`. In this\nguide, the latter points to the current directory, where you run ansible commands\nfrom.\n\nTo verify nodes connectivity, use the command:\n\n    ansible -v -i inventory/hosts -m ping all\n\nIf something is broken, double-check the inventory variables, paths and the\ngenerated `<openstack_inventory_path>/hosts` and `openstack_ssh_config_path` files.\n\nThe `inventory: dynamic` can be used instead to access cluster nodes directly via\nfloating IPs. In this mode you can not use a bastion node and should specify\nthe dynamic inventory file in your ansible commands , like `-i openstack.py`.\n\n## Using Docker on the Ansible host\n\nIf you don't want to worry about the dependencies, you can use the\n[OpenStack Control Host image][control-host-image].\n\n[control-host-image]: https://hub.docker.com/r/redhatcop/control-host-openstack/\n\nIt has all the dependencies installed, but you'll need to map your\ncode and credentials to it. Assuming your SSH keys live in `~/.ssh`\nand everything else is in your current directory (i.e. `ansible.cfg`,\n`keystonerc`, `inventory`, `openshift-ansible`,\n`openshift-ansible-contrib`), this is how you run the deployment:\n\n    sudo docker run -it -v ~/.ssh:/mnt/.ssh:Z \\\n        -v $PWD:/root/openshift:Z \\\n        -v $PWD/keystonerc:/root/.config/openstack/keystonerc.sh:Z \\\n        redhatcop/control-host-openstack bash\n\n(feel free to replace `$PWD` with an actual path to your inventory and\ncheckouts, but note that relative paths don't work)\n\nThe first run may take a few minutes while the image is being\ndownloaded. After that, you'll be inside the container and you can run\nthe playbooks:\n\n    cd openshift\n    ansible-playbook openshift-ansible-contrib/playbooks/provisioning/openstack/provision.yaml\n\n\n### Run the playbook\n\nAssuming your OpenStack (Keystone) credentials are in the `keystonerc`\nthis is how you stat the provisioning process from your ansible control node:\n\n    . keystonerc\n    ansible-playbook openshift-ansible-contrib/playbooks/provisioning/openstack/provision.yaml\n\nNote, here you start with an empty inventory. The static inventory will be populated\nwith data so you can omit providing additional arguments for future ansible commands.\n\nIf bastion enabled, the generates SSH config must be applied for ansible.\nOtherwise, it is auto included by the previous step. In order to execute it\nas a separate playbook, use the following command:\n\n    ansible-playbook openshift-ansible-contrib/playbooks/provisioning/openstack/post-provision-openstack.yml\n\nThe first infra node then becomes a bastion node as well and proxies access\nfor future ansible commands. The post-provision step also configures Satellite,\nif requested, and DNS server, and ensures other OpenShift requirements to be met.\n\n\n## Running Custom Post-Provision Actions\n\nA custom playbook can be run like this:\n\n```\nansible-playbook --private-key ~/.ssh/openshift -i inventory/ openshift-ansible-contrib/playbooks/provisioning/openstack/custom-actions/custom-playbook.yml\n```\n\nIf you'd like to limit the run to one particular host, you can do so as follows:\n\n```\nansible-playbook --private-key ~/.ssh/openshift -i inventory/ openshift-ansible-contrib/playbooks/provisioning/openstack/custom-actions/custom-playbook.yml -l app-node-0.openshift.example.com\n```\n\nYou can also create your own custom playbook. Here are a few examples:\n\n### Adding additional YUM repositories\n\n```\n---\n- hosts: app\n  tasks:\n\n  # enable EPL\n  - name: Add repository\n    yum_repository:\n      name: epel\n      description: EPEL YUM repo\n      baseurl: https://download.fedoraproject.org/pub/epel/$releasever/$basearch/\n```\n\nThis example runs against app nodes. The list of options include:\n\n  - cluster_hosts (all hosts: app, infra, masters, dns, lb)\n  - OSEv3 (app, infra, masters)\n  - app\n  - dns\n  - masters\n  - infra_hosts\n\n### Attaching additional RHN pools\n\n```\n---\n- hosts: cluster_hosts\n  tasks:\n  - name: Attach additional RHN pool\n    become: true\n    command: \"/usr/bin/subscription-manager attach --pool=<pool ID>\"\n    register: attach_rhn_pool_result\n    until: attach_rhn_pool_result.rc == 0\n    retries: 10\n    delay: 1\n```\n\nThis playbook runs against all cluster nodes. In order to help prevent slow connectivity\nproblems, the task is retried 10 times in case of initial failure.\nNote that in order for this example to work in your deployment, your servers must use the RHEL image.\n\n### Adding extra Docker registry URLs\n\nThis playbook is located in the [custom-actions](https://github.com/openshift/openshift-ansible-contrib/tree/master/playbooks/provisioning/openstack/custom-actions) directory.\n\nIt adds URLs passed as arguments to the docker configuration program.\nGoing into more detail, the configuration program (which is in the YAML format) is loaded into an ansible variable\n([lines 27-30](https://github.com/openshift/openshift-ansible-contrib/blob/master/playbooks/provisioning/openstack/custom-actions/add-docker-registry.yml#L27-L30))\nand in its structure, `registries` and `insecure_registries` sections are expanded with the newly added items\n([lines 56-76](https://github.com/openshift/openshift-ansible-contrib/blob/master/playbooks/provisioning/openstack/custom-actions/add-docker-registry.yml#L56-L76)).\nThe new content is then saved into the original file\n([lines 78-82](https://github.com/openshift/openshift-ansible-contrib/blob/master/playbooks/provisioning/openstack/custom-actions/add-docker-registry.yml#L78-L82))\nand docker is restarted.\n\nExample usage:\n```\nansible-playbook -i <inventory> openshift-ansible-contrib/playbooks/provisioning/openstack/custom-actions/add-docker-registry.yml  --extra-vars '{\"registries\": \"reg1\", \"insecure_registries\": [\"ins_reg1\",\"ins_reg2\"]}'\n```\n\n### Adding extra CAs to the trust chain\n\nThis playbook is also located in the [custom-actions](https://github.com/openshift/openshift-ansible-contrib/blob/master/playbooks/provisioning/openstack/custom-actions) directory.\nIt copies passed CAs to the trust chain location and updates the trust chain on each selected host.\n\nExample usage:\n```\nansible-playbook -i <inventory> openshift-ansible-contrib/playbooks/provisioning/openstack/custom-actions/add-cas.yml --extra-vars '{\"ca_files\": [<absolute path to ca1 file>, <absolute path to ca2 file>]}'\n```\n\nPlease consider contributing your custom playbook back to openshift-ansible-contrib!\n\nA library of custom post-provision actions exists in `openshift-ansible-contrib/playbooks/provisioning/openstack/custom-actions`. Playbooks include:\n\n* [add-yum-repos.yml](https://github.com/openshift/openshift-ansible-contrib/blob/master/playbooks/provisioning/openstack/custom-actions/add-yum-repos.yml): adds a list of custom yum repositories to every node in the cluster\n* [add-rhn-pools.yml](https://github.com/openshift/openshift-ansible-contrib/blob/master/playbooks/provisioning/openstack/custom-actions/add-rhn-pools.yml): attaches a list of additional RHN pools to every node in the cluster\n* [add-docker-registry.yml](https://github.com/openshift/openshift-ansible-contrib/blob/master/playbooks/provisioning/openstack/custom-actions/add-docker-registry.yml): adds a list of docker registries to the docker configuration on every node in the cluster\n* [add-cas.yml](https://github.com/openshift/openshift-ansible-contrib/blob/master/playbooks/provisioning/openstack/custom-actions/add-rhn-pools.yml): adds a list of CAs to the trust chain on every node in the cluster\n\n\n## Install OpenShift\n\nOnce it succeeds, you can install openshift by running:\n\n    ansible-playbook openshift-ansible/playbooks/byo/config.yml\n\n## Access UI\n\nOpenShift UI may be accessed via the 1st master node FQDN, port 8443.\n\nWhen using a bastion, you may want to make an SSH tunnel from your control node\nto access UI on the `https://localhost:8443`, with this inventory variable:\n\n   openshift_ui_ssh_tunnel: True\n\nNote, this requires sudo rights on the ansible control node and an absolute path\nfor the `openstack_private_ssh_key`. You should also update the control node's\n`/etc/hosts`:\n\n    127.0.0.1 master-0.openshift.example.com\n\nIn order to access UI, the ssh-tunnel service will be created and started on the\ncontrol node. Make sure to remove these changes and the service manually, when not\nneeded anymore.\n\n## Scale Deployment up/down\n\n### Scaling up\n\nOne can scale up the number of application nodes by executing the ansible playbook\n`openshift-ansible-contrib/playbooks/provisioning/openstack/scale-up.yaml`.\nThis process can be done even if there is currently no deployment available.\nThe `increment_by` variable is used to specify by how much the deployment should\nbe scaled up (if none exists, it serves as a target number of application nodes).\nThe path to `openshift-ansible` directory can be customised by the `openshift_ansible_dir`\nvariable. Its value must be an absolute path to `openshift-ansible` and it cannot\ncontain the '/' symbol at the end.\n\nUsage:\n\n```\nansible-playbook -i <path to inventory> openshift-ansible-contrib/playbooks/provisioning/openstack/scale-up.yaml` [-e increment_by=<number>] [-e openshift_ansible_dir=<path to openshift-ansible>]\n```\n\nNote: This playbook works only without a bastion node (`openstack_use_bastion: False`).\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8b5349594d059fc146cfd48154753b7ebef22d04", "filename": "roles/osp/admin-sec-group/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Manage Security Groups\n  os_security_group:\n    cloud: \"{{ item.cloud | default(osp_default_cloud) | default(omit) }}\"\n    state: \"{{ item.state | default(osp_resource_state) | default('present') }}\"\n    name: \"{{ item.name }}\"\n    description: \"{{ item.description | default(omit) }}\"\n  with_items:\n  - \"{{ osp_security_groups | default([]) }}\"\n  register: security_groups\n\n- name: Create Security Group Rules\n  os_security_group_rule:\n    cloud: \"{{ item.cloud | default(osp_default_cloud) | default(omit) }}\"\n    security_group: \"{{ item.0.item.name }}\"\n    protocol: \"{{ item.1.protocol | default(omit) }}\"\n    port_range_min: \"{{ item.1.port_range_min | default(omit) }}\"\n    port_range_max: \"{{ item.1.port_range_max | default(omit) }}\"\n    direction: \"{{ item.1.direction | default(omit) }}\"\n    remote_ip_prefix: \"{{ item.1.remote_ip_prefix | default(omit) }}\"\n  with_subelements:\n  - \"{{ security_groups.results }}\"\n  - item.rules \n  when: \n  - item.0.invocation.module_args.state == 'present'\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5d1f5d656423707012bfe55f0385ecafd9ab2081", "filename": "roles/dns/manage-dns-records/tasks/route53/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- include_tasks: process-records.yml\n  with_subelements:\n    - \"{{ dns_data.views }}\"\n    - zones\n  loop_control:\n    loop_var: dns\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "e985f9270f6f9bca323e4a4310ac589c5ad05dc3", "filename": "roles/ssh_tunneling/meta/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\ndependencies:\n  - { role: common, tags: common }\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a2d5a6297b024178f5b9f61c5cebfd10725d4a0b", "filename": "roles/config-docker/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\ndocker_username: root"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "03af4243588c5cb7719cc8a7e5b631224f6e3136", "filename": "roles/config-satellite/tasks/manifest.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Copy subscription manifest .zip file\"\n  copy:\n    src: \"{{ manifest_file_path | default(default_manifest_file_path) }}\"\n    dest: /root/manifest.zip\n  register: copy_sub\n\n- name: \"Upload manifest if new or updated file provided\"\n  command: > \n    hammer \n      -u \"{{ satellite_username }}\"\n      -p \"{{ satellite_password }}\"\n      subscription upload\n      --file /root/manifest.zip\n      --organization \"{{ satellite_organization }}\"\n  when: copy_sub|changed\n\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9e4ee86ab2a9da8b085553ae6bc1004e1a575e14", "filename": "playbooks/prep.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Configure RHSM usernames/passwords'\n  hosts: localhost\n  tasks:\n  - block:\n    - pause:\n        prompt: 'Please enter your Red Hat Subscription username'\n      register: username\n    - set_fact:\n        rhsm_username: \"{{ username.user_input }}\"\n    - pause:\n        prompt: 'Please enter your Red Hat Subscription password'\n        echo: no\n      no_log: True\n      register: password\n    - set_fact:\n        rhsm_password: \"{{ password.user_input }}\"\n      no_log: True\n    tags:\n    - configure_rhsm\n    when:\n    - rhsm_register|default(False)\n    - rhsm_activationkey is undefined\n    - rhsm_org_id is undefined\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "dbff85662432013d02018e7b1ba984652d54cb1f", "filename": "roles/docker-storage-setup/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: stop docker\n  service: name=docker state=stopped\n\n- block:\n    - name: create the docker-storage config file\n      template:\n        src: \"{{ role_path }}/templates/docker-storage-setup-overlayfs.j2\"\n        dest: /etc/sysconfig/docker-storage-setup\n        owner: root\n        group: root\n        mode: 0644\n  when:\n    - ansible_distribution_version | version_compare('7.4', '>=')\n    - ansible_distribution == \"RedHat\"\n\n- block:\n    - name: create the docker-storage-setup config file\n      template:\n        src: \"{{ role_path }}/templates/docker-storage-setup-dm.j2\"\n        dest: /etc/sysconfig/docker-storage-setup\n        owner: root\n        group: root\n        mode: 0644\n  when:\n    - ansible_distribution_version | version_compare('7.4', '<')\n    - ansible_distribution == \"RedHat\"\n\n- name: start docker\n  service: name=docker state=started enabled=true\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "12f2f6cf52e08d69d64e81664c6ba6d9c3517692", "filename": "reference-architecture/gcp/ansible/playbooks/openshift-scaleup.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: verify current and desired state\n  hosts: localhost\n  tasks:\n  - name: get current number of master nodes\n    command: gcloud --project {{ gcloud_project }} compute instance-groups managed describe {{ prefix }}-master --region {{ gcloud_region }} --format 'value(targetSize)'\n    register: masters_nb\n    changed_when: false\n\n  - name: get current number of infra nodes\n    command: gcloud --project {{ gcloud_project }} compute instance-groups managed describe {{ prefix }}-infra-node --region {{ gcloud_region }} --format 'value(targetSize)'\n    register: infra_nodes_nb\n    changed_when: false\n\n  - name: get current number of app nodes\n    command: gcloud --project {{ gcloud_project }} compute instance-groups managed describe {{ prefix }}-node --region {{ gcloud_region }} --format 'value(targetSize)'\n    register: nodes_nb\n    changed_when: false\n\n  - name: assert that we are not scaling down\n    assert:\n      that:\n      - \"master_instance_group_size >= masters_nb.stdout | int\"\n      - \"infra_node_instance_group_size >= infra_nodes_nb.stdout | int\"\n      - \"node_instance_group_size >= nodes_nb.stdout | int\"\n      msg: Scaling down is not supported! Desired number of nodes must be equal or bigger than the current state.\n\n  - name: verify that there is something to scale up\n    fail:\n      msg: To scale up, update 'master_instance_group_size', 'infra_node_instance_group_size' and 'node_instance_group_size' variables to the desired number of nodes in your 'config.yaml' file and rerun this command.\n    when:\n    - master_instance_group_size == masters_nb.stdout | int\n    - infra_node_instance_group_size == infra_nodes_nb.stdout | int\n    - node_instance_group_size == nodes_nb.stdout | int\n\n- include: core-infra.yaml\n\n- name: create instance groups\n  hosts: localhost\n  roles:\n  - instance-groups\n\n- include: ../../../../playbooks/prerequisite.yaml\n- include: ../../../../../openshift-ansible/playbooks/byo/openshift_facts.yml\n- include: ../../../../../openshift-ansible/playbooks/byo/openshift-master/scaleup.yml\n  when:\n  - groups['new_masters'] is defined\n  - groups['new_masters'] | length > 0\n- include: ../../../../../openshift-ansible/playbooks/byo/openshift-node/scaleup.yml\n  when:\n  - groups['new_nodes'] is defined\n  - groups['new_nodes'] | length > 0\n- include: openshift-post.yaml\n"}, {"commit_sha": "9e7bed4aaef56159f148d24e4f7d7e8b53be632b", "sha": "216eaa495d8af4dfe0ec808c9f6df3093bb0f22a", "filename": "handlers/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# handlers file for ansible-role-docker-ce\n\n- name: restart docker\n  service:\n    name: docker\n    state: restarted\n  become: true\n\n# Workaround because systemd cannot be used: https://github.com/ansible/ansible/issues/22171\n- name: restart auditd\n  shell: service auditd restart\n  args:\n    warn: false\n  become: true\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7078424fb99d0eafaccedefb5606e2f0df523468", "filename": "roles/user-management/manage-user-passwd/tasks/generate-one-password.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Set password to given value or default\"\n  set_fact:\n    password: \"{{ item.password | default('') }}\"\n\n- name: \"Create random password\"\n  set_fact:\n    password: \"{{ lookup('password','/dev/null length=16') }}\"\n  when:\n  - item.generate_password is defined\n  - item.generate_password == True\n  - password|trim == ''\n\n\n- name: \"Add new user passwords\"\n  set_fact:\n    user_passwords: \"{{ user_passwords|default({}) | combine({ item.user_name: password }) }}\"\n\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "8b134ad5d493351041b19eb4f4cdd11f5b4d6628", "filename": "reference-architecture/aws-ansible/playbooks/roles/cloudformation-infra/files/add-node.json", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "{\n  \"AWSTemplateFormatVersion\": \"2010-09-09\",\n  \"Parameters\": {\n    \"KeyName\": {\n      \"Type\": \"AWS::EC2::KeyPair::KeyName\"\n    },\n    \"Route53HostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"AmiId\": {\n      \"Type\": \"AWS::EC2::Image::Id\"\n    },\n    \"InstanceType\": {\n      \"Type\": \"String\",\n      \"Default\": \"t2.medium\"\n    },\n    \"NodeRootVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"50\"\n    },\n    \"NodeDockerVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"NodeDockerVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"NodeUserData\": {\n      \"Type\": \"String\"\n    },\n    \"NodeEmptyVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"NodeEmptyVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"NodeRootVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"Subnet\": {\n      \"Type\": \"String\"\n    },\n    \"NodeName\": {\n      \"Type\": \"String\"\n    },\n    \"NodeInstanceProfile\": {\n      \"Type\": \"String\"\n    },\n    \"NodeType\": {\n      \"Type\": \"String\"\n    },\n    \"NodeSg\": {\n      \"Type\": \"String\"\n    }\n  },\n  \"Resources\": {\n    \"Route53Records\": {\n      \"Type\": \"AWS::Route53::RecordSetGroup\",\n      \"DependsOn\": [\n        \"NewNode\"\n      ],\n      \"Properties\": {\n        \"HostedZoneName\": { \"Ref\": \"Route53HostedZone\" },\n        \"RecordSets\": [\n          {\n            \"Name\":  { \"Ref\": \"NodeName\" },\n            \"Type\": \"A\",\n\t            \"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"NewNode\", \"PrivateIp\"] }]\n          }\n        ]\n      }\n    },\n    \"NewNode\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InstanceType\"},\n\t  \"SecurityGroupIds\": [{\"Ref\": \"NodeSg\"}],\n          \"SubnetId\" : {\"Ref\": \"Subnet\"},\n          \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Ref\": \"NodeName\"}\n            },\n            { \"Key\": \"provision\",\n              \"Value\": \"node\"\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": {\"Ref\": \"NodeType\"}\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          }\n         ]\n     }\n   }\n }\n}\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "cab27676d474763c70c26a13fcd0c65e0fd2b9e3", "filename": "roles/dns/manage-dns-zones/tasks/route53/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Fail when AWS KEY environment variables are not defined\n  debug:\n    msg: \"Both 'AWS_ACCESS_KEY_ID' and 'AWS_SECRET_ACCESS_KEY' environment variables must be defined\"\n  failed_when: (lookup('env','AWS_ACCESS_KEY_ID')|trim == \"\") or\n               (lookup('env','AWS_SECRET_ACCESS_KEY')|trim == \"\")\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "2a2ddda4ad416fa5c7c50868fa65189b33108afa", "filename": "reference-architecture/gcp/ansible/playbooks/roles/openshift-ansible-installer/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: check the current state of openshift installer dir\n  stat:\n    path: '{{ openshift_ansible_installer_dir }}'\n  register: aoi_dir_stat\n\n- block:\n  - name: check if openshift ansible installer rpm is available\n    command: yum list atomic-openshift-utils\n    register: aoi_available\n    ignore_errors: true\n\n  - block:\n    - name: check if the openshift ansible installer is installed\n      command: rpm -q atomic-openshift-utils\n      register: aoi_installed\n      ignore_errors: true\n\n    - name: assert that the openshift installer is installed\n      assert:\n        that:\n        - aoi_installed | succeeded\n        msg: Please install the atomic-openshift-utils package on the deployer machine.\n\n    - name: create symlink from aoi dir to rpm location\n      file:\n        src: /usr/share/ansible/openshift-ansible\n        dest: '{{ openshift_ansible_installer_dir }}'\n        state: link\n    when: aoi_available | succeeded\n  when:\n  - ansible_distribution == 'RedHat'\n  - ansible_distribution_major_version == '7'\n  - aoi_dir_stat.stat.isdir is not defined or not aoi_dir_stat.stat.isdir\n\n- name: download openshift ansible installer from github\n  git:\n    repo: https://github.com/openshift/openshift-ansible.git\n    dest: '{{ playbook_dir }}/../../../../../openshift-ansible'\n    version: '{{ openshift_ansible_branch }}'\n  when: aoi_available is not defined or aoi_available | failed or aoi_available | skipped\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "83aeb478c30509bda2ebfd0dae6c9711b0883e55", "filename": "tasks/Win32NT/install/tarball.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Mkdir for java installation\n  win_file:\n    path: '{{ java_path }}'\n    state: directory\n\n- name: 'Install java {{ java_full_version }}'\n  win_unzip:\n    src: '{{ java_artifact }}'\n    dest: '{{ java_path }}'\n    creates: '{{ java_path }}\\{{ java_folder }}'\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6d66f8b87e494f2d9d94ecc233ce16581c62b764", "filename": "reference-architecture/ansible-tower-integration/tower_unconfig_azure/tower_unconfig_azure/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n\n- name: Remove workflow-ocp-azure-install\n  command: tower-cli workflow delete --name=\"workflow-ocp-azure-install\"\n\n- name: Remove azure-deploy-ocp job template\n  tower_job_template:\n    name: azure-deploy-ocp\n    job_type: run\n    project: openshift-ansible-contrib\n    playbook: \"reference-architecture/azure-ansible/3.6/ansibledeployocp/playbooks/deploy.yml\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove workflow-ocp-azure-destroy\n  command: tower-cli workflow delete --name=\"workflow-ocp-azure-destroy\"\n\n- name: Remove azure-destroy-ocp job template\n  tower_job_template:\n    name: azure-destroy-ocp\n    job_type: run\n    project: openshift-ansible-contrib\n    playbook: \"reference-architecture/azure-ansible/3.6/ansibledeployocp/playbooks/destroy.yml\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower group\n  tower_group:\n    name: azure\n    inventory: \"azure-inventory\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower inventory\n  tower_inventory:\n    name: \"azure-inventory\"\n    organization: \"Default\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower credential for machine\n  tower_credential:\n    name: azure-privkey\n    kind: ssh\n    organization: \"Default\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower project\n  tower_project:\n    name: \"openshift-ansible-contrib\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Remove tower organization\n  tower_organization:\n    name: \"Default\"\n    description: \"Set to Default since the trial license only allows one organization. You can change it if you have deep pockets\"\n    state: absent\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Azure Login\n  command: azure login -u \"{{ AZURE_USERNAME }}\" -p \"{{ AZURE_PASSWORD }}\"\n\n- name: Find Azure Service Principal Object ID\n  shell: azure ad sp show -c \"{{ AZURE_SP_NAME }}\" |grep \"Object Id:\" |awk -F\" \" '{print $4}'\n  register: azure_sp_object_id\n\n- name: Find Azure Subscription ID\n  shell: azure account show |grep -v Tenant |grep ID |awk -F\" \" '{print $4}'\n  register: azure_subscription_id\n\n- name: Delete contributor to Service Principal\n  command: azure role assignment delete -q --objectId {{ azure_sp_object_id.stdout }} -o contributor -c /subscriptions/{{ azure_subscription_id.stdout }}\n\n- name: Remove Azure Service Principal\n  command: azure ad sp delete -qdo \"{{ azure_sp_object_id.stdout }}\"\n\n- name: Remove  npm azure-cli\n  command: npm rm -g azure-cli\n  become: true\n\n- name: Remove npm\n  yum:\n    name: npm\n    state: absent\n  become: true\n\n- name: Remove tower-cli\n  yum:\n    name: ansible-tower-cli\n    state: absent\n  become: true\n\n- name: Remove pip\n  yum:\n    name: python2-pip\n    state: absent\n  become: true\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "366504bb261ca465dd7e50839a3067e04ec9c877", "filename": "tasks/replication/slave/import_data.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n# Doc: https://www.percona.com/doc/percona-xtrabackup/2.1/howtos/recipes_ibkx_gtid.html\n- name: WAIT_FOR | source data (prevent rsync bug) - TODO find another hack\n  wait_for:\n    host: \"{{ mariadb_slave_import_from }}\"\n    port: 22\n\n- name: COMMAND | Prepare backup another server\n  command: innobackupex --no-timestamp {{ mariadb_backup_dir }} creates={{ mariadb_backup_dir }}\n  delegate_to: \"{{ mariadb_slave_import_from }}\"\n  register: backup\n\n- name: SHELL | Dump\n  shell: \"innobackupex --apply-log {{ mariadb_backup_dir }}\"\n  delegate_to: \"{{ mariadb_slave_import_from }}\"\n  when: backup.changed\n\n- name: FILE | Remove mysql db from backup\n  file:\n    path: \"{{ mariadb_backup_dir }}/mysql\"\n    state: absent\n  delegate_to: \"{{ mariadb_slave_import_from }}\"\n  when: backup.changed and not mariadb_slave_replicate_mysqldb\n\n- name: MYSQL_VARIABLES | Get datadir\n  mysql_variables: variable=datadir\n  register: datadir\n\n- name: SET_FACT | related to mysql datadir\n  set_fact:\n    mariadb_datadir: \"{{ datadir.msg }}\"\n    mariadb_binlog_info: \"{{ datadir.msg }}/xtrabackup_binlog_info\"\n\n- name: SERVICE | Stop MariaDB before importing data\n  service:\n    name: mysql\n    state: stopped\n\n# TODO: add an \"ignore warning\"\n- name: COMMAND | Sync backup to slave - TODO remove vagrant as static user (see why mariadb_backup_user is not working)\n  shell: \"sudo -E rsync --rsync-path='sudo rsync' -a -e 'ssh -o StrictHostKeyChecking=no' {{ mariadb_backup_user }}@{{ mariadb_slave_import_from }}:{{ mariadb_backup_dir }}/ {{ mariadb_datadir }}/\"\n  become: no\n\n- name: FILE | Re-apply owner\n  file:\n    path: \"{{ mariadb_datadir }}\"\n    state: directory\n    owner: mysql\n    group: mysql\n    recurse: yes\n\n- name: SERVICE | Start MariaDB\n  service:\n    name: mysql\n    state: started\n\n- name: SHELL | Get master_log_file\n  command: \"awk '{ print $1 }' {{ mariadb_binlog_info }}\"\n  register: master_log_file\n\n- name: SHELL | Get master_log_pos\n  command: \"awk '{ print $2 }' {{ mariadb_binlog_info }}\"\n  register: master_log_pos\n\n- name: SHELL | Get master GTID\n  command: \"awk '{ print $3 }' {{ mariadb_binlog_info }}\"\n  register: master_gtid\n\n- name: SET_FACT | master_log_file\n  set_fact:\n    mariadb_master_log_file: \"{{ master_log_file.stdout }}\"\n    mariadb_master_log_pos: \"{{ master_log_pos.stdout }}\"\n    mariadb_master_gtid: \"{{ master_gtid.stdout }}\"\n\n- name: FILE | Delete dump\n  file: path={{ mariadb_backup_dir }} state=absent\n  delegate_to: \"{{ mariadb_slave_import_from }}\"\n  when: mariadb_slave_import_flush_dump\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "fe8e646a51894f414897f2b613478b368f15b108", "filename": "roles/samba/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "---\n# Create a smbuser\n#\n\n- name: Create smb user\n  user:\n    name: \"{{ smbuser }}\"\n    shell: /sbin/nologin\n    password: \"{{ smbpassword }}\"\n\n- name: Create the public folder\n  file:\n    dest: \"{{ shared_dir }}\"\n    owner: \"{{ smbuser }}\"\n    group: \"{{ smbuser }}\"\n    mode: 0777\n    state: directory\n\n# Install and configure samba server (requires ports 137, 138, 139, 445 open).\n- name: Ensure Samba-related packages are installed\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n  - samba\n  - samba-client\n  - samba-common\n  - cifs-utils\n  tags:\n    - samba\n    - download\n\n- name: Put our smb.conf in place\n  template:\n    src: smb.conf.j2\n    dest: /etc/samba/smb.conf\n\n- name: Ensure Samba is running and set to start on boot.\n  service:\n    name: \"{{ smb_service }}\"\n    state: started\n    enabled: yes\n  tags:\n    - samba\n  when : samba_enabled\n\n- name: NetBIOS name server is running and set to start on boot\n  service:\n    name: \"{{ nmb_service }}\"\n    state: started\n    enabled: yes\n  tags:\n    - samba\n  when : samba_enabled\n\n- name: Disable Samba if that is wanted\n  service:\n    name: \"{{ smb_service }}\"\n    state: stopped\n    enabled: no\n  tags:\n    - samba\n  when : not samba_enabled\n\n- name: Disable Samba name server if that is wanted\n  service:\n    name: \"{{ nmb_service }}\"\n    state: stopped\n    enabled: no\n  tags:\n    - samba\n  when : not samba_enabled\n\n- name: Add 'samba' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: samba\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: Samba\n    - option: description\n      value: '\"Samba is a Microsoft-compatible network file system that re-implements SMB/CIFS (Common Internet File System).\"'\n    - option: enabled\n      value: \"{{ samba_enabled }}\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "eee5816f0c4ba6e0be297d3d3eb5ec87fe0732ea", "filename": "playbooks/roles/sensor-common/tasks/configure.yml", "repository": "rocknsm/rock", "decoded_content": "---\n######################################################\n################# Data Directory #####################\n######################################################\n###############\n##### NOTE ####\n###############\n# You will want to remount this to your \"good\" storage after the build.\n# This is just to make sure all the paths in the configs are proper.\n###############\n- name: Create ROCK data dir\n  file:\n    path: \"{{ rock_data_dir }}\"\n    mode: 0755\n    owner: \"{{ rock_data_user }}\"\n    group: \"{{ rock_data_group }}\"\n    state: directory\n\n- name: Create ROCK NSM directory\n  file:\n    path: \"{{ rocknsm_dir }}\"\n    mode: 0755\n    owner: root\n    group: root\n    state: directory\n\n######################################################\n######### Configure the monitoring interface #########\n######################################################\n- name: Set monitor interface config\n  template:\n    src: templates/ifcfg-monif.j2\n    dest: /etc/sysconfig/network-scripts/ifcfg-{{ item }}\n    mode: 0644\n    owner: root\n    group: root\n    force: yes\n  with_items: \"{{ rock_monifs }}\"\n\n- name: Configure local ifup script\n  template:\n    src: templates/ifup-local.j2\n    dest: /sbin/ifup-local\n    mode: 0755\n    owner: root\n    group: root\n    force: yes\n  notify: configure monitor interfaces\n\n#######################################################\n#################### Disable IPv6 #####################\n#######################################################\n- name: Disable IPv6 for all interfaces\n  sysctl:\n    name: net.ipv6.conf.all.disable_ipv6\n    value: 1\n    sysctl_file: \"{{ rock_sysctl_file }}\"\n\n- name: Disable IPv6 for default interfaces\n  sysctl:\n    name: net.ipv6.conf.default.disable_ipv6\n    value: 1\n    sysctl_file: \"{{ rock_sysctl_file }}\"\n\n- name: Disable IPv6 in SSHD\n  lineinfile:\n    dest: /etc/ssh/sshd_config\n    regexp: AddressFamily\n    line: AddressFamily inet\n  notify:\n  - sshd restart\n\n- name: Remove localhost6 from hosts file\n  lineinfile:\n    dest: /etc/hosts\n    regexp: localhost6\n    state: absent\n\n#######################################################\n#################### DNS Changes ######################\n#######################################################\n- name: Set hostname in hosts file\n  lineinfile:\n    dest: /etc/hosts\n    insertafter: 127.0.0.1\n    line: 127.0.0.2  {{ rock_fqdn }}  {{ rock_hostname }}\n\n- name: Set system hostname\n  hostname:\n    name: \"{{ rock_fqdn }}\"\n\n#######################################################\n################## Setup Yum Repos ####################\n#######################################################\n\n- name: Setup EPEL repo\n  yum_repository:\n    name: epel\n    description: EPEL YUM repo\n    baseurl: \"{{ epel_baseurl }}\"\n    gpgkey: \"{{ epel_gpgurl }}\"\n    gpgcheck: yes\n  when: rock_online_install\n\n- name: Manually trust CentOS GPG key\n  rpm_key:\n    state: present\n    key: http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-7\n  when: rock_online_install\n\n- name: Setup Elastic repo\n  yum_repository:\n    name: elastic-6.x\n    description: Elastic Stack repository for 6.x\n    baseurl: \"{{ elastic_baseurl }}\"\n    gpgkey:  \"{{ elastic_gpgurl }}\"\n    gpgcheck: no\n  when: rock_online_install\n\n- name: Install RockNSM GPG keys\n  copy:\n    src: \"{{ item }}\"\n    dest: \"/etc/pki/rpm-gpg/{{ item }}\"\n    mode: 0644\n    owner: root\n    group: root\n  with_items:\n    - RPM-GPG-KEY-RockNSM-2\n    - RPM-GPG-KEY-RockNSM-2.1-Testing\n    - RPM-GPG-KEY-RockNSM-pkgcloud-2_1\n\n- name: Trust RockNSM GPG keys\n  rpm_key:\n    state: present\n    key: \"{{ item.path }}\"\n  with_items:\n    - { repoid: \"rocknsm_2_1\", path: \"/etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-2\" }\n    - { repoid: \"rocknsm_2_1\", path: \"/etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-pkgcloud-2_1\" }\n    - { repoid: \"rocknsm-testing\", path: \"/etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-2.1-Testing\"}\n    - { repoid: \"rocknsm-local\", path: \"/etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-2\" }\n  register: registered_keys\n\n- name: Configure RockNSM online repos\n  yum_repository:\n    file: rocknsm\n    name: \"{{ item.name }}\"\n    enabled: \"{{ rock_online_install }}\"\n    description: \"{{ item.name }}\"\n    baseurl: \"{{ item.baseurl }}\"\n    repo_gpgcheck: 1\n    gpgcheck: \"{{ item.gpgcheck }}\"\n    gpgkey:\n      - file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-pkgcloud-2_1\n      - file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-2\n    sslverify: 1\n    sslcacert: /etc/pki/tls/certs/ca-bundle.crt\n    metadata_expire: 300\n    cost: 750\n    state: present\n  with_items:\n    - { name: \"rocknsm_2_1\", gpgcheck: yes, baseurl: \"{{ rocknsm_baseurl }}\" }\n    - { name: \"rocknsm_2_1-source\", gpgcheck: no, baseurl: \"{{ rocknsm_srpm_baseurl }}\" }\n\n- name: Configure RockNSM online testing repos\n  yum_repository:\n    file: \"rocknsm-testing\"\n    name: \"rocknsm-testing\"\n    description: \"RockNSM 2.1 - Testing - $basearch\"\n    baseurl: \"{{ rocknsm_testing_baseurl }}\"\n    skip_if_unavailable: \"True\"\n    gpgcheck: 1\n    gpgkey:\n      - file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-2.1-Testing\n    repo_gpgcheck: 0\n    enabled: \"{{ (rock_online_install and rock_enable_testing) | bool }}\"\n    cost: 750\n    state: present\n\n- name: Setup local offline repo\n  yum_repository:\n    name: rocknsm-local\n    description: ROCKNSM Local Repository\n    baseurl: \"{{ rocknsm_local_baseurl }}\"\n    gpgcheck: 1\n    gpgkey: file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-2\n    repo_gpgcheck: 1\n    cost: 500\n  when: \"{{ not rock_online_install | bool }}\"\n\n- name: Trust RockNSM GPG keys in yum\n  command: \"yum -q makecache -y --disablerepo='*' --enablerepo='{{ item.repoid }}'\"\n  with_items:\n    - { repoid: \"rocknsm_2_1\", test: \"{{ rock_online_install }}\" }\n    - { repoid: \"rocknsm_2_1-source\", test: \"{{ rock_online_install }}\" }\n    - { repoid: \"rocknsm-testing\", test: \"{{ rock_online_install }}\" }\n    - { repoid: \"rocknsm-local\", test: \"{{ not rock_online_install }}\" }\n  when: item.test | bool\n  changed_when: False\n  # TODO: Fix this ^^\n\n- name: Configure default CentOS online repos\n  yum_repository:\n    name: \"{{ item.name }}\"\n    enabled: \"{{ rock_online_install }}\"\n    description: \"CentOS-$releasever - {{ item.name | title }}\"\n    mirrorlist: \"{{ item.mirror }}\"\n    file:  CentOS-Base\n  with_items:\n  - { name: base, mirror: \"http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=os&infra=$infra\" }\n  - { name: updates, mirror: \"http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=updates&infra=$infra\" }\n  - { name: extras, mirror: \"http://mirrorlist.centos.org/?release=$releasever&arch=$basearch&repo=extras&infra=$infra\"}\n\n...\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "80e85b9f6ea17873636fc447298d464da73b188d", "filename": "roles/cloud-digitalocean/tasks/venv.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Clean up the environment\n  file:\n    dest: \"{{ digitalocean_venv }}\"\n    state: absent\n  when: clean_environment\n\n- name: Install requirements\n  pip:\n    name: dopy\n    version: 0.3.5\n    virtualenv: \"{{ digitalocean_venv }}\"\n    virtualenv_python: python2.7\n"}, {"commit_sha": "e9fb46dc84b9c815a69f6de1347c9ece5db01cc8", "sha": "4e12f3d564cb60b23c868a5d427ac6eb041312c6", "filename": "meta/main.yml", "repository": "fubarhouse/ansible-role-nodejs", "decoded_content": "---\ndependencies: []\n\ngalaxy_info:\n  author: fubarhouse\n  description: Installs NVM, NodeJS and NPM packages.\n  license: \"license (BSD, MIT)\"\n  min_ansible_version: 2.1.0.0\n  platforms:\n  - name: Ubuntu\n    versions:\n    - precise\n    - trusty\n    - xenial\n  - name: Debian\n    versions:\n    - all\n  - name: EL\n    versions:\n    - 6\n    - 7\n  categories:\n    - system\n    - database\n    - development\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "df42b4279f4d4d6aa1942817f78ede162c56a675", "filename": "meta/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\ngalaxy_info:\n  author: \"Petr Kubica\"\n  description: \"oVirt Deployment\"\n  company: \"Red Hat\"\n  license: \"GPLv3\"\n  min_ansible_version: 1.9\n  platforms:\n  - name: EL\n    versions:\n    - all\n  galaxy_tags:\n    - installer\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "3ce46a76fa76e02008a37f70672322e360db5237", "filename": "playbooks/libvirt/README.md", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "# libvirt playbooks\n\nThis playbook directory is meant to be driven by [`bin/cluster`](../../bin),\nwhich is community supported and most use is considered deprecated.\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "ecf88dc58efa7d475e8b571ef997c367abbefd2f", "filename": "tasks/create_repo_maven_proxy_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_maven_proxy\n    args: \"{{ _nexus_repos_maven_defaults|combine(item) }}\""}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f4ba5e8f1f4bae31d5f9bd2f19fbd4bd15059746", "filename": "roles/config-ipa-client/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Configure the host for IPA/IdM use'\n  hosts: all\n  roles:\n  - role: config-ipa-client\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "efbf3dffd19c81fb963a5091e932a6d1797a4686", "filename": "roles/config-idm-server/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n# tasks file for idm\n\n- import_tasks: 'prep.yml'\n- import_tasks: 'configure_idm.yml'\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "f27e60470c2d2d1df1db26f833b03aae051c05dc", "filename": "roles/network/tasks/hostapd.yml", "repository": "iiab/iiab", "decoded_content": "- name: Create /etc/hostapd/hostapd.conf from template\n  template:\n    src: hostapd/hostapd.conf.j2\n    dest: /etc/hostapd/hostapd.conf\n    owner: root\n    group: root\n    mode: 0644\n  when: iiab_wireless_lan_iface is defined\n\n- name: Create /etc/hostapd/hostapd.conf.iiab from template\n  template:\n    src: hostapd/iiab-hostapd.conf.j2\n    dest: /etc/hostapd/hostapd.conf.iiab\n    owner: root\n    group: root\n    mode: 0644\n  when: discovered_wireless_iface is defined\n\n- name: Use custom systemd unit file to start 'hostapd' service\n  template:\n    src: hostapd/hostapd.service.j2\n    dest: /etc/systemd/system/hostapd.service\n    owner: root\n    group: root\n    mode: 0644\n\n- name: Create /usr/bin/iiab-hotspot-on from template\n  template:\n    src: network/iiab-hotspot-on\n    dest: /usr/bin/iiab-hotspot-on\n    owner: root\n    group: root\n    mode: 0755\n  when: is_rpi\n\n- name: Create /usr/bin/iiab-hotspot-off from template\n  template:\n    src: network/iiab-hotspot-off\n    dest: /usr/bin/iiab-hotspot-off\n    owner: root\n    group: root\n    mode: 0755\n  when: is_rpi\n\n- name: Disable the Access Point 'hostapd' service\n  systemd:\n    name: hostapd\n    enabled: no\n# cheap workaround for when /etc/init.d is populated\n#  when: not hostapd_enabled\n\n- name: Enable the Access Point 'hostapd' service\n  systemd:\n    name: hostapd\n    enabled: yes\n  when: hostapd_enabled and iiab_wireless_lan_iface is defined and iiab_network_mode != \"Appliance\"\n\n- name: Record HOSTAPD_ENABLED to /etc/iiab/iiab.env\n  lineinfile:\n    dest: /etc/iiab/iiab.env\n    regexp: '^HOSTAPD_ENABLED=*'\n    line: 'HOSTAPD_ENABLED={{ hostapd_enabled }}'\n    state: present\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "891545f0fc10aa5c9b72dbc0c40471730a825937", "filename": "roles/manage-confluence-space/tasks/prepare_confluence_source.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Retrieve all contents from source space\n  uri:\n    url: '{{ confluence_source_url }}/wiki/rest/api/space/{{ atlassian.confluence.source.key }}/content?expand=body.storage,space,ancestors&limit=500'\n    method: GET\n    user: '{{ confluence_source_username }}'\n    password: '{{ confluence_source_password }}'\n    force_basic_auth: yes\n    status_code: 200\n    return_content: yes\n  register: contents_json\n\n- name: Get Confluence Homepage ID\n  uri:\n    url: '{{ confluence_source_url}}/wiki/rest/api/space/{{ atlassian.confluence.source.key }}'\n    method: GET\n    user: '{{ confluence_source_username }}'\n    password: '{{ confluence_source_password }}'\n    return_content: yes\n    force_basic_auth: yes\n    status_code: 200\n    body_format: json\n  register: source_space\n\n- name: Set Source Homepage ID\n  set_fact:\n    source_homepage_id: \"{{ source_space.json._expandable.homepage.split('/')|last }}\"\n\n- name: Create a tempfile for Content JSON\n  command: mktemp\n  register: uptemp\n  delegate_to: 127.0.0.1\n\n- name: Write content to file\n  copy:\n    content: \"{{ contents_json.json['page']['results'] }}\"\n    dest: \"{{ uptemp.stdout }}\"\n\n- name: Initialise old to new id mapping\n  set_fact:\n    id_mapping: {}\n\n- name: Sort contents based on its dependencies\n  command: \"./contents_order_parser.py '{{ uptemp.stdout }}'\"\n  args:\n    chdir: \"{{ role_path }}/files\"\n  register: processed_contents\n  delegate_to: 127.0.0.1\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "15fcc87b5c331da75d4000fb18c97dfc3095b60d", "filename": "roles/docket/tasks/docket_config.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: Check existing secret_key\n  shell: awk '/^SECRET_KEY/ {print $2}' /etc/docket/prod.yaml\n  register: docket_prod\n  changed_when: false\n\n- debug: msg=\"{{ docket_prod }}\"\n\n- name: Keep existing secret_key\n  set_fact: docket_secret=\"{{ docket_prod.stdout }}\"\n  when: '\"CHANGE_THIS\" not in docket_prod.stdout'\n\n- name: Set production docket config\n  template:\n    src: docket_prod.yaml.j2\n    dest: /etc/docket/prod.yaml\n  notify:\n    - Restart docket uwsgi\n    - Restart docket celery services\n\n- name: Set uwsgi config\n  template:\n    src: docket-uwsgi.ini.j2\n    dest: /etc/docket/docket-uwsgi.ini\n  notify:\n    - Restart docket uwsgi\n\n- name: Enable redis\n  service:\n    name: redis\n    enabled: true\n  notify: Restart redis\n  when: \"rock_services | selectattr('name', 'equalto', 'docket') | map(attribute='enabled') \"\n\n- name: Enable docket celery services\n  service:\n    name: \"{{ item }}\"\n    enabled: \"{{ 'True' if rock_services | selectattr('name', 'equalto', 'docket') | map(attribute='enabled') | bool else 'False' }}\"\n  notify: Restart docket celery services\n  loop:\n    - docket-celery-io\n    - docket-celery-query\n\n- name: Enable docket uwsgi service\n  service:\n    name: docket\n    enabled: \"{{ rock_services | selectattr('name', 'equalto', 'docket') | map(attribute='enabled') | bool }}\"\n  notify: Restart docket uwsgi\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "404340d75f153d7d472bb5e8cc59a59b182f7f53", "filename": "roles/update-host/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: update-host.yml\n- import_tasks: reboot-host.yml\n- import_tasks: wait-for-host.yml \n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "55d62ee4deac63dfd10fbbac97ef106f007ae196", "filename": "roles/zookeeper/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "- name: ensure zookeeper is running (and enable it at boot)\n  sudo: yes\n  service:\n    name: zookeeper\n    state: started\n    enabled: yes\n  tags:\n    - zookeeper\n\n- name: Create zookeeper config file\n  template:\n    src: zoo.cfg.j2\n    dest: /etc/zookeeper/conf/zoo.cfg\n  sudo: yes\n  notify:\n    - restart zookeeper\n  tags:\n    - zookeeper\n\n- name: Create zookeeper myid file\n  copy:\n    content: \"{{ zookeeper_id }}\"\n    dest: /etc/zookeeper/conf/myid\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart zookeeper\n  tags:\n    - zookeeper\n\n- name: Set Zookeeper consul service definition\n  sudo: yes\n  template:\n    src: zookeeper-consul.j2\n    dest: \"{{ consul_dir }}/zookeeper.json\"\n  notify:\n    - restart consul\n  tags:\n    - zookeeper\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "e81afc84d8530c8c79f2a381fad797ba6ba46ff9", "filename": "roles/docket/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# tasks file for rocknsm.docket\n\n\n# Ensure `stenographer` and `nginx` groups exists\n- import_tasks: prereqs.yml\n\n# Install packages\n- import_tasks: install.yml\n  when: inventory_hostname in groups['docket']\n\n# Generate/copy x509 client cert/keys and CA certs\n- import_tasks: crypto.yml\n\n# Configure docket app settings\n- import_tasks: docket_config.yml\n  when: inventory_hostname in groups['docket']\n\n# Configure web server settings\n- import_tasks: lighttpd.yml\n  when: inventory_hostname in groups['docket']\n\n# Enable / Activate Services\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "84e0cfd956967dbda6b2f85b7fea90f54ef48ee0", "filename": "roles/cloud-vultr/tasks/prompts.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- pause:\n    prompt: |\n      Enter the local path to your configuration INI file\n      (https://github.com/trailofbits/algo/docs/cloud-vultr.md):\n  register: _vultr_config\n  when: vultr_config is undefined\n\n- name: Set the token as a fact\n  set_fact:\n    algo_vultr_config: \"{{ vultr_config | default(_vultr_config.user_input) | default(lookup('env','VULTR_API_CONFIG'), true) }}\"\n\n- name: Get regions\n  uri:\n    url: https://api.vultr.com/v1/regions/list\n    method: GET\n    status_code: 200\n  register: _vultr_regions\n\n- name: Format regions\n  set_fact:\n    regions: >-\n      [ {% for k, v in _vultr_regions.json.items() %}\n      {{ v }}{% if not loop.last %},{% endif %}\n      {% endfor %} ]\n\n- name: Set regions as a fact\n  set_fact:\n    vultr_regions: \"{{ regions | sort(attribute='country') }}\"\n\n- name: Set default region\n  set_fact:\n    default_region: >-\n      {% for r in vultr_regions %}\n      {%- if r['DCID'] == \"1\" %}{{ loop.index }}{% endif %}\n      {%- endfor %}\n\n- pause:\n    prompt: |\n      What region should the server be located in?\n      (https://www.vultr.com/locations/):\n        {% for r in vultr_regions %}\n        {{ loop.index }}.   {{ r['name'] }}\n        {% endfor %}\n\n      Enter the number of your desired region\n      [{{ default_region }}]\n  register: _algo_region\n  when: region is undefined\n\n- name: Set the desired region as a fact\n  set_fact:\n    algo_vultr_region: >-\n      {% if region is defined %}{{ region }}\n      {%- elif _algo_region.user_input is defined and _algo_region.user_input != \"\" %}{{ vultr_regions[_algo_region.user_input | int -1 ]['name'] }}\n      {%- else %}{{ vultr_regions[default_region | int - 1]['name'] }}{% endif %}\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "b4657806e79e4b2273114aea13d12513bda9e1be", "filename": "ops/playbooks/roles/hpe.openports/tasks/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n\n    - name: Open  ports  in the firewall\n      firewalld:\n        port: \"{{ item }}\"\n        immediate: true\n        permanent: true\n        state: enabled\n      with_items: \"{{ hpe_openports_ports }}\"\n\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "622ffd822eaf0371774ba98d2182b9dd5fd17206", "filename": "ops/playbooks/includes/find_dtr.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n#\n# find first valid DTR server\n#\n#   set dtr_instance to the name of the first instance found\n#   set dtr_instance to \".none.\" if no instance was found\n#\n\n- name: \"Find DTR: Init\" \n  set_fact:\n    checks:\n      status: -1\n\n- name: \"Find DTR: check {{ ping_servers }}\" \n  uri:\n    url: \"https://{{ item }}.{{ domain_name }}/api/v0/accounts/language\" \n    headers:\n      Content-Type: application/json\n    method: GET\n    status_code: 200,401\n    body_format: json\n    validate_certs: no\n#    user: \"{{ ucp_username }}\" \n#    password: \"{{ ucp_password }}\" \n    force_basic_auth: yes\n  register: checks\n  failed_when: false\n  changed_when: false\n  when: checks.status != 200 and checks.status != 401\n  with_items: \"{{ ping_servers }}\"\n  no_log: yes\n\n- set_fact: dtr_instance=\".none.\"\n- name: \"Find DTR: set dtr_instance\" \n  set_fact: dtr_instance={{ item.item }}\n  when: \"'status' in item and item.status == 200\"\n  with_items: \"{{ checks.results }}\"\n  no_log: yes\n\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "0763c16c7de3eea6313212b8a8d08bceae484fd2", "filename": "roles/weave/meta/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\ngalaxy_info:\n  author: Graham Taylor\n  description:\n  company: Capgemini\n  # Some suggested licenses:\n  # - BSD (default)\n  # - MIT\n  # - GPLv2\n  # - GPLv3\n  # - Apache\n  # - CC-BY\n  license: license (MIT)\n  min_ansible_version: 1.2\n  #\n  # Below are all platforms currently available. Just uncomment\n  # the ones that apply to your role. If you don't see your\n  # platform on this list, let us know and we'll get it added!\n  #\n  platforms:\n  #- name: EL\n  #  versions:\n  #  - all\n  #  - 5\n  #  - 6\n  #  - 7\n  #- name: GenericUNIX\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Fedora\n  #  versions:\n  #  - all\n  #  - 16\n  #  - 17\n  #  - 18\n  #  - 19\n  #  - 20\n  #- name: SmartOS\n  #  versions:\n  #  - all\n  #  - any\n  #- name: opensuse\n  #  versions:\n  #  - all\n  #  - 12.1\n  #  - 12.2\n  #  - 12.3\n  #  - 13.1\n  #  - 13.2\n  #- name: Amazon\n  #  versions:\n  #  - all\n  #  - 2013.03\n  #  - 2013.09\n  #- name: GenericBSD\n  #  versions:\n  #  - all\n  #  - any\n  #- name: FreeBSD\n  #  versions:\n  #  - all\n  #  - 8.0\n  #  - 8.1\n  #  - 8.2\n  #  - 8.3\n  #  - 8.4\n  #  - 9.0\n  #  - 9.1\n  #  - 9.1\n  #  - 9.2\n  - name: Ubuntu\n    versions:\n  #  - all\n  #  - lucid\n  #  - maverick\n  #  - natty\n  #  - oneiric\n  #  - precise\n  #  - quantal\n  #  - raring\n  #  - saucy\n     - trusty\n  #- name: SLES\n  #  versions:\n  #  - all\n  #  - 10SP3\n  #  - 10SP4\n  #  - 11\n  #  - 11SP1\n  #  - 11SP2\n  #  - 11SP3\n  #- name: GenericLinux\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Debian\n  #  versions:\n  #  - all\n  #  - etch\n  #  - lenny\n  #  - squeeze\n  #  - wheezy\n  #\n  # Below are all categories currently available. Just as with\n  # the platforms above, uncomment those that apply to your role.\n  #\n  categories:\n  - cloud\n  #- cloud:ec2\n  #- cloud:gce\n  #- cloud:rax\n  #- clustering\n  #- database\n  #- database:nosql\n  #- database:sql\n  #- development\n  #- monitoring\n  #- networking\n  #- packaging\n  - system\n  #- web\ndependencies:\n  - role: docker\n  # List your role dependencies here, one per line. Only\n  # dependencies available via galaxy should be listed here.\n  # Be sure to remove the '[]' above if you add dependencies\n  # to this list.\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "46e5b079ecc8b18ee501acb3206a5d3c63a5a033", "filename": "tasks/create_repo_gitlfs_hosted_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_gitlfs_hosted\n    args: \"{{ _nexus_repos_gitlfs_defaults|combine(item) }}\"\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "30bf01be9656d2b8028e33b5c3dab3591489041f", "filename": "tasks/Linux/fetch/s3.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Download artifact from s3\n  aws_s3:\n    bucket: '{{ transport_s3_bucket }}'\n    object: '{{ transport_s3_path }}'\n    dest: '{{ java_download_path }}/{{ transport_s3_path | basename }}'\n    aws_access_key: '{{ transport_s3_aws_access_key }}'\n    aws_secret_key: '{{ transport_s3_aws_secret_key }}'\n    mode: get\n    overwrite: different\n    ignore_nonexistent_bucket: true\n  retries: 5\n  delay: 2\n\n- name: Set downloaded artifact vars\n  set_fact:\n    file_downloaded:\n      dest: '{{ java_download_path }}/{{ transport_s3_path | basename }}'\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "0c664f0a8520f98924283c9288233bf39514053f", "filename": "roles/haproxy/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for haproxy\nhaproxy_image: asteris/haproxy-consul\nhaproxy_image_tag: latest\n\n# Set the domain that haproxy uses to match URLs to internal apps.\n# For example, if all your apps will be\n#    app1.example.com, app2.example.com, etc.  set this to 'example.com'\nhaproxy_domain: example.com\n\nconsul_template_dir: /mnt/consul-template.d\nconsul_template_loglevel: debug\nconsul_backend: consul.service.consul:8500\nconsul_template_version: 0.10.0\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "4a72c347f66c059f9caa4debbafcb4cfaa7d85be", "filename": "roles/virt-install/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\ndefault_http_dir: \"/var/www/html\"\ndefault_http_host: \"192.168.122.1\"\n\ndefault_connect: \"qemu:///system\"\ndefault_virt_type: \"kvm\"\ndefault_name: \"tmpvm-{{ ansible_date_time.epoch }}\"\ndefault_title: \"tmpvm-title\"\ndefault_description: \"tmpvm-description\"\ndefault_memory: \"1024\"\ndefault_vcpus: \"2\"\ndefault_disk_size: \"10\"\ndefault_disk_pool: \"default\"\ndefault_os_variant: \"rhel7.3\"\ndefault_iso: \"/tmp/rhel-server-7.3-x86_64-dvd.iso\"\ndefault_ksfile: \"/tmp/my.ks\"\ndefault_authorized_keys: \"/tmp/authorized_keys\"\ndefault_network_hostif: \"eth0\"\ndefault_network_model: \"virtio\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "80a4b70571f15d368dddc8f229a2c7ebdcc8db29", "filename": "reference-architecture/rhv-ansible/playbooks/ovirt-vm-infra.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: oVirt infra\n  hosts: localhost\n  connection: local\n  gather_facts: false\n\n  vars_files:\n    - vars/ovirt-infra-vars.yaml\n\n  pre_tasks:\n    - name: Log in to oVirt\n      ovirt_auth:\n        url: \"{{ engine_url }}\"\n        username: \"{{ engine_user }}\"\n        password: \"{{ engine_password }}\"\n        ca_file: \"{{ engine_cafile | default(omit) }}\"\n        insecure: \"{{ engine_insecure | default(true) }}\"\n      tags:\n        - always\n\n  roles:\n    - ovirt-image-template\n    - ovirt-vm-infra\n\n  post_tasks:\n    - name: Logout from oVirt\n      ovirt_auth:\n        state: absent\n        ovirt_auth: \"{{ ovirt_auth }}\"\n      tags:\n        - always\n"}, {"commit_sha": "b2591b9333f6e7e70f6b9d99e55356b30d7e173c", "sha": "8363324266afba40278443266d60bbd19f04d555", "filename": "tasks/install.yml", "repository": "inkatze/wildfly", "decoded_content": "---\n# task file for wildfly\n\n- name: Install OpenJDK\n  yum:\n    name: java-1.8.0-openjdk-headless\n    state: present\n\n- name: Check if wildfly version file exists\n  stat:\n    path: '{{ wildfly_version_file }}'\n  register: wildfly_version_file_st\n\n- name: Check wildfly installation\n  command: grep -q '{{ wildfly_version }}' '{{ wildfly_version_file }}'\n  register: wildfly_installed\n  when: wildfly_version_file_st.stat.exists and\n        not wildfly_version_file_st.stat.isdir\n  changed_when: no\n\n- block:\n\n  - name: Download wildfly tar file\n    get_url:\n      url: '{{ wildfly_download_url }}'\n      dest: '{{ wildfly_download_dir }}'\n\n  - name: Create wildfly group\n    group:\n      name: '{{ wildfly_group }}'\n      state: present\n\n  - name: Create wildfly user\n    user:\n      name: '{{ wildfly_user }}'\n      group: '{{ wildfly_group }}'\n      createhome: no\n      state: present\n\n  - name: Unarchive downloaded file\n    unarchive:\n      src: '{{ wildfly_download_dir }}/{{ wildfly_download_file }}'\n      dest: '{{ wildfly_install_dir }}'\n      owner: '{{ wildfly_user }}'\n      group: '{{ wildfly_group }}'\n      mode: '0750'\n      copy: no\n\n  when: wildfly_installed.stdout is not defined or not wildfly_installed.stdout\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "cf8d95b0e234ad10fe7b6a7ca7edc98d2a35beb4", "filename": "roles/ovirt-engine-setup/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# health page\n- name: check if ovirt-engine running (health page)\n  uri:\n    url: \"http://{{ ansible_fqdn }}/ovirt-engine/services/health\"\n    status_code: 200\n  register: ovirt_engine_status\n  retries: 2\n  delay: 5\n  until: ovirt_engine_status|success\n  ignore_errors: True\n\n# copy default answer file\n- name: copy default answerfile\n  template:\n    src: answerfile_{{ ovirt_engine_version }}_basic.txt.j2\n    dest: /tmp/answerfile.txt\n    mode: 0644\n    owner: root\n    group: root\n  when: ovirt_engine_answer_file_path is undefined\n\n# copy custom answer file\n- name: copy custom answer file\n  template:\n    src: \"{{ ovirt_engine_answer_file_path }}\"\n    dest: /tmp/answerfile.txt\n    mode: 0644\n    owner: root\n    group: root\n  when: ovirt_engine_answer_file_path is defined\n\n- name: update setup packages\n  yum:\n    name: 'ovirt*setup*'\n    state: latest\n  when: ovirt_engine_update\n  tags:\n    - skip_ansible_lint\n\n- name: run engine-setup with answerfile\n  shell: 'engine-setup --config-append=/tmp/answerfile.txt'\n  when: ovirt_engine_status|failed or ovirt_engine_update\n  tags:\n    - skip_ansible_lint\n\n- name: check state of engine\n  service:\n    name: ovirt-engine\n    state: started\n\n- name: restart of ovirt-engine service\n  service:\n    name: ovirt-engine\n    state: restarted\n\n- name: check health status of page\n  uri:\n    url: \"http://{{ ansible_fqdn }}/ovirt-engine/services/health\"\n    status_code: 200\n  register: health_page\n  retries: 12\n  delay: 10\n  until: health_page|success\n\n- name: clean tmp files\n  file:\n    path: '/tmp/answerfile.txt'\n    state: 'absent'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "86337d4fc059952d18f1bd2ba56245b55e298011", "filename": "playbooks/osp/delete-osp-instance.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Delete Instance(s)\n  hosts: osp-provisioner\n  vars:\n    osp_resource_state: absent \n  roles:\n  - osp/admin-instance\n  - osp/admin-sec-group\n  - osp/admin-volume\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e65599b3efeccd561753f923c9ede9186e2050e7", "filename": "roles/user-management/manage-local-user-ssh-authkeys/tasks/authorizedkeys.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: \"Update authorized keys for user: {{ user_name|default(ansible_user) }}\"\n  authorized_key: \n    user: \"{{ user_name|default(ansible_user) }}\"\n    exclusive: \"{{ reset_keyfile|default('no') }}\"\n    key: \"{{ key_url }}\"\n\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "bf3b5de83c676faff840bbedbe529b81ec91ea1c", "filename": "playbooks/site.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- import_playbook: deploy-rock.yml\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "ba359a5ae647fec4609fd1c6fc80531ded0f7abc", "filename": "roles/2-common/tasks/net_mods.yml", "repository": "iiab/iiab", "decoded_content": "- name: Disable systemd-networkd.service (OS's other than centos)\n  service:\n    name: systemd-networkd.service\n    enabled: no\n  when: not is_centos\n\n- name: Mask systemd-networkd.service (OS's other than centos)\n  shell: 'systemctl mask systemd-networkd'\n  when: not is_centos\n\n- name: Disable systemd-hostnamed.service\n  service:\n    name: systemd-hostnamed.service\n    enabled: no\n\n- name: Disable dbus-org.freedesktop.hostname1.service\n  service:\n    name: dbus-org.freedesktop.hostname1\n    enabled: no\n\n- name: Mask dbus-org.freedesktop.hostname1.service\n  shell: 'systemctl mask dbus-org.freedesktop.hostname1'\n\n- name: Disable network.service\n  service:\n    name: network\n    enabled: no\n\n- name: Mask network.service\n  shell: 'systemctl mask network.service'\n\n# Network Manager starts this if needed\n- name: Disable wpa_supplicant\n  service:\n    name: wpa_supplicant\n    enabled: no\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "9b24f0a11659760489e9a3be8811054c38648f2b", "filename": "roles/ovirt-engine-rename/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# grep returns an exit code of 1 when it doesn't match any lines. Ansible then\n# interprets this (actually any status code other than 0 from a shell/command\n# task) as an error and so promptly fails.\n- name: Register the engine name stored inside the files in directory `/etc/ovirt-engine`\n  command: grep -R --include \\*.conf {{ ovirt_engine_rename_new_fqdn }} /etc/ovirt-engine\n  register: grep_output_remote_run\n  changed_when: False\n  failed_when: \"grep_output_remote_run.rc == 2\"\n  ignore_errors: yes\n\n- name: Saving the above diff result\n  copy: content=\"{{ grep_output_remote_run.stdout }}\" dest=/tmp/engine-rename-logs-current.grep\n  notify:\n    - delete engine-rename engine rename logs\n\n- debug: msg=\"The command grep -R {{ ovirt_engine_rename_new_fqdn }} /etc/ovirt-engine returned {{ grep_output_remote_run.stdout }}\"\n\n- name: Copy the expected result to /tmp on the remote vm to be compared with existing engine-rename logs\n  template:\n    src: check-engine-name-expected-result.grep.j2\n    dest: /tmp/check-engine-name-expected-result.grep\n  notify:\n    - delete engine-rename expected log files\n\n# if the engine-name is already at the state where we want it to be\n- name: Comparing the grep diff for the one that we expect and the one we just ran\n  command: diff --ignore-all-space /tmp/check-engine-name-expected-result.grep /tmp/engine-rename-logs-current.grep\n  register: grep_diff\n  changed_when: False\n  failed_when: \"grep_diff.rc == 2\"\n  ignore_errors: yes\n\n- name: Running engine-rename command (if the engine-name diff stdout != \"\")\n  shell: |\n    /usr/share/ovirt-engine/setup/bin/ovirt-engine-rename \\\n    --newname={{ ovirt_engine_rename_new_fqdn }} \\\n    --otopi-environment=\"OSETUP_RENAME/forceIgnoreAIAInCA=bool:'True' \\\n    OVESETUP_CORE/engineStop=bool:'True' \\\n    OSETUP_RENAME/confirmForceOverwrite=bool:'False'\"\n  when: grep_diff.stdout != \"\"\n  # ^ i.e skip when engine-name is already at the state we want it to be\n  tags:\n    - skip_ansible_lint\n\n# this is optional and is to be commented out if your systems use the DNS\n# to get the hostname, if not let it remain\n- name: Changing the system host name to reflect the new engine-name\n  # command: hostnamectl set-hostname {{ ovirt_engine_rename_new_fqdn }}\n  command: hostname {{ ovirt_engine_rename_new_fqdn }}\n  tags:\n    - skip_ansible_lint\n\n- debug: msg=\"change your /etc/hosts file to reflect to the new changed ovirt-engine name\"\n\n- name: Checking if the engine-rename is successful\n  command: grep -R --include \\*.conf {{ ovirt_engine_rename_new_fqdn }} /etc/ovirt-engine\n  changed_when: False\n  register: final_check\n\n- name: Engine rename successful or not?\n  command: echo Engine Rename successful\n  changed_when: False\n  when: final_check.rc == 0\n\n- name: check health status of page\n  uri:\n    url: \"http://{{ ovirt_engine_rename_new_fqdn }}/ovirt-engine/services/health\"\n    status_code: 200\n  register: health_page\n  retries: 12\n  delay: 10\n  until: health_page|success\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "949a323a749599d19e799e7b220f29d03c4fb19c", "filename": "playbooks/provisioning/openstack/sample-inventory/group_vars/OSEv3.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\nopenshift_deployment_type: origin\n#openshift_deployment_type: openshift-enterprise\n#openshift_release: v3.5\nopenshift_master_default_subdomain: \"apps.{{ env_id }}.{{ public_dns_domain }}\"\n\nopenshift_master_cluster_method: native\nopenshift_master_cluster_hostname: \"{{ groups.lb.0|default(groups.masters.0) }}\"\nopenshift_master_cluster_public_hostname: \"{{ groups.lb.0|default(groups.masters.0) }}\"\n\nosm_default_node_selector: 'region=primary'\n\nopenshift_hosted_router_wait: True\nopenshift_hosted_registry_wait: True\n\n## Openstack credentials\n#openshift_cloudprovider_kind=openstack\n#openshift_cloudprovider_openstack_auth_url: \"{{ lookup('env','OS_AUTH_URL') }}\"\n#openshift_cloudprovider_openstack_username: \"{{ lookup('env','OS_USERNAME') }}\"\n#openshift_cloudprovider_openstack_password: \"{{ lookup('env','OS_PASSWORD') }}\"\n#openshift_cloudprovider_openstack_tenant_name: \"{{ lookup('env','OS_TENANT_NAME') }}\"\n#openshift_cloudprovider_openstack_region=\"{{ lookup('env', 'OS_REGION_NAME') }}\"\n\n\n## Use Cinder volume for Openshift registry:\n#openshift_hosted_registry_storage_kind: openstack\n#openshift_hosted_registry_storage_access_modes: ['ReadWriteOnce']\n#openshift_hosted_registry_storage_openstack_filesystem: xfs\n\n## NOTE(shadower): This won't work until the openshift-ansible issue #5657 is fixed:\n## https://github.com/openshift/openshift-ansible/issues/5657\n## If you're using the `cinder_hosted_registry_name` option from\n## `all.yml`, uncomment these lines:\n#openshift_hosted_registry_storage_openstack_volumeID: \"{{ lookup('os_cinder', cinder_hosted_registry_name).id }}\"\n#openshift_hosted_registry_storage_volume_size: \"{{ cinder_hosted_registry_size_gb }}Gi\"\n\n## If you're using a Cinder volume you've set up yourself, uncomment these lines:\n#openshift_hosted_registry_storage_openstack_volumeID: e0ba2d73-d2f9-4514-a3b2-a0ced507fa05\n#openshift_hosted_registry_storage_volume_size: 10Gi\n\n\n# NOTE(shadower): the hostname check seems to always fail because the\n# host's floating IP address doesn't match the address received from\n# inside the host.\nopenshift_override_hostname_check: true\n\n# For POCs or demo environments that are using smaller instances than\n# the official recommended values for RAM and DISK, uncomment the line below.\n#openshift_disable_check: disk_availability,memory_availability\n\n# NOTE(shadower): Always switch to root on the OSEv3 nodes.\n# openshift-ansible requires an explicit `become`.\nansible_become: true\n\n# # Flannel networking\n#osm_cluster_network_cidr: 10.128.0.0/14\n#openshift_use_openshift_sdn: false\n#openshift_use_flannel: true\n#flannel_interface: eth1\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "84785efbab4cb3c2c884c02e8cffddcee42b237e", "filename": "archive/roles/openstack-create/tasks/create-volume.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: \"Create {{ instance_type }} Attached Storage\"\n  shell: \"nova volume-create --display-name {{ item.openstack.name + '-volume' }} {{ storage_volume_size }} |  awk '/ id / {print $4}'\"\n  register: created_volumes\n  failed_when: created_volumes.rc != 0\n  with_items: \"{{ openstack_machines.results }}\"\n\n\n- name: \"Validate {{ instance_type }} Attached Storage\"\n  shell: \"nova volume-show {{ item.stdout }} | grep -cE \\\"status.*available\\\"\"\n  register: validate_volumes\n  changed_when: false\n  until: validate_volumes.stdout == \"1\"\n  retries: 20\n  delay: 5\n  with_items: \"{{ created_volumes.results }}\"\n  \n\n- name: \"Attach {{ instance_type }} Storage\"\n  command: \"nova volume-attach {{ item.1.id }} {{ item.0.stdout }} {{ storage_disk_volume }}\"\n  with_together:\n    - \"{{ created_volumes.results }}\"\n    - \"{{ openstack_machines.results }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "c01b606a9a804f190f2ed17669ef7cdae6504149", "filename": "playbooks/unregister.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: cluster_hosts\n  gather_facts: yes\n  become: yes\n  roles:\n  - rhsm-unregister\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "43d64bc66005ca64dd42badf7e70aac76879f4ea", "filename": "tasks/admin_password_setup.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: update_admin_password\n    args:\n      new_password: \"{{ nexus_admin_password }}\"\n\n- name: Admin password changed\n  set_fact:\n    current_nexus_admin_password: \"{{ nexus_admin_password }}\"\n  no_log: true"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "4ae29cb16957ac8b3163a3bb9ab3738a4c9b1a12", "filename": "playbooks/roles/suricata/defaults/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# defaults file for suricata\nmethod: all\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8a3b6a67a5e3ce966f20077a342a83fa07949b50", "filename": "roles/user-management/list-users-by-group/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Find users part of group and generate list of users\"\n  include_tasks: generate-list-of-users.yml\n  when:\n  - user_group.name == target_group\n  with_items:\n  - \"{{ user_groups }}\"\n  loop_control:\n    loop_var: user_group\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "18245501e1d7c9b159a258e546e94e0bef377d0c", "filename": "roles/dns/manage-dns-records/tests/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\ndns_records_rm:\n- view: private\n  zone: first.example.com\n  server: \"192.168.48.26\"\n  key_name: \"private-first.example.com\"\n  key_secret: \"EhZfRtlHgy7xTIi2LeVSGsBj99Sb8IGB6K30ovg13dE=\"\n  key_algorithm: \"hmac-sha256\"\n  entries:\n  - type: A\n    record: master\n    value: 172.16.10.19\ndns_records_add:\n- view: private\n  zone: first.example.com\n  server: \"192.168.48.26\"\n  key_name: \"private-first.example.com\"\n  key_secret: \"EhZfRtlHgy7xTIi2LeVSGsBj99Sb8IGB6K30ovg13dE=\"\n  key_algorithm: \"hmac-sha256\"\n  entries:\n  - type: A\n    record: master\n    value: 172.16.10.20\n  - type: A\n    record: node1\n    value: 172.16.10.20\n  - type: A\n    record: node2\n    value: 172.16.10.21\n  - type: A\n    record: node3\n    value: 172.16.10.22\n- view: private\n  zone: second.example.com\n  server: \"192.168.48.26\"\n  key_name: \"private-second.example.com\"\n  key_secret: \"+UYdpSzdQyZ20V9/2Ud9RjHFz9Pouqn4aXP3V9X/gq4=\"\n  key_algorithm: \"hmac-sha256\"\n  entries:\n  - type: A\n    record: master\n    value: 172.17.10.20\n  - type: A\n    record: node1\n    value: 172.17.10.20\n  - type: A\n    record: node2\n    value: 172.17.10.21\n- view: public\n  zone: first.example.com\n  server: \"192.168.48.26\"\n  key_name: \"public-first.example.com\"\n  key_secret: \"5RZv5wMtKS/fZtjtc2bXS2s6L5+cXN2x53jSkEtwNjk=\"\n  key_algorithm: \"hmac-sha256\"\n  entries:\n  - type: A\n    record: master\n    value: 10.9.77.20\n  - type: A\n    record: node1\n    value: 10.9.77.20\n  - type: A\n    record: node2\n    value: 10.9.77.21\n  - type: A\n    record: node3\n    value: 10.9.77.22\n- view: public\n  zone: second.example.com\n  server: \"192.168.48.26\"\n  key_name: \"public-second.example.com\"\n  key_secret: \"7VKvn5iZ64l+s42XT/hllJSxS6CjE3369tOy85vkBk4=\"\n  key_algorithm: \"hmac-sha256\"\n  entries:\n  - type: A\n    record: master\n    value: 10.8.88.20\n  - type: A\n    record: node1\n    value: 10.8.88.20\n  - type: A\n    record: node2\n    value: 10.8.88.21\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "f37843250c5c61e65b303a266076dfaf4f1d049d", "filename": "roles/rachel/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Create various directories for rachel\n  file: path={{ item }}\n        owner=root\n        group=root\n        mode=0755\n        state=directory\n  with_items:\n    - /library/rachel\n\n- name: See if rachel content is installed\n  stat: path=\"{{ rachel_content_path }}/index.php\"\n  register: rachel_content\n\n- name: Set rachel_content_found to False\n  set_fact:\n     rachel_content_found: False\n\n- name: Set rachel_content_found True if found\n  set_fact:\n     rachel_content_found: True\n  when: rachel_content.stat.exists  == true\n\n- include_tasks: rachel_enabled.yml\n  when: rachel_enabled and rachel_content_found\n\n- name: Add rachel to service list\n  ini_file: dest='{{ service_filelist }}'\n            section=rachel\n            option='{{ item.option }}'\n            value='{{ item.value }}'\n  with_items:\n    - option: name\n      value: rachel\n    - option: description\n      value: '\"Educational resources, packaged together for download and distribution in places without internet.\"'\n    - option: rachel_content_path\n      value: \"{{ rachel_content_path }}\"\n    - option: rachel_version\n      value: \"{{ rachel_version }}\"\n    - option: rachel_mysqldb_path\n      value: \"{{ rachel_mysqldb_path }}\"\n    - option: rachel_src_url\n      value: \"{{ rachel_src_url }}\"\n    - option: content_found\n      value: \"{{ rachel_content_found }}\"\n    - option: enabled\n      value: \"{{ rachel_enabled }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "35788702b33636cfb9bbb245ddf55f8074f9e853", "filename": "reference-architecture/vmware-ansible/playbooks/ocp-demo.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: yes\n  ignore_errors: yes\n  vars_files:\n  - vars/main.yaml\n  pre_tasks:\n  - name: set fact\n    set_fact:\n      openshift_master_cluster_public_hostname: \"{{ openshift_master_cluster_public_hostname }}\"\n  - name: set fact\n    set_fact:\n      openshift_master_cluster_hostname: \"{{ openshift_master_cluster_hostname }}\"\n  roles:\n  - instance-groups\n\n- name: Perform post validation steps\n  include: ../../../playbooks/post-validation.yaml\n  vars:\n    validate_etcd_short_hostname: true\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "83014490b51254843d4a04ebe719341389cbd1ef", "filename": "roles/ovirt-engine-setup/defaults/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_engine_dwh: True\novirt_engine_type: 'ovirt-engine'\novirt_engine_version: '4.1'\novirt_engine_admin_password: '123456'\n\novirt_provider_ovn_username: 'admin@internal'\novirt_provider_ovn_password: '{{ ovirt_engine_admin_password }}'\n\novirt_engine_db_host: 'localhost'\novirt_engine_db_port: 5432\novirt_engine_db_name: 'engine'\novirt_engine_db_user: 'engine'\novirt_engine_db_password: 'AqbXg4dpkbcVRZwPbY8WOR'\n\novirt_engine_dwh_db_configure: true\n# ^ set this to false if you want to install the dwh on a different host\n# machine rather than the same machine where the engine is installed.\novirt_engine_dwh_db_host: 'localhost'\novirt_engine_dwh_db_port: 5432\novirt_engine_dwh_db_name: 'ovirt_engine_history'\novirt_engine_dwh_db_user: 'ovirt_engine_history'\novirt_engine_dwh_db_password: '37xmBKECANQGm0z3SfylMp'\n\novirt_engine_configure_iso_domain: false\novirt_engine_iso_domain_path: '/var/lib/exports/iso'\novirt_engine_iso_domain_name: 'ISO_DOMAIN'\novirt_engine_iso_domain_acl: '0.0.0.0/0.0.0.0(rw)'\n\novirt_engine_firewall_manager: 'firewalld'\n\novirt_engine_update: false\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "c9c0f58a33aae26023e101208a83a64bf2682fc2", "filename": "playbooks/templates/bro-node.cfg.j2", "repository": "rocknsm/rock", "decoded_content": "[logger]\ntype=logger\nhost=localhost\nenv_vars=fanout_id=0\n\n[manager]\ntype=manager\nhost=localhost\nenv_vars=fanout_id=0\n\n[proxy-1]\ntype=proxy\nhost=localhost\nenv_vars=fanout_id=0\n\n{% set procs_per_worker = (bro_cpu | int) // (rock_monifs|length) %}\n{% for iface in rock_monifs %}\n[{{ iface }}]\ntype=worker\nhost=localhost\n{%if procs_per_worker >=2 %}\ninterface=af_packet::{{ iface }}\nlb_method=custom\nlb_procs={{ (bro_cpu | int) // loop.length }}\n{% else %}\ninterface={{ iface }}\n{% endif %}\nenv_vars=fanout_id={{ 42 + loop.index0 }}\n{# TODO: add logic for pinning processes #}\n{% endfor %}\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "6a5e8766baca1aa1fd59d8d98f21bad943e01e66", "filename": "roles/idm-host-cert/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: \"idm-login.yml\"\n\n- import_tasks: \"register-host.yml\"\n\n- import_tasks: \"generate-csr.yml\"\n\n- import_tasks: \"create-host-cert.yml\"\n\n- import_tasks: \"retrieve-ca-cert.yml\"\n\n- import_tasks: \"write-certs-to-file.yml\"\n\n- import_tasks: \"print-certs.yml\"\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "51d3afc8fadbbc8073940d70ec3504da44662885", "filename": "tasks/install/percona/apt.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n- name: APT_KEY | Install Percona key\n  apt_key:\n    keyserver: \"keyserver.ubuntu.com\"\n    id: \"8507EFA5\"\n\n- name: TEMPLATE | Deploy APT pinning (prevent upgrades from Debian)\n  template:\n    src: etc/apt/preferences.d/95-percona.j2\n    dest: /etc/apt/preferences.d/95-percona\n\n- name: APT_REPOSITORY | Add Percona repository\n  apt_repository:\n    repo: 'deb {{ mariadb_percona_repository }} {{ ansible_distribution_release }} main'\n\n- name: APT_RESPOSITORY | Add Percona (src) repository\n  apt_repository:\n    repo: 'deb-src {{ mariadb_percona_repository }} {{ ansible_distribution_release }} main'\n  when: mariadb_upstream_apt_src\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "7a78f90c264a87ebe7d6f6cfe29af3fb53958da9", "filename": "tasks/section_07_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 7.1.1 Disable IP Forwarding (Scored)\n    sysctl: >\n      name=net.ipv4.ip_forward\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.1\n      - section7.1.1\n\n  - name: 7.1.2.1 Disable Send Packet Redirects (Scored)\n    sysctl: >\n      name=net.ipv4.conf.all.send_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.1\n      - section7.1.2\n      - section7.1.2.1\n\n  - name: 7.1.2.2 Disable Send Packet Redirects (Scored)\n    sysctl: >\n      name=net.ipv4.conf.default.send_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.1\n      - section7.1.2\n      - section7.1.2.2\n\n  - name: 7.2.1.1 Disable Source Routed Packet Acceptance (Scored)\n    sysctl: >\n      name=net.ipv4.conf.all.accept_source_route\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.1\n      - section7.2.1.1\n\n  - name: 7.2.1.2 Disable Source Routed Packet Acceptance (Scored)\n    sysctl: >\n      name=net.ipv4.conf.default.accept_source_route\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.1\n      - section7.2.1.2\n\n  - name: 7.2.2.1 Disable ICMP Redirect Acceptance (Scored)\n    sysctl: >\n      name=net.ipv4.conf.all.accept_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.2\n      - section7.2.2.1\n\n  - name: 7.2.2.2 Disable ICMP Redirect Acceptance (Scored)\n    sysctl: >\n      name=net.ipv4.conf.default.accept_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.2\n      - section7.2.2.2\n\n  - name: 7.2.3.1 Disable Secure ICMP Redirect Acceptance (Scored)\n    sysctl: >\n      name=net.ipv4.conf.all.secure_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.3\n      - section7.2.3.1\n\n  - name: 7.2.3.2 Disable Secure ICMP Redirect Acceptance (Scored)\n    sysctl: >\n      name=net.ipv4.conf.default.secure_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.3\n      - section7.2.3.2\n\n  - name: 7.2.4.1 Log Suspicious Packets (Scored)\n    sysctl: >\n      name=net.ipv4.conf.all.log_martians\n      value=1\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.4\n      - section7.2.4.1\n\n  - name: 7.2.4.2 Log Suspicious Packets (Scored)\n    sysctl: >\n      name=net.ipv4.conf.default.log_martians\n      value=1\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.4\n      - section7.2.4.2\n\n  - name: 7.2.5 Enable Ignore Broadcast Requests (Scored)\n    sysctl: >\n      name=net.ipv4.icmp_echo_ignore_broadcasts\n      value=1\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.5\n\n  - name: 7.2.6 Enable Bad Error Message Protection (Scored)\n    sysctl: >\n      name=net.ipv4.icmp_ignore_bogus_error_responses\n      value=1\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.6\n\n  - name: 7.2.7.1 Enable RFC-recommended Source Route Validation (Scored)\n    sysctl: >\n      name=net.ipv4.conf.all.rp_filter\n      value=1\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.7\n      - section7.2.7.1\n\n  - name: 7.2.7.2 Enable RFC-recommended Source Route Validation (Scored)\n    sysctl: >\n      name=net.ipv4.conf.default.rp_filter\n      value=1\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.7\n      - section7.2.7.2\n\n  - name: 7.2.8 Enable TCP SYN Cookies (Scored)\n    sysctl: >\n      name=net.ipv4.tcp_syncookies\n      value=1\n      state=present\n    when: enable_tcp_syncookies\n    tags:\n      - section7\n      - section7.2\n      - section7.2.8\n\n  - name: 7.3.1.1 Disable IPv6 Router Advertisements (Not Scored)\n    sysctl: >\n      name=net.ipv6.conf.all.accept_ra\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.3\n      - section7.3.1\n      - section7.3.1.1\n\n  - name: 7.3.1.2 Disable IPv6 Router Advertisements (Not Scored)\n    sysctl: >\n      name=net.ipv6.conf.default.accept_ra\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.3\n      - section7.3.1\n      - section7.3.1.2\n\n  - name: 7.3.2.1 Disable IPv6 Redirect Acceptance (Not Scored)\n    sysctl: >\n      name=net.ipv6.conf.all.accept_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.3\n      - section7.3.2\n      - section7.3.2.1\n\n  - name: 7.3.2.2 Disable IPv6 Redirect Acceptance (Not Scored)\n    sysctl: >\n      name=net.ipv6.conf.default.accept_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.3\n      - section7.3.2\n      - section7.3.2.2\n\n  - name: 7.3.3 Disable IPv6 (Not Scored)\n    sysctl: >\n      name={{ item }}\n      value=1\n      state=present\n    with_items:\n      - net.ipv6.conf.all.disable_ipv6\n      - net.ipv6.conf.default.disable_ipv6\n      - net.ipv6.conf.lo.disable_ipv6\n    when: disable_ipv6 == True\n    tags:\n      - section7\n      - section7.3\n      - section7.3.3\n\n  - name: 7.4.1 Install TCP Wrappers (Scored)\n    apt: name=tcpd state=present\n    tags:\n      - section7\n      - section7.4\n      - section7.4.1\n\n  - name: 7.4.2 Create /etc/hosts.allow (Not Scored)\n    debug: msg=\"*** Verify /etc/hosts.allow ***\"\n    tags:\n      - section7\n      - section7.4\n      - section7.4.2\n\n  - name: 7.4.3 Verify Permissions on /etc/hosts.allow (Scored)\n    file: >\n        path=/etc/hosts.allow\n        owner=root\n        group=root\n        mode=0644\n    tags:\n      - section7\n      - section7.4\n      - section7.4.3\n\n  - name: 7.4.4 Create /etc/hosts.deny (Not Scored)\n    debug: msg='*** Verify /etc/hosts.deny ***'\n    tags:\n      - section7\n      - section7.4\n      - section7.4.4\n\n  - name: 7.4.5 Verify Permissions on /etc/hosts.deny (Scored)\n    file: >\n        path=/etc/hosts.deny\n        owner=root\n        group=root\n        mode=0644\n    tags:\n      - section7\n      - section7.4\n      - section7.4.5\n\n  - name: 7.5.0 Check the presence of the file \"cis.conf\" under modprobe.d (Not Scored)\n    stat: >\n        path=/etc/modprobe.d/CIS.conf\n    register: cis_conf_file\n    tags:\n      - section7\n      - section7.5\n\n  - name: 7.5.0 Create the file \"cis.conf\" under modprobe.d if doesn't exist (Not Scored)\n    file: >\n        dest=/etc/modprobe.d/CIS.conf state=touch\n    when: not cis_conf_file.stat.exists\n    tags:\n      - section7\n      - section7.5\n\n  - name: 7.5.1-4 Disable DCCP, SCTP, RDS, TIPC (Not Scored)\n    lineinfile: >\n        dest=/etc/modprobe.d/CIS.conf\n        line='install {{ item }} /bin/true'\n        state=present\n    with_items:\n        - dccp\n        - sctp\n        - rds\n        - tipc\n    tags:\n      - section7\n      - section7.5\n      - section7.5.1\n      - section7.5.2\n      - section7.5.3\n      - section7.5.4\n\n  - name: 7.6 Deactivate Wireless Interfaces (Not Scored)\n    shell: 'lspci -k | grep -i wifi'\n    changed_when: False\n    register: lspci_wifi\n    failed_when: lspci_wifi.rc == 0\n    tags:\n      - section7\n      - section7.6\n\n  - name: 7.7 Ensure Firewall is active (install) (Scored)\n    apt: >\n        name=ufw\n        state=present\n    when: activate_ufw\n    tags:\n      - section7\n      - section7.7\n\n  - name: 7.7 Ensure Firewall is active (Scored)\n    service: >\n        name=ufw\n        state=started\n    when: activate_ufw\n    tags:\n      - section7\n      - section7.7\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "7dfdc964cac8e85338f38f2d071053577add7168", "filename": "roles/manage-confluence-space/tasks/copy_confluence_attachments.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Get attachment data\n  uri:\n    url: '{{ confluence.source.url }}/wiki/rest/api/content/{{ confluence_content_ids.key }}/child/attachment'\n    method: GET\n    user: '{{ confluence.source.username }}'\n    password: '{{ confluence.source.password }}'\n    force_basic_auth: yes\n    status_code: 200\n    return_content: yes\n  register: attachments_json\n\n- name: Create temp directory for downloaded attachments\n  tempfile:\n    state: directory\n  register: attachment_tempdir\n\n- include_tasks: download_attachment.yml\n  with_items: '{{ attachments_json.json.results }}'\n  loop_control:\n    loop_var: attachment_data\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "a48b6867bbfa9988e069b01c3f0fc2cc0ddf0ff8", "filename": "playbooks/templates/elasticsearch.yml.j2", "repository": "rocknsm/rock", "decoded_content": "# ======================== Elasticsearch Configuration =========================\n#\n# NOTE: Elasticsearch comes with reasonable defaults for most settings.\n#       Before you set out to tweak and tune the configuration, make sure you\n#       understand what are you trying to accomplish and the consequences.\n#\n# The primary way of configuring a node is via this file. This template lists\n# the most important settings you may want to configure for a production cluster.\n#\n# Please consult the documentation for further information on configuration options:\n# https://www.elastic.co/guide/en/elasticsearch/reference/index.html\n#\n# ---------------------------------- Cluster -----------------------------------\n#\n# Use a descriptive name for your cluster:\n#\ncluster.name: {{ es_cluster_name }}\n#\n# ------------------------------------ Node ------------------------------------\n#\n# Use a descriptive name for the node:\n#\nnode.name: {{ es_node_name }}\n#\n# Add custom attributes to the node:\n#\n#node.attr.rack: r1\n#\n# ----------------------------------- Paths ------------------------------------\n#\n# Path to directory where to store the data (separate multiple locations by comma):\n#\npath.data: {{ es_data_dir }}\n#\n# Path to log files:\n#\npath.logs: {{ es_log_dir }}\n#\n# ----------------------------------- Memory -----------------------------------\n#\n# Lock the memory on startup:\n#\nbootstrap.memory_lock: true\n#\n# Make sure that the heap size is set to about half the memory available\n# on the system and that the owner of the process is allowed to use this\n# limit.\n#\n# Elasticsearch performs poorly when the system is swapping the memory.\n#\n# ---------------------------------- Network -----------------------------------\n#\n# Set the bind address to a specific IP (IPv4 or IPv6):\n#\nnetwork.host: _local:ipv4_\n#\n# Set a custom port for HTTP:\n#\n#http.port: 9200\n#\n# For more information, consult the network module documentation.\n#\n# --------------------------------- Discovery ----------------------------------\n#\n# Pass an initial list of hosts to perform discovery when new node is started:\n# The default list of hosts is [\"127.0.0.1\", \"[::1]\"]\n#\n#discovery.zen.ping.unicast.hosts: [\"host1\", \"host2\"]\n#\n# Prevent the \"split brain\" by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):\n#\n#discovery.zen.minimum_master_nodes:\n#\n# For more information, consult the zen discovery module documentation.\n#\n# ---------------------------------- Gateway -----------------------------------\n#\n# Block initial recovery after a full cluster restart until N nodes are started:\n#\n#gateway.recover_after_nodes: 3\n#\n# For more information, consult the gateway module documentation.\n#\n# ---------------------------------- Various -----------------------------------\n#\n# Require explicit names when deleting indices:\n#\n#action.destructive_requires_name: true\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "76babaf6c461e693b4f6812b1f604726c91087c2", "filename": "roles/openshift-applier/tasks/process-content.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: \"Include any pre-processing role(s) before applying content\"\n  include: pre-post-step.yml\n  with_items:\n  - \"{{ entry.pre_steps | default('') }}\"\n  loop_control:\n    loop_var: step\n\n- name: \"Create/Apply OpenShift Cluster Content ('{{ entry.object }}') - based on individual files\"\n  include_tasks: process-one-entry.yml\n  with_items:\n  - \"{{ entry.content | default([]) }}\"\n  loop_control:\n    loop_var: content\n\n- name: \"Apply OpenShift Cluster Content ('{{ entry.object }}') - based on directory\"\n  block:\n  - fail:\n      msg: \"WARNING: This functionality is being deprecated. Please use the 'content.file' variable instead. See role docs for more info.\"\n    ignore_errors: true\n  - command: >\n      oc apply -f {{ tmp_inv_dir }}{{ entry.content_dir }}\n  when:\n  - entry.content_dir is defined\n  - entry.content_dir|trim != ''\n\n- name: \"Include any post-processing role(s) after applying content\"\n  include: pre-post-step.yml\n  with_items:\n  - \"{{ entry.post_steps | default('') }}\"\n  loop_control:\n    loop_var: step\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "6db7a4f64b467e2b1e76d53807b64626c46d01d3", "filename": "roles/manage-sshd-config/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'reload sshd'\n  service:\n    name: '{{ item }}'\n    state: restarted\n  with_items:\n  - sshd\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "bd84d471b5fa341fc69119d7de933ca72ae30c91", "filename": "reference-architecture/aws-ansible/playbooks/roles/inventory-file-creation/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: create a new file\n  file:\n    path: ../static-inventory\n    state: absent\n\n- name: Create base file\n  template:\n    src: files/inventory\n    dest: ../static-inventory\n    remote_src: false\n\n- name: command for masters\n  shell: sed -i  '/\\[masters\\]/a {{ hostvars[item].ec2_tag_Name }}' ../static-inventory\n  with_items:\n  - \"{{ groups['tag_openshift_role_master'] }}\"\n  when:\n  - hostvars[item]['ec2_tag_KubernetesCluster'] == \"{{ stack_name }}\"\n\n\n- name: command for masters\n  shell: sed -i  '/\\[etcd\\]/a {{ hostvars[item].ec2_tag_Name }}' ../static-inventory\n  with_items:\n  - \"{{ groups['tag_openshift_role_master'] }}\"\n  when:\n  - hostvars[item]['ec2_tag_KubernetesCluster'] == \"{{ stack_name }}\"\n\n- name: define nodes\n  lineinfile:\n    line: \"{{ hostvars[item].ec2_tag_Name }} openshift_node_labels=\\\"{'role': 'master'}\\\"\"\n    insertafter: \"[nodes]\"\n    state: present\n    dest: ../static-inventory\n  with_items:\n  - \"{{ groups['tag_openshift_role_master'] }}\"\n  when:\n  - hostvars[item]['ec2_tag_KubernetesCluster'] == \"{{ stack_name }}\"\n\n- name: define nodes\n  lineinfile:\n    line: \"{{ hostvars[item].ec2_tag_Name }} openshift_node_labels=\\\"{'role': 'app'}\\\"\"\n    insertafter: [nodes]\n    state: present\n    dest: ../static-inventory\n  with_items:\n  - \"{{ groups['tag_openshift_role_app'] }}\"\n  when:\n  - hostvars[item]['ec2_tag_KubernetesCluster'] == \"{{ stack_name }}\"\n\n- name: define nodes\n  lineinfile:\n    line: \"{{ hostvars[item].ec2_tag_Name }} openshift_node_labels=\\\"{'role': 'infra'}\\\"\"\n    insertafter: [nodes]\n    state: present\n    dest: ../static-inventory\n  with_items:\n  - \"{{ groups['tag_openshift_role_infra'] }}\"\n  when:\n  - hostvars[item]['ec2_tag_KubernetesCluster'] == \"{{ stack_name }}\"\n\n- name: define nodes\n  lineinfile:\n    line: \"{{ hostvars[item].ec2_tag_Name }} openshift_node_labels=\\\"{'role': 'storage'}\\\"\"\n    insertafter: [nodes]\n    state: present\n    dest: ../static-inventory\n  with_items: \"{{ groups['tag_openshift_role_storage'] }}\"\n  when:\n  - groups.tag_openshift_role_storage is defined and hostvars[item]['ec2_tag_KubernetesCluster'] == stack_name\n  ignore_errors: true\n"}, {"commit_sha": "c91b6076e3a957fb0a165131d0ff3b3b208ed419", "sha": "4ca5380a2827c79510cd09d29ebfe9e26066951d", "filename": "tasks/section_03_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n#sections 3.3 and 3.4 have to happen before as the latter overwrite 3.1 and 3.2\n\n  - name: 3.0 Check for /boot/grub/grub.cfg file\n    stat: path=/boot/grub/grub.cfg\n    register: grub_cfg_file\n    tags:\n      - section3\n\n  - name: 3.3.1.1 Set Boot Loader Superuser (check) (Scored)\n    command: grep \"^set superusers\" /boot/grub/grub.cfg\n    register: boot_superusers \n    when: grub_cfg_file.stat.exists == True\n    changed_when: False\n    failed_when: False\n    always_run: True\n    tags:\n      - section3\n      - section3.3\n      - section3.3.1\n      - section3.3.1.1\n\n  - name: 3.3.1.2 Set Boot Loader Superuser (Scored)\n    lineinfile: >\n        dest='/etc/grub.d/40_custom'\n        regexp='^set superusers'\n        line='set superusers=\"root\"'\n        state=present\n        create=yes\n    when: grub_cfg_file.stat.exists == True and boot_superusers.rc == 1\n    tags:\n      - section3\n      - section3.3\n      - section3.3.1\n      - section3.3.1.2\n    \n  - name: 3.3.2.1 Set Boot Loader Password (check) (Scored)\n    command: grep \"^password\" /boot/grub/grub.cfg\n    register: boot_password\n    when: grub_cfg_file.stat.exists == True\n    changed_when: False\n    failed_when: False\n    always_run: True\n    tags:\n      - section3\n      - section3.3\n      - section3.3.2\n      - section3.3.2.1\n\n  - name: 3.3.2.2 Set Boot Loader Password (Scored)\n    lineinfile: >\n        dest='/etc/grub.d/40_custom' \n        regexp='^password'\n        line='password_pbkdf2 root grub.pbkdf2.sha512.10000.529DB4AF052F170948C1DB2A754CEA8A286804DA2D9A4EB5A7CCE4B8636775C83EAF8A1093CBDBC256954BCE789A58EFB3B75D23DFC76583C703922D5DADB69E.4D5BD1EC6736057095CA2EBF55C2DA02DFB0B0784F2105A396F1CEF11FEB1483D5C420F412E2E817E2570DDFC22ABCC329C5FF44091A0ACDE67171FF72E96CFD'\n        state=present\n    when: grub_cfg_file.stat.exists == True and boot_password.rc == 1\n    tags:\n      - section3\n      - section3.3\n      - section3.3.2\n      - section3.3.2.2\n\n  - name: 3.3.3 Disable password protection booting\n    lineinfile: >\n        dest='/etc/grub.d/10_linux'\n        create=yes\n        regexp='^CLASS='\n        line='CLASS=\"--class gnu-linux --class gnu --class os --unrestricted\"'\n        state=present\n    tags:\n      - section3\n      - section3.3\n      - section3.3.3\n\n\n  - name: 3.3.4 Update Grub configuration (Scored)\n    command: update-grub\n    when: grub_cfg_file.stat.exists == True and (boot_superusers.rc == 1 or boot_password.rc == 1)\n    tags:\n      - section3\n      - section3.3\n      - section3.3.4\n\n\n  - name: 3.4.1 Require Authentication for Single-User Mode (check) (Scored)\n    shell: 'grep \"^root:[*\\!]:\" /etc/shadow'\n    register: root_password_set\n    changed_when: False\n    failed_when: False\n    always_run: True\n    tags:\n      - section3\n      - section3.4\n      - section3.4.1\n\n  - name: 3.4.2 Require Authentication for Single-User Mode (Scored)\n    user: name=root state=present password='{{ root_password }}'\n    when: root_password_set.rc == 1\n    tags:\n      - section3\n      - section3.4\n      - section3.4.2\n\n  - name: 3.1 Set User/Group Owner on bootloader config (Scored)\n    file: path=/boot/grub/grub.cfg owner=root group=root\n    when: grub_cfg_file.stat.exists == True\n    tags:\n      - section3\n      - section3.1\n\n  - name: 3.2 Set Permissions on bootloader config (Scored)\n    file: path=/boot/grub/grub.cfg mode=\"o-rwx,g-rwx\"\n    when: grub_cfg_file.stat.exists == True\n    sudo: yes\n    tags:\n      - section3\n      - section3.2\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "cc27d6006101232246395664be2fa6f0d338c5c3", "filename": "roles/config-iscsi-client/tasks/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - iscsi-initiator-utils\n  - device-mapper-multipath\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "ad493128e10ef93271f42a8d182412cce8715ef4", "filename": "roles/ipareplica/vars/Ubuntu.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# vars/Ubuntu.yml\nipareplica_packages: [ \"freeipa-server\" ]\nipareplica_packages_dns: [ \"freeipa-server-dns\" ]\nipareplica_packages_adtrust: [ ]\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "8db268c3733146a043215ff7f8e777b8c8447f97", "filename": "roles/2-common/tasks/yum-historical.yml", "repository": "iiab/iiab", "decoded_content": "- name: install yum deps for arm!!!\n  shell: dnf install -y python-urlgrabber pyxattr yum-metadata-parser\n  when: ansible_distribution == \"Fedora\" and ansible_machine == \"armv7l\" and ansible_distribution_version|int >= 22\n\n- name: install yum from Fedora 23 for arm!!!\n  shell: dnf install -y https://kojipkgs.fedoraproject.org//packages/yum/3.4.3/506.fc23/noarch/yum-3.4.3-506.fc23.noarch.rpm python-dnf \n  when: ansible_distribution == \"Fedora\" and ansible_machine == \"armv7l\" and ansible_distribution_version|int >= 22\n\n- name: install yum if it has been dropped from our distribution -- Fedora 22 uses dnf!!!\n  shell: dnf install -y yum\n  when: ansible_distribution == \"Fedora\" and ansible_distribution_version|int >= 22 and ansible_machine != \"armv7l\"\n\n- name: get the createrepo program\n  package: name=createrepo\n           state=present\n  when: is_redhat\n\n- name: Create local repo\n  shell: createrepo {{ yum_packages_dir }}\n  when: is_redhat\n\n- name: Install local repo file.\n  template: dest=/etc/yum.repos.d/iiab-local.repo\n            src=local.repo\n            owner=root\n            mode=0644\n  when: is_redhat\n\n- name: Install yum packages\n  package: name={{ item }}\n           state=present\n  with_items:\n   - yum-utils\n   - createrepo\n   - wpa_supplicant\n   - linux-firmware\n   - syslog\n   - xml-common\n  when: is_redhat\n\n- name: Install yum packages for Debian\n  package: name={{ item }}\n           state=present\n  with_items:\n    - inetutils-syslogd\n    - wpasupplicant\n  when: is_debuntu\n\n- name: Install common packages\n  package: name={{ item }}\n           state=present\n  with_items:\n   - acpid\n   - mlocate\n   - rsync\n   - htop\n   - etckeeper\n   - python-passlib\n   - usbmount\n   - net-tools\n   - openssh-server\n   - sudo\n   - logrotate\n   - make\n   - tar\n   - unzip\n   - bzip2\n   - i2c-tools\n   - bridge-utils\n   - usbutils\n   - hostapd\n   - wget\n   - openssl   #FC 18 does not supply, but pear requires\n   - gawk\n   - sqlite3\n\n- name: Update common packages (not debian\n  package: name={{ item }}\n           state=latest\n  with_items:\n   - NetworkManager\n   - glibc # CVE-2015-7547\n   - bash\n   - iptables\n  when: is_redhat\n\n- name: Update common packages  (debian)\n  package: name={{ item }}\n           state=latest\n  with_items:\n   - libc6\n   - bash\n   - iptables\n  when: is_debuntu\n\n\n# instuctions state to start with a fully updated system before starting, stop using\n# ansible as a crutch for developers not following the directions and taking short-cuts\n\n#- name: If version of Network manager has changed, subsequent nmcli commands will fail,restart now\n#  service: name=NetworkManager\n#           state=restarted\n#  when: not installing\n# the above should use a handler - all reboots should wait until all \n# mods are preformed\n\n- name: Install optional exFAT packages for CentOS\n  shell: yum --enablerepo=li-nux-ro install exfat-utils fuse-exfat\n  when: exFAT_enabled == \"True\" and ansible_distribution == \"CentOS\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "9835670262ea2542513d8e1bdb647c50978c69cc", "filename": "roles/openstack-stack/tasks/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n\n- name: Generate the templates\n  include: generate-templates.yml\n  when:\n  - stack_state == 'present'\n\n- name: Handle the Stack (create/delete)\n  ignore_errors: False\n  register: stack_create\n  os_stack:\n    name: \"{{ stack_name }}\"\n    state: \"{{ stack_state }}\"\n    template: \"{{ stack_template_path | default(omit) }}\"\n    wait: yes\n\n# NOTE(bogdando) OS::Neutron::Subnet doesn't support live updates for\n# dns_nameservers, so we can't do that for the \"create stack\" task.\n- include: subnet_update_dns_servers.yaml\n  when:\n  - private_dns_server is defined\n  - stack_state == 'present'\n\n- name: CleanUp\n  include: cleanup.yml\n  when:\n  - stack_state == 'present'\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "084913cd0dbffb85f9d9166be0cec07c3421e791", "filename": "roles/validate-app/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- block:\n    - name: Gather facts\n      openshift_facts:\n        role: common\n\n    - name: Create the validation project\n      command: \"{{ openshift.common.client_binary }} new-project {{ project }}\"\n\n    - name: Create Hello world app\n      shell: \"{{ openshift.common.client_binary }} new-app {{ app }}\"\n\n    - name: Wait for build to complete\n      shell: \"{{ openshift.common.client_binary }} get pod | grep -v deploy | awk '/{{ app }}-1-build/{ print $3 }'\"\n      register: build_output\n      until: build_output.stdout | search(\"Completed\")\n      retries: 30\n      delay: 15\n\n    - name: Wait for App to be running\n      shell: \"{{ openshift.common.client_binary }} get pod | grep -v deploy | grep -v build  | awk '/{{ app }}-1-*/{print $3}'\"\n      register: deployer_output\n      until: deployer_output.stdout | search(\"Running\")\n      retries: 30\n      delay: 15\n\n    - name: Sleep to allow for route propegation\n      pause:\n        seconds: 10\n\n    - name: check the status of the page\n      uri:\n        url: \"http://{{ app }}-{{ project }}.{{ hostvars['localhost']['wildcard_zone'] }}\"\n        status_code: 200\n        method: GET\n\n  always:\n    - name: Delete the Project\n      command: \"{{ openshift.common.client_binary }} delete project {{ project }}\"\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "62b13e402e6202560d789bf2cc5d469a9fee26a8", "filename": "roles/config-vnc-server/tasks/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Determine required python firewall package'\n  set_fact:\n    python_firewall_package: \"python3-firewall\"\n  when:\n  - ansible_python_version is match(\"3.*\")\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - firewalld\n  - \"{{ python_firewall_package | default('python-firewall') }}\"\n\n- name: 'Ensure firewalld is running'\n  service:\n    name: firewalld\n    state: started\n    enabled: yes\n\n- name: 'Open Firewall for NFS use'\n  firewalld:\n    port: \"{{ item }}\"\n    permanent: yes\n    state: enabled\n    immediate: yes\n  with_items:\n  - 5900/tcp\n  - 5901/tcp\n  - 5902/tcp\n  - 5903/tcp\n  - 5904/tcp\n  - 5905/tcp\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "7eb7488f4eb7399965cb4963e6b1d2fec4f8fdb1", "filename": "reference-architecture/aws-ansible/playbooks/roles/non-atomic-docker-storage-setup/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Gather facts\n  openshift_facts:\n    role: common\n\n- name: create the docker-storage-setup config file\n  template:\n    src: \"{{ role_path }}/templates/docker-storage-setup.j2\"\n    dest: /etc/sysconfig/docker-storage-setup\n    owner: root\n    group: root\n    mode: 0644\n  when: not openshift.common.is_atomic | bool and ansible_distribution != 'CentOS'\n\n- name: create the docker-storage-setup config file\n  template:\n    src: \"{{ role_path }}/templates/docker-storage-setup-dm.j2\"\n    dest: /etc/sysconfig/docker-storage-setup\n    owner: root\n    group: root\n    mode: 0644\n  when: ansible_distribution == 'CentOS' and not openshift.common.is_atomic | bool\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "e195389a60152d65bd957098f1fafe0c2b9e93de", "filename": "roles/openvpn/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "---\n\n- name: Install OpenVPN packages\n  package:  name={{ item }}\n            state=present\n  with_items:\n        - openvpn\n        - nmap\n  tags:\n    - download\n\n- name: Create the directory for keys\n  file: dest=/etc/openvpn/keys\n        state=directory\n        owner=root\n        group=root\n        mode=0755\n\n- name: Create the directory for scripts\n  file: dest=/etc/openvpn/scripts\n        state=directory\n        owner=root\n        group=root\n        mode=0755\n\n- name: Create a folder for iiab executable not on path\n  file: path=/usr/lib/iiab\n        state=directory\n\n- name: Configure OpenVPN\n  template: src={{ item.src }}\n            dest={{ item.dest }}\n            owner={{ item.owner }}\n            group=root\n            mode={{ item.mode }}\n  with_items:\n    - { src: 'ca.crt', dest: '/etc/openvpn/keys/ca.crt', owner: \"root\" , mode: '0644' }\n    - { src: 'client1.crt', dest: '/etc/openvpn/keys/client1.crt', owner: \"root\" , mode: '0644' }\n    - { src: 'client1.key', dest: '/etc/openvpn/keys/client1.key', owner: \"root\" , mode: '0600' }\n    - { src: 'announce', dest: '/etc/openvpn/scripts/announce', owner: \"root\" , mode: '0755' }\n    - { src: 'announcer', dest: '/etc/openvpn/scripts/announcer', owner: \"root\" , mode: '0755' }\n    - { src: 'silence', dest: '/etc/openvpn/scripts/silence', owner: \"root\" , mode: '0755' }\n    - { src: 'xscenet.conf', dest: '/etc/openvpn/xscenet.conf', owner: \"root\" , mode: '0644' }\n    - { src: 'iiab-vpn.conf.in', dest: '/etc/openvpn/iiab-vpn.conf.in', owner: \"root\" , mode: '0644' }\n    - { src: 'iiab-vpn', dest: '/usr/bin/iiab-vpn', owner: \"root\" , mode: '0755' }\n    - { src: 'iiab-handle', dest: '/usr/bin/iiab-handle', owner: \"root\" , mode: '0755' }\n    - { src: 'up_wan', dest: '/usr/lib/iiab/up_wan', owner: \"root\" , mode: '0755' }\n    - { src: 'start.j2', dest: '/usr/lib/iiab/start', owner: \"root\" , mode: '0755' }\n    - { src: 'iiab-remote-on', dest: '/usr/bin/iiab-remote-on', owner: \"root\" , mode: '0755' }\n    - { src: 'iiab-remote-off', dest: '/usr/bin/iiab-remote-off', owner: \"root\" , mode: '0755' }\n\n- name: Put up_wan in place for Debian\n  template: src=up_wan dest=/usr/lib/iiab/up_wan\n  when: is_debuntu\n\n- name: Put dispatcher up for NM\n  template: src=15-openvpn dest=/etc/NetworkManager/dispatcher.d/\n  when: not is_debuntu\n\n- name: Check for manually configured OpenVPN tunnel\n  stat: path=/etc/openvpn/iiab-vpn.conf\n  register: stat\n\n\n# note that ansible does not currently handle @ in a service name\n- name:  Enable the OpenVPN tunnel at boot time\n  shell: systemctl enable openvpn@xscenet.service\n  when:  openvpn_enabled and not stat.exists is defined and is_debuntu\n\n- name:  Enable the OpenVPN tunnel at boot time for Debian\n  shell: update-rc.d openvpn enable\n  when:  openvpn_enabled and not stat.exists is defined and is_debuntu\n\n- name:  Start the OpenVPN tunnel now\n  shell: systemctl start openvpn@xscenet.service\n  when:  openvpn_enabled and not stat.exists is defined and not installing\n\n\n- name: Make OpenVPN connection automatic\n  lineinfile: dest=/etc/crontab\n            line=\"25 *  *  *  * root (/usr/bin/systemctl start openvpn@xscenet.service) > /dev/null\"\n  when: openvpn_enabled and openvpn_cron_enabled and not stat.exists is defined\n\n- name: Make OpenVPN connection manual\n  lineinfile: dest=/etc/crontab\n            regexp=\".*/usr/bin/systemctl*\"\n            state=absent\n  when: not openvpn_enabled or not openvpn_cron_enabled\n\n\n- name:  Stop starting the OpenVPN tunnel at boot time\n  shell: systemctl disable openvpn@xscenet.service\n  when:  not openvpn_enabled and not is_debuntu\n\n- name:  Stop starting the OpenVPN tunnel at boot time for Debian\n  shell: update-rc.d openvpn disable\n  when:  not openvpn_enabled and is_debuntu\n\n- name:  Stop OpenVPN tunnel immediately\n  shell: systemctl stop openvpn@xscenet.service\n  ignore_errors: True\n  when:  not openvpn_enabled and not installing\n\n\n- name: Add 'openvpn' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: openvpn\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n  - option: name\n    value: OpenVPN\n  - option: description\n    value: '\"OpenVPN is a means of connecting to a server anywhere on the internet, via a middleman server.\"'\n  - option: middleman_url\n    value: \"{{ vpn_presence }}\"\n  - option: port\n    value: \"{{ openvpn_server_port }}\"\n  - option: enabled\n    value: \"{{ openvpn_enabled }}\"\n  - option: cron_enabled\n    value: \"{{ openvpn_cron_enabled }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ba2855b733e917aa6717d05f55bf29bf24c98ec3", "filename": "playbooks/openstack/openshift-cluster/vars.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "# yamllint disable rule:colons\n---\ndebug_level: 2\nopenstack_infra_heat_stack:     \"{{ lookup('oo_option', 'infra_heat_stack' ) |\n                                    default('files/heat_stack.yaml',         True) }}\"\nopenstack_subnet_24_prefix:     \"{{ lookup('oo_option', 'subnet_24_prefix'         ) |\n                                    default('192.168.' + ( ( 1048576 | random % 256 ) | string() ), True) }}\"\nopenstack_network_external_net: \"{{ lookup('oo_option', 'external_net'     ) |\n                                    default('external',                      True) }}\"\nopenstack_network_dns:          \"{{ lookup('oo_option', 'dns'              ) |\n                                    default('8.8.8.8,8.8.4.4',               True) | oo_split() }}\"\nopenstack_ssh_public_key:       \"{{ lookup('file', lookup('oo_option', 'public_key') |\n                                    default('~/.ssh/id_rsa.pub',             True)) }}\"\nopenstack_ssh_access_from:      \"{{ lookup('oo_option', 'ssh_from')          |\n                                    default('0.0.0.0/0',                     True) }}\"\nopenstack_node_port_access_from: \"{{ lookup('oo_option', 'node_port_from')   |\n                                    default('0.0.0.0/0',                     True) }}\"\nopenstack_heat_timeout:         \"{{ lookup('oo_option', 'heat_timeout')   |\n                                    default('3',                             True) }}\"\nopenstack_flavor:\n  etcd:   \"{{ lookup('oo_option', 'etcd_flavor'      ) | default('m1.small',  True) }}\"\n  master: \"{{ lookup('oo_option', 'master_flavor'    ) | default('m1.small',  True) }}\"\n  infra:  \"{{ lookup('oo_option', 'infra_flavor'     ) | default('m1.small',  True) }}\"\n  node:   \"{{ lookup('oo_option', 'node_flavor'      ) | default('m1.medium', True) }}\"\n\ndeployment_rhel7_ent_base:\n  image: \"{{ lookup('oo_option', 'image_name') | default('rhel-guest-image-7.2-20151102.0.x86_64', True) }}\"\n  ssh_user: openshift\n  become: yes\n\ndeployment_vars:\n  origin:\n    image: \"{{ lookup('oo_option', 'image_name') | default('centos-70-raw', True) }}\"\n    ssh_user: openshift\n    become: yes\n  enterprise: \"{{ deployment_rhel7_ent_base }}\"\n  openshift-enterprise: \"{{ deployment_rhel7_ent_base }}\"\n  atomic-enterprise: \"{{ deployment_rhel7_ent_base }}\"\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "2a9d5fece8a73eea11992ca2a6191d4c1f5ca6f3", "filename": "tasks/setup-repository-Fedora.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Ensure python and deps for Ansible modules\n  become: true\n  raw: dnf install -y python2 python2-dnf libselinux-python\n  changed_when: false\n  when:\n    - docker_network_access | bool\n    - not _docker_python3\n\n- name: Include tasks from setup of repositories for CentOS\n  include_tasks: setup-repository-CentOS.yml"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "3660613b67c2c529a7e55021eb3f2a6f4c0363bf", "filename": "roles/cloud-ec2/files/stack.yml", "repository": "trailofbits/algo", "decoded_content": "---\nAWSTemplateFormatVersion: '2010-09-09'\nDescription: 'Algo VPN stack'\nParameters:\n  InstanceTypeParameter:\n    Type: String\n    Default: t2.micro\n  PublicSSHKeyParameter:\n    Type: String\n  ImageIdParameter:\n    Type: String\n  WireGuardPort:\n    Type: String\nResources:\n  VPC:\n    Type: AWS::EC2::VPC\n    Properties:\n      CidrBlock: 172.16.0.0/16\n      EnableDnsSupport: true\n      EnableDnsHostnames: true\n      InstanceTenancy: default\n      Tags:\n        - Key: Name\n          Value: Algo\n        - Key: Environment\n          Value: Algo\n\n  VPCIPv6:\n    Type: AWS::EC2::VPCCidrBlock\n    Properties:\n      AmazonProvidedIpv6CidrBlock: true\n      VpcId: !Ref VPC\n\n  InternetGateway:\n    Type: AWS::EC2::InternetGateway\n    Properties:\n      Tags:\n        - Key: Environment\n          Value: Algo\n        - Key: Name\n          Value: Algo\n\n  Subnet:\n    Type: AWS::EC2::Subnet\n    Properties:\n      CidrBlock: 172.16.254.0/23\n      MapPublicIpOnLaunch: false\n      Tags:\n        - Key: Environment\n          Value: Algo\n        - Key: Name\n          Value: Algo\n      VpcId: !Ref VPC\n\n  VPCGatewayAttachment:\n    Type: AWS::EC2::VPCGatewayAttachment\n    Properties:\n      VpcId: !Ref VPC\n      InternetGatewayId: !Ref InternetGateway\n\n  RouteTable:\n    Type: AWS::EC2::RouteTable\n    Properties:\n      VpcId: !Ref VPC\n      Tags:\n        - Key: Environment\n          Value: Algo\n        - Key: Name\n          Value: Algo\n\n  Route:\n    Type: AWS::EC2::Route\n    DependsOn:\n      - InternetGateway\n      - RouteTable\n      - VPCGatewayAttachment\n    Properties:\n      RouteTableId: !Ref RouteTable\n      DestinationCidrBlock: 0.0.0.0/0\n      GatewayId: !Ref InternetGateway\n\n  RouteIPv6:\n    Type: AWS::EC2::Route\n    DependsOn:\n      - InternetGateway\n      - RouteTable\n      - VPCGatewayAttachment\n    Properties:\n      RouteTableId: !Ref RouteTable\n      DestinationIpv6CidrBlock: \"::/0\"\n      GatewayId: !Ref InternetGateway\n\n  SubnetIPv6:\n    Type: AWS::EC2::SubnetCidrBlock\n    DependsOn:\n      - RouteIPv6\n      - VPC\n      - VPCIPv6\n    Properties:\n      Ipv6CidrBlock:\n        \"Fn::Join\":\n            - \"\"\n            - - !Select [0, !Split [ \"::\", !Select [0, !GetAtt VPC.Ipv6CidrBlocks] ]]\n              - \"::dead:beef/64\"\n      SubnetId: !Ref Subnet\n\n  RouteSubnet:\n    Type: \"AWS::EC2::SubnetRouteTableAssociation\"\n    DependsOn:\n      - RouteTable\n      - Subnet\n      - Route\n    Properties:\n      RouteTableId: !Ref RouteTable\n      SubnetId: !Ref Subnet\n\n  InstanceSecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    DependsOn:\n      - Subnet\n    Properties:\n      VpcId: !Ref VPC\n      GroupDescription: Enable SSH and IPsec\n      SecurityGroupIngress:\n        - IpProtocol: tcp\n          FromPort: '22'\n          ToPort: '22'\n          CidrIp: 0.0.0.0/0\n        - IpProtocol: udp\n          FromPort: '500'\n          ToPort: '500'\n          CidrIp: 0.0.0.0/0\n        - IpProtocol: udp\n          FromPort: '4500'\n          ToPort: '4500'\n          CidrIp: 0.0.0.0/0\n        - IpProtocol: udp\n          FromPort: !Ref WireGuardPort\n          ToPort: !Ref WireGuardPort\n          CidrIp: 0.0.0.0/0\n      Tags:\n        - Key: Name\n          Value: Algo\n        - Key: Environment\n          Value: Algo\n\n  EC2Instance:\n    Type: AWS::EC2::Instance\n    DependsOn:\n      - SubnetIPv6\n      - Subnet\n      - InstanceSecurityGroup\n    Metadata:\n      AWS::CloudFormation::Init:\n        config:\n          files:\n            /home/ubuntu/.ssh/authorized_keys:\n              content:\n                Ref: PublicSSHKeyParameter\n              mode: \"000644\"\n              owner: \"ubuntu\"\n              group: \"ubuntu\"\n    Properties:\n      InstanceType:\n        Ref: InstanceTypeParameter\n      InstanceInitiatedShutdownBehavior: terminate\n      SecurityGroupIds:\n        - Ref: InstanceSecurityGroup\n      ImageId:\n        Ref: ImageIdParameter\n      SubnetId: !Ref Subnet\n      Ipv6AddressCount: 1\n      UserData:\n        \"Fn::Base64\":\n          !Sub |\n            #!/bin/bash -xe\n            apt-get update\n            apt-get -y install python-pip\n            pip install https://s3.amazonaws.com/cloudformation-examples/aws-cfn-bootstrap-latest.tar.gz\n            cfn-init -v --stack ${AWS::StackName} --resource EC2Instance --region ${AWS::Region}\n            cfn-signal -e $? --stack ${AWS::StackName} --resource EC2Instance --region ${AWS::Region}\n      Tags:\n        - Key: Name\n          Value: Algo\n        - Key: Environment\n          Value: Algo\n\n  ElasticIP:\n    Type: AWS::EC2::EIP\n    Properties:\n      Domain: vpc\n      InstanceId: !Ref EC2Instance\n    DependsOn:\n      - EC2Instance\n      - VPCGatewayAttachment\n\nOutputs:\n  ElasticIP:\n    Value: !Ref ElasticIP\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9bfe4b5d4899580a39c6c9e11957eddb256ae8ba", "filename": "roles/dns/config-dns-server/tasks/named/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Ensure required packages are installed\n  package:\n    name: \"{{ item }}\"\n    state: \"{{ package_state }}\"\n  with_items:\n    - bind\n    - bind-utils\n    - firewalld\n    - python-firewall\n    - libsemanage-python\n    - python-dns\n    - libselinux-python\n\n- name: Enable named\n  service:\n    name: named\n    enabled: yes\n\n- name: Enable firewalld\n  service:\n    name: firewalld\n    enabled: yes\n    state: started\n\n- name: Open Firewall for DNS\n  firewalld:\n    port: \"{{item}}\"\n    permanent: yes\n    state: enabled\n    immediate: yes\n  with_items:\n    - 53/tcp\n    - 53/udp\n\n- name: Configure named\n  copy:\n    src: named.conf\n    dest: /etc/named.conf\n    owner: named\n    group: named\n    mode: 0660\n\n- name: Setup Zone Directory\n  file:\n    path: /var/named/static\n    state: directory\n    owner: named\n    group: named\n    mode: 0770\n\n- name: Setup key for service named status to communicate with BIND\n  command: \"/sbin/rndc-confgen -a -r /dev/urandom\"\n\n- name: Ensure various files/directories exists with the proper permissions\n  file:\n    path: \"{{ item }}\"\n    owner: root\n    group: named\n    mode: 0640\n  with_items:\n    - \"/etc/rndc.key\"\n\n- name: Configure SELinux\n  seboolean:\n    name: named_write_master_zones\n    state: yes\n    persistent: yes\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "f184d39133085196f3acf0d3bbd800de27f65cd3", "filename": "reference-architecture/vmware-ansible/playbooks/ocp-install.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: yes\n  ignore_errors: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  # Group systems\n  - instance-groups\n\n- include: prerequisite.yaml\n\n- name: call openshift includes for installer\n  include: /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml\n  vars:\n    openshift_release: \"v3.6\"\n    debug_level: 2\n    console_port: 8443\n    openshift_debug_level: \"{{ debug_level }}\"\n    openshift_node_debug_level: \"{{ node_debug_level | default(debug_level, true) }}\"\n    osm_controller_args:\n      cloud-provider:\n      - \"vsphere\"\n      cloud-config:\n      - \"/etc/vsphere/vsphere.conf\"\n    osm_api_server_args:\n      cloud-provider:\n      - \"vsphere\"\n      cloud-config:\n      - \"/etc/vsphere/vsphere.conf\"\n    openshift_node_kubelet_args:\n      cloud-provider:\n      - \"vsphere\"\n      cloud-config:\n      - \"/etc/vsphere/vsphere.conf\"\n      node-labels:\n      - \"role={{ openshift_node_labels.role }}\"\n    openshift_master_debug_level: \"{{ master_debug_level | default(debug_level, true) }}\"\n    openshift_master_access_token_max_seconds: 2419200\n    openshift_hosted_router_replicas: 1\n    openshift_hosted_registry_replicas: 1\n    openshift_master_api_port: \"{{ console_port }}\"\n    openshift_master_console_port: \"{{ console_port }}\"\n    openshift_master_logging_public_url: \"https://kibana.{{ osm_default_subdomain }}\"\n    openshift_node_local_quota_per_fsgroup: 512Mi\n    osm_cluster_network_cidr: 172.16.0.0/16\n    osm_use_cockpit: false\n    osm_default_node_selector: \"role=app\"\n    openshift_registry_selector: \"role=infra\"\n    openshift_override_hostname_check: true\n    openshift_router_selector: \"role=infra\"\n    openshift_master_cluster_method: native\n    openshift_cloudprovider_kind: vsphere\n    wildcard_zone:\n    osm_default_subdomain: \"{{ wildcard_zone }}\"\n    openshift_master_default_subdomain: \"{{osm_default_subdomain}}\"\n    deployment_type:\n    load_balancer_hostname:\n    openshift_master_cluster_hostname: \"{{ load_balancer_hostname }}\"\n    openshift_master_cluster_public_hostname: \"{{ load_balancer_hostname }}\"\n    os_sdn_network_plugin_name: \"{{ openshift_sdn }}\"\n    openshift_master_identity_providers:\n    - name: Active_Directory\n      challenge: true\n      login: true\n      kind: LDAPPasswordIdentityProvider\n      attributes:\n        id:\n        - dn\n        email:\n        - mail\n        name:\n        - cn\n        preferredUsername:\n        - uid\n      insecure: true\n      url:\n      bindDN:\n      bindPassword:\n    openshift_hosted_registry_storage_kind: nfs\n    openshift_hosted_registry_storage_access_modes: ['ReadWriteMany']\n    openshift_hosted_registry_storage_host:\n    openshift_hosted_registry_storage_nfs_directory:\n    openshift_hosted_registry_storage_volume_name: registry\n    openshift_hosted_registry_storage_volume_size: 20Gi\n    openshift_master_metrics_public_url: \"https://metrics.{{ osm_default_subdomain }}/hawkular/metrics\"\n    openshift_hosted_metrics_storage_kind: nfs\n    openshift_hosted_metrics_storage_access_modes: ['ReadWriteOnce']\n    openshift_hosted_metrics_storage_host:\n    openshift_hosted_metrics_storage_nfs_directory:\n    openshift_hosted_metrics_storage_volume_name: metrics\n    openshift_hosted_metrics_storage_volume_size: 10Gi\n#openshift_master_identity_providers: \"[{'name': 'allow_all', 'login': 'true', 'challenge': 'true', 'kind': 'AllowAllPasswordIdentityProvider'}]\"\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "173d9696708c11628bfae6cb0eb4eb5ec1e3c061", "filename": "roles/cloud-ec2/defaults/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\nec2_vpc_nets:\n  cidr_block: 172.251.0.0/23\n  subnet_cidr: 172.251.1.0/24\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "ea43181c3cbef322147ad703a1389d78fa6f2b48", "filename": "tasks/configure-docker/configure-non-systemd.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Combine Docker daemon environment variable configuration\n  set_fact:\n    docker_service_envs: \"{{ docker_service_envs | combine(_docker_service_opts) | combine(docker_daemon_envs) }}\"\n  vars:\n    _docker_service_opts:\n      DOCKER_OPTS: \"{{ docker_daemon_opts }}\"\n\n- name: Setup Docker environment file {{ docker_envs_dir[_docker_os_dist] }}/docker\n  become: true\n  template:\n    src: docker-envs.j2\n    dest: \"{{ docker_envs_dir[_docker_os_dist] }}/docker\"\n  notify: restart docker\n  vars:\n    docker_envs: \"{{ docker_service_envs }}\""}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "aeb495e5706e8664aa434124d07e1a85ef4c16ca", "filename": "roles/client/tasks/systems/CentOS.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- set_fact:\n    prerequisites:\n      - epel-release\n    configs_prefix: /etc/strongswan\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6c4141fd894695aa92092109bd3af69c871df4c7", "filename": "reference-architecture/gcp/ansible/playbooks/roles/gold-image/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: create gold image\n  include_role:\n    name: deployment-create\n  vars:\n    deployment_name: gold-image\n    deployment_name_with_prefix: '{{ prefix }}-{{ deployment_name }}{{ \"-origin\" if openshift_deployment_type == \"origin\" else \"\" }}'\n\n- name: delete temp instance disk\n  gce_pd:\n    name: '{{ prefix }}-tmp-instance'\n    zone: '{{ gcloud_zone }}'\n    service_account_email: '{{ service_account_id }}'\n    credentials_file: '{{ credentials_file }}'\n    project_id: '{{ gcloud_project }}'\n    state: absent\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b79df48eedf6174c053f3340180a02cbea960f94", "filename": "roles/crs-subscription/tasks/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- block:\n    - name: Allow rhsm a longer timeout to help out with subscription-manager\n      lineinfile:\n        dest: /etc/rhsm/rhsm.conf\n        line: 'server_timeout=600'\n        insertafter: '^proxy_password ='\n\n    - name: Check for sat config file\n      stat: path=/etc/rhsm/rhsm.conf.kat-backup\n      register: sat_cfg\n\n    - name: Remove satellite configuration if using RH CDN\n      command: \"mv -f /etc/rhsm/rhsm.conf.kat-backup /etc/rhsm/rhsm.conf\"\n      when: rhsm_user is defined and rhsm_user and sat_cfg.stat.exists == True\n      ignore_errors: yes\n\n    - name: Remove satellite SSL if using RH CDN\n      command: \"rpm -e $(rpm -qa katello-ca-consumer*)\"\n      when: rhsm_user is defined and rhsm_user and sat_cfg.stat.exists == True\n      ignore_errors: yes\n\n    - name: Is the host already registered?\n      command: \"subscription-manager status\"\n      register: subscribed\n      changed_when: no\n      ignore_errors: yes\n\n    - name: Register host via Activation key\n      redhat_subscription:\n        activationkey: \"{{ rhsm_activation_key }}\"\n        org_id: \"{{ rhsm_org_id }}\"\n        state: present\n        pool: \"{{ rhsm_pool }}\"\n      when: rhsm_activation_key is defined and rhsm_activation_key\n      register: register_key_result\n      ignore_errors: yes\n\n    - name: Register host\n      redhat_subscription:\n        username: \"{{ rhsm_user }}\"\n        password: \"{{ rhsm_password }}\"\n        state: present\n        pool: \"{{ rhsm_pool }}\"\n      when: \"'Current' not in subscribed.stdout and rhsm_user is defined and rhsm_user\"\n\n    - name: Check if subscription is attached\n      command: subscription-manager list --consumed --pool-only --matches=\"{{ rhsm_pool }}\"\n      register: subscription_attached\n      changed_when: no\n\n    - block:\n        - name: Get pool id\n          shell: subscription-manager list --available --pool-only --matches=\"{{ rhsm_pool }}\" | head -n 1\n          register: pool_id\n          changed_when: no\n\n        - name: Fail if no pool ID is returned\n          fail:\n            msg: No subscription matching \"{{ rhsm_pool }}\" found\n          when: pool_id.stdout == \"\"\n\n        - name: Attach subscription\n          command: subscription-manager attach --pool=\"{{ pool_id.stdout }}\"\n\n      when: subscription_attached.stdout == \"\"\n\n  when: ansible_distribution == \"RedHat\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5ec38c87169ed86a33739bfb54c1c3375e0843c5", "filename": "roles/nfs-server/tasks/shares.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Set share basedir\"\n  set_fact:\n    nfs_share_basedir: \"{{ nfs_share_basedir | default(default_nfs_share_basedir) }}\"\n\n- name: \"Create directories\"\n  file:\n    path: \"{{ nfs_share_basedir }}/{{ item.name }}\"\n    state: directory \n    owner: \"{{ item.nfs_owner | default(default_nfs_owner) }}\" \n    group: \"{{ item.nfs_group | default(default_nfs_group) }}\"\n    mode: \"{{ item.nfs_mode | default(default_nfs_mode) }}\"\n  with_items: \n  - \"{{ nfs_shares }}\"\n\n- name: \"Update export file to share out the directory\"\n  lineinfile: \n    path: /etc/exports\n    state: present\n    regexp: \"^{{ nfs_share_basedir }}/{{ item.name }}\"\n    line: \"{{ nfs_share_basedir }}/{{ item.name }} *({{ item.nfs_share_options | default(default_nfs_share_options) }})\" \n  with_items: \n  - \"{{ nfs_shares }}\"\n  notify: Reload NFS\n\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e4a6d54b6fc94ca92f3f33a2e80895d9fc872a7d", "filename": "playbooks/provision-bastion/README.md", "repository": "redhat-cop/infra-ansible", "decoded_content": "# Bastion Host / Control Host playbook\n\nThis playbook uses a variety of roles in this repo to setup a bastion host, also some times called a control host. The inventory can be used (per instructions below) to control which software and services get installed on the bastion host.\n\n\n## Prerequisites\nA running instance (VM or cloud image) such as Fedora, CentOS or Red Hat Enterprise Linux. The instance needs to be subscribed (if applicable) and configured with access to the necessary repos (in most cases, the exsisting repos / configuration is sufficient).\n\nIf the IdM / IPA integration is to be used, it is a prerequisites that the environment is set up with automatic client server discovery vis DNS SRV records (consult your sys admin if this is an unfamiliar area).\n\n## Gotcha's\n1. If running in a cloud environment, for example OpenStack, make sure to have the correct ports open in the security groups (e.g.: 5901 for VNC, 22 for SSH, etc.)\n2. When enabling VNC, and you already have a shared home directory, make sure the proper changes are made to the VNC configuration (typically in `~/.vnc` ) to allow for the service to run correctly.\n\n## Example run\nHow to run the playbook may depend on the options selected. However, below is an example execution whereas the password for IPA/IdM integration (with `ipa_client_install` set to `True` in the inventory) is passed in rather than statically set in the inventory. Modify the inventory to your liking in `playbooks/bastion/inventory`, then at the top level of the repository, execute the following command:\n\n```\n> ansible-playbook -i playbooks/bastion/inventory playbooks/bastion/install.yml -e 'ipa_password=<ipa/IdM password>'\n```\n\n**Note:** If your password contains any special characters, e.g.: a '!', it's important to use the single quotes for the passed in value as it otherwise may be interpereted by the shell.\n\n## Inventory Options\n\n**Note:** If you are intending to use the IdM/IPA integration, and are unfamiliar with the IdM/IPA variables below, please consult the IdM/IPA documentation or your sys admin for details.\n\n**Note:** When installing a GUI (i.e.: XFCE, LXDE, Gnome), it's recommended that only one is selected as running multiple is not supported nor tested by this playbook/roles.\n\n| variable | info |\n|:--------:|:----:|\n|main_user|The username this bastion is primerly being enabled for|\n|ipa_client_install|Set to `True` if you'd like to integrate with a backend IPA/IdM service|\n|ipa_domain|If `ipa_client_install` is set to `True`, set this to the existing IdM / IPA domain your environment uses (obtain from sys admin if not known)|\n|ipa_automount_location|If `ipa_client_install` is set to `True`, set the required automount location for home directories (obtain from sys admin if not known)|\n|ipa_username|If `ipa_client_install` is set to `True`, this is the username of an account that has the permission to join this host to the above IPA/IdM domain (obtain from sys admin if not known)|\n|ipa_password|If `ipa_client_install` is set to `True`, this is the password of an account that has the permission to join this host to the above IPA/IdM domain (obtain from sys admin if not known)\n|docker_install|Set to `True` if you'd like to enable docker on this host|\n|docker_username|Set to the desirable user (your username) to be added to the docker group (to allow for docker admin)|\n|docker_compose_install|Set to `True` if you'd like to have docker-compose installed on this host. NOTE: This will auto set docker_install=True (not supported on CentOS)|\n|xfce_install|Set to `True` if you'd like XFCE enabled on this host for a graphical UI (note MATE, XFCE or LXDE often works better than gnome for VNC)|\n|lxde_install|Set to `True` if you'd like LXDE enabled on this host for a graphical UI (note MATE, XFCE or LXDE often works better than gnome for VNC)|\n|gnome_install|Set to `True` if you'd like gnome enabled on this host for a graphical UI|\n|mate_install|Set to `True` if you'd like MATE Desktop enabled on this host for a graphical UI (note MATE, XFCE or LXDE often works better than gnome for VNC)|\n|vnc_server_install|Set to `True` if you'd like to enable a VNC server on this host for graphical access to the host|\n|list_of_packages_to_install|List of additional packages (RPMs) to be installed at the end of the bastion host preparation, e.g.: `['git', 'vim']`|\n|timezone| `Optional` Timezone of the Bastion ie `America/Denver`|\n|ansible_python_interpreter| `Optional` Required to be set to `/usr/bin/python3` when using systems like Fedora 28 where certain packages are dependent on python3|\n|vnc_password| `Optional` Set VNC password to a specific value instead of accepting the default| \n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b32c4dc56da3c7c975852615c332e670396f54fb", "filename": "roles/dns/manage-dns-zones/tasks/route53/get-zone-records.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Get all hosted zones\n  route53_facts:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    query: hosted_zone\n  register: hosted_zones\n\n- name: Get all records from zones\n  route53_facts:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    query: record_sets\n    hosted_zone_id: \"{{ item.Id | replace('/hostedzone/', '')}}\"\n  with_items:\n    - \"{{ hosted_zones.HostedZones | default({}) }}\"\n  register: zones_records\n  when:\n    - hosted_zones|length > 0\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "dc56a3ed84da7ae3db8c5d15ab8c598ee9f94592", "filename": "reference-architecture/vmware-ansible/playbooks/roles/cloud-provider-setup/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: create /etc/vsphere/\n  file:\n    state: directory\n    path: \"{{ vsphere_conf_dir }}\"\n\n- name: create the vsphere.conf file\n  template:\n    src: \"{{ role_path }}/templates/vsphere.conf.j2\"\n    dest: /etc/vsphere/vsphere.conf\n    owner: root\n    group: root\n    mode: 0644\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "f0ffb9159a2a3e0b93ae646dd183e7131a9918da", "filename": "roles/dns_adblocking/tasks/ubuntu.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Ubuntu | Dnsmasq profile for apparmor configured\n  template: src=usr.sbin.dnsmasq.j2 dest=/etc/apparmor.d/usr.sbin.dnsmasq owner=root group=root mode=0600\n  when: apparmor_enabled is defined and apparmor_enabled == true\n  notify:\n    - restart dnsmasq\n\n- name: Ubuntu | Enforce the dnsmasq AppArmor policy\n  shell: aa-enforce usr.sbin.dnsmasq\n  when: apparmor_enabled is defined and apparmor_enabled == true\n  tags: ['apparmor']\n\n- name: Ubuntu | Ensure that the dnsmasq service directory exist\n  file: path=/etc/systemd/system/dnsmasq.service.d/ state=directory mode=0755  owner=root group=root\n\n- name: Ubuntu | Setup the cgroup limitations for the ipsec daemon\n  template: src=100-CustomLimitations.conf.j2 dest=/etc/systemd/system/dnsmasq.service.d/100-CustomLimitations.conf\n  notify:\n    - daemon-reload\n    - restart dnsmasq\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "23037d2601ff75c71cb94435322cdee5522c1b79", "filename": "roles/kalite/tasks/setup-f18.yml", "repository": "iiab/iiab", "decoded_content": "# This is for Fedora 18, assumed to be an XO\n\n- name: Run the setup using 'kalite manage' (Fedora 18)\n  command: \"/usr/bin/su {{ kalite_user }} -c '{{ kalite_root }}/bin/kalite manage setup --username={{ kalite_user }} --password={{ kalite_password }} --noinput'\"\n  async: 900\n  poll: 10\n\n- name: Finish setup by running 'kalite start' (Fedora 18)\n  command: \"/usr/bin/su {{ kalite_user }} -c '{{ kalite_root }}/bin/kalite start'\"\n  async: 900\n  poll: 10\n\n- name: Stop kalite server started in previous step because we use systemd\n  command: \"/usr/bin/su {{ kalite_user }} -c '{{ kalite_root }}/bin/kalite stop'\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "0da8295694be7f243aeb8b1af2b8e044517e84a6", "filename": "reference-architecture/aws-ansible/playbooks/create-inventory-file.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  connection: local\n  gather_facts: no\n  roles:\n  - cfn-outputs\n  - inventory-file-creation\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "6249336235b4f6bca0946db3cee13bf63d2141d3", "filename": "tasks/checks/distribution-checks-Debian.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Fail if unsupported Debian version\n  fail:\n    msg: \"Debian 7 (wheezy) or later is required!\"\n  when:\n    - _docker_os_dist == \"Debian\"\n    - _docker_os_dist_major_version | int < 7\n"}, {"commit_sha": "a10c5f4577e6e74feb1fadec4bcbab039b8b180a", "sha": "83565e97b1a92c0f0a9e32533f974c2b3859a2ed", "filename": "tasks/postinstall.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Reset internal variables for additional packages to be installed\n  set_fact:\n    _docker_additional_packages_os: []\n    _docker_additional_packages_pip: []\n    _docker_python_system: false\n\n- name: Do best effort detection and set fact to indicate system Python environment is used\n  set_fact:\n    _docker_python_system: true\n  when: ansible_python.executable | regex_search('^/bin') or ansible_python.executable | regex_search('^/usr/bin')\n\n- name: Set facts to install Docker SDK for Python\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + \\\n      docker_predefined_packages_pip[_docker_os_dist]['sdk'] }}\"\n  when:\n    - docker_sdk\n\n- name: Set facts to install Docker Compose\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + \\\n      docker_predefined_packages_pip[_docker_os_dist]['compose'] }}\"\n  when:\n    - docker_compose and not docker_compose_no_pip\n\n- name: Set facts to install Docker Stack dependencies ('docker_stack')\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + \\\n      docker_predefined_packages_pip[_docker_os_dist]['stack'] }}\"\n  when:\n    - docker_stack\n\n- name: Set facts with additional package to be installed\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + docker_additional_packages_pip }}\"\n    _docker_additional_packages_os: \"{{ _docker_additional_packages_os + docker_additional_packages_os }}\"\n\n- name: Determine if pip exists in path\n  become: true\n  shell: type pip\n  register: _docker_pip_cmd\n  changed_when: false\n  failed_when: false\n  check_mode: no\n  tags:\n    - skip_ansible_lint\n  when:\n    - _docker_additional_packages_pip | length > 0\n\n- name: Set fact to install Python PiP\n  set_fact:\n    _docker_additional_packages_os: \"{{ _docker_additional_packages_os + [docker_pip_package] }}\"\n  when:\n    - _docker_additional_packages_pip | length > 0\n    - _docker_pip_cmd.rc != 0\n\n- name: Ensure python-pip-whl is present (Debian 8)\n  set_fact:\n    _docker_additional_packages_os: \"{{ _docker_additional_packages_os + ['python-pip-whl'] }}\"\n  when:\n    - _docker_additional_packages_pip | length > 0\n    - _docker_pip_cmd.rc != 0\n    - _docker_os_dist == \"Debian\"\n    - _docker_os_dist_major_version == '8'\n\n- name: Ensure EPEL release repository is installed\n  become: true\n  package:\n    name: \"epel-release\"\n    state: present\n  register: _pkg_result\n  until: _pkg_result|succeeded\n  when:\n    - _docker_os_dist == \"CentOS\"\n    - _docker_additional_packages_os | length > 0\n\n- name: Install additional packages (OS package manager)\n  become: true\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - \"{{ _docker_additional_packages_os }}\"\n  register: _pkg_result\n  until: _pkg_result|succeeded\n  when: _docker_additional_packages_os | length > 0\n\n- name: Upgrade PiP\n  become: \"{{ docker_pip_sudo | bool }}\"\n  pip:\n    name: pip\n    state: forcereinstall\n  register: _pkg_result\n  until: _pkg_result|succeeded\n  when: docker_pip_upgrade\n\n- name: Install additional packages (PiP)\n  become: \"{{ docker_pip_sudo | bool }}\"\n  pip:\n    name: \"{{ item }}\"\n    state: present\n    extra_args: \"{{ docker_pip_extra_args }}\"\n  with_items:\n    - \"{{ _docker_additional_packages_pip }}\"\n  register: _pkg_result\n  until: _pkg_result|succeeded\n  when: _docker_additional_packages_pip | length > 0\n  environment:\n    PYTHONWARNINGS: ignore\n\n# Not using github_release:  https://github.com/ansible/ansible/issues/45391\n- name: Get latest release of docker-compose\n  uri:\n    url: https://api.github.com/repos/docker/compose/releases/latest\n    body_format: json\n  register: _github_docker_compose\n  until: _github_docker_compose.status == 200\n  retries: 10\n  check_mode: no\n  when:\n    - docker_compose\n\n- name: Set common facts related to docker-compose\n  set_fact:\n    _docker_compose_shasum_url: \"https://github.com/docker/compose/releases/download/\\\n      {{ _github_docker_compose.json.tag_name }}/docker-compose-{{ ansible_system }}-{{ ansible_architecture }}.sha256\"\n    _docker_compose_url: \"https://github.com/docker/compose/releases/download/\\\n      {{ _github_docker_compose.json.tag_name }}/docker-compose-{{ ansible_system }}-{{ ansible_architecture }}\"\n  when:\n    - docker_compose\n\n- name: Fetch docker-compose SHA265 sum file\n  get_url:\n    url: \"{{ _docker_compose_shasum_url }}\"\n    dest: \"/tmp/ansible.docker-compose-sha256\"\n  register: _github_docker_compose_shasum_file\n  changed_when: false\n  until: _github_docker_compose_shasum_file.status_code == 200\n  retries: 10\n  check_mode: no\n  when:\n    - docker_compose\n\n- name: Dump SHA256 file contents to variable\n  command: cat /tmp/ansible.docker-compose-sha256\n  register: _github_docker_compose_shasum\n  changed_when: false\n  check_mode: no\n  when:\n    - docker_compose\n\n- name: Remove temporary file for SHA256 sum\n  file:\n    path: \"/tmp/ansible.docker-compose-sha256\"\n    state: absent\n  changed_when: false\n  check_mode: no\n  when:\n    - docker_compose\n\n- name: Set SHA256 facts related to docker-compose\n  set_fact:\n    _docker_compose_checksum: \"sha256:{{ _github_docker_compose_shasum.stdout | \\\n      regex_replace('^([0-9a-zA-Z]*)[\\\\s\\\\t]+.+', '\\\\1') }}\"\n  when:\n    - docker_compose\n\n# Use when moving to Ansible 2.7 as minimum version\n# - name: Set SHA256 facts related to docker-compose (Ansible >= 2.7)\n#   set_fact:\n#     _docker_compose_checksum: \"sha256:https://github.com/docker/compose/releases/download/\\\n#       {{ _github_docker_compose.json.tag_name }}/\\\n#       docker-compose-{{ ansible_system }}-{{ ansible_architecture }}.sha256\"\n#   when: ansible_version.full is version_compare('2.7', '>=')\n\n- name: Stat /usr/bin/docker-compose\n  stat:\n    path: /usr/bin/docker-compose\n  register: _docker_compose_file\n  check_mode: no\n\n# Official installation of docker-compose (Linux): https://docs.docker.com/compose/install/#install-compose\n- name: Install docker-compose\n  block:\n    - name: Install docker-compose (Linux)\n      become: true\n      get_url:\n        url: \"{{ _docker_compose_url }}\"\n        checksum: \"{{ _docker_compose_checksum }}\"\n        dest: /usr/local/bin/docker-compose\n        mode: 0755\n\n    - name: Create symlink for docker-compose to work with sudo in some distributions\n      become: true\n      file:\n        src: /usr/local/bin/docker-compose\n        dest: /usr/bin/docker-compose\n        state: link\n  when:\n    - docker_compose\n    - not _docker_compose_file.stat.exists\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "c8c9a00c0af370a90976fc869867f8398e32bedf", "filename": "roles/openshift-prep/defaults/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# Defines either to install required packages and update all\nmanage_packages: true\ninstall_debug_packages: false\nrequired_packages:\n  - wget\n  - git\n  - net-tools\n  - bind-utils\n  - bridge-utils\ndebug_packages:\n  - bash-completion\n  - vim-enhanced\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ac79701db8866b00d549d062f784d3516c218729", "filename": "playbooks/openstack/openshift-cluster/lookup_plugins", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "../../../lookup_plugins"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "388e8270b3eb0ae20924b478d6a59f8c54b92b0f", "filename": "tasks/Win32NT/fetch/web.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Download artifact from web\n  win_get_url:\n    url: '{{ transport_web }}'\n    dest: >-\n      {{ java_download_path }}\\{{ (transport_web | urlsplit('path')).split('/')[-1] }}\n    force: false\n  register: file_downloaded\n  retries: 5\n  delay: 2\n  until: file_downloaded is succeeded\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "ff1012b8af5e51b34a344c85a83bc8619e4344f7", "filename": "dev/playbooks/install_elk.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: elk\n  become: yes\n  vars:\n    compose_ver: 1.15.0\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Open required ports for logspout\n      command: firewall-cmd --permanent --zone=public --add-port=5000/udp \n\n    - name: Reload firewalld configuration\n      command: firewall-cmd --reload\n\n    - name: Copy config file\n      copy: src={{ item }} dest=/root\n      with_fileglob:\n         - /root/Docker-SimpliVity/dev/files/elk/*\n\n    # Installing elk stack\n    - name: Installing docker compose\n      shell: curl -L https://github.com/docker/compose/releases/download/{{ compose_ver }}/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n\n    - name: change compose permission\n      file:\n         path: /usr/local/bin/docker-compose\n         mode: 0755\n    - name: copy docker.service file with our proxy configuration\n      shell: cp -f /root/docker.service /usr/lib/systemd/system\n   \n    - name: restart daemon to bring in new docker.service file and restart docker\n      shell: systemctl daemon-reload\n\n    - name: restart docker service\n      shell: systemctl restart docker\n\n    - name: run Dockerfile\n      shell: docker build -f /root/elk-Dockerfile -t docker.elastic.co/logstash/logstash:5.4.2_hpe .\n\n    - name: Set momory para\n      shell: sysctl -w vm.max_map_count=262144\n\n    - name: execute compose file\n      shell: /usr/local/bin/docker-compose -f /root/elk-docker-compose.yml up -d\n\n    - name: sleep for 3 min\n      pause: minutes=3\n\n- hosts: ucp_main \n  become: yes\n\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: run logspout\n      shell: docker service create --name logspout  --mount type=bind,source=/var/run/docker.sock,destination=/var/run/docker.sock --env ROUTE_URIS=logstash://{{ hostvars['nm-elk']['ip_addr_only'] }}:5000 --mode=global bekt/logspout-logstash\n"}, {"commit_sha": "9e7bed4aaef56159f148d24e4f7d7e8b53be632b", "sha": "7d049fd66d3fbe71e83bbefb722cc1629ed9bfd0", "filename": "tasks/main-CentOS.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# tasks file for ansible-role-docker-ce\n\n- name: Ensure yum-utils is installed\n  package:\n    name: yum-utils\n    state: present\n  become: true\n\n- name: Add Docker CE repository\n  get_url:\n    url: https://download.docker.com/linux/centos/docker-ce.repo\n    dest: /etc/yum.repos.d/docker-ce.repo\n    mode: 0644\n  become: true\n  register: yum_repo\n\n- name: Determine Docker CE Edge repo status\n  shell: yum-config-manager docker-ce-edge | grep enabled\n  ignore_errors: yes\n  changed_when: false\n  register: cmd_docker_ce_edge_enabled\n\n- name: Set current Docker CE Edge repo status fact\n  set_fact:\n    fact_docker_ce_edge_enabled: \"{{ cmd_docker_ce_edge_enabled.stdout == 'enabled = True' }}\"\n\n- name: Enable/Disable Docker CE Edge Repository\n  shell: yum-config-manager --{{ (docker_enable_ce_edge == true) | ternary('enable','disable') }} docker-ce-edge\n  become: true\n  when: fact_docker_ce_edge_enabled != docker_enable_ce_edge\n\n- name: Update yum cache\n  shell: yum makecache fast\n  args:\n    warn: false  \n  become: true\n  when: yum_repo.changed\n\n- include: main-Storage.yml\n  when: docker_setup_devicemapper == true\n\n- include: main-Generic.yml\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9b3caa7dae2f6e4cb8277337bfee39a12337aebb", "filename": "roles/osp/admin-volume/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Manage Volumes\n  os_volume:\n    cloud: \"{{ item.cloud | default(osp_default_cloud) | default(omit) }}\"\n    state: \"{{ item.state | default(osp_resource_state) | default('present') }}\"\n    name: \"{{ item.name }}\"\n    size: \"{{ item.size | default(omit) }}\"\n    display_name: \"{{ item.display_name | default(omit) }}\"\n    display_description: \"{{ item.display_description | default(omit) }}\"\n  register: os_volumes\n  with_items:\n  - \"{{ osp_volumes | default([]) }}\"\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "6c3026dc1d7482b511ac90db4c17f1e751621c3e", "filename": "tasks/httpd_reverse_proxy_config.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n\n- name: Copy {{ httpd_package_name }} vhost\n  template:\n    src: \"nexus-vhost.conf\"\n    dest: \"{{ httpd_config_dir }}\"\n  notify:\n    - httpd-service-reload\n    - wait-for-httpd\n\n- name: Copy SSL certificate and optional chain file\n  copy:\n    src: \"{{ item }}\"\n    dest: \"{{ certificate_file_dest }}\"\n    mode: 0600\n  when: (httpd_copy_ssl_files | bool) and (item | length > 0)\n  notify:\n    - httpd-service-reload\n    - wait-for-httpd\n  loop: \"{{ [httpd_ssl_certificate_file] + [httpd_ssl_certificate_chain_file | default()] | unique }}\"\n\n- name: Copy SSL certificate key file\n  copy:\n    src: \"{{ httpd_ssl_certificate_key_file }}\"\n    dest: \"{{ certificate_key_dest }}\"\n    mode: 0600\n  when: httpd_copy_ssl_files | bool\n  notify:\n    - httpd-service-reload\n    - wait-for-httpd\n\n- name: Setsebool httpd_can_network_connect\n  seboolean:\n    name: httpd_can_network_connect\n    persistent: yes\n    state: yes\n  when: ansible_selinux.status is defined and ansible_selinux.status == \"enabled\"\n\n- meta: flush_handlers\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "d3970811220735819591027ad4899ea1fd7b5402", "filename": "playbooks/roles/stenographer/tests/test.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- hosts: localhost\n  remote_user: root\n  roles:\n    - stenographer"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c54cc3058d71627fed413816d69303b6398c0f81", "filename": "roles/config-quay-enterprise/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Restart quay service\n  systemd:\n    name: \"{{ quay_service }}\"\n    enabled: yes\n    state: restarted\n    daemon_reload: yes\n\n- name: restart firewalld\n  service:\n    name: firewalld\n    state: restarted\n\n- name: restart iptables\n  service:\n    name: iptables\n    state: restarted\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "ca8a9e53294134bdbf5a7606d7829f89cb216954", "filename": "roles/zookeeper/defaults/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# defaults file for zookeeper\nmethod: all\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "83e4d8f61d574ffcc7315e09c2a781aee38249fa", "filename": "tasks/main.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - include: check_requirements.yml\n\n  - include: section_01.yml\n    tags: section01\n\n  - include: section_02.yml\n    tags: section02\n\n  - include: section_03.yml\n    tags: section03\n\n  - include: section_04.yml\n    tags: section04\n\n  - include: section_05.yml\n    tags: section05\n\n  - include: section_06.yml\n    tags: section06\n\n  - include: section_07.yml\n    tags: section07\n\n  - include: section_08.yml\n    tags: section08\n\n  - include: section_09.yml\n    tags: section09\n\n  - include: section_10.yml\n    tags: section10\n\n  - include: section_11.yml\n    tags: section11\n\n  - include: section_12.yml\n    tags: section12\n\n  - include: section_13.yml\n    tags: section13\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "322b2a5c7b0dbc7c526d9300948819f07945ad32", "filename": "roles/ansible/tower/manage-credential-types/tests/inventory/group_vars/tower.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\ntower_url: 'https://mytower.local'\ntower_admin_username: 'admin'\ntower_admin_password: \"mypassword\"\n\nansible_tower_credential_types:\n- name: \"CredType1\"\n  description: \"My Credential Type 1\"\n  fields:\n  - type: \"string\"\n    id: \"CUSTOM_ID1\"\n    label: \"My Custom ID 1\"\n    secret: \"false\"\n  - type: \"string\"\n    id: \"CUSTOM_ID2\"\n    label: \"My Custom ID 2\"\n    secret: \"true\"\n  required:\n  - id: \"CUSTOM_ID1\"\n  - id: \"CUSTOM_ID2\"\n  injectors_extra_vars:\n    - name: \"MY_EXTRA_VAR1\"\n      id: \"CUSTOM_ID1\"\n    - name: \"MY_EXTRA_VAR2\"\n      id: \"CUSTOM_ID2\" \n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "85c9745cb34d0e6ab571d3257cbffa8dc6bcea6b", "filename": "roles/manage-aws-infra/tasks/deploy-cluster.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "# Tasks to deploy AWS infra based in the requested intance numbers\n---\n\n################################################################################\n# Masters\n\n- name: \"Prepare facts to Create Master(s)\"\n  set_fact:\n    ec2_name: \"{{ env_id }}-ocp-{{ master_name }}\"\n    ec2_image: \"{{ aws_image_name }}\"\n    ec2_instance_type: \"{{ master_flavor | default('m4.xlarge') }}\"\n    ec2_group: \"{{ aws_master_sgroups | default(['ocp-ssh', 'ocp-master', 'ocp-app-node']) }}\"\n    ec2_instance_tags:\n      group: \"{{ group_masters_tag }}\"\n      node_labels: \"{{ labels_masters_tag }}\"\n      env_id: \"{{ env_id }}\"\n    ec2_volumes:\n      - device_name: \"{{ master_root_volume }}\"\n        volume_size: \"{{ master_root_volume_size }}\"\n        volume_type: gp2\n      - device_name: \"{{ docker_storage_block_device }}\"\n        volume_size: \"{{ docker_storage_volume_size }}\"\n        volume_type: gp2\n    ec2_num_instances: \"{{ aws_num_masters }}\"\n    ec2_sg:\n      - name: \"ocp-ssh\"\n        description: \"OCP SSH\"\n        rules: \"{{ ocp_ssh_sg_rules|default([]) + default_ocp_ssh_sg_rules }}\"\n      - name: \"ocp-master\"\n        description: \"OCP Master\"\n        rules: \"{{ ocp_master_sg_rules|default([]) + default_ocp_master_sg_rules }}\"\n      - name: \"ocp-app-node\"\n        description: \"OCP App Node\"\n        rules: \"{{ ocp_app_node_sg_rules|default([]) + default_ocp_app_node_sg_rules }}\"\n\n- name: \"Ensure the necessary Security Groups Exists for the Master(s)\"\n  vars:\n    sg_name: \"{{ item.name }}\"\n    sg_description: \"{{ item.description }}\"\n    sg_rules: \"{{ item.rules }}\"\n  include_tasks: create-security-group.yml\n  with_items:\n    - \"{{ ec2_sg }}\"\n\n- name: \"Create Master(s) based on above facts\"\n  include_tasks: create-instance.yml\n  when:\n    - ec2_num_instances > 0\n\n- name: \"Store away the instance information\"\n  set_fact:\n    master_instances: \"{{ ec2_instances }}\"\n\n- name: \"Clear facts\"\n  set_fact:\n    ec2_instances: ''\n\n\n################################################################################\n# Infra Nodes\n\n- name: \"Prepare facts to Create Infra Node(s)\"\n  set_fact:\n    ec2_name: \"{{ env_id }}-ocp-{{ infra_node_name }}\"\n    ec2_image: \"{{ infra_image_name | default(aws_image_name) }}\"\n    ec2_instance_type: \"{{ infra_flavor | default('i3.xlarge') }}\"\n    ec2_group: \"{{ aws_infra_node_sgroups | default(['ocp-ssh', 'ocp-infra-node', 'ocp-app-node']) }}\"\n    ec2_instance_tags:\n      group: \"{{ group_infra_nodes_tag }}\"\n      node_labels: \"{{ labels_infra_nodes_tag }}\"\n      env_id: \"{{ env_id }}\"\n    ec2_volumes:\n      - device_name: \"{{ infra_node_root_volume }}\"\n        volume_size: \"{{ infra_node_root_volume_size }}\"\n        volume_type: gp2\n      - device_name: \"{{ docker_storage_block_device }}\"\n        volume_size: \"{{ docker_storage_volume_size }}\"\n        volume_type: gp2\n    ec2_num_instances: \"{{ aws_num_infra_nodes }}\"\n    ec2_sg:\n      - name: \"ocp-ssh\"\n        description: \"OCP SSH\"\n        rules: \"{{ ocp_ssh_sg_rules|default([]) + default_ocp_ssh_sg_rules }}\"\n      - name: \"ocp-app-node\"\n        description: \"OCP App Node\"\n        rules: \"{{ ocp_app_node_sg_rules|default([]) + default_ocp_app_node_sg_rules }}\"\n      - name: \"ocp-infra-node\"\n        description: \"OCP Infra Node\"\n        rules: \"{{ ocp_infra_node_sg_rules|default([]) + default_ocp_infra_node_sg_rules }}\"\n\n- name: \"Ensure the necessary Security Groups Exists for the Infra Node(s)\"\n  vars:\n    sg_name: \"{{ item.name }}\"\n    sg_description: \"{{ item.description }}\"\n    sg_rules: \"{{ item.rules }}\"\n  include_tasks: create-security-group.yml\n  with_items:\n    - \"{{ ec2_sg }}\"\n\n- name: \"Create Infra Node(s) based on above facts\"\n  include_tasks: create-instance.yml\n  when:\n    - ec2_num_instances > 0\n\n- name: \"Store away the instance information\"\n  set_fact:\n    infra_node_instances: \"{{ ec2_instances }}\"\n\n- name: \"Clear facts\"\n  set_fact:\n    ec2_instances: ''\n\n\n################################################################################\n# App Nodes\n\n- name: \"Prepare facts to Create App Node(s)\"\n  set_fact:\n    ec2_name: \"{{ env_id }}-ocp-{{ app_node_name }}\"\n    ec2_image: \"{{ app_node_image_name | default(aws_image_name) }}\"\n    ec2_instance_type: \"{{ app_node_flavor | default('m4.xlarge') }}\"\n    ec2_group: \"{{ aws_app_node_sgroups | default(['ocp-ssh', 'ocp-app-node']) }}\"\n    ec2_instance_tags:\n      group: \"{{ group_app_nodes_tag }}\"\n      node_labels: \"{{ labels_app_nodes_tag }}\"\n      env_id: \"{{ env_id }}\"\n    ec2_volumes:\n      - device_name: \"{{ app_node_root_volume }}\"\n        volume_size: \"{{ app_node_root_volume_size }}\"\n        volume_type: gp2\n      - device_name: \"{{ docker_storage_block_device }}\"\n        volume_size: \"{{ docker_storage_volume_size }}\"\n        volume_type: gp2\n    ec2_num_instances: \"{{ aws_num_app_nodes }}\"\n    ec2_sg:\n      - name: \"ocp-ssh\"\n        description: \"OCP SSH\"\n        rules: \"{{ ocp_ssh_sg_rules|default([]) + default_ocp_ssh_sg_rules }}\"\n      - name: \"ocp-app-node\"\n        description: \"OCP App Node\"\n        rules: \"{{ ocp_app_node_sg_rules|default([]) + default_ocp_app_node_sg_rules }}\"\n\n- name: \"Ensure the necessary Security Groups Exists for the App Node(s)\"\n  vars:\n    sg_name: \"{{ item.name }}\"\n    sg_description: \"{{ item.description }}\"\n    sg_rules: \"{{ item.rules }}\"\n  include_tasks: create-security-group.yml\n  with_items:\n    - \"{{ ec2_sg }}\"\n\n- name: \"Create App Node(s) based on above facts\"\n  include_tasks: create-instance.yml\n  when:\n    - ec2_num_instances > 0\n\n- name: \"Store away the instance information\"\n  set_fact:\n    app_node_instaces: \"{{ ec2_instances }}\"\n\n- name: \"Clear facts\"\n  set_fact:\n    ec2_instances: ''\n\n\n################################################################################\n# CNS Nodes\n\n- name: \"Prepare facts to Create CNS Node(s)\"\n  set_fact:\n    ec2_name: \"{{ env_id }}-ocp-{{ cns_node_name | default('') }}\"\n    ec2_image: \"{{ cns_node_image_name | default(aws_image_name) }}\"\n    ec2_instance_type: \"{{ cns_node_flavor | default('m4.xlarge') }}\"\n    ec2_group: \"{{ aws_cns_node_sgroups | default(['ocp-ssh', 'ocp-app-node', 'ocp-cns-node']) }}\"\n    ec2_instance_tags:\n      group: \"{{ group_cns_nodes_tag | default('') }}\"\n      node_labels: \"{{ labels_cns_nodes_tag | default('') }}\"\n      env_id: \"{{ env_id }}\"\n    ec2_volumes:\n      - device_name: \"{{ cns_node_root_volume }}\"\n        volume_size: \"{{ cns_node_root_volume_size }}\"\n        volume_type: gp2\n      - device_name: \"{{ docker_storage_block_device }}\"\n        volume_size: \"{{ docker_storage_volume_size }}\"\n        volume_type: gp2\n      - device_name: \"{{ cns_node_glusterfs_volume }}\"\n        volume_size: \"{{ cns_node_glusterfs_volume_size }}\"\n        volume_type: gp2\n    ec2_num_instances: \"{{ aws_num_cns_nodes | default(0) }}\"\n    ec2_sg:\n      - name: \"ocp-ssh\"\n        description: \"OCP SSH\"\n        rules: \"{{ ocp_ssh_sg_rules|default([]) + default_ocp_ssh_sg_rules }}\"\n      - name: \"ocp-app-node\"\n        description: \"OCP App Node\"\n        rules: \"{{ ocp_app_node_sg_rules|default([]) + default_ocp_app_node_sg_rules }}\"\n      - name: \"ocp-cns-node\"\n        description: \"OCP CNS Node\"\n        rules: \"{{ ocp_cns_node_sg_rules|default([]) + default_ocp_cns_node_sg_rules }}\"\n\n- name: \"Ensure the necessary Security Groups Exists for the CNS Node(s)\"\n  vars:\n    sg_name: \"{{ item.name }}\"\n    sg_description: \"{{ item.description }}\"\n    sg_rules: \"{{ item.rules }}\"\n  include_tasks: create-security-group.yml\n  with_items:\n    - \"{{ ec2_sg }}\"\n  when:\n    - ec2_num_instances > 0\n\n- name: \"Create CNS Node(s) based on above facts\"\n  include_tasks: create-instance.yml\n  when:\n    - ec2_num_instances > 0\n\n- name: \"Store away the instance information\"\n  set_fact:\n    cns_node_instances: \"{{ ec2_instances }}\"\n\n- name: \"Clear facts\"\n  set_fact:\n    ec2_instances: ''\n\n\n################################################################################\n# Elastic IP assignments\n\n- name: Create elastic IP for Master Node (Non HA Mode)\n  ec2_eip:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    in_vpc: yes\n    state: present\n    release_on_disassociation: yes\n    device_id: \"{{ master_instances.results[0].instances[0].id }}\"\n  when: not ha_mode\n  register: master_eip\n\n- name: Create elastic IP for Infra Node (Non HA Mode)\n  ec2_eip:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    in_vpc: yes\n    state: present\n    release_on_disassociation: yes\n    device_id: \"{{ infra_node_instances.results[0].instances[0].id }}\"\n  when: not ha_mode\n  register: infra_eip\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "f7f4812bb3fee751f2f268d20cef35fdce208cda", "filename": "playbooks/aws/openshift-cluster/service.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Call same systemctl command for openshift on all instance(s)\n  hosts: localhost\n  connection: local\n  become: no\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  - cluster_hosts.yml\n  tasks:\n  - fail: msg=\"cluster_id is required to be injected in this playbook\"\n    when: cluster_id is not defined\n\n  - name: Evaluate g_service_masters\n    add_host:\n      name: \"{{ item }}\"\n      groups: g_service_masters\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    with_items: \"{{ master_hosts | default([]) }}\"\n\n  - name: Evaluate g_service_nodes\n    add_host:\n      name: \"{{ item }}\"\n      groups: g_service_nodes\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    with_items: \"{{ node_hosts | default([]) }}\"\n\n- include: ../../common/openshift-node/service.yml\n- include: ../../common/openshift-master/service.yml\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "3988aee0678971dc06c0acd93cf5b36e7ef1f3d1", "filename": "roles/update-host/tasks/wait-for-host.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Set the host to wait for\"\n  set_fact:\n    update_host: \"{{ (hostvars[inventory_hostname]['ansible_host'] is defined) |\n                      ternary(hostvars[inventory_hostname]['ansible_host'],\n                              hostvars[inventory_hostname]['ansible_ssh_host']) }}\"\n\n- name: \"Waiting for server to come back\"\n  local_action: wait_for\n  args:\n    host: \"{{ update_host }}\"\n    port: 22\n    delay: 15\n    timeout: 300\n    state: started\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9ed3310637804bd5b718f0cea7e9e1b6971efc48", "filename": "roles/config-versionlock/tasks/prereq-RedHat.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install required packages\"\n  package:\n    name: \"{{ item }}\"\n    state: installed\n  with_items:\n  - yum-plugin-versionlock\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "1fe53fa277d083ece89239e2c48e95e1a972d0dc", "filename": "roles/openshift-applier/tasks/check-file-location.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: \"Set default values\"\n  set_fact:\n    option_f: ''\n    file_path: \"{{ path }}\"\n\n- name: \"Check if the passed in path is a remote location (URL)\"\n  uri:\n    url: \"{{ path }}\"\n    method: HEAD\n  failed_when:\n  - url_result.status is defined\n  - url_result.status == -1\n  register: url_result\n\n- name: \"Check if passed in path is a local file (if not remote location)\"\n  stat:\n    path: \"{{ tmp_inv_dir }}{{ path }}\"\n  failed_when: false\n  register: file_result\n  when:\n  - (url_result.status is undefined) or\n    (url_result.status is defined and url_result.status != 200)\n\n- name: \"Set the '-f' option if the template is either a remote or local file\"\n  set_fact:\n    option_f: ' -f '\n    process_local: ' --local '\n  when:\n  - (url_result.status is defined and url_result.status == 200) or\n    (file_result.stat.exists)\n\n- name: \"Pre-pend the temporary dir if the file exists at the local location\"\n  set_fact:\n    file_path: \"{{ tmp_inv_dir }}{{ path }}\"\n  when:\n  - file_result is defined\n  - file_result.stat is defined\n  - file_result.stat.exists\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "63c631ae92a9ec700fbb9590e642c49cf8ecfb4a", "filename": "roles/osp/admin-instance/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Manage OpenStack instance\n  os_server:\n    cloud: \"{{ item.cloud | default(osp_default_cloud) | default(omit) }}\"\n    state: \"{{ item.state | default(osp_resource_state) | default('present') }}\"\n    name: \"{{ item.name }}\"\n    image: \"{{ item.image | default(omit) }}\"\n    key_name: \"{{ item.key_name | default(omit) }}\"\n    timeout: 200\n    flavor: \"{{ item.flavor | default(omit) }}\"\n    network: \"{{ item.network | default(omit) }}\"\n    security_groups: \"{{ item.security_groups | default(omit) }}\"\n    auto_ip: \"{{ item.auto_ip | default(True) }}\"\n    delete_fip: \"{{ item.delete_fip | default(omit) }}\"\n    volumes: \"{{ item.volumes | default(omit) }}\"\n    meta: \"{{ item.meta | default(omit) }}\"\n  register: os_servers\n  with_items:\n  - \"{{ osp_instances | default([]) }}\"\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a863bd055e01c1cf31c130ae48fc002bb67ba5e2", "filename": "roles/config-chrony/tasks/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - chrony\n  - firewalld\n  - python-firewall\n\n- name: 'Start firewalld'\n  service:\n    name: firewalld\n    state: started\n    enabled: yes\n\n- name: 'Open Firewall for NTP/Chrony use'\n  firewalld:\n    service: \"{{ item }}\"\n    permanent: yes\n    state: enabled\n    immediate: yes\n  with_items:\n  - ntp\n\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "e31d53a4ab2162808682d4261da5359fe97c7759", "filename": "reference-architecture/vmware-ansible/playbooks/roles/storage-class-configure/templates/cloud-provider-storage-class.yaml.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "kind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: \"{{ vcenter_datastore }}\"\nprovisioner: kubernetes.io/vsphere-volume\nparameters:\n    diskformat: zeroedthick\n    datastore: \"{{ vcenter_datastore }}\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "40bad9891805059294766bff4dc87e2459b0f9ed", "filename": "playbooks/roles/zookeeper/tasks/configure.yml", "repository": "rocknsm/rock", "decoded_content": "######################################################\n- name: Enable and start zookeeper\n  systemd:\n    name: zookeeper\n    state: \"{{ 'started' if enable_zookeeper else 'stopped' }}\"\n    enabled: \"{{ enable_zookeeper }}\"\n  when: with_zookeeper\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "f1dc77f6b740ecc826312efb22af31c18cc49d2f", "filename": "roles/ipareplica/vars/Fedora-25.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# Fedora-25 defaults file for ipareplica\n# vars/Fedora-25.yml\nipareplica_packages: [ \"ipa-server\", \"libselinux-python\" ]\nipareplica_packages_dns: [ \"ipa-server-dns\" ]\nipareplica_packages_adtrust: [ \"ipa-server-trust-ad\" ]"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "cc3ee72a072a14c95cd3909ac08f6fdf18a64810", "filename": "roles/vpn/defaults/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\nstrongswan_enabled_plugins:\n  - aes\n  - gcm\n  - hmac\n  - kernel-netlink\n  - nonce\n  - openssl\n  - pem\n  - pgp\n  - pkcs12\n  - pkcs7\n  - pkcs8\n  - pubkey\n  - random\n  - revocation\n  - sha2\n  - socket-default\n  - stroke\n  - x509\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8ae4ec2b98d2f70b521ccba39d33dff755e00f0b", "filename": "roles/virt-install/tests/infrahosts.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: 'Create VMs on infra hosts'\n  hosts: infra_hosts\n  roles:\n  - role: virt-install\n  tags:\n  - provision_infra_vms\n  \n- name: 'Check that the VMs are alive'\n  hosts: infra_vms\n  gather_facts: no\n  tasks:\n  - name: 'Wait for VMs to come alive'\n    local_action: wait_for\n    args: \n      host: \"{{ ansible_host }}\"\n      port: 22\n      delay: 30\n      timeout: 300\n  tags:\n  - vm_health_check\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "f3f9f05f62848d2e3f7d9fae538075b3b22b0c61", "filename": "reference-architecture/aws-ansible/playbooks/roles/cloudformation-infra/files/add-cns-storage.json", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "{\n  \"AWSTemplateFormatVersion\": \"2010-09-09\",\n  \"Parameters\": {\n    \"KeyName\": {\n      \"Type\": \"AWS::EC2::KeyPair::KeyName\"\n    },\n    \"Route53HostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"PublicHostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"AmiId\": {\n      \"Type\": \"AWS::EC2::Image::Id\"\n    },\n    \"InstanceType\": {\n      \"Type\": \"String\",\n      \"Default\": \"m4.2xlarge\"\n    },\n    \"NodeRootVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"30\"\n    },\n    \"NodeDockerVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"NodeDockerVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"GlusterVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"GlusterVolSize\": {\n      \"Type\": \"Number\",\n      \"Default\": \"500\"\n    },\n    \"NodeUserData\": {\n      \"Type\": \"String\"\n    },\n    \"NodeEmptyVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"NodeEmptyVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"NodeRootVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"PublicHostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"NodeInstanceProfile\": {\n      \"Type\": \"String\"\n    },\n    \"NodeType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gluster\"\n    },\n    \"GlusterNodeDns1\": {\n      \"Type\": \"String\"\n    },\n    \"GlusterNodeDns2\": {\n      \"Type\": \"String\"\n    },\n    \"GlusterNodeDns3\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet1\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet2\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet3\": {\n      \"Type\": \"String\"\n    },\n    \"NodeSg\": {\n      \"Type\": \"String\"\n    }\n  },\n  \"Resources\": {\n    \"Route53Records\": {\n      \"Type\": \"AWS::Route53::RecordSetGroup\",\n      \"DependsOn\": [\n        \"GlusterNode1\",\n        \"GlusterNode2\",\n        \"GlusterNode3\"\n      ],\n      \"Properties\": {\n        \"HostedZoneName\": { \"Ref\": \"Route53HostedZone\" },\n        \"RecordSets\": [\n          {\n            \"Name\":  {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns1\"},{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t            \"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"GlusterNode1\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\":  {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns2\"},{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t            \"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"GlusterNode2\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\":  {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns3\"},{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t            \"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"GlusterNode3\", \"PrivateIp\"] }]\n          }\n        ]\n      }\n    },\n    \"GlusterNode1\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InstanceType\"},\n\t  \"SecurityGroupIds\": [{\"Ref\": \"NodeSg\"}],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet1\"},\n          \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns1\"},{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"provision\",\n              \"Value\": \"node\"\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"storage\" \n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdd\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"GlusterVolSize\"},\n              \"VolumeType\": {\"Ref\": \"GlusterVolType\"}\n            }\n          }\n         ]\n     }\n   },\n    \"GlusterNode2\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InstanceType\"},\n\t  \"SecurityGroupIds\": [{\"Ref\": \"NodeSg\"}],\n\t  \"SecurityGroupIds\": [{\"Ref\": \"NodeSg\"}],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet2\"},\n          \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns2\"},{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"provision\",\n              \"Value\": \"node\"\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"storage\" \n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdd\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"GlusterVolSize\"},\n              \"VolumeType\": {\"Ref\": \"GlusterVolType\"}\n            }\n          }\n         ]\n     }\n   },\n    \"GlusterNode3\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InstanceType\"},\n\t  \"SecurityGroupIds\": [{\"Ref\": \"NodeSg\"}],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet3\"},\n          \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns3\"},{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"provision\",\n              \"Value\": \"node\"\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"storage\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdd\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"GlusterVolSize\"},\n              \"VolumeType\": {\"Ref\": \"GlusterVolType\"}\n            }\n          }\n         ]\n     }\n   }\n }\n}\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "800560606269737592d4aea772e3842d8d173072", "filename": "tasks/setup_ldap_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: setup_ldap\n    args:\n      name: \"{{ item.ldap_name }}\"\n      protocol: \"{{ item.ldap_protocol }}\"\n      hostname: \"{{ item.ldap_hostname }}\"\n      port: \"{{ item.ldap_port }}\"\n      auth: \"{{ item.ldap_auth | default('none') }}\"\n      username: \"{{ item.ldap_auth_username | default('') }}\"\n      password: \"{{ item.ldap_auth_password | default('') }}\"\n      search_base: \"{{ item.ldap_search_base }}\"\n      user_base_dn: \"{{ item.ldap_user_base_dn | default('ou=users') }}\"\n      user_ldap_filter: \"{{ item.ldap_user_filter | default('') }}\"\n      user_object_class: \"{{ item.ldap_user_object_class }}\"\n      user_id_attribute: \"{{ item.ldap_user_id_attribute }}\"\n      user_real_name_attribute: \"{{ item.ldap_user_real_name_attribute }}\"\n      user_email_attribute: \"{{ item.ldap_user_email_attribute }}\"\n      map_groups_as_roles: \"{{ item.ldap_map_groups_as_roles | default(false) }}\"\n      group_base_dn: \"{{ item.ldap_group_base_dn | default('ou=groups') }}\"\n      group_object_class: \"{{ item.ldap_group_object_class | default('groupOfNames') }}\"\n      group_id_attribute: \"{{ item.ldap_group_id_attribute | default('cn') }}\"\n      group_member_attribute: \"{{ item.ldap_group_member_attribute | default('member') }}\"\n      group_member_format: \"{{ item.ldap_group_member_format | default('uid=${username},ou=users,dc=yourcompany') }}\"\n      user_subtree: \"{{ item.ldap_user_subtree | default(false) }}\"\n      group_subtree: \"{{ item.ldap_group_subtree | default(false) }}\"\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "76f5a05ae02afd2859682adbd8f644ac94300a0f", "filename": "roles/vpn/tasks/client_configs.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Register p12 PayloadContent\n  local_action: >\n    shell cat private/{{ item }}.p12 | base64\n  register:  PayloadContent\n  become: no\n  args:\n    chdir: \"configs/{{ IP_subject_alt_name }}/pki/\"\n  with_items: \"{{ users }}\"\n\n- name: Set facts for mobileconfigs\n  set_fact:\n    proxy_enabled: false\n    PayloadContentCA: \"{{ lookup('file' , 'configs/{{ IP_subject_alt_name }}/pki/cacert.pem')|b64encode }}\"\n\n- name: Build the mobileconfigs\n  local_action:\n    module: template\n    src: mobileconfig.j2\n    dest: configs/{{ IP_subject_alt_name }}/{{ item.0 }}.mobileconfig\n    mode: 0600\n  become: no\n  with_together:\n    - \"{{ users }}\"\n    - \"{{ PayloadContent.results }}\"\n  no_log: True\n\n- name: Build the strongswan app android config\n  local_action:\n    module: template\n    src: sswan.j2\n    dest: configs/{{ IP_subject_alt_name }}/{{ item.0 }}.sswan\n    mode: 0600\n  become: no\n  with_together:\n    - \"{{ users }}\"\n    - \"{{ PayloadContent.results }}\"\n  no_log: True\n\n- name: Build the client ipsec config file\n  local_action:\n    module: template\n    src: client_ipsec.conf.j2\n    dest: configs/{{ IP_subject_alt_name }}/ipsec_{{ item }}.conf\n    mode: 0600\n  become: no\n  with_items:\n    - \"{{ users }}\"\n\n- name: Build the client ipsec secret file\n  local_action:\n    module: template\n    src: client_ipsec.secrets.j2\n    dest: configs/{{ IP_subject_alt_name }}/ipsec_{{ item }}.secrets\n    mode: 0600\n  become: no\n  with_items:\n    - \"{{ users }}\"\n\n- name: Build the windows client powershell script\n  local_action:\n    module: template\n    src: client_windows.ps1.j2\n    dest: configs/{{ IP_subject_alt_name }}/windows_{{ item }}.ps1\n    mode: 0600\n  become: no\n  when: Win10_Enabled is defined and Win10_Enabled == \"Y\"\n  with_items: \"{{ users }}\"\n\n- name: Restrict permissions for the local private directories\n  local_action:\n    module: file\n    path: \"{{ item }}\"\n    state: directory\n    mode: 0700\n  become: no\n  with_items:\n    - configs/{{ IP_subject_alt_name }}\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "e11874c2802ee4a11982caa638fe35b5cc691940", "filename": "playbooks/provisioning/openstack/custom_flavor_check.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Try to get flavor facts\n  os_flavor_facts:\n    name: \"{{ flavor }}\"\n  register: flavor_result\n- name: Check that custom flavor is available\n  assert:\n    that: \"flavor_result.ansible_facts.openstack_flavors\"\n    msg: \"Flavor {{ flavor }} is not available.\"\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "68e6c2e58dd77f36c37d15c6c14898ea37e6eeec", "filename": "tasks/create_repo_bower_hosted_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_bower_hosted\n    args: \"{{ _nexus_repos_bower_defaults|combine(item) }}\""}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "bde2ad491eba6f464bfc664fd9edd1ba23adc36f", "filename": "reference-architecture/gcp/ansible/playbooks/roles/deployment-delete/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: delete deployment {{ deployment_name_with_prefix }}\n  command: gcloud -q --project {{ gcloud_project }} deployment-manager deployments delete {{ deployment_name_with_prefix }}\n  ignore_errors: true\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "3806b4c7ec0eb34f6909062906b41182717e89aa", "filename": "roles/docker/vars/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# vars file for docker\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "76be46aff336f8da3f10bacfca04480b042b6436", "filename": "roles/openshift-applier/tasks/copy-inventory-entry.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: \"Copy 'file' to remote host\"\n  vars:\n    file: \"{{ entry.file }}\"\n  import_tasks: copy-inventory-file.yml\n  when:\n  - entry['file'] is defined\n  - entry['file']|trim != ''\n\n- name: \"Copy 'template' to remote host\"\n  vars:\n    file: \"{{ entry.template }}\"\n  import_tasks: copy-inventory-file.yml\n  when:\n  - entry['template'] is defined\n  - entry['template']|trim != ''\n\n- name: \"Copy 'params' to remote host\"\n  vars:\n    file: \"{{ entry.params }}\"\n  import_tasks: copy-inventory-file.yml\n  when:\n  - entry['params'] is defined\n  - entry['params']|trim != ''\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5a4222f444ad43da513079ee72cff57f3c996e69", "filename": "roles/dns/manage-dns-zones/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: restart named\n  service:\n    name: named\n    state: restarted\n\n- name: reload named\n  service:\n    name: named\n    state: reloaded\n\n- name: cleanup temp\n  file:\n    path: \"{{ dns_zone_temp_config_dir }}\"\n    state: absent\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "324772635e45c0e88c617ab3324127e6f40ffab7", "filename": "playbooks/provision-ansible-tower/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_playbook: ../prep.yml\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../osp/manage-user-network.yml\n  when:\n  - hosting_infrastructure == 'openstack'\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../osp/provision-osp-instance.yml\n  when:\n  - hosting_infrastructure == 'openstack'\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../rhsm.yml\n  tags:\n  - 'never'\n  - 'install'\n\n- hosts: ansible-tower\n  roles:\n  - role: update-host\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../ansible/tower/configure-ansible-tower.yml\n  tags:\n  - 'always'\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "749910ef34690a406dd404e0fe29312ea6e31c8b", "filename": "roles/0-init/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# Initialize\n\n- name: ...IS BEGINNING ============================================\n  stat:\n    path: /etc/iiab/iiab.env\n  register: NewInstall\n\n- name: Setting first run flag\n  set_fact:\n    first_run: True\n  when: not NewInstall.stat.exists\n\n# We need to inialize the ini file and only write the location and version sections once and only\n# once to preserve the install date and git hash.\n- name: Write iiab_ini.yml for the first time\n  include_tasks: first_run.yml\n  when: first_run\n\n#- name: Loading computed_vars\n#  include_tasks: roles/0-init/tasks/computed_vars.yml\n- name: Re-read local_facts.facts from /etc/ansible/facts.d\n  setup:\n    filter: ansible_local\n\n- name: Set top level variables from local facts for convenience\n  set_fact:\n    xo_model: \"{{ ansible_local.local_facts.xo_model }}\"\n    phplib_dir: \"{{ ansible_local.local_facts.phplib_dir }}\"\n    iiab_stage: \"{{ ansible_local.local_facts.stage }}\"\n\n- name: Discover if this is running Raspbian -- if so assume it is a RPi\n  set_fact:\n    rpi_model: \"rpi\"\n    is_rpi: True\n#      no_net_restart: True\n#      nobridge: True\n  when: ansible_local.local_facts.os == \"raspbian\"\n\n- name: Set exFAT enabled for XO laptops\n  set_fact:\n    exFAT_enabled: True\n  when: xo_model != \"none\"\n\n# Discover  do we have a gateway? -- if ansible detects gateway, becomes WAN candidate\n- name: Finding gateway\n  set_fact:\n    discovered_wan_iface: \"{{ ansible_default_ipv4.alias }}\"\n    iiab_wan_iface: \"{{ discovered_wan_iface }}\"\n  when: ansible_default_ipv4.gateway is defined\n\n- name: Verify gateway present\n  shell: ping -c4 \"{{ ansible_default_ipv4.gateway }}\" | grep icmp_seq=4 | wc -l\n  when: discovered_wan_iface != \"none\"\n  register: gw_active_test\n\n- name: Recording gateway response\n  set_fact:\n    gw_active: True\n  when: discovered_wan_iface != \"none\" and gw_active_test.stdout == \"1\"\n\n- name: Test for internet access\n  get_url:\n    url: \"{{ iiab_download_url }}/heart-beat.txt\"\n    dest: /tmp/heart-beat.txt\n    # timeout: \"{{ download_timeout }}\"\n    # @jvonau recommends: 100sec is too much (keep 10sec default)\n  ignore_errors: True\n# async: 10\n# poll: 2\n  register: internet_access_test\n\n- name: Set internet_available true if wget succeeded\n  set_fact:\n    internet_available: True\n  when: not internet_access_test|failed and not disregard_network\n\n- name: Cleanup internet test file\n  file:\n    path: /tmp/heart-beat.txt\n    state: absent\n\n# Put all computed vars here so derive properly from any prior var file\n- name: If the TZ is not set in env, set it to UTC\n  include_tasks: tz.yml\n\n- name: Set port 80 for Admin Console\n  set_fact:\n    gui_port: 80\n  when: not adm_cons_force_ssl\n\n- name: Set port 443 for Admin Console\n  set_fact:\n    gui_port: 443\n  when: adm_cons_force_ssl\n\n- name: Require MySQL to be on (mandatory in Stage 3!)\n  set_fact:\n    mysql_install: True\n    mysql_enabled: True\n\n# we decided to enable mysql unconditionally\n#  when: elgg_enabled or rachel_enabled or owncloud_enabled or phpmyadmin_enabled or wordpress_enabled or iiab_menu_install\n\n# Commenting out MongoDB on a trial basis, for a more basic/lightweight Sugarizer, per https://github.com/iiab/iiab/pull/427\n- name: Turn on mongodb if sugarizer enabled\n  set_fact:\n     mongodb_install: True\n     mongodb_enabled: True\n  when: sugarizer_enabled\n\n# There might be other db's\n- name: Turn on PostgreSQL if Moodle or Pathagar enabled\n  set_fact:\n    postgresql_install: True\n    postgresql_enabled: True\n  when: moodle_enabled or pathagar_enabled\n\n- name: Turn on Docker if SchoolTool is to be installed\n  set_fact:\n    docker_install: True\n    docker_enabled: True\n  when: schooltool_enabled or schooltool_install\n\n- name: Set python_path (redhat)\n  set_fact:\n    python_path: /lib/python2.7/site-packages/\n  when: is_redhat\n\n- name: Set python_path (debuntu)\n  set_fact:\n    python_path: /usr/local/lib/python2.7/dist-packages/\n  when: is_debuntu\n\n# For various reasons the mysql service can not be enabled on fedora 20,\n# but 'mariadb', which is its real name can.\n# On Fedora 18 we need to use 'mysqld'\n\n- name: Set mysql_service to mariadb by default\n  set_fact:\n    mysql_service: mariadb\n\n- name: Set mysql_service to mysqld etc (Fedora 18)\n  set_fact:\n    mysql_service: mysqld\n    no_NM_reload: True\n    is_F18: True\n  when: (ansible_distribution_release == \"based on Fedora 18\" or ansible_distribution_version == \"18\") and ansible_distribution == \"Fedora\"\n\n- name: Set mysql_service to mysql (debuntu)\n  set_fact:\n    mysql_service: mysql\n  when: is_debuntu\n\n- name: Set FQDN\n  set_fact:\n    iiab_fqdn: \"{{ iiab_hostname }}.{{ iiab_domain }}\"\n    FQDN_changed: False\n\n- name: FQDN changed\n  set_fact:\n    FQDN_changed: True\n  when: iiab_fqdn != ansible_fqdn\n\n- name: Setting hostname\n  include_tasks: hostname.yml\n  when: FQDN_changed\n\n- name: Add 'runtime' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ iiab_config_file }}\"\n    section: runtime\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: iiab_stage\n      value: \"{{ iiab_stage }}\"\n    - option: iiab_base_ver\n      value: \"{{ iiab_base_ver }}\"\n    - option: iiab_revision\n      value: \"{{ iiab_revision }}\"\n    - option: runtime_php\n      value: \"{{ phplib_dir }}\"\n    - option: runtime_branch\n      value: \"{{ ansible_local.local_facts.iiab_branch }}\"\n    - option: runtime_commit\n      value: \"{{ ansible_local.local_facts.iiab_commit }}\"\n    - option: runtime_date\n      value: \"{{ ansible_date_time.iso8601 }}\"\n    - option: ansible_version\n      value: \"{{ ansible_local.local_facts.ansible_version }}\"\n    - option: kernel\n      value: \"{{ ansible_kernel }}\"\n    - option: memory_mb\n      value: \"{{ ansible_memtotal_mb }}\"\n    - option: swap_mb\n      value: \"{{ ansible_swaptotal_mb }}\"\n    - option: product_id\n      value: \"{{ ansible_product_uuid }}\"\n    - option: gw_active\n      value:  \"{{ gw_active }}\"\n    - option: internet_available\n      value:  \"{{ internet_available }}\"\n    - option: is_rpi\n      value:  \"{{ is_rpi }}\"\n    - option: first_run\n      value:  \"{{ first_run }}\"\n    - option: local_tz\n      value:  \"{{ local_tz }}\"\n    - option: FQDN_changed\n      value:  \"{{ FQDN_changed }}\"\n\n- name: Now changing FQDN\n  include_tasks: hostname.yml\n  when: FQDN_changed\n\n- name: STAGE 0 HAS COMPLETED ======================================\n  ini_file:\n    dest: \"{{ iiab_config_file }}\"\n    section: runtime\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: is_VM\n      value:  \"yes\"\n  when: is_VM is defined\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "231b3a5673a98ee8eeced4122d9d5afe64a338d7", "filename": "playbooks/openshift/install.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- import_playbook: \"{{ openshift_ansible_path | default('../../roles/openshift-ansible') }}/playbooks/byo/config.yml\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "eb39d6b6afb4bacfb479e596b7bc00c3c93670c6", "filename": "reference-architecture/gcp/ansible/playbooks/teardown.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: check if ssh proxy is configured\n  hosts: localhost\n  tasks:\n  - name: check if ssh proxy is configured\n    command: grep -q 'OPENSHIFT ON GCP BLOCK' {{ ssh_config_file }}\n    register: ssh_proxy_check\n    ignore_errors: true\n\n- include: unregister.yaml\n  when: hostvars['localhost'].ssh_proxy_check | succeeded\n\n- name: teardown the created infrastructure\n  hosts: localhost\n  roles:\n  - registry-bucket-delete\n  - dns-records-delete\n  - role: deployment-delete\n    deployment_name: core\n  - ssl-certificate-delete\n  - role: deployment-delete\n    deployment_name: tmp-instance\n  - temp-instance-disk-delete\n  - ssh-config-tmp-instance-delete\n  - role: deployment-delete\n    deployment_name: network\n  - ssh-proxy-delete\n  - role: deployment-delete\n    deployment_name: gold-image\n    deployment_name_with_prefix: '{{ prefix }}-{{ deployment_name }}{{ \"-origin\" if openshift_deployment_type == \"origin\" else \"\" }}'\n    when: delete_gold_image | bool\n  - role: rhel-image-delete\n    when: delete_image | bool\n  - role: empty-image-delete\n    when: delete_image | bool\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "c2013bf54251f334ef263a36d8cd73e2b373fb84", "filename": "roles/deploy-host/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: \"Obtain currently enabled repos\"\n  shell: 'subscription-manager repos --list-enabled | sed -ne \"s/^Repo ID:[^a-zA-Z0-9]*\\(.*\\)/\\1/p\"'\n  register: enabled_repos\n\n- name: \"Enable specified repositories not already enabled\"\n  command: \"subscription-manager repos --enable={{ item }}\"\n  with_items:\n    - \"{{ openshift_deploy_repos | difference(enabled_repos.stdout_lines) }}\"\n  register: subscribe_repos\n  until: subscribe_repos | succeeded\n\n- name: \"Be sure all pre-req provider packages are installed\"\n  yum: name={{item}} state=installed\n  with_items:\n    - \"{{ openshift_deploy_packages }}\"\n\n- block:\n    - name: \"Establish gcp pre-req packages\"\n      set_fact:\n        openshift_deploy_packages_by_provider: ['qemu-img', 'curl', 'tar', 'which', 'openssl', 'python2-passlib', 'java-1.8.0-openjdk-headless', 'httpd-tools', 'python2-passlib']\n\n    - name: \"Set facts for gcp EPEL packages\"\n      set_fact:\n        openshift_deploy_epel_packages_by_provider: ['python2-libcloud', 'python2-jmespath']\n\n    - name: \"Be sure all pre-req gcp packages are installed\"\n      yum: name={{item}} state=installed\n      with_items:\n        - \"{{ openshift_deploy_packages_by_provider }}\"\n  when: \"'gcp' in provider\"\n\n- block:\n    - name: \"Search for SSH key\"\n      stat:\n        path: \"~/.ssh/id_rsa\"\n      register: ssh_key\n\n    - name: \"Create SSH key if its missing\"\n      command: \"ssh-keygen -N '' -f ~/.ssh/id_rsa\"\n      when: \"not ssh_key.stat.exists\"\n\n    - name: \"Obtain currently enabled repos for rhv\"\n      shell: 'subscription-manager repos --list-enabled | sed -ne \"s/^Repo ID:[^a-zA-Z0-9]*\\(.*\\)/\\1/p\"'\n      register: enabled_repos\n\n    - name: \"Enable rhv mgmt repo for rhv\"\n      command: \"subscription-manager repos --enable=rhel-7-server-rhv-4-mgmt-agent-rpms\"\n      when: \"'rhv' not in enabled_repos.stdout\"\n\n    - name: \"Establish rhv pre-req packages\"\n      set_fact:\n        openshift_deploy_packages_by_provider: ['ovirt-engine-sdk-python']\n\n    - name: \"Be sure all pre-req rhv packages are installed\"\n      yum: name={{item}} state=installed\n      with_items:\n        - \"{{ openshift_deploy_packages_by_provider }}\"\n\n    - name: \"Confirm location of openshift-ansible-contrib for rhv\"\n      stat:\n        path: \"~/git/openshift-ansible-contrib\"\n      register: contrib\n\n    - name: \"Confirm location of ovirt for rhv doesn't already exist\"\n      stat:\n        path: \"~/git/ovirt-ansible\"\n      register: ovirt\n\n    - name: \"Git clone required rhv repo\"\n      git:\n        repo: \"https://github.com/ovirt/ovirt-ansible\"\n        dest: \"~/git/ovirt-ansible\"\n      when: \"contrib.stat.exists and not ovirt.stat.exists\"\n  when: \"'rhv' in provider\"\n\n- block:\n    - name: \"Search for SSH key\"\n      stat:\n        path: \"~/.ssh/id_rsa\"\n      register: ssh_key\n\n    - name: \"Create SSH key if its missing\"\n      command: \"ssh-keygen -N '' -f ~/.ssh/id_rsa\"\n      when: \"not ssh_key.stat.exists\"\n\n    - name: \"Obtain currently enabled repos for vsphere\"\n      shell: 'subscription-manager repos --list-enabled | sed -ne \"s/^Repo ID:[^a-zA-Z0-9]*\\(.*\\)/\\1/p\"'\n      register: enabled_repos\n\n    - name: \"Enable SCL Repo for vsphere\"\n      command: \"subscription-manager repos --enable=rhel-server-rhscl-7-rpms\"\n      when: \"'rhscl' not in enabled_repos.stdout\"\n\n    - name: \"Set facts for vsphere non EPEL packages\"\n      set_fact:\n        openshift_deploy_packages_by_provider: ['python-click', 'python-ldap', 'python27']\n\n    - name: \"Be sure all vsphere pre-req provider packages are installed\"\n      yum: name={{item}} state=installed\n      with_items:\n        - \"{{ openshift_deploy_packages_by_provider }}\"\n\n    - name: \"Set facts for vsphere EPEL packages\"\n      set_fact:\n        openshift_deploy_epel_packages_by_provider: ['python-iptools', 'python2-pyvmomi']\n  when: \"'vsphere' in provider\"\n\n- block:\n    - name: \"Set facts for aws EPEL packages\"\n      set_fact:\n        openshift_deploy_epel_packages_by_provider: ['python2-boto', 'python-netaddr', 'python2-boto3']\n  when: \"'aws' in provider\"\n\n- block:\n    - name: Check if EPEL repo is already configured.\n      stat: path={{ epel_repofile_path }}\n      register: epel_repofile_result\n\n    - name: Install EPEL repo.\n      yum:\n        name: \"{{ epel_repo_url }}\"\n        state: present\n      register: result\n      until: '\"failed\" not in result'\n      retries: 5\n      delay: 10\n      when: not epel_repofile_result.stat.exists\n\n    - name: Import EPEL GPG key.\n      rpm_key:\n        key: \"{{ epel_repo_gpg_key_url }}\"\n        state: present\n      when: not epel_repofile_result.stat.exists\n      ignore_errors: \"{{ ansible_check_mode }}\"\n\n    - name: be sure all pre-req provider EPEL packages are installed\n      yum: name={{item}} state=installed\n      with_items:\n        - \"{{ openshift_deploy_epel_packages_by_provider }}\"\n  when: \"'vsphere' in provider or 'aws' in provider or 'gcp' in provider\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "88504a5f66ea237187ac8b36a0879244a57732f3", "filename": "playbooks/libvirt/openshift-cluster/templates/domain.xml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "<domain type='kvm' id='8'>\n  <name>{{ item }}</name>\n  <memory unit='MiB'>{{ libvirt_instance_memory_mib }}</memory>\n  <metadata xmlns:ansible=\"https://github.com/ansible/ansible\">\n    <ansible:tags>\n      <ansible:tag>environment-{{ cluster_env }}</ansible:tag>\n      <ansible:tag>clusterid-{{ cluster }}</ansible:tag>\n      <ansible:tag>host-type-{{ type }}</ansible:tag>\n      <ansible:tag>sub-host-type-{{ g_sub_host_type }}</ansible:tag>\n    </ansible:tags>\n  </metadata>\n  <vcpu placement='static'>{{ libvirt_instance_vcpu }}</vcpu>\n  <os>\n    <type arch='x86_64' machine='pc'>hvm</type>\n    <boot dev='hd'/>\n  </os>\n  <features>\n    <acpi/>\n    <apic/>\n    <pae/>\n  </features>\n  <cpu mode='host-model'>\n    <model fallback='allow'/>\n  </cpu>\n  <clock offset='utc'>\n    <timer name='rtc' tickpolicy='catchup'/>\n    <timer name='pit' tickpolicy='delay'/>\n    <timer name='hpet' present='no'/>\n  </clock>\n  <on_poweroff>destroy</on_poweroff>\n  <on_reboot>restart</on_reboot>\n  <on_crash>restart</on_crash>\n  <devices>\n    <emulator>/usr/bin/qemu-system-x86_64</emulator>\n    <disk type='file' device='disk'>\n      <driver name='qemu' type='qcow2' discard='unmap'/>\n      <source file='{{ libvirt_storage_pool_path }}/{{ item }}.qcow2'/>\n      <target dev='sda' bus='scsi'/>\n    </disk>\n    <disk type='file' device='disk'>\n      <driver name='qemu' type='qcow2' discard='unmap'/>\n      <source file='{{ libvirt_storage_pool_path }}/{{ item }}-docker.qcow2'/>\n      <target dev='sdb' bus='scsi'/>\n    </disk>\n    <disk type='file' device='cdrom'>\n      <driver name='qemu' type='raw'/>\n      <source file='{{ libvirt_storage_pool_path }}/{{ item }}_cloud-init.iso'/>\n      <target dev='sdc' bus='scsi'/>\n      <readonly/>\n    </disk>\n    <controller type='scsi' model='virtio-scsi' />\n    <interface type='network'>\n      <source network='{{ libvirt_network }}'/>\n      <model type='virtio'/>\n    </interface>\n    <serial type='pty'>\n      <target port='0'/>\n    </serial>\n    <console type='pty'>\n      <target type='serial' port='0'/>\n    </console>\n    <memballoon model='virtio'>\n    </memballoon>\n  </devices>\n</domain>\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "0ff50a095e5113a7ebf1ead3d390c031f50ed796", "filename": "roles/openstack-stack/tasks/generate-templates.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: create HOT stack template prefix\n  register: stack_template_pre\n  tempfile:\n    state: directory\n    prefix: openshift-ansible\n\n- name: set template paths\n  set_fact:\n    stack_template_path: \"{{ stack_template_pre.path }}/stack.yaml\"\n    user_data_template_path: \"{{ stack_template_pre.path }}/user-data\"\n\n- name: generate HOT stack template from jinja2 template\n  template:\n    src: heat_stack.yaml.j2\n    dest: \"{{ stack_template_path }}\"\n\n- name: generate HOT server template from jinja2 template\n  template:\n    src: heat_stack_server.yaml.j2\n    dest: \"{{ stack_template_pre.path }}/server.yaml\"\n\n- name: generate user_data from jinja2 template\n  template:\n    src: user_data.j2\n    dest: \"{{ user_data_template_path }}\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "71d7e5d76d18182ce291c61a20e3d3e5d61bf5ff", "filename": "archive/roles/cicd/handlers/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: reload systemd\n  command: systemctl daemon-reload\n  \n- name: restart httpd\n  service: \n    name: httpd\n    state: restarted\n\n- name: restart nexus\n  service: \n    name: nexus\n    state: restarted\n    \n- name: restart jenkins\n  service: \n    name: jenkins\n    state: restarted\n  \n- name: restart docker\n  service:\n    name: docker\n    state: restarted"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "7506f872bd2f3dc62451c80622cbd35e0bb5278b", "filename": "roles/manage-aws-infra/tasks/create-instance.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "# Tasks to deploy AWS ec2 instance(s) based in the requested parameters\n---\n\n- name: \"Create ec2 instance(s)\"\n  ec2:\n    image: \"{{ ec2_image }}\"\n    instance_type: \"{{ ec2_instance_type }}\"\n    group: \"{{ ec2_group }}\"\n    termination_protection: \"{{ aws_termination_protection }}\"\n    key_name: \"{{ aws_key_name }}\"\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    vpc_subnet_id: \"{{ aws_vpc_subnet_id }}\"\n    exact_count: 1\n    wait: yes\n    count_tag:\n      Name: \"{{ ec2_name }}-{{ item }}\"\n    instance_tags:\n      \"{{ ec2_instance_tags|combine({ 'Name': ec2_name + '-' + item,\n                                      'kubernetes.io/cluster/cluster-' + env_id: env_id } ) }}\"\n    volumes: \"{{ ec2_volumes }}\"\n  with_sequence:\n    count=\"{{ ec2_num_instances }}\"\n  register: ec2_instances\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "551fec4b77363a99e69557f945a599ff5491d45a", "filename": "roles/storage-demo/templates/storage-demo.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "apiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: storage-demo-cluster-admin\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  name: cluster-admin\n  kind: ClusterRole\nsubjects:\n- kind: ServiceAccount\n  name: storage-demo\n  namespace: {{ namespace }}\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: storage-demo-default-admin\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  name: cluster-admin\n  kind: ClusterRole\nsubjects:\n- kind: ServiceAccount\n  name: storage-demo\n  namespace: default\n---\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: storage-demo\n  namespace: \"{{ namespace }}\"\n  labels:\n    app: storage-demo\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: storage-demo\n  template:\n    metadata:\n      labels:\n        app: storage-demo\n    spec:\n      serviceAccountName: storage-demo\n      nodeSelector:\n          \"kubernetes.io/hostname\": \"{{ storage_demo_node_hostname }}\"\n      hostNetwork: true\n      containers:\n      - name: kraken\n        args: [\"demo\"]\n        env:\n        - name: MON_IP\n          value: \"{{ storage_demo_node_hostname }}\"\n        - name: CEPH_PUBLIC_NETWORK\n          value: \"0.0.0.0/0\"\n        - name: RGW_CIVETWEB_PORT\n          value: \"81\" # Don't clash with openshift\n        ports:\n          - containerPort: 6789\n            hostPort: 6789\n        image: ceph/demo\n        volumeMounts:\n        - name: cephvarlib\n          mountPath: /var/lib/ceph\n        - name: cephetc\n          mountPath: /etc/ceph\n      - name: mariadb\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: \"password\"\n        - name: MYSQL_DATABASE\n          value: \"cinder\"\n        - name: MYSQL_USER\n          value: \"cinder\"\n        - name: MYSQL_PASSWORD\n          value: \"password\"\n        image: mariadb\n        volumeMounts:\n        - name: mariadbdata\n          mountPath: \"/var/lib/mysql\"\n      - name: rabbitmq\n        image: rabbitmq\n      - name: cinder-api\n        image: kubevirtci/cinder:ocata\n        command: [\"/scripts/cinder-api.sh\"]\n        env:\n        - name: INIT_DB\n          value: \"true\"\n      - name: cinder-scheduler\n        image: kubevirtci/cinder:ocata\n        command: [\"cinder-scheduler\"]\n      - name: cinder-volume\n        image: kubevirtci/cinder:ocata\n        command: [\"/scripts/ceph-service.sh\"]\n        env:\n        - name: MON_IP\n          value: \"{{ storage_demo_node_hostname }}\"\n        volumeMounts:\n        - name: cephetc\n          mountPath: \"/etc/ceph\"\n      - name: cinder-provisioner\n        image: {{ cinder_provisioner_repo }}/standalone-cinder-provisioner:{{ cinder_provisioner_release }}\n        env:\n        - name: OS_CINDER_ENDPOINT\n          value: http://127.0.0.1:8776/v3\n      volumes:\n      - name: cephvarlib\n        emptyDir: {}\n      - name: cephetc\n        emptyDir: {}\n      - name: mariadbdata\n        emptyDir: {}\n---\nkind: StorageClass\napiVersion: storage.k8s.io/v1beta1\nmetadata:\n  name: kubevirt\nprovisioner: openstack.org/standalone-cinder\nparameters:\n  smartclone: \"true\"\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kubevirt-cephx-secret\ntype: \"kubernetes.io/rbd\"\ndata:\n  key: QVFDMkI0SmE2MDgzT3hBQTdOR0dpb0xpR1lqOHlJTFpkYUI1T0E9PQo=\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "77672b7833c8c1de66f60da66dbbc2a576d20b3c", "filename": "ops/playbooks/config_scheduler.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- name: Config UCP scheduler\n  hosts: local\n  connection: local\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  tasks:\n\n#\n# find out an UCP VM that works\n#\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n\n\n    - name: Retrieve a token for the UCP API\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/auth/login\"\n        headers:\n          Content-Type: application/json\n        method: POST\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        body: '{\"username\":\"{{ ucp_username }}\",\"password\":\"{{ ucp_password }}\"}'\n        use_proxy: no\n      register: login\n      until: login.status == 200\n      retries: 20\n      delay: 5\n\n    - name: Remember the token\n      set_fact:\n        auth_token:  \"{{ login.json.auth_token }}\"\n\n    - name: Retrieve Docker-Datacenter Org ID\n      uri:\n        url: \"https://{{ ucp_instance}}.{{ domain_name }}/accounts/?filter=orgs&limit=100&contains=docker-datacenter\"\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n        use_proxy: no\n      when: inventory_hostname in groups.local\n      register: resp\n\n    - name: Memorize Org ID\n      set_fact:\n        orgid:  \"{{resp.json.accounts[0].id}}\"\n\n    - name: Do not let users schedule on all nodes\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/collectionGrants/{{orgid}}/swarm/scheduler\" \n        headers:\n          accept: application/json\n          Authorization: Bearer {{auth_token}}\n        method: DELETE\n        validate_certs: no\n        use_proxy: no\n        status_code: 204\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "dd474444a5e86cb448db1083c0e4cbf3443d29e8", "filename": "roles/config-packages/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Test installing additional packages\"\n  hosts: all\n  roles:\n    - role: config-packages \n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "11a31411ed3a777e9a3e932bf39275993054a84c", "filename": "playbooks/provisioning/openstack/prerequisites.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  tasks:\n\n  # Sanity check of inventory variables\n  - include: net_vars_check.yaml\n\n  # Check ansible\n  - name: Check Ansible version\n    assert:\n      that: >\n        (ansible_version.major == 2 and ansible_version.minor >= 3) or\n        (ansible_version.major > 2)\n      msg: \"Ansible version must be at least 2.3\"\n\n  # Check shade\n  - name: Try to import python module shade\n    command: python -c \"import shade\"\n    ignore_errors: yes\n    register: shade_result\n  - name: Check if shade is installed\n    assert:\n      that: 'shade_result.rc == 0'\n      msg: \"Python module shade is not installed\"\n\n  # Check jmespath\n  - name: Try to import python module shade\n    command: python -c \"import jmespath\"\n    ignore_errors: yes\n    register: jmespath_result\n  - name: Check if jmespath is installed\n    assert:\n      that: 'jmespath_result.rc == 0'\n      msg: \"Python module jmespath is not installed\"\n\n  # Check python-dns\n  - name: Try to import python DNS module\n    command: python -c \"import dns\"\n    ignore_errors: yes\n    register: pythondns_result\n  - name: Check if python-dns is installed\n    assert:\n      that: 'pythondns_result.rc == 0'\n      msg: \"Python module python-dns is not installed\"\n\n  # Check jinja2\n  - name: Try to import jinja2 module\n    command: python -c \"import jinja2\"\n    ignore_errors: yes\n    register: jinja_result\n  - name: Check if jinja2 is installed\n    assert:\n      that: 'jinja_result.rc == 0'\n      msg: \"Python module jinja2 is not installed\"\n\n  # Check Glance image\n  - name: Try to get image facts\n    os_image_facts:\n      image: \"{{ openstack_default_image_name }}\"\n    register: image_result\n  - name: Check that image is available\n    assert:\n      that: \"image_result.ansible_facts.openstack_image\"\n      msg: \"Image {{ openstack_default_image_name }} is not available\"\n\n  # Check network name\n  - name: Try to get network facts\n    os_networks_facts:\n      name: \"{{ openstack_external_network_name }}\"\n    register: network_result\n    when: not openstack_provider_network_name|default(None)\n  - name: Check that network is available\n    assert:\n      that: \"network_result.ansible_facts.openstack_networks\"\n      msg: \"Network {{ openstack_external_network_name }} is not available\"\n    when: not openstack_provider_network_name|default(None)\n\n  # Check keypair\n  # TODO kpilatov: there is no Ansible module for getting OS keypairs\n  #                (os_keypair is not suitable for this)\n  #                this method does not force python-openstackclient dependency\n  - name: Try to show keypair\n    command: >\n             python -c 'import shade; cloud = shade.openstack_cloud();\n             exit(cloud.get_keypair(\"{{ openstack_ssh_public_key }}\") is None)'\n    ignore_errors: yes\n    register: key_result\n  - name: Check that keypair is available\n    assert:\n      that: 'key_result.rc == 0'\n      msg: \"Keypair {{ openstack_ssh_public_key }} is not available\"\n\n# Check that custom images and flavors exist\n- hosts: localhost\n\n  # Include variables that will be used by heat\n  vars_files:\n  - stack_params.yaml\n\n  tasks:\n  # Check that custom images are available\n  - include: custom_image_check.yaml\n    with_items:\n    - \"{{ openstack_master_image }}\"\n    - \"{{ openstack_infra_image }}\"\n    - \"{{ openstack_node_image }}\"\n    - \"{{ openstack_lb_image }}\"\n    - \"{{ openstack_etcd_image }}\"\n    - \"{{ openstack_dns_image }}\"\n    loop_control:\n      loop_var: image\n\n  # Check that custom flavors are available\n  - include: custom_flavor_check.yaml\n    with_items:\n    - \"{{ master_flavor }}\"\n    - \"{{ infra_flavor }}\"\n    - \"{{ node_flavor }}\"\n    - \"{{ lb_flavor }}\"\n    - \"{{ etcd_flavor }}\"\n    - \"{{ dns_flavor }}\"\n    loop_control:\n      loop_var: flavor\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f1cbe81f3ef63ba92e497def24062ed31c3bf5a5", "filename": "roles/nfs-server/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\ndefault_nfs_owner: \"nfsnobody\"\ndefault_nfs_group: \"nfsnobody\"\ndefault_nfs_mode: \"0777\"\n\ndefault_nfs_vg_name: \"nfs\"\ndefault_nfs_lv_name: \"exports\"\ndefault_nfs_share_basedir: \"/exports\"\ndefault_nfs_share_options: \"rw,root_squash,no_wdelay\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "aaae48663a9c3287dc198ac196ebcf478c263691", "filename": "playbooks/ansible/tower/README.md", "repository": "redhat-cop/infra-ansible", "decoded_content": "# Ansible Tower Playbooks\n\nThis playbook directory has the playbook(s) necessary to manage your Ansible Tower.\n\n## Prerequisites\n\nCurrently, the playbook(s) in here do not manage the instances (a.k.a.: VMs) themselves, so you need to ensure you already have running instances (and subscribed if applicable) before running these playbook(s) with a valid inventory.\n\n## Example\n\n### Inventory\n\nPlease see the inventory in the respective role for more details:\n\n- [tower](../../../roles/ansible/tower/README.md)\n\n\n### Playbook execution\n\nInitial run needs the `tags='install,all'` set to ensure all necessary software is installed:\n\n```bash\n> ansible-playbook -i inventory configure-ansible-tower.yml --tags='install,all'\n```\n\nAny consecutive runs can be done without the tags to speed up execution:\n```bash\n> ansible-playbook -i inventory configure-ansible-tower.yml\n```\n\n\nLicense\n-------\n\nApache License 2.0\n\n\nAuthor Information\n------------------\n\nRed Hat Community of Practice & staff of the Red Hat Open Innovation Labs.\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "d91fab1559c9ed37c80894922675cf5e62481e20", "filename": "roles/atomic-update/tasks/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# Fail as early as possible if Atomic and old version of Docker\n- block:\n  - name: Determine Atomic Host Docker Version\n    shell: 'CURLY=\"{\"; docker version --format \"$CURLY{json .Server.Version}}\"'\n    register: l_atomic_docker_version\n\n  - assert:\n      msg: Installation on Atomic Host requires Docker 1.12 or later. Attempting to patch.\n      that:\n      - l_atomic_docker_version.stdout | replace('\"', '') | version_compare('1.12','>=')\n\n  rescue:\n  - name: Is the host already registered?\n    command: \"subscription-manager status\"\n    register: subscribed\n    changed_when: no\n    ignore_errors: yes\n    when: ansible_distribution == \"RedHat\"\n\n  - name: RedHat subscriptions\n    redhat_subscription:\n      username: \"{{ rhsm_user }}\"\n      password: \"{{ rhsm_password }}\"\n    when: \"'Current' not in subscribed.stdout and rhsm_user is defined and rhel_subscription_user\"\n\n  - name: Retrieve the OpenShift Pool ID\n    command: subscription-manager list --available --matches=\"{{ rhsm_pool }}\" --pool-only\n    register: openshift_pool_id\n    changed_when: False\n    when: ansible_distribution == \"RedHat\"\n\n  - name: Determine if OpenShift Pool Already Attached\n    command: subscription-manager list --consumed --matches=\"{{ rhsm_pool }}\" --pool-only\n    register: openshift_pool_attached\n    changed_when: False\n    when: openshift_pool_id.stdout == '' and ansible_distribution == \"RedHat\"\n\n  - fail:\n      msg: \"Unable to find pool matching {{ rhsm_pool }} in available or consumed pools\"\n    when: openshift_pool_id.stdout == '' and openshift_pool_attached is defined and openshift_pool_attached.stdout == ''\n\n  - name: Attach to OpenShift Pool\n    command: subscription-manager subscribe --pool {{ openshift_pool_id.stdout_lines[0] }}\n    when: openshift_pool_id.stdout != '' and ansible_distribution == \"RedHat\"\n\n  - name: Patching Atomic instances\n    shell: atomic host upgrade\n    register: patched\n\n  - name: Reboot when patched\n    shell: sleep 5 && shutdown -r now \"Reboot due to Atomic Patching\"\n    async: 1\n    poll: 0\n    ignore_errors: true\n    when: patched.changed\n\n  - name: Wait for hosts to be back\n    pause:\n      seconds: 60\n    delegate_to: 127.0.0.1\n    when: patched.changed\n\n  when: openshift.common.is_atomic | bool\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "08ca3cd41d08440bc36dc54ffefac185f9c7b5c9", "filename": "ops/templates/restore_dtr_metadata.sh.j2", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n#\n# Restore DTR metadata\n#\n\nreplica_id=$1\nif [ -e ./ca.pem ]\nthen\n  ucp_insecure_switch=\"\"\n  export UCP_CA=$(cat ./ca.pem)\n  env_switch=\"--env UCP_CA\"\nelse\n  ucp_insecure_switch=\"--ucp-insecure-tls\"\n  env_switch=\"\"\nfi\n\nssh -oStrictHostKeyChecking=no {{ backup_server }} \"cat {{ backup_dtr_meta }}\" | zcat | \\\n  docker run -i --rm --env UCP_USERNAME --env UCP_PASSWORD $env_switch docker/dtr:{{ dtr_version }} restore \\\n    --ucp-url https://{{ ucp_instance }}.{{ domain_name }} \\\n    $ucp_insecure_switch \\\n    --ucp-node {{ groups.dtr_main[0] }}.{{ domain_name }} \\\n    --replica-id ${replica_id}  \\\n    --dtr-external-url https://{{ groups.dtr[0] }}.{{ domain_name }} \\\n    --nfs-storage-url nfs://{{ groups.nfs[0] }}.{{ domain_name }}/docker-images \\\n    {{ http_proxy_switch }} {{ https_proxy_switch }} {{ no_proxy_switch }} \n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "cbadf8debdea8141fb368788f1cdb8159632c4b3", "filename": "roles/cloud-azure/tasks/venv.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Clean up the environment\n  file:\n    dest: \"{{ azure_venv }}\"\n    state: absent\n  when: clean_environment\n\n- name: Install requirements\n  pip:\n    name:\n      - packaging\n      - requests[security]\n      - azure-mgmt-compute>=2.0.0,<3\n      - azure-mgmt-network>=1.3.0,<2\n      - azure-mgmt-storage>=1.5.0,<2\n      - azure-mgmt-resource>=1.1.0,<2\n      - azure-storage>=0.35.1,<0.36\n      - azure-cli-core>=2.0.12,<3\n      - msrest==0.4.29\n      - msrestazure==0.4.31\n      - azure-mgmt-dns>=1.0.1,<2\n      - azure-mgmt-keyvault>=0.40.0,<0.41\n      - azure-mgmt-batch>=4.1.0,<5\n      - azure-mgmt-sql>=0.7.1,<0.8\n      - azure-mgmt-web>=0.32.0,<0.33\n      - azure-mgmt-containerservice>=2.0.0,<3.0.0\n      - azure-mgmt-containerregistry>=1.0.1\n      - azure-mgmt-rdbms==1.2.0\n      - azure-mgmt-containerinstance==0.4.0\n    state: latest\n    virtualenv: \"{{ azure_venv }}\"\n    virtualenv_python: python2.7\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "18aea396eeea6768c8d16cd05f949b22e88d4376", "filename": "reference-architecture/vmware-ansible/playbooks/prod.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - create-vm-prod-ose\n\n- name: fulfill OSE3 prerequisites on production hosts roles\n  hosts: production_group\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - rhsm\n    - vmware-guest-setup\n    - cloud-provider-setup\n    - docker-storage-setup\n    - openshift-volume-quota\n  ignore_errors: yes\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "58f818aed0d6811dad47830bbfd7e536d4532d3e", "filename": "roles/suricata/vars/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# vars file for suricata\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "266baed251308d5a331e3bd6cd1c8db9091a7e76", "filename": "roles/manage-aws-infra/defaults/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\ndelete_vpc: false\n\ndefault_root_volume: '/dev/sda1'\ndefault_docker_volume: '/dev/xvdb'\ndefault_cns_volume: '/dev/xvdg'\n\ndefault_root_volume_size: '50'\ndefault_docker_volume_size: '40'\ndefault_cns_volume_size: '200'\n\n\ndefault_ocp_ssh_sg_rules:\n  - proto: tcp\n    from_port: 22\n    to_port: 22\n    cidr_ip: 0.0.0.0/0\n\ndefault_ocp_master_sg_rules:\n  - proto: udp\n    from_port: 8053\n    to_port: 8053\n    cidr_ip: 0.0.0.0/0\n  - proto: tcp\n    from_port: 8053\n    to_port: 8053\n    cidr_ip: 0.0.0.0/0\n  - proto: tcp\n    from_port: 8443\n    to_port: 8443\n    cidr_ip: 0.0.0.0/0\n  - proto: tcp\n    from_port: 443\n    to_port: 443\n    cidr_ip: 0.0.0.0/0\n  - proto: udp\n    from_port: 53\n    to_port: 53\n    cidr_ip: 0.0.0.0/0\n  - proto: tcp\n    from_port: 53\n    to_port: 53\n    cidr_ip: 0.0.0.0/0\n  - proto: udp\n    from_port: 4789\n    to_port: 4789\n    cidr_ip: 0.0.0.0/0\n\ndefault_ocp_app_node_sg_rules:\n  - proto: udp\n    from_port: 4789\n    to_port: 4789\n    cidr_ip: 0.0.0.0/0\n  - proto: tcp\n    from_port: 10250\n    to_port: 10250\n    cidr_ip: 0.0.0.0/0\n\ndefault_ocp_infra_node_sg_rules:\n  - proto: tcp\n    from_port: 80\n    to_port: 80\n    cidr_ip: 0.0.0.0/0\n  - proto: tcp\n    from_port: 443\n    to_port: 443\n    cidr_ip: 0.0.0.0/0\n\ndefault_ocp_cns_node_sg_rules:\n  - proto: tcp\n    from_port: 24007\n    to_port: 24007\n    cidr_ip: 0.0.0.0/0\n  - proto: tcp\n    from_port: 24008\n    to_port: 24008\n    cidr_ip: 0.0.0.0/0\n  - proto: tcp\n    from_port: 2222\n    to_port: 2222\n    cidr_ip: 0.0.0.0/0\n  - proto: tcp\n    from_port: 49152\n    to_port: 49664\n    cidr_ip: 0.0.0.0/0\n  - proto: tcp\n    from_port: 24010\n    to_port: 24010\n    cidr_ip: 0.0.0.0/0\n  - proto: tcp\n    from_port: 3260\n    to_port: 3260\n    cidr_ip: 0.0.0.0/0\n  - proto: tcp\n    from_port: 111\n    to_port: 111\n    cidr_ip: 0.0.0.0/0\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "1ad40ffb948bfbc40c7480053c72598b48aa847c", "filename": "roles/nextcloud/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "nextcloud_install: True\nnextcloud_enabled: False\n\n# REMOVE /opt/nextcloud/version.php TO FORCE AN INSTALL OR REINSTALL OR UPGRADE\nnextcloud_force_install: False\n\nnextcloud_url: /nextcloud\nnextcloud_prefix: /opt\nnextcloud_data_dir: \"{{ content_base }}/nextcloud/data\"\nnextcloud_dl_url: https://download.nextcloud.com/server/releases/\nnextcloud_orig_src_file: latest-13.tar.bz2\nnextcloud_src_file: nextcloud_{{ nextcloud_orig_src_file }}\n\n# we install on mysql with these setting or those from default_vars, etc.\nnextcloud_dbname: nextcloud\nnextcloud_dbhost: localhost\nnextcloud_dbuser: nextcloud\nnextcloud_dbpassword: nextcloudmysql\nnextcloud_user: nextcloud\nnextcloud_user_password: nextcloudmysql\n\nnextcloud_admin_user: 'Admin'\nnextcloud_admin_password: 'changeme'\n\nnextcloud_required_ip: 10.0.0.0/8 192.168.0.0/16\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "4813ad8c9162c2f9ff54f62cb95656cdcc21c53b", "filename": "playbooks/roles/docket/tasks/lighttpd.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: docket | configure lighttpd + uwsgi\n  template:\n    src: docket_lighttpd_scgi.conf.j2\n    dest: /etc/lighttpd/conf.d/docket_scgi.conf\n\n- name: docket | configure lighttpd TLS listener\n  template:\n    src: docket_lighttpd_vhost.conf.j2\n    dest: /etc/lighttpd/vhosts.d/docket.conf\n  notify: docket | restart lighttpd\n\n- name: docket | create vhost logdir\n  file:\n    state: directory\n    path: \"/var/log/lighttpd/{{ docket_web_server_name }}/\"\n    owner: lighttpd\n    group: lighttpd\n    mode: 0755\n\n- name: docket | enable lighttpd vhosts\n  lineinfile:\n    path: /etc/lighttpd/lighttpd.conf\n    regexp: '^#?\\s*include.*vhosts\\.d/.*$'\n    line: include \"/etc/lighttpd/vhosts.d/*.conf\"\n  notify: docket | restart lighttpd\n\n- name: docket | add lighttpd into docket group\n  user:\n    name: lighttpd\n    append: yes\n    groups: \"{{ docket_group }}\"\n  notify: docket | restart lighttpd\n\n- name: docket | enable lighttpd service\n  service:\n    name: lighttpd\n    enabled: yes\n  notify: docket | restart lighttpd\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "a4da31bfe1c59bc58cd216f54aea35a7aa66f5ef", "filename": "playbooks/provisioning/openstack/stack_params.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\nstack_name: \"{{ env_id }}.{{ public_dns_domain }}\"\ndns_domain: \"{{ public_dns_domain }}\"\ndns_nameservers: \"{{ public_dns_nameservers }}\"\nsubnet_prefix: \"{{ openstack_subnet_prefix }}\"\nmaster_hostname: \"{{ openstack_master_hostname | default('master') }}\"\ninfra_hostname: \"{{ openstack_infra_hostname | default('infra-node') }}\"\nnode_hostname: \"{{ openstack_node_hostname | default('app-node') }}\"\nlb_hostname: \"{{ openstack_lb_hostname | default('lb') }}\"\netcd_hostname: \"{{ openstack_etcd_hostname | default('etcd') }}\"\ndns_hostname: \"{{ openstack_dns_hostname | default('dns') }}\"\nssh_public_key: \"{{ openstack_ssh_public_key }}\"\nopenstack_image: \"{{ openstack_default_image_name }}\"\nlb_flavor: \"{{ openstack_lb_flavor | default(openstack_default_flavor) }}\"\netcd_flavor: \"{{ openstack_etcd_flavor | default(openstack_default_flavor) }}\"\nmaster_flavor: \"{{ openstack_master_flavor | default(openstack_default_flavor) }}\"\nnode_flavor: \"{{ openstack_node_flavor | default(openstack_default_flavor) }}\"\ninfra_flavor: \"{{ openstack_infra_flavor | default(openstack_default_flavor) }}\"\ndns_flavor: \"{{ openstack_dns_flavor | default(openstack_default_flavor) }}\"\nopenstack_master_image: \"{{ openstack_master_image_name | default(openstack_default_image_name) }}\"\nopenstack_infra_image: \"{{ openstack_infra_image_name | default(openstack_default_image_name) }}\"\nopenstack_node_image: \"{{ openstack_node_image_name | default(openstack_default_image_name) }}\"\nopenstack_lb_image: \"{{ openstack_lb_image_name | default(openstack_default_image_name) }}\"\nopenstack_etcd_image: \"{{ openstack_etcd_image_name | default(openstack_default_image_name) }}\"\nopenstack_dns_image: \"{{ openstack_dns_image_name | default(openstack_default_image_name) }}\"\nopenstack_private_network: >-\n  {% if openstack_provider_network_name | default(None) -%}\n  {{ openstack_provider_network_name }}\n  {%- else -%}\n  {{ openstack_private_network_name | default ('openshift-ansible-' + stack_name + '-net') }}\n  {%- endif -%}\nprovider_network: \"{{ openstack_provider_network_name | default(None) }}\"\nexternal_network: \"{{ openstack_external_network_name | default(None) }}\"\nnum_etcd: \"{{ openstack_num_etcd | default(0) }}\"\nnum_masters: \"{{ openstack_num_masters }}\"\nnum_nodes: \"{{ openstack_num_nodes }}\"\nnum_infra: \"{{ openstack_num_infra }}\"\nnum_dns: \"{{ openstack_num_dns | default(1) }}\"\nmaster_server_group_policies: \"{{ openstack_master_server_group_policies | default([]) | to_yaml }}\"\ninfra_server_group_policies: \"{{ openstack_infra_server_group_policies | default([]) | to_yaml }}\"\nmaster_volume_size: \"{{ docker_master_volume_size | default(docker_volume_size) }}\"\ninfra_volume_size: \"{{ docker_infra_volume_size | default(docker_volume_size) }}\"\nnode_volume_size: \"{{ docker_node_volume_size | default(docker_volume_size) }}\"\netcd_volume_size: \"{{ docker_etcd_volume_size | default('2') }}\"\ndns_volume_size: \"{{ docker_dns_volume_size | default('1') }}\"\nlb_volume_size: \"{{ docker_lb_volume_size | default('5') }}\"\nnodes_to_remove: \"{{ openstack_nodes_to_remove | default([]) |  to_yaml }}\"\nuse_bastion: \"{{ openstack_use_bastion|default(False) }}\"\nui_ssh_tunnel: \"{{ openshift_ui_ssh_tunnel|default(False) }}\"\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "3cd904e520cfe534e1c729d79c2535c36501ccdd", "filename": "tasks/replication/master.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n- name: TEMPLATE | Deploy master configuration\n  template:\n    src: etc/mysql/conf.d/50-master.cnf.j2\n    dest: /etc/mysql/conf.d/50-master.cnf\n  notify: restart mariadb\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "fffb0dc10f06f4af8c4526adbe23d3bb3ddbc501", "filename": "reference-architecture/ansible-tower-integration/tower_config_azure/tower_config_azure/meta/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ngalaxy_info:\n  author: James Labocki\n  description: Configures Tower to sync openshift-ansible-contrib\n  company: Red Hat, Inc.\n  license: MIT\n  min_ansible_version: 1.2\n  platforms:\n  - name: EL\n    versions:\n    - 6\n    - 7\n  categories:\n  - packaging\n  - system\n  dependencies: []\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "9cce05582bef8742e4b72747ecdbec43470cb38f", "filename": "roles/ovirt-collect-logs/vars/db.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_collect_logs_tar_optional_params: \"\"\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "5a439c1c937f40259500858ab2e943f6ad843ee6", "filename": "tasks/Linux/install/Debian.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Add apt-key and repository for AdoptOpenJDK\n  block:\n  - name: Add apt-key for AdoptOpenJDK\n    apt_key:\n      url: https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public\n      state: present\n    register: package_install\n    until: package_install is succeeded\n\n  - name: Add repository for AdoptOpenJDK\n    apt_repository:\n      repo: 'deb https://adoptopenjdk.jfrog.io/adoptopenjdk/deb/ bionic main'\n      filename: adoptopenjdk\n      state: present\n      codename: trusty\n      update_cache: true\n  when:\n    - java_distribution == \"adoptopenjdk\"\n\n- name: Install java packages\n  apt:\n    deb: '{{ java_artifact | default(omit) }}'\n    name: '{{ (jdk_package if transport == \"repositories\") | default(omit) }}'\n    state: present\n    update_cache: true\n    cache_valid_time: 3600\n  register: package_install\n  until: package_install is succeeded\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "3f14dc1576ebd935ac9e8ac56a15e069ba4ea700", "filename": "roles/consul/vars/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# vars file for consul\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6b421770d28684a1532f22f1c358e7de6c9370ce", "filename": "playbooks/libvirt/openshift-cluster/templates/meta-data", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "instance-id: {{ item[0] }}\nhostname: {{ item[0] }}\nlocal-hostname: {{ item[0] }}.example.com\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "a6d8d69958d488ff275803d5356a2949ca6d667b", "filename": "playbooks/openstack/README.md", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "# OpenStack playbooks\n\nThis playbook directory is meant to be driven by [`bin/cluster`](../../bin),\nwhich is community supported and most use is considered deprecated.\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "4d9d5227eeea1768b784718fbdc5142f01078b0b", "filename": "roles/cups/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "cups_install: True\ncups_enabled: False\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "20dee97d27de374b1dff4510b099795b5f7d4726", "filename": "roles/mysql/tasks/fedora.yml", "repository": "iiab/iiab", "decoded_content": "    - name: Install MySQL\n      package: name={{ item }}\n               state=present\n      with_items:\n        - mysql-server\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4c282ec715be022b129930c87ad0f41bffb6bffb", "filename": "roles/ansible/tower/manage-credentials/tests/inventory/group_vars/tower.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\ntower_admin_password: \"admin01\"\n\nansible_tower:\n  credentials:\n  - name: \"Cred1\"\n    description: \"My Credential 1\"\n    organization: \"Default\"\n    credential_type: \"Machine\"\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "b39df6f3f66ea09e223d45482aaf3f98a443b979", "filename": "tasks/Linux/install/RedHat.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Add repository for AdoptOpenJDK\n  yum_repository:\n    name: AdoptOpenJDK\n    description: AdoptOpenJDK\n    baseurl: http://adoptopenjdk.jfrog.io/adoptopenjdk/rpm/centos/7/x86_64\n    enabled: true\n    gpgcheck: true\n    gpgkey: https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public\n  when:\n    - java_distribution == \"adoptopenjdk\"\n\n- name: Install java packages\n  yum:\n    name: '{{ (transport == \"repositories\") | ternary(jdk_package, java_artifact) }}'\n    state: present\n  register: package_install\n  until: package_install is succeeded\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b7a7ac6e3ea18001fd050821e05f9bac022f08e2", "filename": "roles/gluster-rhsm-repos/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- block:\n    - name: disable unneeded repos\n      command: subscription-manager repos --disable='*'\n\n    - name: ensure proper repos are assigned\n      command: subscription-manager repos --enable={{ item }}\n      with_items: \"{{ gluster_repos }}\"\n\n    - name: check to see if rhui exists\n      stat:\n        path: /etc/yum.repos.d/redhat-rhui.repo\n      register: rhui\n\n    - name: check to see if rhui client exists\n      stat:\n        path: /etc/yum.repos.d/redhat-rhui-client-config.repo\n      register: client\n\n    - name: disable rhui repo\n      replace:\n        dest: /etc/yum.repos.d/redhat-rhui.repo\n        regexp: 'enabled=1'\n        replace: 'enabled=0'\n      when: rhui.stat.exists\n\n    - name: disable rhui client repos\n      replace:\n        dest: /etc/yum.repos.d/redhat-rhui-client-config.repo\n        regexp: 'enabled=1'\n        replace: 'enabled=0'\n      when: client.stat.exists\n\n  when: ansible_distribution == \"RedHat\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a68de661ab1970b6ead613deb377aa83e8dfedf0", "filename": "roles/ansible/tower/config-ansible-tower/tests/inventory/group_vars/tower.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nansible_connection: local\n\n# NOTE: below is an example on how these params and files can be specified\n#       - please replace with valid values and files\n\nansible_tower:\n  admin_password: \"admin01\"\n  install:\n    license_file: \"{{ inventory_dir }}/../files/tower-license.json\"\n    ssl_certificate:\n      cert: \"{{ inventory_dir }}/../certs/tower.cert\"\n      key: \"{{ inventory_dir }}/../certs/tower.key\"\n    pg:\n      password: \"pg_password01\"\n    rabbitmq:\n      password: \"rabbitmq_password01\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "8d81b6a46377627140b10e1bf66fc0ef398e923c", "filename": "reference-architecture/vmware-ansible/playbooks/roles/heketi-install/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Install heketi RPMS\n  package:\n    name: \"{{ item }}\"\n    state: latest\n  with_items:\n  - heketi\n  - heketi-client\n\n- name: Copy ssh private key in place\n  template:\n    src: ~/.ssh/id_rsa\n    dest: /etc/heketi/heketi_key\n    owner: heketi\n    group: heketi\n\n- name: Copy ssh private key in place\n  template:\n    src: ~/.ssh/id_rsa.pub\n    dest: /etc/heketi/heketi_key.pub\n    owner: heketi\n    group: heketi\n\n- stat: path=/etc/heketi/heketi.json\n  register: heketi_cfg\n\n- name: Copy heketi configuration in place.\n  template:\n    src: heketi.json.j2\n    dest: /etc/heketi/heketi.json\n  notify: restart heketi\n  when: heketi_cfg.stat.exists == True\n\n- name: restart heketi\n  service: name=heketi state=restarted enabled=yes\n\n- name: Verify heketi is started and configured properly\n  uri:\n    url: http://{{ ansible_default_ipv4.address }}:8080/hello\n    status_code: 200\n    method: GET\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "59970ae8b2f8fc7cdb6836d99c682cf56b50ae87", "filename": "reference-architecture/gcp/ansible/playbooks/roles/empty-image/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: check if empty image is already present in gce\n  command: gcloud --project {{ gcloud_project }} compute images describe {{ empty_image_gce }}\n  register: empty_image_gce_exists\n  changed_when: false\n  ignore_errors: true\n\n- block:\n  - name: create temporary build directory\n    tempfile:\n      suffix: build\n      state: directory\n    register: temp_image_directory\n\n  - name: create empty 1g image\n    command: truncate -s 1g '{{ temp_image_directory.path }}/{{ empty_image_raw }}' creates={{ empty_image_raw }}\n\n  - name: archive raw image\n    command: /usr/bin/tar -Szcf '{{ temp_image_directory.path }}/{{ empty_image_archive }}' -C '{{ temp_image_directory.path }}' '{{ empty_image_raw }}' creates={{ empty_image_archive }}\n\n  - name: check if a bucket in gcs exists\n    command: gsutil ls -p {{ gcloud_project }} {{ empty_image_bucket }}\n    register: bucket_exists\n    ignore_errors: true\n\n  - name: create a bucket in gcs\n    command: gsutil mb -p {{ gcloud_project }} -l {{ gcloud_region }} {{ empty_image_bucket }}\n    when: bucket_exists | failed\n\n  - name: check if the bucket contains image\n    command: gsutil ls -p {{ gcloud_project }} {{ empty_image_in_bucket }}\n    register: image_in_bucket_exists\n    ignore_errors: true\n\n  - name: upload image to the bucket\n    command: gsutil cp '{{ temp_image_directory.path }}/{{ empty_image_archive }}' {{ empty_image_bucket }}\n    when: image_in_bucket_exists | failed\n\n  - name: create gce image from the uploaded archive\n    command: gcloud --project {{ gcloud_project }} compute images create {{ empty_image_gce }} --source-uri {{ empty_image_in_bucket }}\n\n  - name: delete bucket with uploaded archive\n    command: gsutil -m rm -r \"{{ empty_image_bucket }}\"\n  when: empty_image_gce_exists | failed\n  always:\n  - name: delete temporary directory\n    file:\n      path: '{{ temp_image_directory.path }}'\n      state: absent\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ba4bd8652e1767886cddc9cd2a85a82fa407222a", "filename": "reference-architecture/vmware-ansible/playbooks/roles/keepalived_haproxy/templates/keepalived.conf.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "global_defs {\n   router_id LVS_DEVEL\n}\n\nvrrp_script haproxy_check {\n   script \"killall -0 haproxy\"\n   interval 2\n   weight 2 \n}\n\nvrrp_instance OCP_EXT {\n   interface {{ external_interface }}\n\n   virtual_router_id 51\n\n   priority {% if groups.haproxy_group.index(inventory_hostname) == 0 %} {{ keepalived_priority_start }}{% else %} {{  keepalived_priority_start - 2 }}{% endif %}\n\n   state {% if groups.haproxy_group.index(inventory_hostname) == 0 %} {{ \"MASTER\" }}{% else %} {{ \"BACKUP\" }}{% endif %}\n\n   virtual_ipaddress {\n       {{ openshift_master_cluster_public_ip }}  dev {{ external_interface }}\n\n   }\n   track_script {\n       haproxy_check\n   }\n   authentication {\n      auth_type PASS\n      auth_pass {{ keepalived_pass.stdout }}\n   }\n}\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "5cbeac07c8d0c3d836b279cd7d354189c8a6def3", "filename": "roles/network/tasks/enable_wan.yml", "repository": "iiab/iiab", "decoded_content": "- name: Turn off ONBOOT for WAN on reboot if disabled\n  lineinfile: state=present\n              backrefs=yes\n              regexp='^ONBOOT'\n              line='ONBOOT=\"no\"'\n              dest=/etc/sysconfig/network-scripts/ifcfg-WAN\n  when: has_WAN and iiab_wan_iface == \"none\"\n\n#testpoint Need to ensure we have only one entry\n- name: Ensure macaddress is correct\n  lineinfile: state=present\n              backrefs=yes\n              regexp='^HWADDR'\n              line='HWADDR=\"{{ hostvars[inventory_hostname]['ansible_' + iiab_wan_iface]['macaddress'] }}\"'\n              dest=/etc/sysconfig/network-scripts/ifcfg-WAN\n  when: has_WAN and iiab_wan_iface != \"none\"\n\n- name: Fix the DEVICE\n  lineinfile: state=present\n              backrefs=yes\n              regexp='^NAME'\n              line='NAME=\"iiab-WAN\"'\n              dest=/etc/sysconfig/network-scripts/ifcfg-WAN\n  when: has_WAN and iiab_wan_iface != \"none\"\n\n- name: Turn on ONBOOT for WAN on reboot if enabled\n  lineinfile: state=present\n              backrefs=yes\n              regexp=\"^ONBOOT\"\n              line=\"ONBOOT=yes\"\n              dest=/etc/sysconfig/network-scripts/ifcfg-WAN\n  when: has_WAN and iiab_wan_iface != \"none\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "59a01af41e732dc11c04e7b4c2b757be570d9cc3", "filename": "roles/setup-slack/tasks/get_channel_info.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Get channel info\n  uri:\n    url: \"https://slack.com/api/channels.list?token={{ slack_token }}\"\n    method: GET\n    status: [200]\n    return_content: yes\n  register: channel_data\n\n- name: Get group info\n  uri:\n    url: \"https://slack.com/api/groups.list?token={{ slack_token }}\"\n    method: GET\n    status: [200]\n    return_content: yes\n  register: group_data\n\n- name: Update channel mapping\n  with_items: \"{{ channel_data.json.channels }}\"\n  loop_control:\n    loop_var: channel\n  set_fact:\n    channel_mapping: \"{{ channel_mapping|combine({ channel.name: channel.id }) }}\"\n\n- name: Update group mapping\n  with_items: \"{{ group_data.json.groups }}\"\n  loop_control:\n    loop_var: group\n  set_fact:\n    channel_mapping: \"{{ channel_mapping|combine({ group.name: group.id }) }}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "e368ae8cff69bd9da33cd91d82b373897ea4cd3b", "filename": "roles/awstats/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- include_tasks: install.yml\n  when: awstats_install\n\n- name: Add 'awstats' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: awstats\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: AWStats\n    - option: description\n      value: '\"AWStats (originally known as Advanced Web Statistics) is a package written in Perl which generates static or dynamic html summaries based upon web server logs.\"'\n    - option: installed\n      value: \"{{ awstats_install }}\"\n    - option: enabled\n      value: \"{{ awstats_enabled }}\"\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "34ba5f86da15b63595b04eec4a1be4dd9a339a9d", "filename": "roles/cloud-digitalocean/defaults/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\ndigitalocean_venv: \"{{ playbook_dir }}/configs/.venvs/digitalocean\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7fc3e4481935b4a5721256357f19fbb92a07f494", "filename": "roles/config-iscsi-client/tasks/lock-lvm.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Obtain currently configured LVMs (pvs)\"\n  shell: 'pvs | sed -ne \"s/\\(\\/dev\\/[^ ]*\\) .*/\\1/p\"'\n  register: pvs_device_list\n\n- name: \"Build fact for setting the LVM global_filter\"\n  set_fact:\n    lvm_global_filter: '{{ lvm_global_filter | default(\"\") }} \"a|^{{ item|trim }}$|\",'\n  with_items:\n  - \"{{ pvs_device_list.stdout_lines }}\"\n\n- name: \"Ensure LVM does NOT claim too many devices\"\n  lineinfile:\n    path: /etc/lvm/lvm.conf\n    state: present\n    regexp: \"\\t# global_filter = \"\n    line: \"\\tglobal_filter = [ {{ lvm_global_filter }} \\\"a/^/dev/loop.*/\\\", \\\"r/.*/\\\" ]\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "ba10034faadecaf354aaffc2c98be27ef6a874a4", "filename": "dev/playbooks/config_docker_lvs.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- hosts: docker\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n    - name: Check for partitions on disk\n      shell: parted -s {{ disk2 }} print 1\n      register: partPresent\n      failed_when: partPresent.rc is not defined\n\n    - name: Create partition on second disk\n      parted:\n        label: gpt\n        part_type: primary\n        device: \"{{ disk2 }}\"\n        flags: [ lvm ]\n        state: present\n        number: 1\n        part_start: 0%\n        part_end: 100%\n      when: partPresent.rc != 0\n#      ignore_errors: yes\n        \n    - name: Create Docker VG\n      lvg:\n        vg: docker\n        pvs: \"{{ disk2_part }}\"\n      when: partPresent.rc != 0\n\n    - name: Create thinpool LV\n      lvol:\n        lv: thinpool\n        opts: --wipesignatures y\n        vg: docker\n        size: 95%VG\n      when: partPresent.rc != 0\n\n    - name: Create thinpoolmeta LV\n      lvol:\n        lv: thinpoolmeta\n        opts: --wipesignatures y\n        vg: docker\n        size: 1%VG\n      when: partPresent.rc != 0\n\n    - name: Convert LVs to thinpool and storage for metadata\n      command: lvconvert -y --zero n -c 512K  --thinpool docker/thinpool --poolmetadata docker/thinpoolmeta\n      when: partPresent.rc != 0\n\n    - name: Config thinpool profile\n      copy: src=../files/docker-thinpool.profile dest=/etc/lvm/profile/docker-thinpool.profile\n\n    - name: Apply the LVM profile\n      command: lvchange --metadataprofile docker-thinpool docker/thinpool\n#      ignore_errors: yes\n\n    - name: Enable monitoring for LVs\n      command: lvs -o+seg_monitor\n\n    - name: Create /etc/docker directory\n      file:\n        path: /etc/docker\n        state: directory\n\n    - name: Config Docker daemon\n      template: src=../templates/daemon.json.j2 dest=/etc/docker/daemon.json\n\n    #- name: Enable and start docker service\n    #  systemd:\n    #    name: docker\n    #    enabled: yes\n    #    state: started\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "dcb968ee1819a8a549e8c1c418915b1be09ecb20", "filename": "roles/validate-masters/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Validate the public address\n  uri:\n    url: \"https://{{ hostvars['localhost']['openshift_master_cluster_public_hostname'] }}:{{ hostvars['localhost']['console_port'] }}/healthz/ready\"\n    validate_certs: False\n    status_code: 200\n    method: GET\n- name: Validate the internal address\n  uri:\n    url: \"https://{{ hostvars['localhost']['openshift_master_cluster_hostname'] }}:{{ hostvars['localhost']['console_port'] }}/healthz/ready\"\n    validate_certs: False\n    status_code: 200\n    method: GET\n- name: Validate the master address\n  uri:\n    url: \"https://{{ inventory_hostname }}:{{ hostvars['localhost']['console_port'] }}/healthz/ready\"\n    validate_certs: False\n    status_code: 200\n    method: GET\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "2427c4d770d0c5575a156b724387337d8c09ea50", "filename": "ops/playbooks/create_main_dtr.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- name: Install DTR\n  hosts: dtr_main\n  serial: 1\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - includes/internal_vars.yml\n\n  vars:\n    nfs_server: \"{{ hostvars[groups['nfs'][0]].ip_addr | ipaddr('address') }}\"\n    http_proxy_switch:  \"{% if  env.http_proxy is defined %} --http-proxy {{ env.http_proxy }} {% endif %}\"\n    https_proxy_switch:  \"{% if  env.https_proxy is defined %} --https-proxy {{ env.https_proxy }} {% endif %}\"\n    no_proxy_switch:  \"{% if  env.no_proxy is defined %} --no-proxy '{{ env.no_proxy }}' {% endif %}\"\n\n  environment:\n    - UCP_USERNAME: \"{{ ucp_username }}\" \n    - UCP_PASSWORD: \"{{ ucp_password }}\" \n    - UCP_CA: \"{{ ucp_ca_cert | default('') }}\"\n    - DTR_CA: \"{{ dtr_ca_cert | default('') }}\"\n    - DTR_CERT: \"{{ dtr_server_cert | default('') }}\"\n    - DTR_KEY: \"{{ dtr_server_key | default('') }}\"\n\n\n  pre_tasks:\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n\n  roles:\n    - role: worker\n      ARG_UCP_IP:         \"{{ ucp_instance }}.{{ domain_name }}\"\n      ARG_UCP_USER:       \"{{ ucp_username }}\"\n      ARG_UCP_PASSWORD:   \"{{ ucp_password }}\"\n      ARG_ADVERTIZE_IP:   \"{{ ucp_instance }}.{{ domain_name }}:2377\"\n      worker_role_ports:  \"{{ internal_dtr_ports }}\"\n      worker_join_delay:  180\n\n  tasks:\n\n#\n# Is DTR running ?\n#\n    - include_tasks: includes/find_dtr.yml\n      vars:\n        ping_servers: \"{{ groups.dtr }}\" \n#\n# if DTR is running, log something\n#\n    - name: \"Create Main DTR: DTR Already running in this environment\" \n      debug: msg=\"DTR is running on {{ dtr_instance }}\" \n      when: dtr_instance != \".none.\" \n\n#\n# Load Certificates\n#\n    - include_tasks: includes/load_certificates.yml\n\n#\n# Install DTR if it is not running, \n#\n\n    - name: Install first DTR node\n      command: docker run --env UCP_USERNAME --env UCP_PASSWORD {{ switch_env_ucp_ca }} {{ switch_env_dtr_certificates }} docker/dtr:{{ dtr_version }} install \n             --nfs-storage-url nfs://{{ nfs_server }}{{ images_folder }}\n             --ucp-node {{ inventory_hostname }}.{{ domain_name }}\n             --ucp-url https://{{ ucp_instance }}.{{ domain_name }}\n             --dtr-external-url https://{{ groups.dtr[0] }}.{{ domain_name }}\n             {{ switch_ucp_insecure }}\n             {{ http_proxy_switch }} {{ https_proxy_switch }} {{ no_proxy_switch }}\n      vars:\n        switch_ucp_insecure:         \"{% if ucp_ca_cert is defined %}{% else %}--ucp-insecure-tls{% endif %}\" \n        switch_env_ucp_ca:           \"{% if ucp_ca_cert is defined %}--env UCP_CA{% endif %}\" \n        switch_env_dtr_certificates: \"{% if dtr_ca_cert is defined %}--env DTR_CA --env DTR_CERT --env DTR_KEY{% endif %}\" \n      when: dtr_instance == \".none.\" \n      register: dtrlog\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f7df2f32c9542ac26755aee7e34bdc4608ed05a5", "filename": "roles/ansible/tower/manage-projects/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: tower\n  roles:\n  - role: ansible/tower/manage-projects\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "55d21155f71903174a6ad1fe2a076d9c587f2c7e", "filename": "roles/elasticsearch/tasks/after.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: Make sure cluster is green\n  uri:\n    method: \"GET\"\n    url: \"http://{{ es_interface }}:9200/_cluster/health\"\n    return_content: true\n    timeout: 2\n  register: result\n  until: result.json is defined and result.json.status == \"green\"\n  retries: 300\n  delay: 3\n  run_once: true\n\n- name: \"Fail if Elasticsearch is RED\"\n  fail:\n    msg: \"Elasticsearch cluster has a red status\"\n  when: result.json.status == \"red\"\n\n- name: Check for default mapping template\n  uri:\n    method: \"GET\"\n    url: \"{{ es_url }}/_template/default\"\n    status_code: [200, 404]\n    return_content: true\n    timeout: 2\n  register: default_index_template\n  run_once: true\n\n- name: Load default elasticsearch mapping template\n  uri:\n    method: PUT\n    url: \"{{ es_url }}/_template/default\"\n    body: \"{{ lookup('file', 'default-mapping.json') }}\"\n    body_format: json\n  when: (rock_services | selectattr('name', 'equalto', 'elasticsearch') | map(attribute='installed')) and default_index_template.status != 200\n  run_once: true\n\n- name: Blanket install/update elasticsearch mappings\n  command: ./import-index-templates.sh \"{{ es_url }}\"\n  args:\n    chdir: \"{{ rock_module_dir }}/configuration/elasticsearch\"\n  register: result\n  changed_when: 'result.stdout.find(\"Changed: 0\") != -1'\n  run_once: true\n...\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "afe269b7ceecbdb0686d00eea64719051a2e08b9", "filename": "playbooks/gce/openshift-cluster/terminate.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Terminate instance(s)\n  hosts: localhost\n  connection: local\n  become: no\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  tasks:\n  - add_host:\n      name: \"{{ item }}\"\n      groups: oo_hosts_to_terminate\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    with_items: \"{{ (groups['tag_clusterid-' ~ cluster_id] | default([])) | difference(['localhost']) }}\"\n\n- name: Unsubscribe VMs\n  hosts: oo_hosts_to_terminate\n  vars_files:\n  - vars.yml\n  roles:\n  - role: rhel_unsubscribe\n    when: deployment_type in ['atomic-enterprise', 'enterprise', 'openshift-enterprise'] and\n          ansible_distribution == \"RedHat\" and\n          lookup('oo_option', 'rhel_skip_subscription') | default(rhsub_skip, True) |\n            default('no', True) | lower in ['no', 'false']\n\n- name: Terminate instances(s)\n  hosts: localhost\n  become: no\n  connection: local\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  tasks:\n  - name: Terminate instances that were previously launched\n    local_action:\n      module: gce\n      state: 'absent'\n      name: \"{{ item }}\"\n      service_account_email: \"{{ lookup('env', 'gce_service_account_email_address') }}\"\n      pem_file: \"{{ lookup('env', 'gce_service_account_pem_file_path') }}\"\n      project_id: \"{{ lookup('env', 'gce_project_id') }}\"\n      zone: \"{{ lookup('env', 'zone') }}\"\n    with_items: \"{{ groups['oo_hosts_to_terminate'] | default([], true) }}\"\n    when: item is defined\n\n#- include: ../openshift-node/terminate.yml\n#  vars:\n#    gce_service_account_email: \"{{ lookup('env', 'gce_service_account_email_address') }}\"\n#    gce_pem_file: \"{{ lookup('env', 'gce_service_account_pem_file_path') }}\"\n#    gce_project_id: \"{{ lookup('env', 'gce_project_id') }}\"\n#\n#- include: ../openshift-master/terminate.yml\n#  vars:\n#    gce_service_account_email: \"{{ lookup('env', 'gce_service_account_email_address') }}\"\n#    gce_pem_file: \"{{ lookup('env', 'gce_service_account_pem_file_path') }}\"\n#    gce_project_id: \"{{ lookup('env', 'gce_project_id') }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "4a8a155246af885439fc7fcbb0f76d8416bbdaf4", "filename": "reference-architecture/vmware-ansible/playbooks/cns-storage.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- include: prod-ose-cns.yaml\n  tags: ['vms']\n\n- include: cns-node-setup.yaml\n  tags: [ 'node-setup']\n\n- include: node-setup.yaml\n  tags: [ 'node-setup']\n\n- include: cleanup-cns.yaml\n  tags: ['clean']\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "9f6dfafb6c05228dd7a901a7776b7f03dbde5f80", "filename": "roles/openshift-applier/tasks/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: \"Pull in Galaxy requirements - if set\"\n  include: install-dependencies.yml\n  with_items:\n  - \"{{ openshift_cluster_content }}\"\n  loop_control:\n    loop_var: dependencies\n  when:\n  - dependencies.galaxy_requirements is defined\n  delegate_to: localhost\n\n- name: \"Copy inventory content to remote host(s) if not running 'locally'\"\n  import_tasks: copy-inventory-to-remote.yml\n  when:\n  - ansible_connection != 'local'\n\n- name: \"Create OpenShift objects\"\n  include_tasks: process-content.yml\n  with_items:\n  - \"{{ openshift_cluster_content | filter_applier_items(filter_tags) | default([]) }}\"\n  loop_control:\n    loop_var: entry\n  when:\n  - entry.content is defined or entry.content_dir is defined\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "972d5db65c0e03f14fdac59a6e9df9c47144fb8b", "filename": "roles/ansible/tower/config-ansible-tower-ldap/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: tower\n  roles:\n  - role: ansible/tower/config-ansible-tower-ldap\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "91d50613fd16322b74c01469c84fece301af9355", "filename": "reference-architecture/gcp/ansible/playbooks/roles/rhel-image-delete/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: delete rhel gce image\n  command: gcloud --project {{ gcloud_project }} compute images delete {{ rhel_image_gce }}\n  ignore_errors: true\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "3672ce2385dbf7d06d0ff5c873ca8a2a3eb7dc86", "filename": "playbooks/provision-satellite-server/README.md", "repository": "redhat-cop/infra-ansible", "decoded_content": "# Satellite Server Playbook\n\nThis playbook directory has the playbook(s) necessary to manage your Satellite server(s).\n\n## Prerequisites\n\nOne of the two:\n- a set of running instance(s)\n- a IaaS that allow for provisioning through these playbooks\n\n\n## Example\n\n### Inventory\n\nPlease see the **sample** inventory in the inventory area:\n\n- [satellite-server](../../inventory/satellite-server/README.md)\n\nYou will need to modify this sample inventory to fit your desired configuration.\n\n### Playbook execution\n\nDepending on how this is being hosted, the initial may need the `tags='install'` set to ensure all necessary software is installed:\n\n```bash\n> ansible-playbook -i inventory main.yml --tags='install'\n```\n\nAny consecutive runs can be done without the 'install' tag to speed up execution:\n```bash\n> ansible-playbook -i inventory main.yml\n```\n\nLicense\n-------\n\nApache License 2.0\n\n\nAuthor Information\n------------------\n\nRed Hat Community of Practice & staff of the Red Hat Open Innovation Labs.\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "16d956648a1a639058dd9bdb8c623f61c2ca86d7", "filename": "roles/kafka/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# handlers file for kafka\n\n- name: Create kafka bro topic\n  command: >\n    /opt/kafka/bin/kafka-topics.sh\n       --zookeeper 127.0.0.1:2181\n       --create\n       --replication-factor 1\n       --topic bro-raw\n       --partitions 1\n\n- name: Create kafka suricata topic\n  command: >\n    /opt/kafka/bin/kafka-topics.sh\n       --zookeeper 127.0.0.1:2181\n       --create\n       --replication-factor 1\n       --topic suricata-raw\n       --partitions 1\n\n- name: Create kafka fsf topic\n  command: >\n    /opt/kafka/bin/kafka-topics.sh\n       --zookeeper 127.0.0.1:2181\n       --create\n       --replication-factor 1\n       --topic fsf-raw\n       --partitions 1\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ee85117fdd78088c02243fe228991f1b17215892", "filename": "reference-architecture/vmware-ansible/playbooks/roles/haproxy-server/templates/haproxy.cfg.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "global\n    log         127.0.0.1 local2\n\n    chroot      /var/lib/haproxy\n    pidfile     /var/run/haproxy.pid\n    maxconn     4000\n    user        haproxy\n    group       haproxy\n    daemon\n\n    # turn on stats unix socket\n    stats socket /var/lib/haproxy/stats\n\ndefaults\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n#     option forwardfor       except 127.0.0.0/8\n    option                  redispatch\n    retries                 3\n    timeout http-request    10s\n    timeout queue           1m\n    timeout connect         10s\n    timeout client          1m\n    timeout server          1m\n    timeout http-keep-alive 10s\n    timeout check           10s\n    maxconn                 3000\nlisten  stats :9000\n        stats enable\n        stats realm Haproxy\\ Statistics\n        stats uri /haproxy_stats\n        stats auth admin:password\n        stats refresh 30\n        mode http\n\nfrontend  main *:80\n    default_backend             router80\n\nbackend router80\n    balance source\n    mode tcp\n    {% for host in groups['infra'] %}\n    server {{ hostvars[host]['ansible_fqdn'] }} {{ hostvars[host]['ansible_fqdn'] }}:80 check\n    {% endfor %}\n\nfrontend  main *:443\n    default_backend             router443\n\nbackend router443\n    balance source\n    mode tcp\n    {% for host in groups['infra'] %}\n    server {{ hostvars[host]['ansible_fqdn'] }} {{ hostvars[host]['ansible_fqdn'] }}:443 check\n    {% endfor %}\n\nfrontend  main *:8443\n    default_backend             mgmt8443\n\nbackend mgmt8443\n    balance source\n    mode tcp\n    {% for host in groups['master'] %}\n    server {{ hostvars[host]['ansible_fqdn'] }} {{ hostvars[host]['ansible_fqdn'] }}:8443 check\n    {% endfor %}\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "51ab7bfce86b6e0e81cfca4fb9d616936e18cd83", "filename": "roles/ipaclient/vars/CentOS-7.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# defaults file for ipaclient\n# vars/rhel.yml\nipaclient_packages: [ \"ipa-client\", \"libselinux-python\" ]\n#ansible_python_interpreter: '/usr/bin/python2'\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "b59e73102510bb66b2c70a1788aeb034f2ea0a05", "filename": "tasks/section_03_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n#sections 3.3 and 3.4 have to happen before as the latter overwrite 3.1 and 3.2\n\n  - name: 3.0 Check for /boot/grub/grub.cfg file (Not Scored)\n    stat: path=/boot/grub/grub.cfg\n    register: grub_cfg_file\n    tags:\n      - section3\n\n  - name: 3.3.1.1 Set Boot Loader Superuser (check) (Scored)\n    command: grep \"^set superusers\" /boot/grub/grub.cfg\n    register: boot_superusers\n    when: grub_cfg_file.stat.exists == True\n    changed_when: False\n    failed_when: False\n    always_run: True\n    tags:\n      - section3\n      - section3.3\n      - section3.3.1\n      - section3.3.1.1\n\n  - name: 3.3.1.2 Set Boot Loader Superuser (Scored)\n    lineinfile: >\n        dest='/etc/grub.d/40_custom'\n        regexp='^set superusers'\n        line='set superusers=\"root\"'\n        state=present\n        create=yes\n    when: grub_cfg_file.stat.exists == True and boot_superusers.rc == 1\n    tags:\n      - section3\n      - section3.3\n      - section3.3.1\n      - section3.3.1.2\n\n  - name: 3.3.2.1 Set Boot Loader Password (check) (Scored)\n    command: grep \"^password\" /boot/grub/grub.cfg\n    register: boot_password\n    when: grub_cfg_file.stat.exists == True\n    changed_when: False\n    failed_when: False\n    always_run: True\n    tags:\n      - section3\n      - section3.3\n      - section3.3.2\n      - section3.3.2.1\n\n  - name: 3.3.2.2 Set Boot Loader Password (Scored)\n    lineinfile: >\n        dest='/etc/grub.d/40_custom'\n        regexp='^password'\n        line='password_pbkdf2 root grub.pbkdf2.sha512.10000.529DB4AF052F170948C1DB2A754CEA8A286804DA2D9A4EB5A7CCE4B8636775C83EAF8A1093CBDBC256954BCE789A58EFB3B75D23DFC76583C703922D5DADB69E.4D5BD1EC6736057095CA2EBF55C2DA02DFB0B0784F2105A396F1CEF11FEB1483D5C420F412E2E817E2570DDFC22ABCC329C5FF44091A0ACDE67171FF72E96CFD'\n        state=present\n    when: grub_cfg_file.stat.exists == True and boot_password.rc == 1\n    tags:\n      - section3\n      - section3.3\n      - section3.3.2\n      - section3.3.2.2\n\n  - name: 3.3.3 Disable password protection booting (Scored)\n    lineinfile: >\n        dest='/etc/grub.d/10_linux'\n        create=yes\n        regexp='^CLASS='\n        line='CLASS=\"--class gnu-linux --class gnu --class os --unrestricted\"'\n        state=present\n    tags:\n      - section3\n      - section3.3\n      - section3.3.3\n\n\n  - name: 3.3.4 Update Grub configuration (Scored)\n    command: update-grub\n    when: grub_cfg_file.stat.exists == True and (boot_superusers.rc == 1 or boot_password.rc == 1)\n    tags:\n      - section3\n      - section3.3\n      - section3.3.4\n\n\n  - name: 3.4.1 Require Authentication for Single-User Mode (check) (Scored)\n    shell: 'grep \"^root:[*\\!]:\" /etc/shadow'\n    register: root_password_set\n    changed_when: False\n    failed_when: False\n    always_run: True\n    tags:\n      - section3\n      - section3.4\n      - section3.4.1\n\n  - name: 3.4.2 Require Authentication for Single-User Mode (Scored)\n    user: name=root state=present password='{{ root_password }}'\n    when: root_password_set.rc == 1\n    tags:\n      - section3\n      - section3.4\n      - section3.4.2\n\n  - name: 3.1 Set User/Group Owner on bootloader config (Scored)\n    file: path=/boot/grub/grub.cfg owner=root group=root\n    when: grub_cfg_file.stat.exists == True\n    tags:\n      - section3\n      - section3.1\n\n  - name: 3.2 Set Permissions on bootloader config (Scored)\n    file: path=/boot/grub/grub.cfg mode=\"o-rwx,g-rwx\"\n    when: grub_cfg_file.stat.exists == True\n    sudo: yes\n    tags:\n      - section3\n      - section3.2\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "db727359b0736026ea3fbd6046834d7f21d4209d", "filename": "roles/ovirt-engine-config/handlers/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# oVirt engine restart is required if configuration changed\n- name: restart of ovirt-engine service\n  service:\n    name: ovirt-engine\n    state: restarted\n\n- name: check health status of page\n  uri:\n    url: \"http://localhost/ovirt-engine/services/health\"\n    status_code: 200\n  register: health_page\n  retries: 12\n  delay: 10\n  until: health_page|success\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ea662c6e0a1a37c0565acce49213933e3a97ade2", "filename": "reference-architecture/aws-ansible/playbooks/vars/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# AWS instance specific configuration\nbastion_instance_type: t2.micro\ndocker_storage: \"25\"\netcd_storage: \"25\"\nemptydir_storage: \"50\"\nvpc_prefix: \"ose-multi-az-vpc-{{ stack_name }}\"\npublic_subnet_prefix: OSE-Public-Subnet\nprivate_subnet_prefix: OSE-Private-Subnet\ncidr_block: 10.20.0.0/16\nsubnet_blocks: 10.20.1.0/24,10.20.2.0/24,10.20.3.0/24,10.20.4.0/24,10.20.5.0/24,10.20.6.0/24\nbyo_bastion: no\nhost_up_time: 80\n\n# OpenShift variables\nopenshift_registry_selector: \"role=infra\"\nopenshift_router_selector: \"role=infra\"\nopenshift_master_cluster_method: native\nopenshift_master_cluster_hostname: \"internal-openshift-master.{{ public_hosted_zone }}\"\nopenshift_master_cluster_public_hostname: \"openshift-master.{{ public_hosted_zone }}\"\nosm_default_subdomain: \"{{ wildcard_zone }}\"\n\n# Gluster variables\ngluster_node01: \"ose-{{ glusterfs_stack_name }}-node01\"\ngluster_node02: \"ose-{{ glusterfs_stack_name }}-node02\"\ngluster_node03: \"ose-{{ glusterfs_stack_name }}-node03\"\ngluster_group: \"{{ stack_name }}-{{ gluster_stack_name }}\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "99445a6ad1dc7d9a92d31d3891a13a9b91bfb448", "filename": "playbooks/roles/stenographer/tasks/config.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# tasks file for stenographer\n  ######################################################\n  ################# Config Stenographer #################\n  ######################################################\n- name: Set stenographer config\n  template:\n    src: templates/stenographer-config.j2\n    dest: \"/etc/stenographer/config.{{ item.1 }}\"\n  with_indexed_items: \"{{ stenographer_monitor_interfaces }}\"\n\n- name: Create Stenographer directories\n  file:\n    path: \"{{ stenographer_data_dir }}/{{ item[0] }}/{{ item[1] }}\"\n    mode: 0755\n    owner: \"{{ stenographer_user }}\"\n    group: \"{{ stenographer_group }}\"\n    state: directory\n  with_nested:\n    - \"{{ stenographer_monitor_interfaces }}\"\n    - [ 'index', 'packets' ]\n\n- name: Install stenographer service files\n  copy:\n    src: \"{{ item }}\"\n    dest: \"/etc/systemd/system/{{ item }}\"\n    mode: 0644\n    owner: root\n    group: root\n  with_items:\n    - stenographer.service\n    - stenographer@.service\n\n- name: Generate stenographer keys\n  command: >\n    /usr/bin/stenokeys.sh {{ stenographer_user }} {{ stenographer_group }}\n  args:\n    creates: /etc/stenographer/certs/client_key.pem\n\n- name: Configure Stenographer service\n  service:\n    name: stenographer\n    enabled: \"{{ enable_stenographer }}\"\n  notify: start stenographer service\n\n- name: configure stenographer per interface\n  service:\n    name: \"stenographer@{{ item }}\"\n    enabled: \"{{ enable_stenographer }}\"\n  with_items: \"{{ stenographer_monitor_interfaces }}\"\n  notify: start stenographer per interface\n...\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "239d42325420fae8689927564775df05afc707df", "filename": "ops/playbooks/roles/ucp/defaults/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n\nucp_role_join_delay: 10\n\nucp_role_ports:\n  - 443/tcp\n  - 2376/tcp\n  - 2377/tcp\n  - 4789/tcp\n  - 4789/udp\n  - 7946/tcp\n  - 7946/udp\n  - 12376/tcp\n  - 12379-12387/tcp\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "fc7d9e4a9afe4c900a7f786ac20b396266abcf2b", "filename": "roles/config-hostname/tasks/set-hostname.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Setting Default Hostname if new one is not supplied\n  set_fact:\n    hostname: \"{{ hostname | default(inventory_hostname_short) }}\"\n\n- name: Setting Hostname Related Facts\n  set_fact:\n    hostname: \"{{ hostname }}.{{ dns_domain }}\"\n  when:\n  - dns_domain is defined\n  - dns_domain|trim != ''\n\n- name: Setting hostname (FQDN)\n  hostname:\n    name: \"{{ hostname }}\"\n\n- name: Check for cloud.cfg\n  stat:\n    path: \"/etc/cloud/cloud.cfg\"\n  register: cloud_cfg\n\n- name: Prevent cloud-init updates of hostname/fqdn (if applicable)\n  lineinfile:\n    dest: /etc/cloud/cloud.cfg\n    state: present\n    regexp: \"{{ item.regexp }}\"\n    line: \"{{ item.line }}\"\n  with_items:\n  - { regexp: '^ - set_hostname', line: '# - set_hostname' }\n  - { regexp: '^ - update_hostname', line: '# - update_hostname' }\n  when:\n  - cloud_cfg.stat.exists\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "d9f1f611fbe1cde3d5d45db9cf46ea2b619d3462", "filename": "reference-architecture/vmware-ansible/playbooks/node-setup.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- include: /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-node/scaleup.yml\n  vars:\n    debug_level: 2\n    openshift_debug_level: \"{{ debug_level }}\"\n    openshift_node_debug_level: \"{{ node_debug_level | default(debug_level, true) }}\"\n    osm_controller_args:\n    osm_api_server_args:\n    openshift_node_kubelet_args:\n      cloud-provider:\n      - \"vsphere\"\n      cloud-config:\n      - \"/etc/vsphere/vsphere.conf\"\n      node-labels:\n      - \"role={{ openshift_node_labels.role }}\"\n    openshift_master_debug_level: \"{{ master_debug_level | default(debug_level, true) }}\"\n    openshift_master_access_token_max_seconds: 2419200\n    openshift_master_api_port: \"{{ console_port }}\"\n    openshift_master_console_port: \"{{ console_port }}\"\n    osm_cluster_network_cidr: 172.16.0.0/16\n    openshift_registry_selector: \"role=infra\"\n    openshift_router_selector: \"role=infra\"\n    openshift_node_local_quota_per_fsgroup: 512Mi\n    openshift_master_cluster_method: native\n    openshift_cloudprovider_kind: vsphere\n    os_sdn_network_plugin_name: \"{{ openshift_sdn }}\"\n    deployment_type:\n    load_balancer_hostname:\n    openshift_master_cluster_hostname: \"{{ load_balancer_hostname }}\"\n    openshift_master_cluster_public_hostname: \"{{ load_balancer_hostname }}\"\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "a08e2342a6dc67efa22d8283ab24387cd304ce1f", "filename": "roles/dns_adblocking/tasks/freebsd.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: FreeBSD / HardenedBSD | Enable dnsmasq\n  lineinfile: dest=/etc/rc.conf regexp=^dnsmasq_enable= line='dnsmasq_enable=\"YES\"'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "d73cb036daf2fe107132f0b51857c9ce365c65ae", "filename": "roles/dns/manage-dns-zones/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: dns-servers\n  roles:\n    - dns/manage-dns-zones\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "3fb0ab1b666b26e9f74250d4cdeddd82d88a47e0", "filename": "playbooks/subscribe-host.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# Example run (localhost needed to source username/password with \"prep.yml\") \n# > ansible-playbook -i <inventory> subscribe-host.yml -l \"myhosts,localhost\" \n\n- import_playbook: \"prep.yml\"\n- import_playbook: \"rhsm.yml\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "a951d6221318a23ffcb0f80c3f90a74d9a4356fa", "filename": "reference-architecture/vmware-ansible/playbooks/roles/vmware-guest-setup/vars/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\nlocale: en_US.UTF-8\ntimezone: UTC\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "cfe91a61eaf1dc1efd9bf5be7ab337a68f2f9f55", "filename": "reference-architecture/vmware-ansible/playbooks/openshift-validate.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: yes\n  vars_files:\n  - vars/main.yaml\n  pre_tasks:\n  - name: set fact\n    set_fact:\n      openshift_master_cluster_public_hostname: \"{{ openshift_master_cluster_public_hostname }}\"\n  - name: set fact\n    set_fact:\n      openshift_master_cluster_hostname: \"{{ openshift_master_cluster_hostname }}\"\n  roles:\n  # Group systems\n  - instance-groups\n\n- include: ../../../playbooks/post-validation.yaml\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "b8d0286591e20e656875ce7c2b1637a81cec398a", "filename": "playbooks/roles/suricata/tasks/all.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- import_tasks: install.yml\n- import_tasks: configure.yml\n...\n"}, {"commit_sha": "b2591b9333f6e7e70f6b9d99e55356b30d7e173c", "sha": "35ee2c00de43b592a6b6bc4d86a41cb09e9b55a6", "filename": "meta/main.yml", "repository": "inkatze/wildfly", "decoded_content": "---\n\ngalaxy_info:\n  author: Juan Diego Romero Gonz\u00e1lez\n  description: Installs Wildfly's application runtime\n  license: BSD\n  min_ansible_version: 1.9\n  platforms:\n  - name: EL\n    versions:\n    - 7\n  categories:\n  - development\n  - web\ndependencies: []\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c98d8c8deab38b822bbf189616e6cc03e0ba8853", "filename": "playbooks/provision-bastion/bastion.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- import_playbook: ../osp/manage-user-network.yml\n  when:\n  - hosting_infrastructure == 'openstack'\n\n- import_playbook: ../osp/provision-osp-instance.yml\n  when:\n  - hosting_infrastructure == 'openstack'\n\n\n- import_playbook: install.yml\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "3ace0b7c4dbeb3f49b8bfdb29ab734276173a3ca", "filename": "playbooks/provision-idm-server/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_playbook: ../prep.yml\n  when:\n  - rhsm_register|default(False)\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../osp/manage-user-network.yml\n  when:\n  - hosting_infrastructure == 'openstack'\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../osp/provision-osp-instance.yml\n  when:\n  - hosting_infrastructure == 'openstack'\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../rhsm.yml\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: setup-idm-dns.yml\n  tags:\n  - 'never'\n  - 'install'\n\n- hosts: idm-server\n  roles:\n  - role: update-host\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: configure-idm-server.yml\n  tags:\n  - 'always'\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "da6ca7309704689134952c321db1345ee0f42343", "filename": "roles/ovirt-iso-uploader-conf/defaults/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_iso_uploader_conf: '/etc/ovirt-engine/isouploader.conf'\novirt_iso_uploader_user: 'admin@internal'\novirt_iso_uploader_password: '123456'\novirt_iso_uploader_engine: \"\"\novirt_iso_uploader_cert_file: \"\"\novirt_iso_uploader_iso_domain: \"\"\novirt_iso_uploader_nfs_server: \"\"\novirt_iso_uploader_ssh_user: \"\"\novirt_iso_uploader_ssh_port: \"\"\novirt_iso_uploader_key_file: \"\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "e4493183b6367bd40a3c6020c12e11ec56dbef60", "filename": "ops/playbooks/roles/worker/meta/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\nallow_duplicates: no\ndependencies:\n  - role: hpe.openports\n    hpe_openports_ports: \"{{ worker_role_ports }}\"\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "8964faa18cce732483280962a948f838e675f94a", "filename": "roles/vpn/tasks/freebsd.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: FreeBSD / HardenedBSD | Get the existing kernel parameters\n  command: sysctl -b kern.conftxt\n  register: kern_conftxt\n  when: rebuild_kernel is defined and rebuild_kernel == \"true\"\n\n- name: FreeBSD / HardenedBSD | Set the rebuild_needed fact\n  set_fact:\n    rebuild_needed: true\n  when: item not in kern_conftxt.stdout and rebuild_kernel is defined and rebuild_kernel == \"true\"\n  with_items:\n    - \"IPSEC\"\n    - \"IPSEC_NAT_T\"\n    - \"crypto\"\n\n- name: FreeBSD / HardenedBSD | Make the kernel config\n  shell: >\n    sysctl -b kern.conftxt > /tmp/IPSEC\n  when: rebuild_needed is defined and rebuild_needed == true\n\n- name: FreeBSD / HardenedBSD | Ensure the all options are enabled\n  lineinfile:\n    dest: /tmp/IPSEC\n    line: \"{{ item }}\"\n    insertbefore: BOF\n  with_items:\n    - \"options\tIPSEC\"\n    - \"options IPSEC_NAT_T\"\n    - \"device\tcrypto\"\n  when: rebuild_needed is defined and rebuild_needed == true\n\n- name: HardenedBSD | Determine the sources\n  set_fact:\n    sources_repo: https://github.com/HardenedBSD/hardenedBSD.git\n    sources_version: \"hardened/{{ ansible_distribution_release.split('.')[0] }}-stable/master\"\n  when: \"'Hardened' in ansible_distribution_version\"\n\n- name: FreeBSD | Determine the sources\n  set_fact:\n    sources_repo: https://github.com/freebsd/freebsd.git\n    sources_version: \"stable/{{ ansible_distribution_major_version }}\"\n  when: \"'Hardened' not in ansible_distribution_version\"\n\n- name: FreeBSD / HardenedBSD | Increase the git postBuffer size\n  git_config:\n    name: http.postBuffer\n    scope: global\n    value: 1048576000\n\n- block:\n    - name: FreeBSD / HardenedBSD | Fetching the sources...\n      git:\n        repo: \"{{ sources_repo }}\"\n        dest: /usr/krnl_src\n        version: \"{{ sources_version }}\"\n        accept_hostkey: true\n      async: 1000\n      poll: 0\n      register: fetching_sources\n\n    - name: FreeBSD / HardenedBSD | Fetching the sources...\n      async_status: jid={{ fetching_sources.ansible_job_id }}\n      when: rebuild_needed is defined and rebuild_needed == true\n      register: result\n      until: result.finished\n      retries: 600\n      delay: 30\n  rescue:\n    - debug: var=fetching_sources\n\n    - fail:\n        msg: \"Something went wrong. Check the debug output above.\"\n\n- block:\n    - name: FreeBSD / HardenedBSD | The kernel is being built...\n      shell: >\n          mv /tmp/IPSEC /usr/krnl_src/sys/{{ ansible_architecture }}/conf &&\n          make buildkernel KERNCONF=IPSEC &&\n          make installkernel KERNCONF=IPSEC\n      args:\n        chdir: /usr/krnl_src\n        executable: /usr/local/bin/bash\n      when: rebuild_needed is defined and rebuild_needed == true\n      async: 1000\n      poll: 0\n      register: building_kernel\n\n    - name: FreeBSD / HardenedBSD | The kernel is being built...\n      async_status: jid={{ building_kernel.ansible_job_id }}\n      when: rebuild_needed is defined and rebuild_needed == true\n      register: result\n      until: result.finished\n      retries: 600\n      delay: 30\n  rescue:\n    - debug: var=building_kernel\n\n    - fail:\n        msg: \"Something went wrong. Check the debug output above.\"\n\n- name: FreeBSD / HardenedBSD | Reboot\n  shell: >\n    sleep 2 && shutdown -r now\n  args:\n    executable: /usr/local/bin/bash\n  when: rebuild_needed is defined and rebuild_needed == true\n  async: 1\n  poll: 0\n  ignore_errors: true\n\n- name: FreeBSD / HardenedBSD | Enable strongswan\n  lineinfile: dest=/etc/rc.conf regexp=^strongswan_enable= line='strongswan_enable=\"YES\"'\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "f9433a9c086c1d9e8aa0786824cccb346eba0b9a", "filename": "roles/mesos/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "# used for leader election amongst masters\n- name: Set ZooKeeper URL\n  template:\n    src: zk.j2\n    dest: /etc/mesos/zk\n    mode: 0644\n  sudo: yes\n\n# Tasks for Master nodes\n- name: Set Mesos Master ip\n  copy:\n    content: \"{{ mesos_ip }}\"\n    dest: /etc/mesos-master/ip\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart mesos master\n  when: mesos_install_mode == \"master\"\n\n- name: Set Mesos Master hostname\n  copy:\n    content: \"{{ mesos_hostname }}\"\n    dest: /etc/mesos-master/hostname\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart mesos master\n  when: mesos_install_mode == \"master\"\n\n  # The Mesos quorum value is based on the number of Mesos Masters. Take the\n  # number of masters, divide by 2, and round-up to nearest integer. For example,\n  # if there are 1 or 2 masters the quorum count is 1. If there are 3 or 4\n  # masters then the quorum count is 2. For 5 or 6 masters it's 3 and so on.\n- name: Set Mesos Master quorum count\n  template:\n    src: quorum.j2\n    dest: /etc/mesos-master/quorum\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart mesos master\n  when: mesos_install_mode == \"master\"\n\n- name: Set Mesos Master Cluster name\n  copy:\n    content: \"{{ mesos_cluster_name }}\"\n    dest: /etc/mesos-master/cluster\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart mesos master\n  when: mesos_install_mode == \"master\"\n\n- name: Set Mesos Master consul service definition\n  sudo: yes\n  template:\n    src: mesos-master-consul.j2\n    dest: \"{{ consul_dir }}/mesos-master.json\"\n  notify:\n    - restart consul\n  when: mesos_install_mode == \"master\"\n\n- name: remove mesos-master override\n  file:\n    path: /etc/init/mesos-master.override\n    state: absent\n  notify:\n    - restart mesos master\n  when: mesos_install_mode == \"master\"\n\n- name: start mesos-master (and enable it at boot)\n  service:\n    name: mesos-master\n    state: started\n    enabled: yes\n  when: mesos_install_mode == \"master\"\n\n# Tasks for Slave nodes\n- name: remove mesos-slave override\n  file:\n    path: /etc/init/mesos-slave.override\n    state: absent\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: Set Mesos Slave hostname\n  copy:\n    content: \"{{ mesos_hostname }}\"\n    dest: /etc/mesos-slave/hostname\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: set executor registration timeout\n  sudo: yes\n  copy:\n    dest: /etc/mesos-slave/executor_registration_timeout\n    content: \"{{ mesos_executor_registration_timeout }}\"\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: set containerizers\n  sudo: yes\n  copy:\n    dest: /etc/mesos-slave/containerizers\n    content: \"{{ mesos_containerizers }}\"\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: set slave resources\n  sudo: yes\n  copy:\n    dest: /etc/mesos-slave/resources\n    content: \"{{ mesos_resources }}\"\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: Set Mesos Slave ip\n  copy:\n    content: \"{{ mesos_ip }}\"\n    dest: /etc/mesos-slave/ip\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: Create Mesos Slave work area\n  file:\n    dest: \"{{ mesos_slave_work_dir }}\"\n    mode: 0755\n    state: directory\n  sudo: yes\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: Set Mesos Slave work area\n  copy:\n    content: \"{{ mesos_slave_work_dir }}\"\n    dest: /etc/mesos-slave/work_dir\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: start mesos-slave (and enable it at boot)\n  service:\n    name: mesos-slave\n    state: started\n    enabled: yes\n  when: mesos_install_mode == \"slave\"\n\n- meta: flush_handlers\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "9d66f76f08c78b4521c941967bc62b27413f1088", "filename": "reference-architecture/gcp/ansible/playbooks/roles/ansible-gcp/templates/gce.ini.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "# Copyright 2013 Google Inc.\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\n# The GCE inventory script has the following dependencies:\n#   1. A valid Google Cloud Platform account with Google Compute Engine\n#      enabled.  See https://cloud.google.com\n#   2. An OAuth2 Service Account flow should be enabled.  This will generate\n#      a private key file that the inventory script will use for API request\n#      authorization.  See https://developers.google.com/accounts/docs/OAuth2\n#   3. Convert the private key from PKCS12 to PEM format\n#      $ openssl pkcs12 -in pkey.pkcs12 -passin pass:notasecret \\\n#      > -nodes -nocerts | openssl rsa -out pkey.pem\n#   4. The libcloud (>=0.13.3) python libray.  See http://libcloud.apache.org\n#\n# (See ansible/test/gce_tests.py comments for full install instructions)\n#\n# Author: Eric Johnson <erjohnso@google.com>\n\n[gce]\n# GCE Service Account configuration information can be stored in the\n# libcloud 'secrets.py' file.  Ideally, the 'secrets.py' file will already\n# exist in your PYTHONPATH and be picked up automatically with an import\n# statement in the inventory script.  However, you can specify an absolute\n# path to the secrets.py file with 'libcloud_secrets' parameter.\n# This option will be deprecated in a future release.\n#libcloud_secrets =\n\n# If you are not going to use a 'secrets.py' file, you can set the necessary\n# authorization parameters here.\ngce_service_account_email_address={{ service_account_id }}\ngce_service_account_pem_file_path={{ credentials_file }}\ngce_project_id={{ gcloud_project }}\ngce_zone={{ gcloud_region }}\n\n# Filter inventory based on on state. Leave undefined to return instances regardless of state.\n# example: Uncomment to only return inventory in the running or provisioning state\n#instance_states = RUNNING,PROVISIONING\n\n\n#[inventory]\n# The 'inventory_ip_type' parameter specifies whether 'ansible_ssh_host' should\n# contain the instance internal or external address. Values may be either\n# 'internal' or 'external'. If 'external' is specified but no external instance\n# address exists, the internal address will be used.\n# The INVENTORY_IP_TYPE environment variable will override this value.\n#inventory_ip_type =\n\n#[cache]\n# directory in which cache should be created\n#cache_path = ~/.ansible/tmp\n\n# The number of seconds a cache file is considered valid. After this many\n# seconds, a new API call will be made, and the cache file will be updated.\n# To disable the cache, set this value to 0\n#cache_max_age = 300\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "792c90d47f9b618ce437ecd62042983b251f11a9", "filename": "roles/config-container-storage-setup/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "docker_dev: /dev/vdb\ndocker_vg: docker-vol\ndocker_data_size: 95%VG\ncontainer_root_lv_name: dockerlv\ncontainer_root_lv_mount_path: /var/lib/docker"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "76aec3419123205f76ec1dcec97774394a761d8c", "filename": "playbooks/deploy-rock.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- hosts: all\n  tags:\n    - common\n  pre_tasks:\n    - name: Include user-override vars\n      include_vars: \"{{ rock_config }}\"\n  become: true\n  roles:\n    - common\n\n- hosts: elasticsearch\n  tags:\n    - elasticsearch\n    - elastic\n    - es_before\n  pre_tasks:\n    - name: Include user-override vars\n      include_vars: \"{{ rock_config }}\"\n  vars:\n    - es_step: \"before\"\n  become: true\n  roles:\n    - role: elasticsearch\n      when: \"rock_services | selectattr('name', 'equalto', 'elasticsearch') | map(attribute='installed')\"\n\n- hosts: elasticsearch\n  tags:\n    - elasticsearch\n    - elastic\n    - es_restart\n  pre_tasks:\n    - name: Include user-override vars\n      include_vars: \"{{ rock_config }}\"\n  vars:\n    - es_step: \"restart\"\n  serial: 1\n  become: true\n  roles:\n    - role: elasticsearch\n      when: \"rock_services | selectattr('name', 'equalto', 'elasticsearch') | map(attribute='installed') and (es_restart is defined and es_restart)\"\n\n- hosts: elasticsearch\n  tags:\n    - elasticsearch\n    - elastic\n    - es_after\n  pre_tasks:\n    - name: Include user-override vars\n      include_vars: \"{{ rock_config }}\"\n  vars:\n    - es_step: \"after\"\n  become: true\n  roles:\n    - role: elasticsearch\n      when: \"rock_services | selectattr('name', 'equalto', 'elasticsearch') | map(attribute='installed')\"\n\n- hosts: zookeeper\n  tags:\n    - zookeeper\n  pre_tasks:\n    - name: Include user-override vars\n      include_vars: \"{{ rock_config }}\"\n  become: true\n  roles:\n    - role: zookeeper\n      when: \"rock_services | selectattr('name', 'equalto', 'zookeeper') | map(attribute='installed')\"\n\n- hosts: kafka\n  tags:\n    - kafka\n  pre_tasks:\n    - name: Include user-override vars\n      include_vars: \"{{ rock_config }}\"\n  become: true\n  roles:\n    - role: kafka\n      when: \"rock_services | selectattr('name', 'equalto', 'kafka') | map(attribute='installed')\"\n\n- hosts: stenographer\n  tags:\n    - docket\n    - stenographer\n  pre_tasks:\n    - name: Include user-override vars\n      include_vars: \"{{ rock_config }}\"\n  become: true\n  roles:\n    - role: stenographer\n      when: \"rock_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='installed')\"\n      stenographer_monitor_interfaces: \"{{ rock_monifs }}\"\n\n- hosts: bro\n  tags:\n    - bro\n    - sensor\n  pre_tasks:\n    - name: Include user-override vars\n      include_vars: \"{{ rock_config }}\"\n  become: true\n  roles:\n    - role: bro\n      when: \"rock_services | selectattr('name', 'equalto', 'bro') | map(attribute='installed')\"\n\n- hosts: suricata\n  tags:\n    - suricata\n    - sensor\n  pre_tasks:\n    - name: Include user-override vars\n      include_vars: \"{{ rock_config }}\"\n  become: true\n  roles:\n    - role: suricata\n      when: \"rock_services | selectattr('name', 'equalto', 'suricata') | map(attribute='installed')\"\n\n- hosts: fsf\n  tags:\n    - fsf\n    - sensor\n  pre_tasks:\n    - name: Include user-override vars\n      include_vars: \"{{ rock_config }}\"\n  become: true\n  roles:\n    - role: fsf\n      when: \"rock_services | selectattr('name', 'equalto', 'fsf') | map(attribute='installed')\"\n\n- hosts:\n    - docket\n    - kibana\n  tags:\n    - docket\n    - kibana\n    - lighttpd\n    - web\n  pre_tasks:\n    - name: Include user-override vars\n      include_vars: \"{{ rock_config }}\"\n  become: true\n  roles:\n    - role: lighttpd\n      when: \"rock_services | selectattr('name', 'equalto', 'lighttpd') | map(attribute='installed')\"\n\n- hosts:\n    - docket\n    - stenographer\n  tags:\n    - docket\n    - stenographer\n  pre_tasks:\n    - name: Include user-override vars\n      include_vars: \"{{ rock_config }}\"\n  become: true\n  roles:\n    - role: docket\n      when: \"rock_services | selectattr('name', 'equalto', 'docket') | map(attribute='installed')\"\n      docket_enable: \"{{ (rock_services | selectattr('name', 'equalto', 'docket') | map(attribute='enabled') | bool }}\"\n\n- hosts: kibana\n  tags:\n    - kibana\n    - elastic\n  pre_tasks:\n    - name: Include user-override vars\n      include_vars: \"{{ rock_config }}\"\n  become: true\n  roles:\n    - role: kibana\n      when: \"rock_services | selectattr('name', 'equalto', 'kibana') | map(attribute='installed')\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "54e83a10bd7835c965c0f58c8e65fa8d366755ef", "filename": "playbooks/openshift/provision-instances.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n# Provision Openstack Instances\n- import_playbook: provision-openstack.yml\n  when: hosting_infrastructure == \"openstack\"\n\n# Provision Openstack Instances\n- import_playbook: provision-aws.yml\n  when: hosting_infrastructure == \"aws\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "43f43786c00cc7ad0b98bd4f956342b4f85fdcf0", "filename": "reference-architecture/vmware-ansible/playbooks/library/rpm_q.py", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "/usr/share/ansible/openshift-ansible/library/rpm_q.py"}, {"commit_sha": "218cdc58f9fe9d7ece7d43e5f100fe9631fde5cc", "sha": "a80e66c27151137a050ec86b14b266ec507b324a", "filename": "tasks/cleanup.yml", "repository": "fubarhouse/ansible-role-golang", "decoded_content": "---\n\n- name: \"Go-Lang | Removing GOROOT\"\n  file:\n    path: \"{{ GOROOT }}\"\n    state: absent\n  failed_when: false\n\n- name: \"Go-Lang | Define shell exports to cleanup\"\n  set_fact:\n    shell_exports:\n    - regex: \"export GOROOT={{ GOROOT }}\"\n      lineinfile: \"export GOROOT={{ GOROOT }}\"\n    - regex: \"export GOPATH={{ GOPATH }}/bin\"\n      lineinfile: \"export GOPATH={{ GOPATH }}/bin\"\n    - regex: \"export PATH=$PATH:{{ GOPATH }}/bin\"\n      lineinfile: \"export PATH=$PATH:{{ GOPATH }}/bin\"\n  when: shell_exports is undefined\n\n- name: \"Go-Lang | Ensure shell profiles are clean\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  lineinfile:\n    dest: \"{{ fubarhouse_user_dir }}/{{ item[0] }}\"\n    regexp: \"{{ item[1].regex }}\"\n    line: \"{{ item[1].lineinfile }}\"\n    state: absent\n  with_nested:\n  - \"{{ shell_profiles }}\"\n  - \"{{ shell_exports_static }}\"\n  ignore_errors: yes\n  when: shell_exports is defined\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "06427981cddec97a1870f01eb2ac69bc41ac9066", "filename": "playbooks/roles/stenographer/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- include_tasks: \"{{ action }}.yml\"\n...\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "3c8c5d870ead6131ed56e07e002756cac436a67a", "filename": "roles/setup-slack/tasks/create_channels.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Create channel\n  uri:\n    url: \"https://slack.com/api/channels.create?token={{ slack_token }}&name={{ channel.name }}\"\n    method: GET\n    status: [200]\n    return_content: yes\n  register: channel_data\n  when: channel.private == False\n\n- name: Create group\n  uri:\n    url: \"https://slack.com/api/groups.create?token={{ slack_token }}&name={{ channel.name }}\"\n    method: GET\n    status: [200]\n    return_content: yes\n  register: channel_data\n  when: channel.private == True\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "6bfadbc568e04d07cadd5dc25d27a27e0b06e098", "filename": "roles/dokuwiki/tasks/install.yml", "repository": "iiab/iiab", "decoded_content": "- name: Download DokuWiki software\n  get_url:\n    url: \"{{ iiab_download_url }}/{{ dokuwiki_version }}.tgz\"\n    dest: \"{{ downloads_dir }}/\"\n    timeout: \"{{ download_timeout }}\"\n  when: internet_available\n\n- name: Copy it to permanent location /library\n  unarchive:\n    src: \"{{ downloads_dir }}/{{ dokuwiki_version }}.tgz\"\n    dest: /library\n    creates: \"/library/{{ dokuwiki_version }}/VERSION\"\n\n- name: Symlink /library/{{ dokuwiki_version }} from /library/dokuwiki\n  #shell: if [ ! -d /library/dokuwiki ]; then ln -sf /library/{{ dokuwiki_version }} /library/dokuwiki; fi\n  #shell: ln -sf /library/{{ dokuwiki_version }} /library/dokuwiki\n  #BOTH LINES ABOVE FAIL TO UPDATE LINK; Ansible approach below works\n  file:\n    path: /library/dokuwiki\n    src: /library/{{ dokuwiki_version }}\n    state: link\n    force: yes\n\n- name: Install config file for DokuWiki in Apache\n  template:\n    src: dokuwiki.conf.j2\n    dest: \"/etc/{{ apache_config_dir }}/dokuwiki.conf\"\n  when: dokuwiki_enabled\n\n- name: Enable the DokuWiki (debuntu)\n  file:\n    src: /etc/apache2/sites-available/dokuwiki.conf\n    dest: /etc/apache2/sites-enabled/dokuwiki.conf\n    state: link\n  when: dokuwiki_enabled and is_debuntu\n\n- name: Disable the DokuWiki (debuntu)\n  file:\n    path: /etc/apache2/sites-enabled/dokuwiki.conf\n    state: absent\n  when: not dokuwiki_enabled and is_debuntu\n\n\n- name: Change permissions on engine directory so Apache can write\n  file:\n    path: \"/library/{{ dokuwiki_version }}\"\n    owner: \"{{ apache_user }}\"\n    mode: 0755\n    state: directory\n    recurse: yes\n\n- name: Restart Apache, so it picks up the new aliases\n  service:\n    name: \"{{ apache_service }}\"\n    state: restarted\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "1e8a4f6b352a3864adce28327ea324a30d121473", "filename": "tasks/create_repo_nuget_proxy_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_nuget_proxy\n    args: \"{{ _nexus_repos_nuget_defaults|combine(item) }}\""}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "32885b5fd081e8816fda07297b9c5bd3e841edac", "filename": "roles/vpn/handlers/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: restart strongswan\n  service: name=strongswan state=restarted\n\n- name: daemon-reload\n  shell: systemctl daemon-reload\n\n- name: restart apparmor\n  service: name=apparmor state=restarted\n\n- name: save iptables\n  shell: service netfilter-persistent save\n\n- name: restart iptables\n  service: name=netfilter-persistent state=restarted\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "d36cae2e3726f1e9760b0b1824e39edda5e96557", "filename": "roles/storage-demo/tasks/main.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- include_tasks: \"{{ action }}.yml\"\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "d67cbde4d7729588934df443d37a359fc2c983dc", "filename": "playbooks/ubuntu.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Ubuntu | Install prerequisites\n  raw: sleep 10 && sudo apt-get update -qq && sudo apt-get install -qq -y python2.7\n\n- name: Ubuntu | Configure defaults\n  raw: sudo update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1\n  tags:\n    - update-alternatives\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "e3e973f4f44af7fab6607e6a47e39f3e30e39156", "filename": "reference-architecture/aws-ansible/playbooks/roles/cloudformation-infra/files/brownfield.json.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "{\n  \"AWSTemplateFormatVersion\": \"2010-09-09\",\n  \"Parameters\": {\n    \"VpcName\": {\n      \"Type\": \"String\",\n      \"Default\": \"ose-on-aws\"\n    },\n    \"S3BucketName\": {\n      \"Type\": \"String\"\n    },\n    \"S3User\": {\n       \"Type\": \"String\"\n    },\n    \"MasterApiPort\": {\n      \"Type\": \"Number\"\n    },\n    \"MasterHealthTarget\": {\n      \"Type\": \"String\"\n    },\n    \"Route53HostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"PublicHostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"MasterClusterPublicHostname\": {\n      \"Type\": \"String\"\n    },\n    \"MasterClusterHostname\": {\n      \"Type\": \"String\"\n    },\n    \"AppWildcardDomain\": {\n      \"Type\": \"String\"\n    },\n    \"KeyName\": {\n      \"Type\": \"AWS::EC2::KeyPair::KeyName\"\n    },\n    \"MasterInstanceType\": {\n      \"Type\": \"String\"\n    },\n    \"AmiId\": {\n      \"Type\": \"AWS::EC2::Image::Id\"\n    },\n    \"BastionInstanceType\": {\n      \"Type\": \"String\"\n    },\n    \"BastionRootVolSize\": {\n      \"Type\": \"String\"\n    },\n    \"BastionRootVolType\": {\n      \"Type\": \"String\"\n    },\n    \"BastionUserData\": {\n      \"Type\": \"String\"\n    },\n    \"MasterRootVolSize\": {\n      \"Type\": \"String\"\n    },\n    \"MasterDockerVolSize\": {\n      \"Type\": \"String\"\n    },\n    \"MasterEtcdVolSize\": {\n      \"Type\": \"String\"\n    },\n    \"MasterEmptyVolSize\": {\n      \"Type\": \"String\"\n    },\n    \"MasterEtcdVolType\": {\n      \"Type\": \"String\"\n    },\n    \"MasterEmptyVolType\": {\n      \"Type\": \"String\"\n    },\n    \"MasterUserData\": {\n      \"Type\": \"String\"\n    },\n    \"MasterDockerVolType\": {\n      \"Type\": \"String\"\n    },\n    \"MasterRootVolType\": {\n      \"Type\": \"String\"\n    },\n    \"InfraInstanceType\": {\n      \"Type\": \"String\"\n    },\n    \"InfraRootVolSize\": {\n      \"Type\": \"String\"\n    },\n    \"InfraDockerVolSize\": {\n      \"Type\": \"String\"\n    },\n    \"InfraDockerVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"InfraRootVolType\": {\n      \"Type\": \"String\"\n    },\n    \"AppNodeInstanceType\": {\n      \"Type\": \"String\"\n    },\n    \"NodeRootVolSize\": {\n      \"Type\": \"String\"\n    },\n    \"NodeUserData\": {\n      \"Type\": \"String\"\n    },\n    \"NodeDockerVolSize\": {\n      \"Type\": \"String\"\n    },\n    \"NodeDockerVolType\": {\n      \"Type\": \"String\"\n    },\n    \"NodeEmptyVolSize\": {\n      \"Type\": \"String\"\n    },\n    \"NodeEmptyVolType\": {\n      \"Type\": \"String\"\n    },\n    \"NodeRootVolType\": {\n      \"Type\": \"String\"\n    },\n    \"Vpc\": {\n      \"Type\": \"String\"\n    },\n    \"PublicSubnet1\": {\n      \"Type\": \"String\"\n    },\n    \"PublicSubnet2\": {\n      \"Type\": \"String\"\n    },\n    \"PublicSubnet3\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet1\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet2\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet3\": {\n      \"Type\": \"String\"\n    }\n  },\n  \"Resources\": {\n    \"BastionSg\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"bastion-sg\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"bastion_sg\"} ],\n        \"SecurityGroupIngress\": [\n          {\n            \"IpProtocol\": \"tcp\",\n            \"FromPort\": \"22\",\n            \"ToPort\": \"22\",\n            \"CidrIp\": \"0.0.0.0/0\"\n          }\n        ]\n      }\n    },\n    \"EtcdSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"etcd\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_etcd_sg\"} ]\n      }\n    },\n    \"InfraElbSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Infra Load Balancer\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_router_sg\"} ],\n        \"SecurityGroupIngress\": [\n          {\n            \"IpProtocol\": \"tcp\",\n            \"FromPort\": \"80\",\n            \"ToPort\": \"80\",\n            \"CidrIp\": \"0.0.0.0/0\"\n          },\n          {\n            \"IpProtocol\": \"tcp\",\n            \"FromPort\": \"443\",\n            \"ToPort\": \"443\",\n            \"CidrIp\": \"0.0.0.0/0\"\n          }\n        ]\n      }\n    },\n    \"MasterExtElbSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Master External Load Balancer\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_elb_master_sg\"} ],\n        \"SecurityGroupIngress\": [\n          {\n            \"IpProtocol\": \"tcp\",\n            \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n            \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n            \"CidrIp\": \"0.0.0.0/0\"\n          }\n        ]\n      }\n    },\n    \"MasterIntElbSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Master Internal Load Balancer\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_internal_elb_master_sg\"} ]\n      }\n    },\n    \"InfraSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Infra\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_infra_node_sg\"} ]\n      }\n    },\n    \"NodeSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Node\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_node_sg\"}, {\"Key\": \"KubernetesCluster\", \"Value\": { \"Ref\": \"AWS::StackName\" }}]\n      }\n    },\n    \"MasterSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Master\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_master_sg\"} ]\n      }\n    },\n    \"InfraElbEgressHTTP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"80\",\n        \"ToPort\": \"80\",\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] }\n      }\n    },\n    \"InfraElbEgressHTTPS\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"443\",\n        \"ToPort\": \"443\",\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] }\n      }\n    },\n    \"ElasticApi\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"9200\",\n        \"ToPort\": \"9200\",\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] }\n      }\n    },\n    \"ElasticCluster\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"9300\",\n        \"ToPort\": \"9300\",\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterExtElbEgressAPI\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterExtElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIntElbEgressAPI\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterIntElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIntElbIngressMasters\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterIntElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIntElbIngressNodes\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterIntElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"InfraIngressHTTP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"80\",\n        \"ToPort\": \"80\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] }\n      }\n    },\n    \"InfraIngressHTTPS\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"443\",\n        \"ToPort\": \"443\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterDaemon\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24007\",\n        \"ToPort\": \"24007\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterManagement\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24008\",\n        \"ToPort\": \"24008\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterSsh\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"2222\",\n        \"ToPort\": \"2222\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterNfs\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"49152\",\n        \"ToPort\": \"49664\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"NodeIngressMasterKubelet\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"10250\",\n        \"ToPort\": \"10250\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"NodeIngressNodeKubelet\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"10250\",\n        \"ToPort\": \"10250\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"NodeIngressNodeVXLAN\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"udp\",\n        \"FromPort\": \"4789\",\n        \"ToPort\": \"4789\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"NodeIngressSsh\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"22\",\n        \"ToPort\": \"22\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"BastionSg\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressIntLB\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterIntElbSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressExtLB\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterExtElbSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressNodesDNSUDP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"udp\",\n        \"FromPort\": \"8053\",\n        \"ToPort\": \"8053\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressNodesDNSTCP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"8053\",\n        \"ToPort\": \"8053\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressNodesAPITCP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"LoggingTCP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24224\",\n        \"ToPort\": \"24224\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"LoggingUDP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"udp\",\n        \"FromPort\": \"24224\",\n        \"ToPort\": \"24224\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressMastersAPITCP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"EtcdIngressEtcd\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"EtcdSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"2379\",\n        \"ToPort\": \"2379\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"EtcdSG\", \"GroupId\" ] }\n      }\n    },\n    \"EtcdIngressMasters\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"EtcdSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"2379\",\n        \"ToPort\": \"2379\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"EtcdIngressEtcdPeer\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"EtcdSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"2380\",\n        \"ToPort\": \"2380\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"EtcdSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIntElb\": {\n      \"Type\": \"AWS::ElasticLoadBalancing::LoadBalancer\",\n      \"Properties\": {\n        \"CrossZone\": \"true\",\n        \"ConnectionSettings\": {\"IdleTimeout\" : 300},\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_internal_master_elb\"} ],\n        \"HealthCheck\": {\n          \"HealthyThreshold\" : \"2\",\n          \"Interval\" : \"5\",\n          \"Target\" : { \"Ref\": \"MasterHealthTarget\" },\n          \"Timeout\" : \"2\",\n          \"UnhealthyThreshold\" : \"2\"\n        },\n        \"Listeners\":[\n          {\n            \"InstancePort\": { \"Ref\" : \"MasterApiPort\" },\n            \"InstanceProtocol\": \"TCP\",\n            \"LoadBalancerPort\": { \"Ref\" : \"MasterApiPort\" },\n            \"Protocol\": \"TCP\"\n          }\n        ],\n        \"Scheme\": \"internal\",\n        \"SecurityGroups\": [ { \"Ref\": \"MasterIntElbSG\" } ],\n        \"Subnets\": [\n          {\"Ref\": \"PrivateSubnet1\"},\n          {\"Ref\": \"PrivateSubnet2\"},\n          {\"Ref\": \"PrivateSubnet3\"}\n            ],\n        \"Instances\": [\n          {\"Ref\": \"Master01\"},\n          {\"Ref\": \"Master02\"},\n          {\"Ref\": \"Master03\"}\n            ]\n        }\n    },\n    \"MasterExtElb\": {\n      \"Type\": \"AWS::ElasticLoadBalancing::LoadBalancer\",\n      \"Properties\": {\n        \"CrossZone\": \"true\",\n        \"ConnectionSettings\": {\"IdleTimeout\" : 300},\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_master_elb\"} ],\n        \"HealthCheck\": {\n          \"HealthyThreshold\" : \"2\",\n          \"Interval\" : \"5\",\n          \"Target\" : { \"Ref\": \"MasterHealthTarget\" },\n          \"Timeout\" : \"2\",\n          \"UnhealthyThreshold\" : \"2\"\n        },\n        \"Listeners\":[\n          {\n            \"InstancePort\": { \"Ref\" : \"MasterApiPort\" },\n            \"InstanceProtocol\": \"TCP\",\n            \"LoadBalancerPort\": { \"Ref\" : \"MasterApiPort\" },\n            \"Protocol\": \"TCP\"\n          }\n        ],\n        \"SecurityGroups\": [{\"Ref\": \"MasterExtElbSG\"}],\n        \"Subnets\": [\n          {\"Ref\": \"PublicSubnet1\"},\n          {\"Ref\": \"PublicSubnet2\"},\n          {\"Ref\": \"PublicSubnet3\"}\n            ],\n        \"Instances\": [\n          {\"Ref\": \"Master01\"},\n          {\"Ref\": \"Master02\"},\n          {\"Ref\": \"Master03\"}\n            ]\n      }\n    },\n    \"InfraElb\": {\n      \"Type\": \"AWS::ElasticLoadBalancing::LoadBalancer\",\n      \"Properties\": {\n        \"CrossZone\": \"true\",\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_router_elb\"} ],\n        \"HealthCheck\": {\n          \"HealthyThreshold\" : \"2\",\n          \"Interval\" : \"5\",\n          \"Target\" : \"TCP:443\",\n          \"Timeout\" : \"2\",\n          \"UnhealthyThreshold\" : \"2\"\n        },\n        \"Listeners\":[\n          {\n            \"InstancePort\": \"443\",\n            \"InstanceProtocol\": \"TCP\",\n            \"LoadBalancerPort\": \"443\",\n            \"Protocol\": \"TCP\"\n          },\n          {\n            \"InstancePort\": \"80\",\n            \"InstanceProtocol\": \"TCP\",\n            \"LoadBalancerPort\": \"80\",\n            \"Protocol\": \"TCP\"\n          }\n        ],\n        \"SecurityGroups\": [ { \"Ref\": \"InfraElbSG\" } ],\n        \"Subnets\": [\n          {\"Ref\": \"PublicSubnet1\"},\n          {\"Ref\": \"PublicSubnet2\"},\n          {\"Ref\": \"PublicSubnet3\"}\n        \t],\n        \"Instances\": [\n          {\"Ref\": \"InfraNode01\"},\n          {\"Ref\": \"InfraNode02\"},\n          {\"Ref\": \"InfraNode03\"}\n            ]\n      }\n    },\n    \"NodePolicy\": {\n      \"Type\": \"AWS::IAM::Role\",\n      \"Properties\": {\n        \"AssumeRolePolicyDocument\": {\n          \"Version\": \"2012-10-17\",\n          \"Statement\": [\n            {\n              \"Effect\": \"Allow\",\n              \"Principal\": { \"Service\": [ \"ec2.amazonaws.com\" ] },\n              \"Action\": [ \"sts:AssumeRole\" ]\n            }\n          ]\n        },\n        \"Policies\": [\n          {\n            \"PolicyName\": \"node-describe\",\n            \"PolicyDocument\": {\n              \"Version\" : \"2012-10-17\",\n              \"Statement\": [\n                {\n                  \"Effect\": \"Allow\",\n                  \"Action\": [\n                     \"ec2:DescribeInstance*\"\n                  ], \n                  \"Resource\": \"*\"\n                }\n              ]\n            }\n          }\n        ]\n      }\n    },\n    \"MasterPolicy\": {\n      \"Type\": \"AWS::IAM::Role\",\n      \"Properties\": {\n        \"AssumeRolePolicyDocument\": {\n          \"Version\": \"2012-10-17\",\n          \"Statement\": [\n            {\n              \"Effect\": \"Allow\",\n              \"Principal\": { \"Service\": [ \"ec2.amazonaws.com\" ] },\n              \"Action\": [ \"sts:AssumeRole\" ]\n            }\n          ]\n        },\n        \"Policies\": [\n          {\n            \"PolicyName\": \"master-ec2-all\",\n            \"PolicyDocument\": {\n              \"Version\" : \"2012-10-17\",\n              \"Statement\": [\n                {\n                  \"Effect\": \"Allow\",\n                  \"Action\": [\n                     \"ec2:DescribeVolume*\",\n                     \"ec2:CreateVolume\",\n                     \"ec2:CreateTags\",\n                     \"ec2:DescribeInstance*\",\n                     \"ec2:AttachVolume\",\n                     \"ec2:DetachVolume\",\n                     \"ec2:DeleteVolume\",\n                     \"ec2:DescribeSubnets\",\n                     \"ec2:CreateSecurityGroup\",\n                     \"ec2:DescribeSecurityGroups\",\n                     \"elasticloadbalancing:DescribeTags\",\n                     \"elasticloadbalancing:CreateLoadBalancerListeners\",\n                     \"ec2:DescribeRouteTables\",\n                     \"elasticloadbalancing:ConfigureHealthCheck\",\n                     \"ec2:AuthorizeSecurityGroupIngress\",\n                     \"elasticloadbalancing:DeleteLoadBalancerListeners\",\n                     \"elasticloadbalancing:RegisterInstancesWithLoadBalancer\",\n                     \"elasticloadbalancing:DescribeLoadBalancers\",\n                     \"elasticloadbalancing:CreateLoadBalancer\",\n                     \"elasticloadbalancing:DeleteLoadBalancer\",\n                     \"elasticloadbalancing:ModifyLoadBalancerAttributes\",\n                     \"elasticloadbalancing:DescribeLoadBalancerAttributes\"\n                  ],\n                  \"Resource\": \"*\"\n                }\n              ]\n            }\n          }\n        ]\n      }\n    },\n    \"MasterInstanceProfile\": {\n      \"Type\": \"AWS::IAM::InstanceProfile\",\n      \"Properties\": {\n        \"Roles\": [ { \"Ref\": \"MasterPolicy\" } ]\n      }\n    },\n    \"NodeInstanceProfile\": {\n      \"Type\": \"AWS::IAM::InstanceProfile\",\n      \"Properties\": {\n        \"Roles\": [ { \"Ref\": \"NodePolicy\" } ]\n      }\n    },\n    \"RegistryBucket\": {\n    \"Type\": \"AWS::S3::Bucket\",\n    \"Properties\" : {\n       \"BucketName\": { \"Ref\": \"S3BucketName\"}\n                   }\n     },\n    \"Route53Records\": {\n      \"Type\": \"AWS::Route53::RecordSetGroup\",\n      \"DependsOn\": [\n        \"InfraElb\",\n        \"MasterIntElb\",\n        \"Bastion\",\n        \"Master01\",\n        \"Master02\",\n        \"Master03\",\n        \"MasterExtElb\"\n      ],\n      \"Properties\": {\n        \"HostedZoneName\": { \"Ref\": \"Route53HostedZone\" },\n        \"RecordSets\": [\n          {\n            \"Name\":  { \"Ref\": \"MasterClusterPublicHostname\" },\n            \"Type\": \"A\",\n            \"AliasTarget\": {\n                \"HostedZoneId\": { \"Fn::GetAtt\" : [\"MasterExtElb\", \"CanonicalHostedZoneNameID\"] },\n                \"DNSName\": { \"Fn::GetAtt\" : [\"MasterExtElb\",\"CanonicalHostedZoneName\"] }\n            }\n          },\n          {\n            \"Name\": { \"Ref\": \"MasterClusterHostname\" },\n            \"Type\": \"A\",\n            \"AliasTarget\": {\n                \"HostedZoneId\": { \"Fn::GetAtt\" : [\"MasterIntElb\", \"CanonicalHostedZoneNameID\"] },\n                \"DNSName\": { \"Fn::GetAtt\" : [\"MasterIntElb\",\"DNSName\"] }\n            }\n          },\n          {\n            \"Name\": { \"Ref\": \"AppWildcardDomain\" },\n            \"Type\": \"A\",\n            \"AliasTarget\": {\n                \"HostedZoneId\": { \"Fn::GetAtt\" : [\"InfraElb\", \"CanonicalHostedZoneNameID\"] },\n                \"DNSName\": { \"Fn::GetAtt\" : [\"InfraElb\",\"CanonicalHostedZoneName\"] }\n            }\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-master01\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"Master01\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-master02\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"Master02\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-master03\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"Master03\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-infra-node01\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"InfraNode01\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-infra-node02\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"InfraNode02\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-infra-node03\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"InfraNode03\", \"PrivateIp\"] }]\n          },\n{% for idx in range(1, app_node_count|int + 1) %}\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-app-node{{ '%02d' % idx }}\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n             \"TTL\": \"300\",\n            \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"AppNode{{ '%02d' % idx }}\", \"PrivateIp\"] }]\n          },\n{% endfor %}\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"bastion\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n                        \"TTL\": \"300\",\n                    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"Bastion\", \"PublicIp\"] }]\n          }\n        ]\n      }\n    },\n    \"Bastion\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"BastionUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t      \"InstanceType\": {\"Ref\": \"BastionInstanceType\"},\n\t\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"BastionSg\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PublicSubnet1\"},\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"bastion\",{\"Ref\": \"PublicHostedZone\"}]]}\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n\t\t\t          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"BastionRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"BastionRootVolType\"}\n                   }\n            }\n          ]\n       }\n    },\n    \"BastionEip\" : {\n       \"Type\" : \"AWS::EC2::EIP\",\n        \"Properties\" : {\n        \"Domain\" : \"vpc\"\n           }\n       },\n    \"BastionEipAssoc\" : {\n      \"DependsOn\": [\"Bastion\"],\n      \"Type\" : \"AWS::EC2::EIPAssociation\",\n      \"Properties\" : {\n        \"InstanceId\" : {\"Ref\" : \"Bastion\"},\n        \"AllocationId\" : { \"Fn::GetAtt\" : [\"BastionEip\", \"AllocationId\"]}\n        }\n       },\n    \"Master01\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"MasterUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t      \"InstanceType\": {\"Ref\": \"MasterInstanceType\"},\n          \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"MasterSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"EtcdSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet1\"},\n          \"IamInstanceProfile\": { \"Ref\": \"MasterInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-master01\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"master\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"false\",\n              \"VolumeSize\": {\"Ref\": \"MasterEtcdVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterEtcdVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdd\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterEmptyVolType\"}\n            }\n          }\n         ]\n     }\n  },\n    \"Master02\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"MasterUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t      \"InstanceType\": {\"Ref\": \"MasterInstanceType\"},\n          \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"MasterSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"EtcdSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet2\"},\n          \"IamInstanceProfile\": { \"Ref\": \"MasterInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-master02\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"master\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"false\",\n              \"VolumeSize\": {\"Ref\": \"MasterEtcdVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterEtcdVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdd\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterEmptyVolType\"}\n            }\n          }\n         ]\n     }\n   },\n    \"Master03\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"MasterUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t      \"InstanceType\": {\"Ref\": \"MasterInstanceType\"},\n          \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"MasterSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"EtcdSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet3\"},\n          \"IamInstanceProfile\": { \"Ref\": \"MasterInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-master03\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"master\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"false\",\n              \"VolumeSize\": {\"Ref\": \"MasterEtcdVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterEtcdVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdd\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterEmptyVolType\"}\n            }\n          }\n         ]\n     }\n   },\n    \"InfraNode01\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t      \"InstanceType\": {\"Ref\": \"InfraInstanceType\"},\n\t\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"InfraSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet1\"},\n\t\t  \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-infra-node01\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"infra\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          }\n         ]\n     }\n    },\n    \"InfraNode02\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t      \"InstanceType\": {\"Ref\": \"InfraInstanceType\"},\n\t\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"InfraSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet2\"},\n\t\t  \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-infra-node02\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"infra\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          }\n         ]\n     }\n },\n    \"InfraNode03\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InfraInstanceType\"},\n\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"InfraSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet3\"},\n\t  \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-infra-node03\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"infra\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          }\n         ]\n     }\n },\n{% set rotator = 1 %}\n{% for idx in range(1, app_node_count|int + 1) %}\n{% if rotator == 4 %}\n  {% set rotator = 1 %}\n{% endif %}\n    \"AppNode{{ '%02d' % idx }}\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"DependsOn\": [\"NodeInstanceProfile\"],\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n          \"InstanceType\": {\"Ref\": \"AppNodeInstanceType\"},\n          \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet{{ rotator }}\"},\n          \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n             \"Value\": {\"Fn::Join\": [\".\", [\"ose-app-node{{ '%02d' % idx }}\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"app\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          }\n         ]\n     }\n   },\n{% set rotator = rotator + 1 %}\n{% endfor %}\n    \"S3UserName\" : {\n      \"Type\" : \"AWS::IAM::User\",\n      \"Properties\" : {\n        \"Path\" : \"/\",\n        \"UserName\": { \"Ref\": \"S3User\" },\n        \"Policies\" : [ {\n          \"PolicyName\" : \"accessalls3\",\n          \"PolicyDocument\" : {\n            \"Version\": \"2012-10-17\",\n            \"Statement\" : [ {\n              \"Effect\" : \"Allow\",\n              \"Action\" : [ \"s3:*\" ],\n              \"Resource\": \"*\"\n            } ]\n          }\n        } ]\n      }\n    },\n    \"CFNKeys\" : {\n      \"Type\" : \"AWS::IAM::AccessKey\",\n      \"Properties\" : {\n        \"UserName\" : { \"Ref\": \"S3UserName\" }\n      }\n    }\n  },\n  \"Outputs\" : {\n    \"StackVpc\" : {\n      \"Value\" : { \"Ref\" : \"Vpc\" },\n      \"Description\" : \"VPC that was created\"\n    },\n    \"PrivateSubnet1\" : {\n      \"Value\" : { \"Ref\" : \"PrivateSubnet1\" },\n      \"Description\" : \"Private Subnet 1\"\n    },\n    \"PrivateSubnet2\" : {\n      \"Value\" : { \"Ref\" : \"PrivateSubnet2\" },\n      \"Description\" : \"Private Subnet 2\"\n    },\n    \"PrivateSubnet3\" : {\n      \"Value\" : { \"Ref\" : \"PrivateSubnet3\" },\n      \"Description\" : \"Private Subnet 3\"\n    },\n    \"NodeSGId\" : {\n      \"Value\" : { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ]},\n      \"Description\" : \"Node SG id\"\n    },\n    \"InfraSGId\" : {\n      \"Value\" : { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ]},\n      \"Description\" : \"Infra Node SG id\"\n    },\n    \"BastionSGId\" : {\n      \"Value\" : { \"Fn::GetAtt\": [ \"BastionSg\", \"GroupId\" ]},\n      \"Description\" : \"Bastion SG id\"\n    },\n    \"NodeARN\" : {\n      \"Value\" : { \"Ref\": \"NodeInstanceProfile\" },\n      \"Description\": \"ARN for the Node instance profile\"\n    },\n    \"S3UserAccessId\" : {\n      \"Value\" : { \"Ref\" : \"CFNKeys\" },\n      \"Description\" : \"AWSAccessKeyId of user\"\n    },\n    \"S3UserSecretKey\" : {\n      \"Value\" : { \"Fn::GetAtt\" : [\"CFNKeys\", \"SecretAccessKey\"]},\n      \"Description\" : \"AWSSecretKey of new S3\"\n    },\n    \"S3Bucket\" : {\n      \"Value\" : { \"Ref\" : \"RegistryBucket\"},\n      \"Description\" : \"Name of S3 bucket\"\n    }\n }\n}\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "01fefd6196f847661c2a9e4b7125fa1ba8d1d555", "filename": "roles/config-repo-server/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: repo_server\n  roles:\n    - config-repo-server\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ebf7cbcedc6642edd6efc3f8ab31a9fb2d480db5", "filename": "reference-architecture/rhv-ansible/playbooks/roles/instance-groups/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Add masters to requisite groups\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: masters, etcd, nodes, cluster_hosts\n    openshift_node_labels:\n      role: master\n  with_items: \"{{ groups['tag_openshift_master'] }}\"\n\n- name: Add one master to the single_master group\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: single_master\n    openshift_node_labels:\n      role: master\n  with_items: \"{{ groups['tag_openshift_master'][0] }}\"\n\n- name: Add infrastructure nodes to requisite groups\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: nodes, cluster_hosts, schedulable_nodes\n    openshift_node_labels:\n      role: infra\n  with_items: \"{{ groups['tag_openshift_infra'] }}\"\n\n- name: Add app nodes to requisite groups\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: nodes, cluster_hosts, schedulable_nodes\n    openshift_node_labels:\n      role: app\n  with_items: \"{{ groups['tag_openshift_node'] }}\"\n\n- name: Add load balancer node to lb group\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: lb, cluster_hosts, nodes\n    openshift_node_labels:\n      role: lb\n  with_items: \"{{ groups['tag_openshift_lb'] }}\"\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "9d082d70baa502a662d980269766143e7b2b73cc", "filename": "tasks/Win32NT/fetch/adoptopenjdk-fallback.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Fetch download page\n  win_uri:\n    url: \"{{ adoptopenjdk_api_page }}\\\n      info/releases/\\\n      openjdk{{ java_major_version }}\\\n      ?openjdk_impl={{ adoptopenjdk_impl }}\\\n      &os=windows&\\\n      arch={{ (java_arch == 'x64') | ternary('x64', 'x32') }}\\\n      &release=latest\\\n      &type={{ java_package }}&heap_size=normal\"\n    return_content: true\n    follow_redirects: all\n  register: download_page\n\n- name: Find release url\n  set_fact:\n    release_url: >-\n      {{ (download_page.content | from_json).binaries | map(attribute='binary_link') | list +\n        (download_page.content | from_json).binaries | map(attribute='checksum_link') | list }}\n\n- name: Exit if AdobtOpenJDK version is not found\n  fail:\n    msg: 'AdoptOpenJDK version {{ java_major_version }} not found'\n  when: release_url[0] is not defined\n\n- name: 'Fetch artifact checksum file {{ release_url[1] }}'\n  win_uri:\n    url: '{{ release_url[1] }}'\n    return_content: true\n  register: artifact_checksum_file\n\n- name: 'Get artifact checksum from file {{ release_url[1] }}'\n  set_fact:\n    artifact_checksum:\n      content: >-\n        {{ artifact_checksum_file['content'] |\n          regex_search('([^\\s]+)')\n        }}\n\n- name: 'Download artifact from {{ release_url[0] }}'\n  win_get_url:\n    url: '{{ release_url[0] }}'\n    dest: '{{ java_download_path }}'\n    force: true\n    checksum: '{{ artifact_checksum.content }}'\n    checksum_algorithm: sha256\n  register: file_downloaded\n  retries: 20\n  delay: 5\n  until: file_downloaded is succeeded\n  when: ansible_version.full is version('2.8.0', '>=')\n\n- name: Old fetch (Ansible < 2.8)\n  include_tasks: fetch_fallback_old.yml\n  when: ansible_version.full is version('2.8.0', '<')\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "56457306aa2a2f11068016b0ed2ed7e0285a96c7", "filename": "reference-architecture/vmware-ansible/playbooks/roles/create-vm-crs-prod-ose/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Set cluster_id crs annoation\n  set_fact:\n    crs_annoation: \"{{ cluster_id }}-crs\"\n\n- name: Create CRS production VMs on vCenter\n  vmware_guest:\n    hostname: \"{{ vcenter_host }}\"\n    username: \"{{ vcenter_username }}\"\n    password: \"{{ vcenter_password }}\"\n    validate_certs: False\n    name: \"{{ item.value.guestname }}\"\n    cluster: \"{{ vcenter_cluster}}\"\n    datacenter: \"{{ vcenter_datacenter }}\"\n    resource_pool: \"{{ vcenter_resource_pool }}\"\n    template: \"{{vcenter_template_name}}\"\n    state: poweredon\n    wait_for_ip_address: true\n    folder: \"/{{ vcenter_folder }}\"\n    annotation: \"{{ crs_annoation }}\"\n    disk:\n    - size_gb: 60\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 40\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 40\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 300\n      datastore: \"{{ vcenter_datastore }}\"\n      type: eagerZeroedThick\n    hardware:\n      memory_mb: 32768\n    networks:\n    - name: \"{{ vm_network }}\"\n      ip: \"{{ item.value.ip4addr }}\"\n      netmask: \"{{ vm_netmask }}\"\n      gateway: \"{{ vm_gw }}\"\n    customization:\n      domain: \"{{dns_zone}}\"\n      dns_servers:\n      - \"{{ vm_dns }}\"\n      dns_suffix: \"{{dns_zone}}\"\n      hostname: \"{{ item.value.guestname}}\"\n  with_dict: \"{{host_inventory}}\"\n  when: \"'crs' in item.value.guestname\"\n\n- name: Add CRS production VMs to inventory\n  add_host: hostname=\"{{ item.value.guestname }}\" ansible_ssh_host=\"{{ item.value.ip4addr }}\" groups=\"{{ crs_annoation }}, crs, production_group\"\n  with_dict: \"{{host_inventory}}\"\n  when: \"'crs' in item.value.guestname\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "e40fd3c823cdbe695b36bbfa2b4b8a901dba7e4e", "filename": "reference-architecture/vmware-ansible/playbooks/roles/vmware-guest-setup/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: be sure all pre-req packages are installed\n  yum: name={{item}} state=installed\n  with_items:\n    - open-vm-tools\n    - PyYAML\n    - perl\n    - net-tools\n    - chrony\n    - python-six\n    - iptables\n    - iptables-services\n    - docker\n\n- name: be sure katello-agent is installed\n  yum: name=katello-agent state=installed\n  when: rhsm_activation_key is defined and rhsm_activation_key\n\n- name: be sure chrony is configured\n  template: src=chrony.conf.j2 dest=/etc/chrony.conf\n  notify:\n    - restart chronyd\n\n- name: be sure chronyd is running and enabled\n  service: name=chronyd state=started enabled=yes\n\n- name: be sure openvmtools is running and enabled\n  service: name=vmtoolsd state=started enabled=yes\n\n- name: set link to localtime\n  command: timedatectl set-timezone {{timezone}}\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "da46534a5f1302359d9ba7cf84e87a66933d6878", "filename": "roles/cloud-ec2/tasks/encrypt_image.yml", "repository": "trailofbits/algo", "decoded_content": "- name: Check if the encrypted image already exist\n  ec2_ami_find:\n    aws_access_key: \"{{ aws_access_key | default(lookup('env','AWS_ACCESS_KEY_ID'))}}\"\n    aws_secret_key: \"{{ aws_secret_key | default(lookup('env','AWS_SECRET_ACCESS_KEY'))}}\"\n    owner: self\n    sort: creationDate\n    sort_order: descending\n    sort_end: 1\n    state: available\n    ami_tags:\n      Algo: \"encrypted\"\n    region: \"{{ region }}\"\n  register: search_crypt\n\n- set_fact:\n    ami_image: \"{{ search_crypt.results[0].ami_id }}\"\n  when: search_crypt.results\n\n- name: Copy to an encrypted image\n  ec2_ami_copy:\n    aws_access_key: \"{{ aws_access_key | default(lookup('env','AWS_ACCESS_KEY_ID'))}}\"\n    aws_secret_key: \"{{ aws_secret_key | default(lookup('env','AWS_SECRET_ACCESS_KEY'))}}\"\n    encrypted: yes\n    name: algo\n    kms_key_id: \"{{ kms_key_id | default(omit) }}\"\n    region: \"{{ region }}\"\n    source_image_id: \"{{ ami_image }}\"\n    source_region: \"{{ region }}\"\n    tags:\n      Algo: \"encrypted\"\n    wait: true\n  register: enc_image\n  when: not search_crypt.results\n\n- set_fact:\n    ami_image: \"{{ enc_image.image_id }}\"\n  when: not search_crypt.results\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "accda5820ce8fe2a9a4cf851c0ad1586b6e40924", "filename": "roles/marathon/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for marathon\n# Restart service marathon, in all cases\n- name: Restart marathon\n  service: name=marathon state=restarted\n  sudo: yes\n"}, {"commit_sha": "b2591b9333f6e7e70f6b9d99e55356b30d7e173c", "sha": "e2600d152f37828024619516380853a9c70bc8fd", "filename": "tasks/ssl.yml", "repository": "inkatze/wildfly", "decoded_content": "---\n# task file for wildfly\n\n# See the README file on how to generate this file.\n- name: Copy keystore file to the configuration folder\n  copy:\n    src: '{{ wildfly_keystore_name }}'\n    dest: '{{ wildfly_keystore_path }}'\n    owner: '{{ wildfly_user }}'\n    group: '{{ wildfly_group }}'\n    mode: '0750'\n  notify:\n    - restart wildfly\n    - change standalone data mode\n\n- name: Add SSL identity to the management realm\n  lineinfile:\n    dest: '{{ wildfly_standalone_config_path }}'\n    insertafter: <security-realm name=\"ManagementRealm\">\n    line: '{{ wildfly_application_ssl_identity }}'\n    owner: '{{ wildfly_user }}'\n    group: '{{ wildfly_group }}'\n    mode: '0750'\n  notify:\n    - restart wildfly\n    - change standalone data mode\n\n- name: Add https listener for applications\n  lineinfile:\n    dest: '{{ wildfly_standalone_config_path }}'\n    insertafter: <http-listener name=*\n    line: '{{ wildfly_https_listener }}'\n    owner: '{{ wildfly_user }}'\n    group: '{{ wildfly_group }}'\n    mode: '0750'\n  notify:\n    - restart wildfly\n    - change standalone data mode\n\n- name: Add https socket binding to management interfaces\n  lineinfile:\n    dest: '{{ wildfly_standalone_config_path }}'\n    insertafter: <http-interface security-realm=\"ManagementRealm\"*\n    line: <socket-binding https=\"management-https\"/>\n    owner: '{{ wildfly_user }}'\n    group: '{{ wildfly_group }}'\n    mode: '0750'\n  notify:\n    - restart wildfly\n    - change standalone data mode\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "a5a392bcea4a4615867317d8304608d815f0f5ad", "filename": "tasks/create_repo_raw_hosted_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_raw_hosted\n    args: \"{{ _nexus_repos_raw_defaults|combine(item) }}\""}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "876b7a6a579d82b6bfdb60ee758874d23a5a4967", "filename": "roles/dns/manage-dns-zones/tasks/route53/empty-zone.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: \"Remove records in Private zone\"\n  route53:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    state: absent\n    zone: \"{{ zone.dns_domain }}\"\n    private_zone: \"true\"\n    vpc_id: \"{{ zone.route53.vpc_id }}\"\n    value: \"{{ r53_record.1.Value }}\"\n    record: \"{{ r53_record.0.Name }}\"\n    ttl: \"{{ r53_record.0.TTL }}\"\n    type: \"{{ r53_record.0.Type }}\"\n  when: (zone.dns_domain + \".\" == r53_zone.item.Name) and\n        (view.name == \"private\") and\n        ((r53_record.0.Name != r53_zone.item.Name) and\n        (r53_record.0.Type != \"SOA\") and\n        (r53_record.0.Type != \"NS\"))\n\n- name: \"Remove records in Public zone\"\n  route53:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    state: absent\n    zone: \"{{ zone.dns_domain }}\"\n    record: \"{{ r53_record.0.Name }}\"\n    ttl: \"{{ r53_record.0.TTL }}\"\n    value: \"{{ r53_record.1.Value }}\"\n    type: \"{{ r53_record.0.Type }}\"\n  when: (zone.dns_domain + \".\" == r53_zone.item.Name) and\n        (view.name == \"public\") and\n        ((r53_record.0.Name != r53_zone.item.Name) and\n        (r53_record.0.Type != \"SOA\") and\n        (r53_record.0.Type != \"NS\"))\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "0f56d324b4b47b708a2e10c2eb71e88d89198a3e", "filename": "roles/config-nagios-target/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: restart docker\n  service:\n    name: docker\n    state: restarted\n\n- name: reload docker\n  service:\n    name: docker\n    state: reloaded\n\n- name: restart firewalld\n  service:\n    name: firewalld\n    state: restarted\n\n- name: reload firewalld\n  service:\n    name: firewalld\n    state: reloaded\n\n- name: restart iptables\n  service:\n    name: iptables\n    state: restarted\n\n- name: reload iptables\n  service:\n    name: iptables\n    state: reloaded\n\n- name: restart nrpe\n  service:\n    name: nrpe\n    state: restarted\n\n- name: reload nrpe\n  service:\n    name: nrpe\n    state: reloaded\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "08a2481aef3e7801bc25a6a191f621c7f67ab41b", "filename": "playbooks/roles/sensor-common/files/RPM-GPG-KEY-RockNSM-pkgcloud-2_1", "repository": "rocknsm/rock", "decoded_content": "-----BEGIN PGP PUBLIC KEY BLOCK-----\nVersion: GnuPG v1.4.11 (GNU/Linux)\n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\nXaxyC0F7/8t8qyLImRUIS87C/C+2wY6zUDwoE11f9KZF20sHylU=\n=lIij\n-----END PGP PUBLIC KEY BLOCK-----\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "861a14c07d2d253d57cb7d82775b3a47d44fd720", "filename": "roles/network/tasks/computed_network.yml", "repository": "iiab/iiab", "decoded_content": "# just lie about active gateway present on XOs to suppress ifcfg-WAN\n# auto-creation/moving with XOs using NM/system-connections/ via keyfile.\n# ifcfg-rh acts on /etc/sys*/net*/ifcfg-* where we search for devices.\n- name: Setting XO has WiFi gateway\n  set_fact:\n    user_wan_iface: \"{{ discovered_wan_iface }}\"\n  when: discovered_wan_iface != \"none\" and xo_model != \"none\" and has_ifcfg_gw == \"none\"\n\n#- name: Checking for NetworkManager-config-server\n#  shell: rpm -qa | grep  NetworkManager-config-server | wc -l\n#  register: strict_networking_check\n\n#- name: Found Checking for NetworkManager-config-server\n#  set_fact:\n#     strict_networking: True\n#  when: strict_networking_check == \"1\"\n\n#- name: Use restricted network features\n#  set_fact:\n#     iiab_demo_mode: True\n#  when: teamviewer_install and not strict_networking\n\n- name: XO laptop wants USB WiFi interface as AP mode\n  set_fact:\n    iiab_wireless_lan_iface: \"{{ discovered_lan_iface }}\"\n  when: num_wifi_interfaces >= \"2\" and xo_model != \"none\" and discovered_wan_iface != \"none\" and discovered_wireless_iface == \"eth0\"\n\n# static backout suppy new template file\n- name: gui-static-wan\n  set_fact:\n    wan_ip: dhcp\n    gui_static_wan_ip: undefined\n  when: gui_static_wan_ip != \"unset\" and not gui_static_wan\n\n- name: Undo gui-static-wan by requesting new template file\n  set_fact:\n    has_WAN: False\n  when: gui_static_wan_ip != \"unset\" and not gui_static_wan\n\n# figure out more than one interfaces to detect.\n- name: Using GUI_STATIC info\n  set_fact:\n    has_WAN: False\n    has_ifcfg_gw: \"none\"\n    wan_ip: \"{{ gui_static_wan_ip }}\"\n    wan_netmask: \"{{ gui_static_wan_netmask }}\"\n    wan_gateway: \"{{ gui_static_wan_gateway }}\"\n    wan_nameserver: \"{{ gui_static_wan_nameserver }}\"\n  when: gui_static_wan or user_wan_iface != \"auto\"\n\n# we need to have an interface name for ifcfg-WAN to be able to change gateway\n# the DEVICE from the gui. Thanks to George for proving my point about knowing\n# what device to switch to.\n#- name: Using GUI_WAN info\n#  set_fact:\n#    user_wan_iface: \"{{ gui_wan_iface }}\"\n#  when: gui_wan_iface != \"unset\" and gui_desired_network_role is defined and gui_desired_network_role != \"LanController\"\n\n# should make the GUI buttons the last call\n- name: Checking iiab_wan_enabled\n  set_fact:\n    user_wan_iface: \"none\"\n  when: 'not iiab_wan_enabled'\n\n# gui wants LanController # keeps ifcfg-WAN but onboot=no\n# the change over might be a little bumpy ATM.\n- name: Setting GUI wants 'LanController'\n  set_fact:\n    device_gw: \"none\"\n    user_wan_iface: \"none\"\n    iiab_gateway_enabled: \"False\"\n  when: gui_desired_network_role is defined and gui_desired_network_role == \"LanController\"\n\n# device_gw is used with the LAN detection and LAN's ifcfg file deletion.\n# single interface vars/ users would need to set iiab_wan_enabled False as above, to disable the WAN\n# and set user_lan_iface = <device> to suppress the auto detection for the same effect.\n\n- name: Setting user_lan_iface for 'LanController' for single interface\n  set_fact:\n    user_lan_iface: \"{{ discovered_wan_iface }}\"\n  when: discovered_wan_iface != \"none\" and num_lan_interfaces == \"0\" and gui_desired_network_role is defined and gui_desired_network_role == \"LanController\"\n\n# override with user_wan_iface setting if no longer in auto\n- name: Setting user WAN fact\n  set_fact:\n    iiab_wan_iface: \"{{ user_wan_iface }}\"\n  when: user_wan_iface != \"auto\"\n\n# user disabled interface - overriding all other entries\n- name: Checking iiab_lan_enabled\n  set_fact:\n    user_lan_iface: \"none\"\n  when: 'not iiab_lan_enabled'\n\n# gui wants Appliance Note: could of used iiab_lan_enabled false\n- name: Setting GUI wants 'Appliance'\n  set_fact:\n    user_lan_iface: \"none\"\n    iiab_gateway_enabled: \"False\"\n  when: gui_desired_network_role is defined and gui_desired_network_role == \"Appliance\"\n\n# gui wants Gateway\n- name: Setting GUI wants and has active 'Gateway'\n  set_fact:\n    user_lan_iface: \"auto\"\n    user_wan_iface: \"{{ iiab_wan_iface }}\"\n  when: gui_desired_network_role is defined and gui_desired_network_role == \"Gateway\" and iiab_wan_iface != \"none\"\n\n# make it so number 2 vars should use user_wan_iface but we can cover a single\n# wired if dhcp fails the interface should revert to LAN, static address should\n# stick around but testing gateway response is not performed.\n- name: User wants single wired interface as static or dhcp gateway\n  set_fact:\n    user_wan_iface: \"{{ discovered_wan_iface }}\"   # Jan 2018: Holt discovered_lan_iface was UNDEFINED on WiFi-installed\n                                                   # RPi (when re-running ./iiab-network) so \"discovered_wan_iface\" is a\n                                                   # workaround -- please see https://github.com/iiab/iiab/pull/649\n                                                   # This workaround can and should evolve as IIAB 6.5 matures!\n  when: num_lan_interfaces == \"1\" and user_lan_iface == \"auto\" and user_wan_iface == \"auto\"\n\n- name: Use old gateway device info if not detected and using static ip\n  set_fact:\n    iiab_wan_iface: \"{{ device_gw }}\"\n  when: wan_ip != \"dhcp\" and iiab_wan_iface == \"none\"\n\n- name: No LAN configured - 'Appliance' mode\n  set_fact:\n    iiab_network_mode: \"Appliance\"\n  when: iiab_lan_iface == \"none\"\n\n- name: LAN configured - 'LanController' mode\n  set_fact:\n    iiab_network_mode: \"LanController\"\n  when: iiab_lan_iface != \"none\" and iiab_wan_iface == \"none\"\n\n- name: LAN configured - 'Gateway' mode\n  set_fact:\n    iiab_network_mode: \"Gateway\"\n  when: (iiab_lan_iface != \"none\" and iiab_wan_iface != \"none\") or is_rpi\n\n- name: Force iiab_lan_iface if is_rpi\n  set_fact:\n    iiab_lan_iface: \"br0\"\n    iiab_wireless_lan_iface: \"wlan0\"\n  when: is_rpi\n\n- name: Enable hostapd if discovered_wireless_iface is not WAN\n  set_fact:\n    hostapd_enabled: True\n  when: is_rpi and iiab_wan_iface != discovered_wireless_iface\n\n# override with user_lan_iface setting if no longer in auto\n- name: Setting user LAN fact\n  set_fact:\n    iiab_lan_iface: \"{{ user_lan_iface }}\"\n  when: 'user_lan_iface != \"auto\"'\n\n# so this works\n- name: Interface count\n  shell: ls /sys/class/net | grep -v -e lo | wc | awk '{print $1}'\n  register: adapter_count\n\n# well if there ever was a point to tell the user things are FUBAR this is it.\n- name: We're hosed no work interfaces\n  set_fact:\n    iiab_network_mode: \"No_network_found\"\n  when: adapter_count.stdout|int == \"0\"\n\n# well if there ever was a point to tell the user things are FUBAR this is it.\n- name: I'm not guessing declare gateway please\n  set_fact:\n    iiab_network_mode: \"Undetectable_use_local_vars\"\n    iiab_wan_iface: \"none\"\n  when: adapter_count.stdout|int >= \"5\" and device_gw == \"none\" and gui_wan_iface == \"unset\" and gui_static_wan is defined\n\n- name: Record IIAB_WAN_DEVICE to /etc/iiab/iiab.env\n  lineinfile:\n    dest: /etc/iiab/iiab.env\n    regexp: '^IIAB_WAN_DEVICE=*'\n    line: 'IIAB_WAN_DEVICE=\"{{ iiab_wan_iface }}\"'\n    state: present\n  when: not installing   #REMOVE THIS LINE IF installing IS ALWAYS false AS SET IN roles/0-init/defaults/main.yml\n  tags:\n    - network\n\n- name: Record IIAB_LAN_DEVICE to /etc/iiab/iiab.env\n  lineinfile:\n    dest: /etc/iiab/iiab.env\n    regexp: '^IIAB_LAN_DEVICE=*'\n    line: 'IIAB_LAN_DEVICE=\"{{ iiab_lan_iface }}\"'\n    state: present\n  when: not installing   #REMOVE THIS LINE IF installing IS ALWAYS false AS SET IN roles/0-init/defaults/main.yml\n  tags:\n    - network\n\n- name: Add location section to config file\n  ini_file:\n    dest: \"{{ iiab_config_file }}\"\n    section: computed_network\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n  - option: iiab_wan_enabled\n    value: \"{{ iiab_wan_enabled }}\"\n  - option: user_wan_iface\n    value: \"{{ user_wan_iface }}\"\n  - option: iiab_wan_iface\n    value: \"{{ iiab_wan_iface }}\"\n  - option: iiab_lan_enabled\n    value: \"{{ iiab_lan_enabled }}\"\n  - option: user_lan_iface\n    value: \"{{ user_lan_iface }}\"\n  - option: iiab_lan_iface\n    value: \"{{ iiab_lan_iface }}\"\n  - option: iiab_network_mode\n    value: \"{{ iiab_network_mode }}\"\n  - option: hostapd_enabled\n    value: \"{{ hostapd_enabled }}\"\n  - option: host_ssid\n    value: \"{{ host_ssid }}\"\n  - option: host_wifi_mode\n    value: \"{{ host_wifi_mode }}\"\n  - option: host_channel\n    value: \"{{ host_channel }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a2535b9e3d11882bcf33485b9264a76d9a1ed89e", "filename": "roles/user-management/manage-local-user-password/tasks/password.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: \"Change {{ user_name }}'s password\"\n  user: \n    name: \"{{ user_name}}\"\n    password: \"{{ clear_text_password|encrypt_password }}\"\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "5c4bedb6c97a3d0977da76b76c758e248b294c3e", "filename": "tasks/configure-docker.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# https://wiki.ubuntu.com/SystemdForUpstartUsers\n# Important! systemd is only fully supported in Ubuntu 15.04 and later releases\n- name: Determine usage of systemd\n  become: true\n  shell: \"ps -p1 | grep systemd 1>/dev/null && echo systemd || echo upstart\"\n  changed_when: no\n  check_mode: no\n  register: _determine_systemd_usage\n  tags:\n    - skip_ansible_lint\n\n- name: Set fact to indicate systemd is used or not\n  set_fact:\n    _docker_systemd_used: \"{{ _determine_systemd_usage is defined and _determine_systemd_usage.stdout == 'systemd' }}\"\n\n- name: Configure systemd service\n  include_tasks: configure-docker/configure-systemd.yml\n  when: _docker_systemd_used | bool\n\n- name: Configure non-systemd service\n  include_tasks: configure-docker/configure-non-systemd.yml\n  when: not _docker_systemd_used | bool\n\n- name: Ensure /etc/docker directory exists\n  become: true\n  file:\n    path: /etc/docker\n    state: directory\n    mode: 0755\n\n- name: Configure Docker daemon (file)\n  become: true\n  copy:\n    src: \"{{ docker_daemon_config_file }}\"\n    dest: /etc/docker/daemon.json\n  notify: restart docker\n  when: docker_daemon_config_file is defined\n\n- name: Fetch Docker daemon status\n  become: true\n  service:\n    name: docker\n  register: _docker_status_check\n  when: docker_plugins | length > 0\n\n- name: Configure Docker daemon (variables)\n  become: true\n  copy:\n    content: \"{{ docker_daemon_config | to_nice_json }}\"\n    dest: /etc/docker/daemon.json\n  notify: restart docker\n  when:\n    - docker_daemon_config_file is not defined\n    - docker_daemon_config is defined\n    - _docker_status_check.status is not defined or\n      (_docker_status_check.status is defined and\n      _docker_status_check.status.SubState is defined and\n      _docker_status_check.status.SubState != \"running\")\n\n- name: Ensure Docker default user namespace is defined in subuid and subgid\n  become: true\n  lineinfile:\n    path: \"{{ item }}\"\n    regexp: '^dockremap'\n    line: 'dockremap:500000:65536'\n  loop:\n    - /etc/subuid\n    - /etc/subgid\n  when: (_docker_os_dist == \"CentOS\" or _docker_os_dist == \"RedHat\") and\n        ((docker_daemon_config is defined and\n        docker_daemon_config['userns-remap'] is defined and\n        docker_daemon_config['userns-remap'] == 'default') or\n        docker_bug_usermod | bool)\n\n- name: Ensure Docker users are added to the docker group\n  become: true\n  user:\n    name: \"{{ item }}\"\n    groups: docker\n    append: true\n  loop: \"{{ docker_users }}\"\n\n- name: Ensure devicemapper prerequisites are fulfilled\n  block:\n    - name: Ensure lvm2 is installed\n      become: true\n      package:\n        name: lvm2\n        state: present\n      register: _pkg_result\n      until: _pkg_result is succeeded\n\n    - name: Ensure thin-provisioning-tools is installed\n      become: true\n      package:\n        name: thin-provisioning-tools\n        state: present\n      register: _pkg_result\n      until: _pkg_result is succeeded\n      when: (_docker_os_dist == \"Ubuntu\" or (_docker_os_dist == \"Debian\" and _docker_os_dist_major_version | int > 7))\n  when:\n    - docker_daemon_config['storage-driver'] is defined\n    - docker_daemon_config['storage-driver'] == 'devicemapper'\n\n- name: Enable Docker service\n  become: true\n  service:\n    name: docker\n    enabled: yes\n  notify: restart docker\n  register: _docker_service\n\n- name: Trigger start/restart of Docker\n  service:\n    name: docker\n  notify: restart docker\n  changed_when: _docker_service.status.SubState != \"running\"\n  when: _docker_service.status is defined and _docker_service.status.SubState is defined\n\n- name: Install and configure Docker plugins\n  include_tasks: configure-docker/configure-docker-plugins.yml\n  when: docker_plugins | length > 0"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "9743c2f0d5780a99c0e5d6803e2a138669e3257b", "filename": "roles/config-quay-enterprise/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Verify Storage Type\n  fail:\n    msg: \"Invalid Database Type. 'quay_database_type' must be 'postgres' or 'mysql'\"\n  when: quay_database_type not in [\"postgresql\",\"mysql\"]\n\n- name: Verify Clair Endpoint\n  fail:\n    msg: Clar Endpoint Must Be Specified\n  when: quay_clair_enable is defined and quay_clair_enable|bool and (quay_clair_endpoint is not defined or quay_clair_endpoint|trim == \"\")\n\n- name: Set PostgreSQL Facts\n  set_fact:\n    quay_db_uri: \"{{ postgresql_db_uri }}\"\n  when: quay_database_type == \"postgresql\"\n\n- name: Set MySQL Facts\n  set_fact:\n    quay_db_uri: \"{{ mysql_db_uri }}\"\n  when: quay_database_type == \"mysql\"\n\n- name: Set HTTP Protocol\n  set_fact:\n    quay_http_protocol: \"{{ (quay_ssl_enable|bool)| ternary('https','http') }}\"\n\n- name: Include Container Credentials\n  include_tasks: container_credentials.yml\n  when: (quay_registry_server | trim != \"\") and ((quay_registry_auth | trim != \"\") or (quay_registry_email | trim != \"\"))\n\n- name: Configure Storage Directories\n  file:\n    state: directory\n    owner: root\n    group: root\n    mode: g+rw\n    path: \"{{ item }}\"\n  with_items:\n    - \"{{ quay_config_dir }}\"\n    - \"{{ quay_storage_dir }}\"\n\n- name: Include systemd configurations\n  include_tasks: configure_systemd.yml\n\n- name: Set SSL Facts\n  set_fact:\n    quay_ssl_enable: \"{{ quay_ssl_enable }}\"\n\n- name: Set Fact for Custom SSL Certificates\n  set_fact:\n    quay_ssl_cert_file: \"{{ ssl_local_tmp_dir.stdout }}/ssl.cert\"\n    quay_ssl_key_file: \"{{ ssl_local_tmp_dir.stdout }}/ssl.key\"\n  when: quay_ssl_enable|bool and (quay_ssl_key_file is defined and quay_ssl_key_file|trim != \"\" and quay_ssl_cert_file is defined and quay_ssl_cert_file|trim != \"\")\n\n- name: Create SSL Certificates\n  block:\n    - name: Create Temporary SSL Directory\n      command: mktemp -d /tmp/quay-ssl-XXXXXXX\n      register: quay_ssl_remote_tmp_dir_mktemp\n      delegate_to: \"{{ groups['quay_enterprise'][0] }}\"\n      when: quay_ssl_remote_tmp_dir is undefined and quay_ssl_remote_tmp_dir|trim == \"\"\n\n    - name: Set Fact for Remote SSL Directory\n      set_fact:\n        quay_ssl_remote_tmp_dir: \"{{ quay_ssl_remote_tmp_dir if quay_ssl_remote_tmp_dir is defined and quay_ssl_remote_tmp_dir|trim == '' else quay_ssl_remote_tmp_dir_mktemp.stdout }}\"\n      when: quay_ssl_remote_tmp_dir is undefined and quay_ssl_remote_tmp_dir|trim == \"\"\n\n    - name: Create SSL Certificate\n      command: openssl req -nodes -x509 -newkey rsa:4096 -keyout {{ quay_ssl_remote_tmp_dir }}/ssl.key -out {{ quay_ssl_remote_tmp_dir }}/ssl.cert -subj \"/C={{ quay_ssl_generate_country }}/ST={{ quay_ssl_generate_state }}/L={{ quay_ssl_generate_city }}/O={{ quay_ssl_generate_organization }}/OU={{ quay_ssl_generate_organizational_unit }}/CN={{ quay_server_hostname }}\" -days {{ quay_ssl_generate_days_validity }}\n      delegate_to: \"{{ groups['quay_enterprise'][0] }}\"\n  \n    - name: Set Fact for Custom SSL Certificates\n      set_fact:\n        quay_ssl_cert_file: \"{{ quay_ssl_local_tmp_dir }}/ssl.cert\"\n        quay_ssl_key_file: \"{{ quay_ssl_local_tmp_dir }}/ssl.key\"\n\n    - name: Fetch SSL Certifictes\n      fetch:\n        src:  \"{{ item.src }}\"\n        dest: \"{{ item.dest }}\"\n        flat: true\n        fail_on_missing: yes\n      delegate_to: \"{{ groups['quay_enterprise'][0] }}\"\n      with_items:\n        - { src: \"{{ quay_ssl_remote_tmp_dir }}/ssl.key\", dest: \"{{ quay_ssl_key_file }}\" }\n        - { src: \"{{ quay_ssl_remote_tmp_dir }}/ssl.cert\", dest: \"{{ quay_ssl_cert_file }}\" }\n\n    - name: Delete Remote SSL Certificates\n      file:\n        state: absent\n        path: \"{{ quay_ssl_remote_tmp_dir }}\"\n      delegate_to: \"{{ groups['quay_enterprise'][0] }}\"\n  when: quay_ssl_enable|bool and (quay_ssl_key_file is not defined or quay_ssl_key_file|trim == \"\" or quay_ssl_cert_file is not defined or quay_ssl_cert_file|trim == \"\")\n\n- name: Copy SSL Certificates\n  copy:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: root\n    group: root\n    mode: g+rw\n  notify: Restart quay service\n  with_items:\n    - { src: \"{{ quay_ssl_key_file }}\", dest: \"{{ quay_config_dir }}/ssl.key\" }\n    - { src: \"{{ quay_ssl_cert_file }}\", dest: \"{{ quay_config_dir }}/ssl.cert\" }\n  when: quay_ssl_enable|bool\n\n\n- name: Check if quay configuration exists\n  stat:\n    path: \"{{ quay_config_dir }}/config.yaml\"\n  register: quay_config_stat_result\n\n- name: Configure BitTorrent Pepper Value\n  set_fact:\n    bittorrent_filename_pepper: \"{{ 'hostname' | to_uuid | upper }}\"\n\n- name: Setup initial quay configuration file\n  template:\n    src: config.yaml.j2\n    dest: \"{{ quay_config_dir }}/config.yaml\"\n    owner: root\n    group: root\n    mode: g+rw\n  notify: Restart quay service\n  when: not quay_config_stat_result.stat.exists\n\n- name: Include firewall tasks\n  include_tasks: firewall.yml\n\n- name: Setup Initial User and Configuration\n  include_tasks: complete_setup.yml\n  when: not quay_config_stat_result.stat.exists and quay_superuser_username is defined and quay_superuser_username|trim != \"\" and quay_superuser_password is defined and quay_superuser_password|trim != \"\" and quay_superuser_email is defined and quay_superuser_email|trim != \"\" "}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "15025433ce3e010c37ad6709b384a3b472820d57", "filename": "examples/playbooks/remote_db.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n- hosts: database\n  remote_user: root\n  become: true\n  roles:\n    - {role: ovirt-common}\n    - {role: ovirt-engine-remote-db}\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "251375b85480c7aa0684cb73616bf744978e10cc", "filename": "playbooks/roles/docket/templates/docket_lighttpd_vhost.conf.j2", "repository": "rocknsm/rock", "decoded_content": "#######################################################################\n## Docket virtual host config\n## file: /etc/lighttpd/vhosts.d/docket.conf\n##\n## {{ ansible_managed }}\n##\n\n$SERVER[\"socket\"] == \"{{docket_listen_ip}}:{{docket_listen_port}}\" {\n  var.server_name = \"{{ docket_web_server_name }}\"\n  server.name = server_name\n  accesslog.filename = log_root + \"/\" + server.name + \"/access.log\"\n\n  ssl.engine  = \"{{ 'enable' if docket_tls == true else 'disable' }}\"\n  ssl.pemfile = \"{{ docket_web_pemfile }}\"\n  ssl.dh-file = \"{{ docket_web_dhparams }}\"\n\n  include \"conf.d/docket_scgi.conf\"\n}\n\n##\n#######################################################################\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "defac5b9645b446650e8883144cb4723114d0188", "filename": "ops/playbooks/backup_swarm.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: ucp_main\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/backups\n    - ../group_vars/vault\n\n  environment: \"{{ env }}\"\n\n  tasks:\n#\n# configure password less ssh from ucp_main to our ansible box\n#\n    - name: Register key\n      stat: path=/root/.ssh/id_rsa\n      register: key\n    - name: Create keypairs\n      shell: ssh-keygen -t rsa -f /root/.ssh/id_rsa -N ''\n      when: key.stat.exists == False\n    - name: Fetch all public ssh keys\n      shell: cat ~/.ssh/id_rsa.pub\n      register: ssh_keys\n    - name: Deploy keys on Ansible Box\n      authorized_key: user=root key=\"{{ item }}\"\n      delegate_to: localhost\n      with_items:\n        - \"{{ ssh_keys.stdout }}\"\n\n#\n# Get a timestamp, will be used to name the backup\n#\n    - name: Get TimeStamp\n      command: date -u '+%Y_%m_%d_%H%M%S'\n      register: timestamp\n    - name: Store the timestamp\n      set_fact:\n        timestamp: \"{{ timestamp.stdout }}\"\n    - name: Creates directory on the VM \n      file:\n        path: /root/scripts\n        state: directory\n\n    - set_fact:\n        swarm_backup_offline: \"{{ swarm_backup_offline | default('false') }}\"\n    - name: Copy backup scripts to backup VM\n      template: src=../templates/backup_swarm.sh.j2 dest=/root/scripts/backup_swarm.sh\n    - file:\n        path: /root/scripts/backup_swarm.sh\n        mode: 0744\n\n    - set_fact:\n        backup_name:  \"backup_swarm_{{ inventory_hostname }}_{{ timestamp }}\"\n      when: backup_name is undefined\n\n#\n# Compute the quorum\n#\n    - name: How many nodes are active now\n      shell: >\n            (\n             docker node ls -f \"role=manager\" --format \"{{ '{{' }}.Status{{ '}}' }}\" | grep Ready | wc -l )\n      register: active_nodes\n    - shell: echo $(( ({{ total }}+1)/2))\n      vars:\n        total: \"{{ groups['ucp'] | length }}\"\n      register: quorum\n    - debug:\n        msg: \"total={{ total }} quorum = {{ quorum.stdout }} active = {{ active_nodes.stdout }}\"\n      vars:\n        total: \"{{ groups['ucp'] | length }}\"\n\n    - fail:\n        msg: \"Aborting: Reason number of active nodes {{ active_nodes.stdout }} <= quorum = {{ quorum.stdout }} and swarm_backup_online != true\"\n      when: active_nodes.stdout <= quorum.stdout and swarm_backup_offline  == \"true\" \n     \n    - name: \"Backup swarm data now Offline mode {{ swarm_backup_offline }}\"\n      shell: /root/scripts/backup_swarm.sh {{ \"unused\" }} {{ backup_name }}\n      register: res\n\n    - debug: var=res\n      when: _debug is defined\n\n    - name: Create a temporary directory to receive files that contains metadata\n      tempfile:\n        state: directory\n        suffix: temp\n      register: res\n      delegate_to: localhost\n\n    - template:\n        src: ../templates/backup_meta.yml.j2\n        dest: \"{{ res.path }}/meta.yml\"\n      delegate_to: localhost\n\n    - copy:\n         src: \"{{ item }}\"\n         dest: \"{{ res.path }}/\"\n      with_items:\n        - ../vm_hosts\n        - ../group_vars/vars\n        - ../group_vars/backups\n      delegate_to: localhost\n\n    - name: Backup the Metadata as well\n      archive:\n        path:\n          - \"{{ res.path }}/\"\n        dest: \"{{ backup_dest }}/{{ backup_name }}.vars.tgz\"\n      delegate_to: localhost\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b80ad05cc21d8a9c9093c94c1c8e04d389fff553", "filename": "playbooks/osp/manage-user-network.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# See the roles README for detailed info on inventory requirements.\n# https://github.com/redhat-cop/infra-ansible/blob/master/roles/osp/admin-network/README.md\n\n- hosts: osp-provisioner\n  roles:\n  - osp/admin-network\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "597cb965ff1beaa2d4f59cf22ff18382a18a9b7d", "filename": "dev/playbooks/distribute_keys.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: vms\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - stat: path=/root/.ssh/id_rsa\n      register: key\n\n    - name: Create keypairs\n      shell: ssh-keygen -t rsa -f /root/.ssh/id_rsa -N ''\n      when: key.stat.exists == False\n\n    - name: Fetch all public ssh keys\n      shell: cat ~/.ssh/id_rsa.pub\n      register: ssh_keys\n\n    - name: Deploy keys on all servers\n      authorized_key: user=root key=\"{{ item[0] }}\"\n      delegate_to: \"{{ item[1] }}\"\n      with_nested: \n        - \"{{ ssh_keys.stdout }}\"\n        - \"{{ groups.all }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "2c78a33a3416aa54df8d6dffbcd5b4ee99fb8b25", "filename": "reference-architecture/vmware-ansible/playbooks/add-node-prerequisite.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: new_nodes\n  gather_facts: yes\n  become: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - rhsm\n\n- hosts: new_nodes\n  gather_facts: no\n  become: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - prerequisites\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "3e980ff60cd8ff4b42fe215d160aab2bf344d3e1", "filename": "roles/ansible/tower/manage-inventories/tasks/process-inventory.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Get the org id based on the org name\"\n  set_fact:\n    org_id: \"{{ item.id }}\"\n  when:\n  - item.name|trim == inventory.organization|trim\n  with_items:\n  - \"{{ existing_organizations_output.rest_output }}\"\n\n- name: \"Load up the inventory\"\n  uri:\n    url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/inventories/\"\n    user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n    password: \"{{ ansible_tower.admin_password }}\"\n    force_basic_auth: yes\n    method: POST\n    body: \"{{ lookup('template', 'inventory.j2') }}\"\n    body_format: 'json'\n    headers:\n      Content-Type: \"application/json\"\n      Accept: \"application/json\"\n    validate_certs: no\n    status_code: 200,201,400\n  register: inventory_output\n\n# Utilize the `rest_get` library routine to ensure REST pagination is handled\n- name: \"Get the updated list of existing inventories\"\n  rest_get:\n    host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n    rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n    rest_password: \"{{ ansible_tower.admin_password }}\"\n    api_uri: \"/api/v2/inventories/\"\n  register: existing_inventories_output\n\n- name: \"Get the inventory id based on the inventory name\"\n  set_fact:\n    inv_id: \"{{ item.id }}\"\n  when:\n  - item.name|trim == inventory.name|trim\n  with_items:\n  - \"{{ existing_inventories_output.rest_output }}\"\n\n- name: \"Process the inventory host entries\"\n  include_tasks: process-host.yml\n  with_items:\n  - \"{{ inventory.hosts }}\"\n  loop_control:\n    loop_var: host\n\n- name: \"Process the inventory group entries\"\n  include_tasks: process-group.yml\n  with_items:\n  - \"{{ inventory.groups }}\"\n  loop_control:\n    loop_var: group\n\n- name: \"Clear/Update facts\"\n  set_fact:\n    org_id: ''\n    inv_id: ''\n    processed_inventories: \"{{ processed_inventories + [ { 'name': inventory.name } ] }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4ecafae6b40c757d6e92ea23c9a80c38148c1eb9", "filename": "roles/user-management/manage-atlassian-users/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- include_tasks: create_atlassian_groups.yml\n  with_items: \"{{ atlassian.groups }}\"\n  loop_control:\n    loop_var: group\n  when: atlassian.groups|length > 0\n\n- include_tasks: create_atlassian_users.yml\n  with_items: \"{{ atlassian.user }}\"\n  loop_control:\n    loop_var: atlassian_user\n  when: atlassian.user|length > 0\n\n- include_tasks: add_user_to_groups.yml\n  with_items: \"{{ atlassian.user }}\"\n  loop_control:\n    loop_var: atlassian_user\n  when: atlassian.user|length > 0\n\n- include_tasks: delete_atlassian_users.yml\n  with_items: \"{{ atlassian.user }}\"\n  loop_control:\n    loop_var: atlassian_user\n  when: atlassian.user|length > 0\n\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "b1d815f58f6d42aabb389f0a59672e5fcc836759", "filename": "roles/ovirt-collect-logs/meta/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\ngalaxy_info:\n  author: \"Lukas Bednar\"\n  description: \"Collects logs from oVirt deployment\"\n  company: \"Red Hat\"\n  license: \"GPLv3\"\n  min_ansible_version: 1.9\n  platforms:\n    - name: EL\n      versions:\n        - all\n  galaxy_tags:\n    - installer\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "5a2d849715e2b3cf12f568b6eea0ff3dd1c35c9c", "filename": "reference-architecture/vmware-ansible/playbooks/cleanup-cns.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  user: root\n  become: false\n  ignore_errors: yes\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - instance-groups\n\n- hosts: cns\n  user: root\n  become: false\n  ignore_errors: yes\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - rhsm-unregister\n\n- hosts: localhost\n  user: root\n  become: false\n  ignore_errors: yes\n  vars_files:\n    - vars/main.yaml\n  tasks:\n    - name: Delete cns VMs\n      vmware_guest:\n        hostname: \"{{ vcenter_host }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        datacenter: \"{{ vcenter_datacenter }}\"\n        folder: \"/{{ vcenter_folder }}\"\n        name: \"{{ item.value.guestname }}\"\n        state: absent\n        force: true\n      with_dict: \"{{host_inventory}}\"\n      when: \"'cns' in item.value.guestname\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "f29f2c0f2cd49d20bf5264d6bdfedca20b5a0a2c", "filename": "roles/schooltool/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# the defaults on this image are user:manager, password:schooltool\n- name: Get the schooltool container if it is missing\n  docker: image=ghunt/schooltool:latest\n          ports=7080:7080\n          state=present\n          pull=missing\n  when: schooltool_install and docker_install and internet_available \n\n\n- name: Configure schooltool to run in docker container under systemd\n  template: backup=yes\n            src={{ item.src }}\n            dest={{ item.dest }}\n            owner=root\n            group=root\n            mode={{ item.mode }}\n  with_items:\n    -  dest: '/etc/systemd/system/schooltool.service'\n       src: 'schooltool.service'\n       mode: '0644'\n    -  dest: '/etc/{{ apache_config_dir }}/schooltool.conf'\n       src:  'schooltool.conf'\n       mode: '0644'\n\n# create a schooltool user who can have limited capability\n- name: Make a user for schooltool\n  user: name=schooltool\n        shell=/sbin/nologin\n        createhome=no\n\n# create the permanent storage directory, and give world write access\n# -- will eventually change the docker container to give itself write access and demote itself\n- name: create Permanent Storage for persistent schooltool data\n  file: path=/var/lib/schooltool\n        state=directory\n        owner=schooltool\n        mode=0777\n\n#Experience teaches that enabling schooltool is more likely to succeed after docker restart\n- name: Restart docker\n  service: name=docker\n           state=restarted\n           enabled=yes\n  when: schooltool_enabled\n\n- name: Enable schooltool\n  service: name=schooltool\n           state=started\n           enabled=yes\n  when: schooltool_enabled\n\n- name: Disable schooltool\n  service: name=schooltool\n           state=stopped\n           enabled=no\n  when: not schooltool_enabled\n\n- name: add schooltool to service list\n  ini_file: dest='{{ service_filelist }}'\n            section=schooltool\n            option='{{ item.option }}'\n            value='{{ item.value }}'\n  with_items:\n    - option: name\n      value: Schooltool\n    - option: description\n      value: '\"SchoolTool is an open source, web based student information system designed for schools in the developing world, with strong support for translation, localization and automated deployment and updates.\"'\n    - option: enabled\n      value: \"{{ schooltool_enabled }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "4b66e8594dc621b89e9a3c3fe3d6e585df85eded", "filename": "reference-architecture/vmware-ansible/playbooks/minor-update.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- include: /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/upgrades/{{ openshift_vers | default('v3_6') }}/upgrade.yml\n  vars:\n    debug_level: 2\n    openshift_debug_level: \"{{ debug_level }}\"\n    openshift_master_cluster_method: native\n    openshift_node_debug_level: \"{{ node_debug_level | default(debug_level, true) }}\"\n    openshift_master_debug_level: \"{{ master_debug_level | default(debug_level, true) }}\"\n    openshift_master_access_token_max_seconds: 2419200\n    openshift_master_api_port: \"{{ console_port }}\"\n    openshift_master_console_port: \"{{ console_port }}\"\n    osm_cluster_network_cidr: 172.16.0.0/16\n    openshift_cloudprovider_kind: vsphere\n    load_balancer_hostname:\n    deployment_type:\n    openshift_master_cluster_hostname: \"{{ load_balancer_hostname }}\"\n    openshift_master_cluster_public_hostname: \"{{ load_balancer_hostname }}\"\n    osm_default_node_selector: \"role=app\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "e460fbf124c0a6e4918950645b5ec74fd3fe3bac", "filename": "playbooks/provisioning/openstack/post-provision-openstack.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: cluster_hosts\n  name: Wait for the the nodes to come up\n  become: False\n  gather_facts: False\n  tasks:\n    - when: not openstack_use_bastion|default(False)|bool\n      wait_for_connection:\n    - when: openstack_use_bastion|default(False)|bool\n      delegate_to: bastion\n      wait_for_connection:\n\n- hosts: cluster_hosts\n  gather_facts: True\n  tasks:\n    - name: Debug hostvar\n      debug:\n        msg: \"{{ hostvars[inventory_hostname] }}\"\n        verbosity: 2\n\n- name: OpenShift Pre-Requisites (part 1)\n  include: pre-install.yml\n\n- name: Assign hostnames\n  hosts: cluster_hosts\n  gather_facts: False\n  become: true\n  roles:\n    - role: hostnames\n\n- name: Subscribe DNS Host to allow for configuration below\n  hosts: dns\n  gather_facts: False\n  become: true\n  roles:\n    - role: subscription-manager\n      when: hostvars.localhost.rhsm_register|default(False)\n      tags: 'subscription-manager'\n\n- name: Determine which DNS server(s) to use for our generated records\n  hosts: localhost\n  gather_facts: False\n  become: False\n  roles:\n    - dns-server-detect\n\n- name: Build the DNS Server Views and Configure DNS Server(s)\n  hosts: dns\n  gather_facts: False\n  become: true\n  roles:\n    - role: dns-views\n    - role: infra-ansible/roles/dns-server\n\n- name: Build and process DNS Records\n  hosts: localhost\n  gather_facts: True\n  become: False\n  roles:\n    - role: dns-records\n      use_bastion: \"{{ openstack_use_bastion|default(False)|bool }}\"\n    - role: infra-ansible/roles/dns\n\n- name: Switch the stack subnet to the configured private DNS server\n  hosts: localhost\n  gather_facts: False\n  become: False\n  vars_files:\n    - stack_params.yaml\n  tasks:\n    - include_role:\n        name: openstack-stack\n        tasks_from: subnet_update_dns_servers\n\n- name: OpenShift Pre-Requisites (part 2)\n  hosts: OSEv3\n  gather_facts: true\n  become: true\n  vars:\n    interface: \"{{ flannel_interface|default('eth1') }}\"\n    interface_file: /etc/sysconfig/network-scripts/ifcfg-{{ interface }}\n    interface_config:\n      DEVICE: \"{{ interface }}\"\n      TYPE: Ethernet\n      BOOTPROTO: dhcp\n      ONBOOT: 'yes'\n      DEFTROUTE: 'no'\n      PEERDNS: 'no'\n  pre_tasks:\n    - name: \"Include DNS configuration to ensure proper name resolution\"\n      lineinfile:\n        state: present\n        dest: /etc/sysconfig/network\n        regexp: \"IP4_NAMESERVERS={{ hostvars['localhost'].private_dns_server }}\"\n        line: \"IP4_NAMESERVERS={{ hostvars['localhost'].private_dns_server }}\"\n    - name: \"Configure the flannel interface options\"\n      when: openshift_use_flannel|default(False)|bool\n      block:\n        - file:\n            dest: \"{{ interface_file }}\"\n            state: touch\n            mode: 0644\n            owner: root\n            group: root\n        - lineinfile:\n            state: present\n            dest: \"{{ interface_file }}\"\n            regexp: \"{{ item.key }}=\"\n            line: \"{{ item.key }}={{ item.value }}\"\n          with_dict: \"{{ interface_config }}\"\n  roles:\n    - node-network-manager\n\n- include: prepare-and-format-cinder-volume.yaml\n  when: >\n    prepare_and_format_registry_volume|default(False) or\n    (cinder_registry_volume is defined and\n      cinder_registry_volume.changed|default(False))\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7205f0f3847261c2bf93e1d60d14320ba2704473", "filename": "roles/config-nagios-target/tasks/prerequisites.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# Ensure the correct repos and software packages are installed\n#- import_tasks: enable-repos.yml\n#- import_tasks: install-epel.yml\n#- import_tasks: install-nagios.yml\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "fe10fe604abe0e3b5a48e7fed20e1bd16df7137b", "filename": "archive/playbooks/openstack/terminate.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n# Use this to delete all OpenStack instances and their attached volumes matching a specified 'env_id'. \n#\n# Call it as such:\n#\n#            ansible-playbook terminate.yml -e \"env_id=testenv1\"\n\n- hosts: localhost\n  gather_facts: false\n  vars:\n    ansible_ssh_user: cloud-user\n    max_instances: 6\n    min_env_id_length: 8\n    really_really_sure: false\n    dry_run: false\n    force: false\n    prompt: true\n    uuid_regex: \"[[:alnum:]]{8}-[[:alnum:]]{4}-[[:alnum:]]{4}-[[:alnum:]]{4}-[[:alnum:]]{12}\"\n    newline: \"\\n\"\n  tasks:\n\n  - name: \"Set 'env_id' to 'uuid_regex' if override requested\"\n    set_fact:\n      env_id: \"{{ uuid_regex }}\"\n    when:\n      - env_id is not defined or env_id is none or env_id|trim == ''\n      - really_really_sure\n\n  - name: \"Verify environment ID was set\"\n    fail:\n      msg: \"No 'env_id' set, refusing to delete all instances and volumes, please provide a string to match via 'env_id'. Override with 'really_really_sure=true'\"\n    when: \n      - env_id is not defined or env_id is none or env_id|trim == ''\n      - not really_really_sure\n\n  - name: \"Verify environment ID is of sufficient length\"\n    fail:\n      msg: \"'env_id' is too short at only '{{ env_id|length }}' characters, risk of deleting too many instances. Override 'min_env_id_length' or with 'really_really_sure=true'\"\n    when: \n      - env_id|length < min_env_id_length|int\n      - not really_really_sure\n\n  - name: \"Verify connectivity to OpenStack\"\n    command: \"nova credentials\"\n    register: nova_result\n    failed_when: nova_result.rc != 0\n\n  - name: \"Determine number of matching instances\"\n    shell: nova list | grep -v \"ERROR\" | grep -E \"{{ env_id }}\" | awk '{print $2}' | sed ':a;N;$!ba;s/\\n/, /g'\n    register: instances_to_delete\n\n  - debug:\n      var:\n        instances_to_delete\n    when: dry_run\n\n  - name: \"Set instance count fact\"\n    set_fact:\n      instance_count: \"{{ instances_to_delete.stdout.split(', ')|length }}\"\n\n  - name: \"Fix instance count if list is empty\"\n    # Python counts an empty list as length 1, so fixing it if so\n    set_fact:\n      instance_count: 0\n    when:\n      - instances_to_delete.stdout.split(', ').0|trim == ''\n\n  - name: \"Determine if there are matching instances\"\n    fail:\n      msg: \"'{{ env_id }}' does not match any instances.\"\n    when:\n      - instance_count == 0\n      - not really_really_sure\n\n  - name: \"Verify environment ID does not match too many instances\"\n    fail:\n      msg: \"'{{ env_id }}' matches {{ instance_count|int }} instances, risk of deleting too many instances. Override 'max_instances' or with 'really_really_sure=true'\"\n    when: \n      - instance_count|int > max_instances|int\n      - not really_really_sure\n\n  - name: \"Determine instance names to delete\"\n    shell: nova list | grep -v \"ERROR\" | grep -E \"{{ env_id }}\" | awk '{print $4}' | sed ':a;N;$!ba;s/\\n/, /g'\n    register: names_to_delete\n\n  - debug:\n      var:\n        names_to_delete\n    when: dry_run\n\n  - name: \"Query Neutron services\"\n    command: neutron agent-list\n    register: neutron\n    ignore_errors: true\n\n  - name: \"Check for Neutron services - (a failure assumes Legacy Networking (Nova Network)\"\n    set_fact:\n      neutron_in_use: true\n    when: neutron.rc == 0\n\n  # There is a possibility of an instance not having a floating IP thus the extra sed to delete an empty field\n  - name: \"Determine list of public IPs\"\n    shell: nova list | grep -v \"ERROR\" | grep -E \"{{ env_id }}\" | awk '{print $13}' | sed '/|/d' | sed ':a;N;$!ba;s/\\n/, /g'\n    register: ips_to_delete\n\n  - name: \"Set IP count fact\"\n    set_fact:\n      ip_count: \"{{ ips_to_delete.stdout.split(', ')|length }}\"\n\n  - name: \"Fix IP count if list is empty\"\n    # Python counts an empty list as length 1, so fixing it if so\n    set_fact:\n      ip_count: 0\n    when:\n      - ips_to_delete.stdout.split(', ').0|trim == ''\n\n\n  - debug:\n      var:\n        ips_to_delete\n    when: dry_run\n\n  - name: \"Determine list of Neutron Floating IP IDs to delete\"\n    shell: for floatingip in $(echo {{ ips_to_delete.stdout }}  | sed -n 1'p' | tr ',' ' ' | while read ip; do echo ${ip}; done); do neutron floatingip-list | awk \"/${floatingip}/\"'{print $2}'; done | sed ':a;N;$!ba;s/\\n/, /g'\n    register: floatingips_to_delete\n    when: neutron_in_use is defined\n\n  - debug:\n      var:\n        floatingips_to_delete\n    when: dry_run\n\n  # It is possible for a volume to exist but not be attached possibly intentionally to save data, so adding an extra grep to ensure only in-use volumes are deleted. This is because the volume names are set the ID of the instance which is difficult to determine when it is attached or not by looking for an attached ID.\n  - name: \"Determine list of volumes\"\n    shell: for instance in $(echo {{ instances_to_delete.stdout }}  | sed -n 1'p' | tr ',' ' ' | while read id; do echo ${id}; done); do nova volume-list | grep \"in-use\" | awk \"/${instance}/\"'{print $2}'; done | sed ':a;N;$!ba;s/\\n/, /g'\n    register: volumes_to_delete\n\n  - debug:\n      var:\n        volumes_to_delete\n    when: dry_run\n\n  - name: \"Set Volume count fact\"\n    set_fact:\n      volume_count: \"{{ volumes_to_delete.stdout.split(', ')|length }}\"\n\n  - name: \"Fix Volume count if list is empty\"\n    # Python counts an empty list as length 1, so fixing it if so\n    set_fact:\n      volume_count: 0\n    when:\n      - volumes_to_delete.stdout.split(', ').0|trim == ''\n\n  - name: \"Determine images used in instances\"\n    shell: for instance in $(echo {{ instances_to_delete.stdout }}  | sed -n 1'p' | tr ',' ' ' | while read id; do echo ${id}; done); do nova show ${instance} | awk \"/image/\"'{print $4}'; done | sed ':a;N;$!ba;s/\\n/, /g'\n    register: images_to_delete\n\n  - debug:\n      var:\n        images_to_delete\n    when: dry_run\n\n  - name: \"Initialize image fact to first image found\"\n    set_fact:\n      image: \"{{ images_to_delete.stdout.split(', ').0 }}\"\n\n  - name: \"Determine if any instance uses a different image\"\n    set_fact:\n      images_differ: true\n    when: \"'{{ item }}' != image\"\n    with_items: images_to_delete.stdout.split(', ')\n\n  - name: \"Set Neutron Port ID fact\"\n    set_fact:\n      floatingips_to_delete:\n        stdout: \"Neutron not in use\"\n    when: neutron_in_use is undefined\n\n  - name: \"Warn if images are not unique\"\n    pause:\n      prompt: \"{{ newline }}\nWARNING! Images used for matching instances are not unique. It is possible that different users are required for SSH based on the image used. This can lead to some termination tasks such as subscription-manager not being executed successfully. It is recommended that each environment is deleted in batches of matching images.{{ newline }}\nProceed with caution.{{ newline }}{{ newline }}\n[Unique Images]{{':'}} '{{ images_to_delete.stdout.split(', ') | unique | join(', ') }}'{{ newline }}{{ newline }}\nPress ENTER to continue or CTRL+c to cancel\"\n    when:\n      - images_differ is defined\n      - prompt\n\n  - name: \"Build a group containing instance IPs\"\n    add_host:\n      hostname: \"{{ item }}\"\n      ansible_ssh_host: \"{{ item }}\"\n      ansible_ssh_user: \"{{ ansible_ssh_user }}\"\n      groups: instance_ips\n    with_items: \"ips_to_delete.stdout.split(', ')\"\n    when:\n      - item is defined\n      - item is not none\n      - item|trim != ''\n\n  - name: \"Set 'env_id' fact for display purposes if override requested\"\n    set_fact:\n      env_id: \"*\"\n    when:\n      - really_really_sure\n      - env_id == uuid_regex\n\n  - name: \"Pause for confirmation on normal run.\"\n    pause:\n      prompt: \"{{ newline }}\nWARNING! About to delete the following objects matching the environment ID '{{ env_id}}'{{':'}}{{ newline }}\n{{ instance_count|int }} instances, {{ ip_count|int }} IPs and {{ volume_count|int }} attached volumes{{':'}}{{ newline }}{{ newline }}\n[Instance IDs]{{':'}} '{{ instances_to_delete.stdout }}'{{ newline }}{{ newline }}\n[Instance Names]{{':'}} '{{ names_to_delete.stdout }}'{{ newline }}{{ newline }}\n[Instance IPs]{{':'}} '{{ ips_to_delete.stdout }}'{{ newline }}{{ newline }}\n[Floating IP IDs]{{':'}} '{{ floatingips_to_delete.stdout }}'{{ newline }}{{ newline }}\n[Attached Volumes]{{':'}} '{{ volumes_to_delete.stdout }}'{{ newline }}{{ newline }}\n[Unique Images]{{':'}} '{{ images_to_delete.stdout.split(', ') | unique | join(', ') }}'{{ newline }}{{ newline }}\nPress ENTER to delete these or CTRL+c to cancel\"\n    when:\n      - not dry_run\n      - prompt\n\n  - name: \"Pause for confirmation on dry run.\"\n    pause:\n      prompt: \"{{ newline }}\nNOTE{{':'}} A normal run would delete the following objects matching the environment ID '{{ env_id}}'{{':'}}{{ newline}}\n{{ instance_count|int }} instances, {{ ip_count|int }} IPs and {{ volume_count|int }} attached volumes{{':'}}{{ newline }}{{ newline }}\n[Instance IDs]{{':'}} '{{ instances_to_delete.stdout }}'{{ newline }}{{ newline }}\n[Instance Names]{{':'}} '{{ names_to_delete.stdout }}'{{ newline }}{{ newline }}\n[Instance IPs]{{':'}} '{{ ips_to_delete.stdout }}'{{ newline }}{{ newline }}\n[Floating IP IDs]{{':'}} '{{ floatingips_to_delete.stdout }}'{{ newline }}{{ newline }}\n[Attached Volumes]{{':'}} '{{ volumes_to_delete.stdout }}'{{ newline }}{{ newline }}\n[Unique Images]{{':'}} '{{ images_to_delete.stdout.split(', ') | unique | join(', ') }}'{{ newline }}{{ newline }}\nPress ENTER to view tasks that will be skipped or CTRL+c to cancel\"\n    when:\n      - dry_run\n      - prompt\n\n- hosts: instance_ips\n  gather_facts: false\n  vars:\n    ansible_sudo: true\n    dry_run: false\n    unregister: true\n  tasks:\n\n  - debug:\n      var: hostvars[inventory_hostname]\n    when: dry_run\n\n  - name: \"Attempt to unregister from Subscription Manager\"\n    command: \"subscription-manager unregister\"\n    ignore_errors: yes\n    when:\n      - not dry_run\n      - unregister\n\n- hosts: localhost\n  gather_facts: false\n  vars:\n    dry_run: false\n  tasks:\n\n  - name: \"Delete instance\"\n    command: \"nova delete {{ item }}\"\n    ignore_errors: yes\n    with_items: \"instances_to_delete.stdout.split(', ')\"\n    when:\n      - not dry_run\n      - item is defined\n      - item is not none\n      - item|trim != ''\n\n  - name: \"Wait for instance delete and volume detach\"\n    shell: nova volume-list | awk \"/{{ item }}/\"'{ print $4 }'\n    register: volume_check\n    until: volume_check.stdout != \"in-use\"\n    retries: 5\n    delay: 10\n    with_items: \"volumes_to_delete.stdout.split(', ')\"\n    when:\n      - not dry_run\n      - item is defined\n      - item is not none\n      - item|trim != ''\n\n  - name: \"Delete volume\"\n    command: \"nova volume-delete {{ item }}\"\n    ignore_errors: yes\n    with_items: \"volumes_to_delete.stdout.split(', ')\"\n    when:\n      - not dry_run\n      - item is defined\n      - item is not none\n      - item|trim != ''\n\n  - name: \"Release Neutron Floating IP\"\n    command: \"neutron floatingip-delete {{ item }}\"\n    ignore_errors: yes\n    with_items: \"floatingips_to_delete.stdout.split(', ')\"\n    when:\n      - not dry_run\n      - neutron_in_use is defined\n      - item is defined\n      - item is not none\n      - item|trim != ''\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "ced52f25bbe37dc518b9e1b6d7d1276662a0ef9f", "filename": "playbooks/delete-data.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- hosts: all\n  vars:\n    rock_debug: \"{{ lookup('env', 'DEBUG') }}\"\n    http_proxy: \"{{ lookup('env','http_proxy') }}\"\n    https_proxy: \"{{ lookup('env', 'https_proxy') }}\"\n  tasks:\n    - name: Get default settings\n      include_vars: rocknsm_config.dist.yml\n    - name: Apply override settings, if available\n      include_vars: \"{{ rock_config }}\"\n      ignore_errors: true\n      failed_when: false\n    - name: Debug variables\n      include: debug.yml\n      when: rock_debug is defined and rock_debug\n\n    ######################################################\n    ######### Stop Services ##############################\n    ######################################################\n    - name: Stop rocknsm services\n      command: /sbin/rock_stop\n\n    ######################################################\n    ######### Delete Data ################################\n    ######################################################\n    - name: Remove rock_data_dir\n      file:\n        state: absent\n        path: \"{{ rock_data_dir }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "68faaae842bc8c596d2f9090c6ba7f1f516b8b8d", "filename": "roles/openshift-pv-cleanup/tasks/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n\n- name: Get all projects\n  command: >\n    oc get projects -o jsonpath='{ .items[*].metadata.name }'\n  register: projects\n\n- name: Delete all persistent volume claims in cluster\n  command: >\n    oc delete pvc --all -n {{ item }}\n  with_items: \"{{ projects.stdout.split(' ') }}\"\n\n- name: Delete all persistent volumes in cluster\n  command: >\n    oc delete pvc --all\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "7dd29e1f33b6f56fea06366b8597a676b7aada1c", "filename": "reference-architecture/vmware-ansible/playbooks/roles/nfs-server/handlers/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: restart nfs\n  service: name=nfs state=restarted\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "88932ebd08e2a8469a4b1803222b7ca7d560ba0d", "filename": "tasks/Win32NT/install/package.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Install java packages\n  win_package:\n    path: '{{ java_artifact }}'\n    product_id: '{{ java_product_id }}'\n    state: present\n    arguments: '/s INSTALLDIR=\"{{ java_path }}\\{{ java_folder }}\"'\n    creates_path: '{{ java_path }}\\{{ java_folder }}'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9ffba24fe3f384431a004ef97033a2fa54b2644d", "filename": "roles/osp/packstack-install/tasks/packstack-install-prep.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Include prereq\"\n  import_tasks: prereq.yml\n\n- name: \"Initialize variables\"\n  set_fact:\n    config: []\n\n- name: \"Get the list of hosts part of this OSP cluster\"\n  set_fact:\n    cluster_hosts: \"{{ cluster_hosts | default(groups[group_names[0]]) | intersect(groups[item]) }}\"\n  with_items:\n  - \"{{ group_names }}\"\n\n- name: \"Build OSP fact: Compute(nova) Host(s)\"\n  set_fact:\n    compute_hosts: \"{{ compute_hosts|default([]) }} + [ '{{ hostvars[item].ansible_host }}' ]\"\n  with_items:\n  - \"{{ cluster_hosts }}\"\n  when:\n  - \"'nova' in hostvars[item].osp_roles.split(',')\"\n\n- name: \"Build OSP fact: Network(neutron) Host(s)\"\n  set_fact:\n    network_hosts: \"{{ network_hosts|default([]) }} + [ '{{ hostvars[item].ansible_host }}' ]\"\n  with_items:\n  - \"{{ cluster_hosts }}\"\n  when:\n  - \"'neutron' in hostvars[item].osp_roles.split(',')\"\n\n- name: \"Set OSP fact: Build Config Dictionary\"\n  set_fact:\n    config:\n      compute: \"{{ compute_hosts|join(',') }}\"\n      network: \"{{ network_hosts|join(',') }}\"\n\n- name: \"Set OSP fact: Build Config Dictionary\"\n  set_fact:\n    config: \"{{ config | combine( { item.1: hostvars[item.0].ansible_host } ) }}\"\n  with_nested:\n  - \"{{ cluster_hosts }}\"\n  - [ 'controller', 'cinder', 'amqp', 'mariadb', 'mongodb', 'redis' ]\n  when:\n  - \"item[1] in hostvars[item.0].osp_roles.split(',')\"\n\n- name: \"Set answer-file fact\"\n  set_fact:\n    answer_file: \"answer-file-{{ ansible_date_time.date }}-{{ ansible_date_time.time }}\"\n\n- name: \"Generate the Initial answer file\"\n  command: \"packstack --gen-answer-file={{ answer_file }}\"\n\n- name: \"Populate the yes/no choices in the answer file\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^{{ item.key }}'\n    line: '{{ item.key }}={{ item.value }}'\n  with_dict: \"{{ osp_install_choices }}\"\n\n- name: \"Populate the answer file with OSP host values\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^{{ item.answer_key }}='\n    line: '{{ item.answer_key }}={{ config[item.config_key] }}'\n  with_items:\n  - { config_key: 'amqp', answer_key: 'CONFIG_AMQP_HOST' }\n  - { config_key: 'compute', answer_key: 'CONFIG_COMPUTE_HOSTS' }\n  - { config_key: 'controller', answer_key: 'CONFIG_CONTROLLER_HOST' }\n  - { config_key: 'mariadb', answer_key: 'CONFIG_MARIADB_HOST' }\n  - { config_key: 'mongodb', answer_key: 'CONFIG_MONGODB_HOST' }\n  - { config_key: 'network', answer_key: 'CONFIG_NETWORK_HOSTS' }\n  - { config_key: 'redis', answer_key: 'CONFIG_REDIS_HOST' }\n\n- name: \"Update SSL_CERT Subject CN\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_SSL_CERT_SUBJECT_CN='\n    line: 'CONFIG_SSL_CERT_SUBJECT_CN={{ osp_public_fqdn }}'\n\n- name: \"Update SSL_CERT Subject MAIL\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_SSL_CERT_SUBJECT_MAIL='\n    line: 'CONFIG_SSL_CERT_SUBJECT_MAIL=admin@{{ osp_public_fqdn }}'\n\n- name: \"Replace PW placeholders\"\n  replace:\n    path: \"{{ answer_file }}\"\n    regexp: 'PW_PLACEHOLDER'\n    replace: '{{ pw_placeholder_replacement }}'\n\n- name: \"Update MariaDB Password\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_MARIADB_PW='\n    line: 'CONFIG_MARIADB_PW={{ mariadb_passwd }}'\n\n- name: \"Update Keystone DB Password\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_KEYSTONE_DB_PW='\n    line: 'CONFIG_KEYSTONE_DB_PW={{ keystone_db_password }}'\n\n- name: \"Update Keystone Admin Password\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_KEYSTONE_ADMIN_PW='\n    line: 'CONFIG_KEYSTONE_ADMIN_PW={{ keystone_admin_password }}'\n\n- name: \"Update the default password\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_DEFAULT_PASSWORD='\n    line: 'CONFIG_DEFAULT_PASSWORD={{ default_password }}'\n\n- name: \"Update to NOT create cinder volumes\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_CINDER_VOLUMES_CREATE='\n    line: 'CONFIG_CINDER_VOLUMES_CREATE=n'\n\n- name: \"Allow for SSH migration of instances\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_NOVA_COMPUTE_MIGRATE_PROTOCOL='\n    line: 'CONFIG_NOVA_COMPUTE_MIGRATE_PROTOCOL=ssh'\n\n- name: \"Set Neutron L3 External Bridge to 'provider'\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_NEUTRON_L3_EXT_BRIDGE='\n    line: 'CONFIG_NEUTRON_L3_EXT_BRIDGE=provider'\n\n- name: \"Set Neutron ML2 Type Drivers\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_NEUTRON_ML2_TYPE_DRIVERS='\n    line: 'CONFIG_NEUTRON_ML2_TYPE_DRIVERS={{ neutron_ml2_type_drivers }}'\n\n- name: \"Set Neutron ML2 VLAN Ranges\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_NEUTRON_ML2_VLAN_RANGES='\n    line: 'CONFIG_NEUTRON_ML2_VLAN_RANGES={{ neutron_vlan_ranges }}'\n\n- name: \"Set OVS Bridge Interfaces\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_NEUTRON_OVS_BRIDGE_IFACES='\n    line: 'CONFIG_NEUTRON_OVS_BRIDGE_IFACES={{ neutron_ovs_bridge_ifaces }}'\n\n- name: \"Set OVS Tunnel Interface\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_NEUTRON_OVS_TUNNEL_IF='\n    line: 'CONFIG_NEUTRON_OVS_TUNNEL_IF={{ neutron_ovs_tunnel_if }}'\n\n- name: \"Set OVS Bridge Mappings\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_NEUTRON_OVS_BRIDGE_MAPPINGS='\n    line: 'CONFIG_NEUTRON_OVS_BRIDGE_MAPPINGS={{ neutron_ovs_bridge_mappings }}'\n\n- name: \"Set OVS Bridges Compute\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_NEUTRON_OVS_BRIDGES_COMPUTE='\n    line: 'CONFIG_NEUTRON_OVS_BRIDGES_COMPUTE={{ neutron_ovs_bridges_compute }}'\n\n- name: \"Set NTP servers\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_NTP_SERVERS='\n    line: 'CONFIG_NTP_SERVERS={{ ntp_servers }}'\n\n- name: \"Do not provision demo usage and testing\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_PROVISION_DEMO='\n    line: 'CONFIG_PROVISION_DEMO=n'\n\n- name: \"Enable SSL for Horizon (dashboard)\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_HORIZON_SSL='\n    line: 'CONFIG_HORIZON_SSL=y'\n\n- name: \"Disable self-signed certs\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_SSL_CACERT_SELFSIGN='\n    line: 'CONFIG_SSL_CACERT_SELFSIGN=n'\n\n- name: \"Set the cert directory\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_SSL_CERT_DIR='\n    line: 'CONFIG_SSL_CERT_DIR={{ ssl_cert_directory }}'\n\n- name: \"Set the SSL CA cert\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_SSL_CACERT_FILE='\n    line: 'CONFIG_SSL_CACERT_FILE={{ ssl_cacert_file }}'\n\n- name: \"Set the SSL CA Cert key file\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_SSL_CACERT_KEY_FILE='\n    line: 'CONFIG_SSL_CACERT_KEY_FILE={{ ssl_host_key_file }}'\n\n- name: \"Set the VNC SSL cert\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_VNC_SSL_CERT='\n    line: 'CONFIG_VNC_SSL_CERT={{ ssl_host_cert_file }}'\n\n- name: \"Set the VNC SSL key file\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_VNC_SSL_KEY='\n    line: 'CONFIG_VNC_SSL_KEY={{ ssl_host_key_file }}'\n\n- name: \"Set the Horizon CA cert\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_HORIZON_SSL_CACERT='\n    line: 'CONFIG_HORIZON_SSL_CACERT={{ ssl_cacert_file }}'\n\n- name: \"Set the Horizon SSL cert\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_HORIZON_SSL_CERT='\n    line: 'CONFIG_HORIZON_SSL_CERT={{ ssl_host_cert_file }}'\n\n- name: \"Set the Horizon SSL key file\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_HORIZON_SSL_KEY='\n    line: 'CONFIG_HORIZON_SSL_KEY={{ ssl_host_key_file }}'\n\n- name: \"Provision Cinder related settings\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_CINDER_{{ item.key|upper }}='\n    line: 'CONFIG_CINDER_{{ item.key|upper }}={{ item.value }}'\n  with_dict: \"{{ cinder }}\"\n\n- name: \"Configure keystone identity backend\"\n  lineinfile:\n    path: \"{{ answer_file }}\"\n    regexp: '^CONFIG_KEYSTONE_IDENTITY_BACKEND='\n    line: 'CONFIG_KEYSTONE_IDENTITY_BACKEND={{ keystone_identity_backend }}'\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "152419f7c5cbcba9a459fa5e1c5b984fb202e07e", "filename": "tasks/create_repo_maven_hosted_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_maven_hosted\n    args: \"{{ _nexus_repos_maven_defaults|combine(item) }}\""}, {"commit_sha": "bf6e08dcb2440421477b6536ff6a8d11adc2be17", "sha": "fa00f2d16f3e1727be23bc0034b12f738fdd9a1c", "filename": "roles/serverspec/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for serverspec\n\n- name: upload serverspecs\n  synchronize:\n    src: ../../../tests\n    dest: \"{{ serverspec_tests_path }}\"\n    recursive: yes\n    delete: yes\n  when: serverspec_run_tests and serverspec_install_bundler and serverspec_upload_folder\n  tags:\n    - serverspec\n\n- name: install bundler\n  command: gem install bundler --no-ri --no-rdoc\n  args:\n    creates: /usr/local/bin/bundler\n  when: serverspec_run_tests and serverspec_install_bundler\n  tags:\n    - serverspec\n\n- name: install bundle files\n  command: bundle install --path vendor\n  args:\n    chdir: \"{{ serverspec_tests_path }}/tests\"\n    creates: \"{{ serverspec_tests_path }}/tests/vendor\"\n  when: serverspec_run_tests\n  tags:\n    - serverspec\n\n- name: run serverspec tests\n  sudo: yes\n  command: \"bundle exec rake serverspec:{{ test_role }}\"\n  args:\n    chdir: \"{{ serverspec_tests_path }}/tests\"\n  when: test_role is defined and serverspec_run_tests\n  tags:\n    - serverspec\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "38f9e19fd447b876177dfd1d1263ea66de1cac8e", "filename": "roles/dnsmasq/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for dnsmasq\n- name: restart dnsmasq\n  service:\n    name: dnsmasq\n    state: restarted\n  sudo: yes\n\n- name: reload dnsmasq\n  service:\n    name: dnsmasq\n    state: reloaded\n  sudo: yes\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "053a471b6806f28c6be2853139a478b9f2c0db65", "filename": "roles/config-hostname/tasks/main.yaml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- block:\n  - import_tasks: prep.yml\n  - import_tasks: set-hostname.yml\n  become: True\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b8d2a060f7becd20f504293aa7f5746341b277ff", "filename": "reference-architecture/gcp/ansible/playbooks/roles/pre-flight-validation/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: check required packages\n  include: check-package.yaml\n  with_items:\n  - curl\n  - python\n  - which\n  - openssl\n  - git\n  - python-libcloud\n  - python2-jmespath\n\n- name: check required packages for enterprise deployments\n  include: check-package.yaml\n  with_items:\n  - tar\n  - qemu-img\n  when: openshift_deployment_type == 'openshift-enterprise'\n\n- name: check required packages for metrics\n  include: check-package.yaml\n  with_items:\n  - java-1.8.0-openjdk-headless\n  - httpd-tools\n  - python2-passlib\n  when: openshift_hosted_metrics_deploy\n\n- name: check gcloud utility\n  command: which gcloud\n  ignore_errors: true\n  changed_when: false\n  register: gcloud_state\n\n- name: assert that gcloud is present\n  assert:\n    that:\n    - gcloud_state | succeeded\n    msg: gcloud utility seems to be missing. For installation information, see https://cloud.google.com/sdk/downloads\n\n- block:\n  - name: stat rhel image file\n    stat:\n      path: '{{ rhel_image_path }}'\n    register: rhel_image_path_stat\n\n  - name: verify that the image is readable\n    assert:\n      that:\n      - 'rhel_image_path_stat.stat.exists'\n      - 'rhel_image_path_stat.stat.readable'\n      msg: RHEL image file must exist and it must be readable\n  when: openshift_deployment_type == 'openshift-enterprise'\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "a90251a748fe8d216ba2fe62d05f03796325fc2b", "filename": "ops/playbooks/resize_syspart.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n####################################################################################\n#\n# play 1: Resize Disk Partition of Linux Systems\n#\n#####################################################################################\n- name: Resize System Partition (Linux))\n  hosts: vms\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n    - name: Resize system partition (Linux)\n      shell: |\n        start=$(parted /dev/sda unit mb print free | tail -2 | awk \"/Free Space/ { print \\$1 }\")\n        if [ \"$start\" == \"\" ]\n        then \n          echo \"changed: false\"\n        else  \n          parted /dev/sda mkpart primary ext4 $start 100%\n          partprobe -s\n          pvcreate /dev/sda3\n          vgextend rhel /dev/sda3\n          lvextend -l +100%FREE /dev/rhel/root\n          xfs_growfs /\n          echo \"changed: true\"\n        fi\n      register: res\n      changed_when: '\"changed: true\" in res.stdout_lines'\n\n####################################################################################\n#\n# play 2: Resize Disk Partition of Windows Systems\n#\n#####################################################################################\n- name: Resize System Partition (Windows)\n  hosts: win_worker\n  gather_facts: false\n  connection: local\n  user: remote\n  become: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  tasks:\n\n    - name: Resize Windows Partition\n      win_shell: | \n        $supported = (Get-PartitionSupportedSize -DiskNumber 0 -PartitionNumber 2)\n        $current = Get-Partition -DiskNumber 0 -PartitionNumber 2\n        if ( $current.Size -lt $supported.SizeMax )\n        {\n          Resize-Partition -DiskNumber 0 -PartitionNumber 2 -Size $supported.SizeMax\n          \"changed: true\"\n        } \n        else \n        {\n          \"changed: false\"\n        }\n      register: res\n      changed_when: '\"changed: true\" in res.stdout_lines'\n      when: inventory_hostname in groups.win_worker\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "1d513f0bd12afb752197f30cc01002c4ba1d453b", "filename": "roles/config-quay-enterprise/tasks/firewall.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Check if firewalld is installed\n  command: systemctl status firewalld\n  register: firewalld_status\n  failed_when: false\n  changed_when: false\n\n- name: Check if iptables is installed\n  command: systemctl status iptables\n  register: iptables_status\n  failed_when: false\n  changed_when: false\n\n- name: Open port in firewalld\n  firewalld:\n    port: \"{{ item }}/TCP\"\n    permanent: true\n    state: enabled\n  when: firewalld_status.rc == 0\n  with_items:\n    - \"{{ quay_http_port }}\"\n    - \"{{ quay_https_port }}\"\n  notify:\n  - restart firewalld\n\n- name: Ensure iptables is correctly configured \n  lineinfile:\n    insertafter: \"^-A INPUT .* --dport {{ item }} .* ACCEPT\"\n    state: present\n    dest: /etc/sysconfig/iptables\n    regexp: \"^-A INPUT .* --dport {{ item }} .* ACCEPT\"\n    line: \"-A INPUT -p TCP -m state --state NEW -m TCP --dport {{ item }} -j ACCEPT\"\n  with_items:\n    - \"{{ quay_http_port }}\"\n    - \"{{ quay_https_port }}\"\n  when: iptables_status.rc == 0 and firewalld_status.rc != 0 \n  notify:\n  - restart iptables\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "3edace493d56e21cb14f64e5688b18eefa14e7ec", "filename": "playbooks/aws/openshift-cluster/launch.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Launch instance(s)\n  hosts: localhost\n  connection: local\n  become: no\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  tasks:\n  - include: ../../common/openshift-cluster/tasks/set_etcd_launch_facts.yml\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ etcd_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"default\"\n\n  - include: ../../common/openshift-cluster/tasks/set_master_launch_facts.yml\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ master_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"default\"\n\n  - include: ../../common/openshift-cluster/tasks/set_node_launch_facts.yml\n    vars:\n      type: \"compute\"\n      count: \"{{ num_nodes }}\"\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ node_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"{{ sub_host_type }}\"\n\n  - include: ../../common/openshift-cluster/tasks/set_node_launch_facts.yml\n    vars:\n      type: \"infra\"\n      count: \"{{ num_infra }}\"\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ node_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"{{ sub_host_type }}\"\n\n  - add_host:\n      name: \"{{ master_names.0 }}\"\n      groups: service_master\n    when: master_names is defined and master_names.0 is defined\n\n- include: update.yml\n- include: list.yml\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "1f1de192470bc39496f4676565f96d2758c1e5b4", "filename": "tasks/section_12.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - include: section_12_level1.yml\n    tags:\n      - section12\n      - level1\n\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "7980f94f066f10feed2f0331049acfae44d26c66", "filename": "roles/ipaserver/meta/main.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# dependencies:\n#   - role: t_woerner.ipaclient\n\ngalaxy_info:\n  author: Thomas Woerner\n  description: A role to setup an iPA domain server\n  company: Red Hat, Inc\n  license: GPLv3\n  min_ansible_version: 2.5\n  platforms:\n  - name: Fedora\n    versions:\n    - all\n  - name: EL\n    versions:\n    - 7\n    # - 8\n  galaxy_tags:\n    - identity\n    - ipa\n    - freeipa\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b1dc1fb0545a93ef8e460415b78466c073cc8d8b", "filename": "roles/scm/gitlab.com/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n# defaults file for gitlab\ngitlab_api_base: https://gitlab.com/api/v4\ngitlab_api_projects: \"{{ gitlab_api_base }}/projects\"\ngitlab_api_groups: \"{{ gitlab_api_base }}/groups\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "66c49e344024f03bf35a180f346d55138724871c", "filename": "roles/dhcp/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n# Location on target machine to place the config file\ndhcp_config_dest_file: /etc/dhcp/dhcpd.conf\n\n# default name for temp file that is created from the template\ndhcp_config_temp_file: dhcpTempConfigFile\n\n# Temp Directory\ndhcp_config_temp_dir: /tmp\n\n# default location on target machine for testing syntax\ndhcp_config_temp_loc: '{{dhcp_config_temp_dir }}/{{ dhcp_config_temp_file }}'\n\n#default null entries\ndhcp_host_entries: {}\ndhcp_group_entries: {}\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "293280d6cec36342a911a539e9b2a8bf00a59859", "filename": "roles/2-common/tasks/udev.yml", "repository": "iiab/iiab", "decoded_content": "- name: Does systemd-udevd.service exist\n  stat:\n    path: \"{{ systemd_location }}/systemd-udevd.service\"\n  register: udev_unit\n\n- name: Copy udevd service to /etc/systemd/system to modify\n  copy:\n    src: \"{{ systemd_location }}/systemd-udevd.service\"\n    dest: /etc/systemd/system/systemd-udevd.service\n    owner: root\n    group: root\n    mode: 0644\n  when: udev_unit.stat.exists is defined and udev_unit.stat.exists\n\n- name: Change MountFlags from slave to shared\n  lineinfile:\n    backup: no\n    dest: /etc/systemd/system/systemd-udevd.service\n    regexp: '^MountFlags'\n    line: 'MountFlags=shared'\n    state: present\n  when: udev_unit.stat.exists is defined and udev_unit.stat.exists\n\n# ubuntu 16.04 comes with ansible 2.0.0.2 -- no systemd module\n- name: Ask systemd to reread unit files (daemon-reload)\n  systemd:\n    daemon_reload: yes\n  when: udev_unit.stat.exists is defined and udev_unit.stat.exists\n\n- name: Restart so systemd recognizes the changes\n  shell: systemctl restart systemd-udevd.service\n  when: udev_unit.stat.exists is defined and udev_unit.stat.exists\n\n- name: Reload systemd-udevd so it has rootfs open read-write\n  template:\n    src: udev-reload.service\n    dest: /etc/systemd/system/\n\n- name: Enable the reload service\n  shell: systemctl enable udev-reload.service\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "9ee638d9b64d36bd2fe268eaf5221b1bc479f68f", "filename": "archive/roles/openstack-create/tasks/security_groups.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: \"Create Security Groups if required\"\n  os_security_group:\n    name: \"{{ item.name }}\"\n    state: present\n  with_items: \"{{ security_groups }}\"\n  when: neutron_in_use\n\n- name: \"Create SSH Rule in matching Security Group if required\"\n  os_security_group_rule:\n    security_group: \"{{ item.0.name }}\"\n    protocol: \"{{ item.1.protocol }}\"\n    port_range_min: \"{{ item.1.from_port }}\"\n    port_range_max: \"{{ item.1.to_port }}\"\n    remote_ip_prefix: \"{{ item.1.cidr }}\"\n  when:\n    - item.1.name is defined\n    - item.1.protocol is defined\n    - item.1.from_port is defined\n    - item.1.to_port is defined\n    - item.1.cidr is defined\n    - neutron_in_use\n  with_subelements:\n    - \"{{ security_groups }}\"\n    - rules\n\n# Build a comma-separated list of security groups defined in the array\n# Initialize list variable so previous runs are not concatenated\n- set_fact:\n    security_groups_list: \"\"\n\n- name: \"Build a list of Security Groups\"\n  set_fact:\n    security_groups_list: \"{{ [item.name,security_groups_list | default('')] | join(',') }}\"\n  with_items: \"{{ security_groups }}\"\n\n# This is an optional task as it cleans up the string variable, otherwise the way it is constructed would result in an extra comma at the end as such 'group1,group2,\" and this can be passed to the nova_compute module just fine but this extra step just cleans it up\n- set_fact:\n    security_groups_list: \"{{ security_groups_list | regex_replace('^(.*),$', '\\\\1') }}\"\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "5998d6de6196e5d2134c3d76c2bdf69d63a868e0", "filename": "roles/ipareplica/vars/Fedora.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# Fedora defaults file for ipareplica\n# vars/Fedora.yml\nipareplica_packages: [ \"freeipa-server\", \"python3-libselinux\" ]\nipareplica_packages_dns: [ \"freeipa-server-dns\" ]\nipareplica_packages_adtrust: [ \"freeipa-server-trust-ad\" ]"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b0e4c832c2cdae7d19a146bf2a5c6afe796ab0f6", "filename": "roles/static_inventory/tasks/sshtun.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Create ssh tunnel systemd service\n  template:\n    src: ssh-tunnel.service.j2\n    dest: /etc/systemd/system/ssh-tunnel.service\n    mode: 0644\n\n- name: reload the systemctl daemon after file update\n  command: systemctl daemon-reload\n\n- name: Enable ssh tunnel service\n  service:\n    name: ssh-tunnel\n    enabled: true\n    state: restarted\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "d92f27b0a011db88f6a63af413c05a9f6b065236", "filename": "roles/update-host/tasks/update-host.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Update the host\"\n  package:\n    name: \"*\"\n    state: latest\n  register: host_updated\n  when: \n    - pkg_update|default(False) \n\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "b35b7e479d98497f8f8d1be294b2dd6deceba885", "filename": "roles/openshift-management/tasks/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: Validate OpenShift Token Provided\n  fail: msg=\"OpenShift Token Not Provided\"\n  failed_when: (openshift_token is undefined) or (openshift_token is none) or (openshift_token|trim == '')\n  tags: always\n\n- name: Set Facts\n  set_fact: kubeconfig=\"/tmp/openshift-management-{{ ansible_date_time.epoch }}/config\"\n  tags: always\n\n- name: Create Directory\n  file: path=\"{{ kubeconfig | dirname }}\" state=directory\n  notify: cleanup openshift login\n  tags: always\n\n- name: Login to OpenShift\n  shell: oc login --token={{ openshift_token }} {{ openshift_login_insecure_flag }} {{ openshift_master_url }} \n  environment:\n    KUBECONFIG: \"{{ kubeconfig }}\"\n  tags: always\n\n- import_tasks: prune-builds.yml\n  when: \"{{ openshift_prune_builds | default(False) }}\"\n  tags: openshift-management-builds\n\n- import_tasks: prune-deployments.yml\n  when: \"{{ openshift_prune_deployments | default(False) }}\"\n  tags: openshift-management-deployments\n\n- import_tasks: prune-images.yml\n  when: \"{{ openshift_prune_images | default(False) }}\"\n  tags: openshift-management-images\n\n- import_tasks: prune-projects.yml\n  when: \"{{ openshift_prune_projects | default(False) }}\"\n  tags: openshift-management-projects\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4ecf02929c80bba3c53026102c0e98bc4b52435f", "filename": "playbooks/provision-ansible-tower/README.md", "repository": "redhat-cop/infra-ansible", "decoded_content": "# Ansible Tower Provisioning playbook\n\nThis playbooks runs through the steps to provision a VM, set it up (subscriptions, updates, etc.), and install Ansible Tower.\nCurrently it is configured to provision OpenStack resources, but other providers can easily be added.\n\n## Prerequisites\nAccess to an Ansible Tower software and license.\n\nOne of the two:\n- a set of running instance(s)\n- a IaaS that allow for provisioning through these playbooks\n\n\n## Example\n\n### Inventory\n\nPlease see the **sample** inventory in the inventory area:\n\n- [ansible-tower](../../inventory/ansible-tower/README.md)\n\nYou will need to modify this sample inventory to fit your desired configuration.\n\n### Playbook execution\n\nDepending on how this is being hosted, the initial may need the `tags='install'` set to ensure all necessary software is installed:\n\n```bash\n> ansible-playbook -i inventory main.yml --tags='install'\n```\n\nAny consecutive runs can be done without the 'install' tag to speed up execution:\n```bash\n> ansible-playbook -i inventory main.yml\n```\n\nLicense\n-------\n\nApache License 2.0\n\n\nAuthor Information\n------------------\n\nRed Hat Community of Practice & staff of the Red Hat Open Innovation Labs.\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "fa204f6d54f7d2a107ca7af9f9bd3553c062e173", "filename": "reference-architecture/vmware-ansible/playbooks/nfs.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - create-vm-nfs\n\n- name: Deploy NFS server\n  hosts: nfs_group\n  gather_facts: true\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - rhsm\n    - vmware-guest-setup\n    - nfs-server\n  ignore_errors: yes\n"}, {"commit_sha": "4a9aaf0951e383c57077cf651b93e78eeea1b5ac", "sha": "f5a87e55e4826da242a3d88e68d40227bd28f115", "filename": "tasks/trim-fat.yml", "repository": "geerlingguy/ansible-role-solr", "decoded_content": "---\n- name: Remove the downloaded Solr archive.\n  file:\n    path: \"{{ item }}\"\n    state: absent\n  with_items:\n    - \"{{ solr_workspace }}/{{ solr_filename }}.tgz\"\n    - \"{{ solr_workspace }}/{{ solr_filename }}\"\n\n- name: Remove docs, if not needed.\n  file:\n    path: \"{{ solr_install_path }}/docs\"\n    state: absent\n  when: solr_remove_cruft\n\n- name: Remove example dir, if not needed.\n  file:\n    path: \"{{ solr_install_path }}/example\"\n    state: absent\n  when:\n    - solr_remove_cruft\n    - solr_version.split('.')[0] >= '5'\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "bae10969e2def66aa831a4bf80a62566784398ae", "filename": "roles/ovirt-engine-db-dump/meta/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\ngalaxy_info:\n  author: \"Lucie Leistnerova\"\n  description: \"oVirt DB dump\"\n  company: \"Red Hat\"\n  license: \"GPLv3\"\n  min_ansible_version: 1.9\n  platforms:\n  - name: EL\n    versions:\n    - all\n  galaxy_tags:\n    - migration\n    - export\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "03379b692a43f7158c254217982a7d7f65d4f88f", "filename": "roles/dns/config-dns-server/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: named/main.yml\n\n# Add calls to additional providers here ...\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "282caa6c224daa029362bbc4d05215b279bd4c98", "filename": "tasks/create_repo_npm_proxy_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_npm_proxy\n    args: \"{{ _nexus_repos_npm_defaults|combine(item) }}\""}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "a71df7433de4fbdcbeb6ecac24430f04ea6cb091", "filename": "roles/ipaserver/tasks/main.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "---\n# tasks file for ipaserver\n\n- name: Import variables specific to distribution\n  include_vars: \"{{ item }}\"\n  with_first_found:\n    - \"vars/{{ ansible_distribution }}-{{ ansible_distribution_version }}.yml\"\n    - \"vars/{{ ansible_distribution }}-{{ ansible_distribution_major_version }}.yml\"\n    - \"vars/{{ ansible_distribution }}.yml\"\n    - \"vars/default.yml\"\n\n- name: Install IPA server\n  include_tasks: tasks/install.yml\n  when: state|default('present') == 'present'\n\n- name: Uninstall IPA server\n  include_tasks: tasks/uninstall.yml\n  when: state|default('present') == 'absent'\n"}, {"commit_sha": "4a9aaf0951e383c57077cf651b93e78eeea1b5ac", "sha": "6123e2dd7a90b5725c78545141f89dda948d96a4", "filename": "tasks/configure.yml", "repository": "geerlingguy/ansible-role-solr", "decoded_content": "---\n- name: Remove existing SOLR_HEAP configuration.\n  lineinfile:\n    dest: \"{{ solr_config_file }}\"\n    regexp: \"^SOLR_HEAP\"\n    state: absent\n  notify: restart solr\n\n- name: Apply Solr configuration changes.\n  lineinfile:\n    dest: \"{{ solr_config_file }}\"\n    regexp: \"{{ item.regexp }}\"\n    line: \"{{ item.line }}\"\n    state: present\n  with_items:\n    - regexp: \"^.?SOLR_JAVA_MEM=\"\n      line: 'SOLR_JAVA_MEM=\"-Xms{{ solr_xms }} -Xmx{{ solr_xmx }}\"'\n    - regexp: \"^SOLR_PORT=\"\n      line: 'SOLR_PORT=\"{{ solr_port }}\"'\n    - regexp: \"^.?SOLR_TIMEZONE=\"\n      line: 'SOLR_TIMEZONE=\"{{ solr_timezone }}\"'\n  notify: restart solr\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "a83b3406b552ca4cfaed95ea9f31bc556cb52992", "filename": "playbooks/delete-data.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- hosts: all\n  vars:\n    rock_debug: \"{{ lookup('env', 'DEBUG') }}\"\n    http_proxy: \"{{ lookup('env','http_proxy') }}\"\n    https_proxy: \"{{ lookup('env', 'https_proxy') }}\"\n  tasks:\n  - name: Get default settings\n    include_vars: rocknsm_config.dist.yml\n  - name: Apply override settings, if available\n    include_vars: /etc/rocknsm/config.yml\n    ignore_errors: true\n    failed_when: false\n  - name: Debug variables\n    include: debug.yml\n    when: rock_debug is defined and rock_debug\n\n    ######################################################\n    ######### Stop Services ##############################\n    ######################################################\n\n  - name: Stop rocknsm services\n    command: /sbin/rock_stop\n\n    ######################################################\n    ######### Delete Data ################################\n    ######################################################\n\n  - name: Remove rock_data_dir\n    file:\n      state: absent\n      path: \"{{ rock_data_dir }}\"\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "f49335b92676307b2917ed8ba5fc14918e277121", "filename": "tasks/Win32NT/fetch/security-fetch/security-winfetch-oracle-fallback.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Download security policy artifact from Oracle OTN\n  win_get_url:\n    url: '{{ fallback_oracle_security_policy_artifacts[java_major_version|int] }}'\n    dest: >-\n      {{ java_download_path }}\\{{ (fallback_oracle_security_policy_artifacts[java_major_version|int]\n        | urlsplit('path')).split('/')[-1] }}\n    headers:\n      Cookie: 'oraclelicense=accept-securebackup-cookie'\n    force: false\n  register: policy_file_downloaded\n  retries: 15\n  delay: 5\n  until: policy_file_downloaded is succeeded\n\n- name: Downloaded security policy artifact\n  set_fact:\n    security_policy_java_artifact: '{{ policy_file_downloaded.dest }}'\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "5804bc65c2d68f7904bc7059f4ae02f0e81ff41f", "filename": "roles/ovirt-engine-backup/defaults/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_engine_type: 'ovirt-engine'\novirt_backup_mode: 'backup'\novirt_backup_archive: '/tmp/engine-backup.gzip'\novirt_backup_log_file: '/tmp/engine-backup.log'\novirt_backup_scope: 'all'\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "fa6f778a2e80c1fbf03917e985481426f111d6e5", "filename": "roles/ovirt-engine-db-dump/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# get engine and dwh database settings\n- name: set variables\n  set_fact:\n    ovirt_engine_db_dump_engine_db: {}\n    ovirt_engine_db_dump_dwh_db: {}\n\n- name: grep engine variables\n  shell: grep \"^ENGINE_DB\" {{ ovirt_engine_etc_conf_path }}/10-setup-database.conf\n  register: cmd\n  tags:\n    - skip_ansible_lint\n\n- name: set facts for engine variables\n  set_fact:\n    ovirt_engine_db_dump_engine_db: >\n      {{\n        ovirt_engine_db_dump_engine_db |\n        combine(\n          dict([ item.partition('=')[::2] ])\n        )\n      }}\n  with_items: \"{{ cmd.stdout_lines|list }}\"\n\n- name: grep dwh variables\n  shell: grep \"^DWH_DB\" {{ ovirt_engine_etc_conf_path }}/10-setup-dwh-database.conf\n  register: cmd\n  when: ovirt_engine_db_dump_dwh == True\n  tags:\n    - skip_ansible_lint\n\n- name: set facts for dwh variables\n  set_fact:\n    ovirt_engine_db_dump_dwh_db: >\n      {{\n        ovirt_engine_db_dump_dwh_db |\n        combine(\n          dict([ item.partition('=')[::2] ])\n        )\n      }}\n  with_items: \"{{ cmd.stdout_lines|list }}\"\n  when: ovirt_engine_db_dump_dwh == True\n\n# stop engine and dwh services that it won't generate more data to database\n- name: stop necessary service\n  service:\n    name: \"{{ item }}\"\n    state: stopped\n  with_items:\n    - ovirt-engine-dwhd\n    - ovirt-engine\n\n# get engine and dwh database dumps\n- name: create local dir\n  local_action:\n    file path={{ playbook_dir }}/engine_dump state=directory\n\n- name: get dump of engine database\n  become: yes\n  become_user: postgres\n  shell: pg_dump -F c {{ ovirt_engine_db_dump_engine_db['ENGINE_DB_DATABASE'] }} -f engine.sql\n  tags:\n    - skip_ansible_lint\n\n- name: get dump of dwh database\n  become: yes\n  become_user: postgres\n  shell: pg_dump -F c {{ ovirt_engine_db_dump_dwh_db['DWH_DB_DATABASE'] }} -f dwh.sql\n  when: ovirt_engine_db_dump_dwh == True\n  tags:\n    - skip_ansible_lint\n\n# print variables to file\n- name: print engine variables to file\n  local_action:\n        copy content=\"{{ ovirt_engine_db_dump_engine_db }}\" dest=\"{{ playbook_dir }}/engine_dump/engine_variables.json\"\n\n- name: print DWH variables to file\n  local_action:\n        copy content=\"{{ ovirt_engine_db_dump_dwh_db }}\" dest=\"{{ playbook_dir }}/engine_dump/dwh_variables.json\"\n  when: ovirt_engine_db_dump_dwh == True\n\n# copy files to localhost\n- name: get postgres path\n  become: yes\n  become_user: postgres\n  become_method: su\n  become_flags: \"-\"\n  shell: echo $HOME\n  register: pgpath\n  tags:\n    - skip_ansible_lint\n\n- name: copy engine dump to local\n  fetch:\n    src: \"{{ pgpath.stdout }}/engine.sql\"\n    dest: \"{{ playbook_dir }}/engine_dump/engine.sql\"\n    flat: yes\n\n- name: copy dwh dump to local\n  fetch:\n    src: \"{{ pgpath.stdout }}/dwh.sql\"\n    dest: \"{{ playbook_dir }}/engine_dump/dwh.sql\"\n    flat: yes\n  when: ovirt_engine_db_dump_dwh == True\n\n# start needed services\n- name: start necessary service\n  service:\n    name: \"{{ item }}\"\n    state: started\n  with_items:\n    - ovirt-engine\n    - ovirt-engine-dwhd\n  when: ovirt_engine_db_dump_start_services == True\n\n# clean\n- name: remove dumps from engine\n  file:\n    path: \"{{ pgpath.stdout }}/{{ item }}.sql\"\n    state: absent\n  with_items:\n    - engine\n    - dwh\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "5db3ab6d07aef9878d52305772f2dcf9af678795", "filename": "tasks/section_12_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 12.1 Verify Permissions on /etc/passwd (Scored)\n    file: path=/etc/passwd mode=0644\n    tags:\n      - section12\n      - section12.1\n\n  - name: 12.2 Verify Permissions on /etc/shadow (Scored)\n    file: path=/etc/shadow mode='o-rwx,g-rw'\n    tags:\n      - section12\n      - section12.2\n\n  - name: 12.3 Verify Permissions on /etc/group (Scored)\n    file: path=/etc/group mode=0644\n    tags:\n      - section12\n      - section12.3\n\n  - name: 12.4 Verify User/Group Ownership on /etc/passwd (Scored)\n    file: path=/etc/passwd owner=root group=root\n    tags:\n      - section12\n      - section12.4\n\n  - name: 12.5 Verify User/Group Ownership on /etc/shadow (Scored)\n    file: path=/etc/shadow owner=root group=shadow\n    tags:\n      - section12\n      - section12.5\n\n  - name: 12.6 Verify User/Group Ownership on /etc/group (Scored)\n    file: path=/etc/group owner=root group=root\n    tags:\n      - section12\n      - section12.6\n\n  - name: 12.7 Find World Writable Files (check) (Not Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -type f -perm -0002 -print\n    changed_when: False\n    failed_when: False\n    register: world_files\n    tags:\n      - section12\n      - section12.7\n\n  - name: 12.7 Find World Writable Files (Not Scored)\n    debug: msg=\"{{ item }}\"\n    with_items:\n        world_files.stdout_lines\n    tags:\n      - section12\n      - section12.7\n\n  - name: 12.8 Find Un-owned Files and Directories (check) (Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -nouser -ls\n    changed_when: False\n    failed_when: False\n    register: unowned_files\n    tags:\n      - section12\n      - section12.8\n\n  - name: 12.8 Find Un-owned Files and Directories (Scored)\n    debug: msg=\"{{ item }}\"\n    with_items:\n        unowned_files.stdout_lines\n    tags:\n      - section12\n      - section12.9\n\n  - name: 12.9 Find Un-grouped Files and Directories (check) (Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -nogroup -ls\n    changed_when: False\n    failed_when: False\n    register: ungrouped_files\n    tags:\n      - section12\n      - section12.9\n\n  - name: 12.9 Find Un-grouped Files and Directories (Scored)\n    debug: >\n        msg=\"{{ item }}\"\n    with_items:\n        ungrouped_files.stdout_lines\n    tags:\n      - section12\n      - section12.9\n\n  - name: 12.10 Find SUID System Executables (check) (Not Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -type f -perm -4000 -print\n    changed_when: False\n    failed_when: False\n    register: suid_files\n    tags:\n      - section12\n      - section12.10\n\n  - name: 12.10 Find SUID System Executables (Not Scored)\n    debug: msg=\"{{ item }}\"\n    with_items:\n        suid_files.stdout_lines\n    tags:\n      - section12\n      - section12.10\n\n  - name: 12.11 Find SGID System Executables (check) (Not Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -type f -perm -2000 -print\n    changed_when: False\n    failed_when: False\n    register: gsuid_files\n    tags:\n      - section12\n      - section12.11\n\n  - name: 12.11 Find SGID System Executables (Not Scored)\n    debug: msg=\"{{ item }}\"\n    with_items:\n        gsuid_files.stdout_lines\n    tags:\n      - section12\n      - section12.10\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "29856ccddbe652b15b8421d135230ba933b157a5", "filename": "roles/gluster-ports/handlers/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: restart iptables\n  service: name=iptables state=restarted\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "8ce3163acd185ccfdb7dcb8d70cee22ace015f8b", "filename": "roles/vpn/handlers/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: restart strongswan\n  service: name=strongswan state=restarted\n\n- name: daemon-reload\n  shell: systemctl daemon-reload\n\n- name: restart apparmor\n  service: name=apparmor state=restarted\n\n- name: save iptables\n  shell: service netfilter-persistent save\n\n- name: restart iptables\n  service: name=netfilter-persistent state=restarted\n\n- name: rereadcrls\n  shell: ipsec rereadcrls; ipsec purgecrls\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "209746310e7c6b42eae17e87dcc86f4788cc5189", "filename": "roles/config-nagios-target/tasks/install-epel.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Installing EPEL Software Repo\n  package:\n    name=\"{{item}}\"\n    state=present\n  with_items:\n  - http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n  tags: epel\n\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "33434081a22cc642025cc2121202710271cca79b", "filename": "roles/wireguard/tasks/keys.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Delete the lock files\n  file:\n    dest: \"{{ config_prefix|default('/') }}etc/wireguard/private_{{ item }}.lock\"\n    state: absent\n  when: keys_clean_all|bool == True\n  with_items:\n    - \"{{ users }}\"\n    - \"{{ IP_subject_alt_name }}\"\n\n- name: Generate private keys\n  command: wg genkey\n  register: wg_genkey\n  args:\n    creates: \"{{ config_prefix|default('/') }}etc/wireguard/private_{{ item }}.lock\"\n  with_items:\n    - \"{{ users }}\"\n    - \"{{ IP_subject_alt_name }}\"\n\n- block:\n  - name: Save private keys\n    copy:\n      dest: \"{{ wireguard_config_path }}/private/{{ item['item'] }}\"\n      content: \"{{ item['stdout'] }}\"\n      mode: \"0600\"\n    no_log: true\n    when: item.changed\n    with_items: \"{{ wg_genkey['results'] }}\"\n    delegate_to: localhost\n    become: false\n\n  - name: Touch the lock file\n    file:\n      dest: \"{{ config_prefix|default('/') }}etc/wireguard/private_{{ item }}.lock\"\n      state: touch\n    with_items:\n      - \"{{ users }}\"\n      - \"{{ IP_subject_alt_name }}\"\n  when: wg_genkey.changed\n\n- name: Generate public keys\n  shell: echo \"{{ lookup('file', wireguard_config_path + '/private/' + item) }}\" | wg pubkey\n  register: wg_pubkey\n  changed_when: false\n  args:\n    executable: bash\n  with_items:\n    - \"{{ users }}\"\n    - \"{{ IP_subject_alt_name }}\"\n\n- name: Save public keys\n  copy:\n    dest: \"{{ wireguard_config_path }}/public/{{ item['item'] }}\"\n    content: \"{{ item['stdout'] }}\"\n    mode: \"0600\"\n  no_log: true\n  with_items: \"{{ wg_pubkey['results'] }}\"\n  delegate_to: localhost\n  become: false\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "955c780d05df33492657dfb2e3d49978e501f37b", "filename": "tasks/checks/distribution-checks-Fedora.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Fail if unsupported Fedora version\n  fail:\n    msg: \"Fedora 24 or later is required!\"\n  when: _docker_os_dist == \"Fedora\" and\n        _docker_os_dist_major_version | int < 24\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "2000d11de69d24df1f7f76b002482e7acf013dbd", "filename": "reference-architecture/gcp/ansible/playbooks/roles/wait-for-instance-group/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: wait until the instance group {{ instance_group }} is ready\n  command: gcloud --project {{ gcloud_project }} compute instance-groups managed describe {{ instance_group }} --region {{ gcloud_region }} --format 'yaml(targetSize, currentActions.none)'\n  register: instance_group_size_and_action_none\n  until: (instance_group_size_and_action_none.stdout | from_yaml).targetSize == (instance_group_size_and_action_none.stdout | from_yaml).currentActions.none\n  retries: 20\n  delay: 5\n  changed_when: false\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "8b137891791fe96927ad78e64b0aad7bded08bdc", "filename": "roles/registrator/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "286d0ac1bbc5db6fa9bd5b541323031aa3c6b866", "filename": "playbooks/deploy-host.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  tasks:\n    - name: Check for required variables\n      fail:\n        msg: \"Please pass a valid provider: vsphere,aws,gcp,rhv,osp e.g. -e provider=vsphere\"\n      when: provider is not defined\n\n- hosts: localhost\n  gather_facts: yes\n  become: yes\n  roles:\n    - rhsm\n    - deploy-host\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "e322b02434083777a4e8535e4a533a475ad64202", "filename": "playbooks/storage.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- hosts: localhost\n  vars:\n    storage_role: storage-demo\n  connection: local\n  gather_facts: False\n  # unset http_proxy. required for running in the CI\n  environment:\n    http_proxy: \"\"\n  roles:\n    - role: \"{{ storage_role }}\"\n    - role: \"cdi\"\n\n- hosts: masters nodes\n  vars:\n    storage_role: storage-demo\n  gather_facts: False\n  # unset http_proxy. required for running in the CI\n  environment:\n    http_proxy: \"\"\n  roles:\n    - role: storage-demo-nodeconfig\n      when: storage_role == 'storage-demo'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a2ab48d811d7291be0477c3fdf59f8ec2c6b43d2", "filename": "playbooks/provision-nfs-server/README.md", "repository": "redhat-cop/infra-ansible", "decoded_content": "# NFS Server playbook\n\nThis playbook is for provisioning a server to host NFS shares.\n\n## Example run\n\n```\n> ansible-playbook -i inventory/provision-nfs-server/ playbooks/provision-nfs-server/main.yml --tags='install'\n```\n\n## Inventory Options\n\n| variable | info |\n|:--------:|:----:|\n|nfs-shares|List of nfs shares to create, more details at [nfs-server role](https://github.com/redhat-cop/infra-ansible/tree/master/roles/nfs-server)|\n|nfs_storage_device|The storage device where the nfs shares will be running.|\n|rhsm_username|Subscription manager username|\n|rhsm_password|Subscription manager password|\n|rhsm_pool|The pool id to register the server with|\n\n\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "2cccca3fe6fd7e19677ba259319f24cb587347bc", "filename": "ops/playbooks/restore_dtr_images.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- hosts: nfs\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/backups\n\n  environment: \"{{ env }}\"\n\n  tasks:\n#\n# configure passwordless ssh to our ansible box\n#\n    - name: Register key\n      stat: path=/root/.ssh/id_rsa\n      register: key\n    - name: Create keypairs\n      shell: ssh-keygen -t rsa -f /root/.ssh/id_rsa -N ''\n      when: key.stat.exists == False\n    - name: Fetch all public ssh keys\n      shell: cat ~/.ssh/id_rsa.pub\n      register: ssh_keys\n    - name: Deploy keys on localhost\n      authorized_key: user=root key=\"{{ item }}\"\n      delegate_to: localhost\n      with_items:\n        - \"{{ ssh_keys.stdout }}\"\n\n#\n# Create the restore script on the target machine\n#\n    - name: Creates directory\n      file:\n        path: /root/scripts\n        state: directory\n\n    - name: Copy restore script to target\n      template: src=../templates/restore_dtr_images.sh.j2 dest=/root/scripts/restore_dtr_images.sh\n\n    - file:\n        path: /root/scripts/restore_dtr_images.sh\n        mode: 0744\n\n    - name: Do not restore if the directoyr containing the image data is not empty\n      stat:\n        path: \"{{ images_folder }}/docker\"\n      register: res\n\n    - fail:\n        msg: \"Could not 'stat' {{ images_folder }}/docker, Aborting\"\n      when: res.failed != false\n\n    - debug:\n        msg: \"Folder {{ images_folder }}/docker is not empty, will not restore data\"\n      when: res.stat.exists != false\n\n    - name: restore Data\n      shell: /root/scripts/restore_dtr_images.sh\n      register: res\n      when: res.stat.exists == false\n\n#\n# Could not find a way to specify a remote source\n#\n#    - name: Restore DTR Image data\n#      unarchive:\n#        src: \"{{ backup_dtr_data }}\"\n#        dest: \"{{ images_folder }}\"\n\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "322a7ad6ca443af175ee0110347af184bf3cf0e0", "filename": "handlers/main.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n- name: restart mariadb \n  service:\n    name: mysql\n    state: restarted\n  when: mariadb_notify_restart\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "9db75e4dcc8f1fd1ac36f701bb9eb731335b490f", "filename": "roles/manage-aws-infra/tasks/remove_vpc.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "# Remove VPC used for OCP Cluster\n---\n- name: Register VPC to be terminated\n  ec2_vpc_net_facts:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    filters:\n      \"tag:env_id\": \"{{ env_id }}\"\n  register: destroy_vpc\n  when:\n    - operation == \"absent\"\n    - delete_vpc\n\n- name: Register Security Groups to be terminated\n  ec2_group_facts:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    filters:\n      \"tag:env_id\": \"{{ env_id }}\"\n  register: destroy_sgroups\n  when:\n    - operation == \"absent\"\n    - delete_vpc\n\n- name: Ensure VPC has no dependencies\n  ec2_vpc:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    vpc_id: \"{{ destroy_vpc.vpcs[0].id }}\"\n    state: \"present\"\n    resource_tags: \"{}\"\n    subnets: []\n    internet_gateway: False\n    route_tables: []\n    wait: yes\n  when:\n    - operation == \"absent\"\n    - delete_vpc\n\n- name: Ensure Security Groups are removed\n  ec2_group:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    state: \"{{ operation }}\"\n    group_id: \"{{ item.group_id }}\"\n  when:\n    - operation == \"absent\"\n    - delete_vpc\n  with_items:\n    - \"{{ destroy_sgroups.security_groups }}\"\n\n- name: Ensure VPC is removed\n  ec2_vpc_net:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    name: \"{{ destroy_vpc.vpcs[0].tags.Name }}\"\n    cidr_block: \"{{ destroy_vpc.vpcs[0].cidr_block }}\"\n    region: \"{{ aws_region }}\"\n    state: \"{{ operation }}\"\n    tags:\n      env_id: \"{{ env_id }}\"\n  when:\n    - operation == \"absent\"\n    - delete_vpc\n"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "60c6b4a188dcab4e496fb027d69cb9e0cb749936", "filename": "tasks/fetch/win-chocolatey.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Chocolatay will download artifact itself\"\n  debug:\n    msg: \"Chocolatay will download artifact itself\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "8a7ae0c2194590212e22ced8aa8e9d05c5686c64", "filename": "playbooks/openshift/start-aws.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- hosts: localhost\n  roles:\n  - role: manage-aws-infra\n    operation: running\n"}, {"commit_sha": "1054e6b543fd736ac9c3616cb81e484bb0af134c", "sha": "82bd83ef0b01c43a180bb1826df8a4915ef7b6c6", "filename": "meta/main.yml", "repository": "antoiner77/caddy-ansible", "decoded_content": "---\ngalaxy_info:\n  author: antoiner77\n  description: Installs and configures a Caddy webserver\n\n  # Some suggested licenses:\n  # - BSD (default)\n  # - MIT\n  # - GPLv2\n  # - GPLv3\n  # - Apache\n  # - CC-BY\n  license: license (GPLv2, CC-BY, etc)\n\n  min_ansible_version: 1.2\n\n  # Below are all platforms currently available. Just uncomment\n  # the ones that apply to your role. If you don't see your\n  # platform on this list, let us know and we'll get it added!\n  #\n  platforms:\n  - name: EL\n    versions:\n  #  - all\n  #  - 5\n  #  - 6\n     - 7\n  #- name: GenericUNIX\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Solaris\n  #  versions:\n  #  - all\n  #  - 10\n  #  - 11.0\n  #  - 11.1\n  #  - 11.2\n  #  - 11.3\n  #- name: Fedora\n  #  versions:\n  #  - all\n  #  - 16\n  #  - 17\n  #  - 18\n  #  - 19\n  #  - 20\n  #  - 21\n  #  - 22\n  #- name: Windows\n  #  versions:\n  #  - all\n  #  - 2012R2\n  #- name: SmartOS\n  #  versions:\n  #  - all\n  #  - any\n  #- name: opensuse\n  #  versions:\n  #  - all\n  #  - 12.1\n  #  - 12.2\n  #  - 12.3\n  #  - 13.1\n  #  - 13.2\n  #- name: Amazon\n  #  versions:\n  #  - all\n  #  - 2013.03\n  #  - 2013.09\n  #- name: GenericBSD\n  #  versions:\n  #  - all\n  #  - any\n  #- name: FreeBSD\n  #  versions:\n  #  - all\n  #  - 8.0\n  #  - 8.1\n  #  - 8.2\n  #  - 8.3\n  #  - 8.4\n  #  - 9.0\n  #  - 9.1\n  #  - 9.1\n  #  - 9.2\n  - name: Ubuntu\n    versions:\n  #  - all\n  #  - lucid\n  #  - maverick\n  #  - natty\n  #  - oneiric\n     - precise\n  #  - quantal\n  #  - raring\n  #  - saucy\n     - trusty\n  #  - utopic\n  #  - vivid\n  #- name: SLES\n  #  versions:\n  #  - all\n  #  - 10SP3\n  #  - 10SP4\n  #  - 11\n  #  - 11SP1\n  #  - 11SP2\n  #  - 11SP3\n  #- name: GenericLinux\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Debian\n  #  versions:\n  #  - all\n  #  - etch\n  #  - jessie\n  #  - lenny\n  #  - squeeze\n  #  - wheezy\n\n  galaxy_tags:\n      - system\n      - web\n\n  dependencies: []\n  # List your role dependencies here, one per line.\n  # Be sure to remove the '[]' above if you add dependencies\n  # to this list.\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "f912f50adcbcfa14fcf44754cdcd959bb0974ddc", "filename": "reference-architecture/aws-ansible/playbooks/add-node.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  connection: local\n  gather_facts: no\n  become: no\n  vars_files:\n  - vars/main.yaml\n  roles:\n  # Group systems\n  - instance-groups\n  - cfn-outputs\n\n- hosts: new_nodes\n  gather_facts: yes\n  become: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - non-atomic-docker-storage-setup\n  - openshift-versions\n\n- include: ../../../playbooks/add-node-prerequisite.yaml\n\n- include: node-setup.yaml\n\n- hosts: single_master\n  gather_facts: yes\n  become: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - ../../../roles/router-scaleup\n  - ../../../roles/registry-scaleup\n\n- hosts: localhost\n  connection: local\n  gather_facts: no\n  become: no\n  vars_files:\n  - vars/main.yaml\n  post_tasks:\n  - name: add instance to ELB if node is infra\n    ec2_elb:\n      instance_id: \"{{ hostvars[item].ec2_id }}\"\n      ec2_elbs: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['InfraLb'] | default(infra_elb_name) }}\"\n      region: \"{{ region }}\"\n      wait: no\n      state: present\n    with_items: \"{{ groups['tag_provision_node'] }}\"\n    when: node_type == \"infra\"\n\n  - name: tag a resource\n    ec2_tag:\n      region: \"{{ region }}\"\n      resource: \"{{ hostvars[item].ec2_id }}\"\n      state: absent\n      tags:\n        provision: node\n    with_items: \"{{ groups['tag_provision_node'] }}\"\n\n- hosts: cluster_hosts\n  gather_facts: yes\n  become: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - gluster-ports\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "9a3129fd2f9f5d3d2f513517055eba269f707f3d", "filename": "playbooks/roles/docket/test.sh", "repository": "rocknsm/rock", "decoded_content": "#!/bin/bash\n##\n## Copyright (c) 2017, 2018 RockNSM.\n## \n## This file is part of RockNSM\n## (see http://rocknsm.io).\n## \n## Licensed under the Apache License, Version 2.0 (the \"License\");\n## you may not use this file except in compliance with the License.\n## You may obtain a copy of the License at\n## \n##   http://www.apache.org/licenses/LICENSE-2.0\n## \n## Unless required by applicable law or agreed to in writing,\n## software distributed under the License is distributed on an\n## \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n## KIND, either express or implied.  See the License for the\n## specific language governing permissions and limitations\n## under the License.\n## \n##\n#\n# Ansible role test shim.\n#\n# Usage: [OPTIONS] ./tests/test.sh\n#   - playbook: a playbook in the tests directory (default = \"test.yml\")\n#   - test_idempotence: whether to test playbook's idempotence (default = true)\n#\n# License: MIT\n\n# Exit on any individual command failure.\nset -e\n\n# Pretty colors.\nred='\\033[0;31m'\ngreen='\\033[0;32m'\nneutral='\\033[0m'\n\ntimestamp=$(date +%s)\n\n# Allow environment variables to override defaults.\nplaybook=${playbook:-\"test.yml\"}\ntest_idempotence=${test_idempotence:-\"true\"}\n\nexport ANSIBLE_ROLES_PATH=$(pwd)/../\n\n# Install requirements if `requirements.yml` is present.\nif [ -f \"$PWD/tests/requirements.yml\" ]; then\n  printf ${green}\"Requirements file detected; installing dependencies.\"${neutral}\"\\n\"\n  TERM=xterm ansible-galaxy install -r tests/requirements.yml\nfi\n\nprintf \"\\n\"\n\n# Test Ansible syntax.\nprintf ${green}\"Checking Ansible playbook syntax.\"${neutral}\nTERM=xterm ansible-playbook tests/$playbook --syntax-check\n\nprintf \"\\n\"\n\n# Run Ansible playbook.\nprintf ${green}\"Running command: TERM=xterm ansible-playbook tests/$playbook\"${neutral}\nTERM=xterm ANSIBLE_FORCE_COLOR=1 ansible-playbook --become --inventory tests/inventory tests/$playbook\n\nif [ \"$test_idempotence\" = true ]; then\n  # Run Ansible playbook again (idempotence test).\n  printf ${green}\"Running playbook again: idempotence test\"${neutral}\n  idempotence=$(mktemp)\n  ansible-playbook --become --inventory tests/inventory tests/$playbook | tee -a $idempotence\n  tail $idempotence \\\n    | grep -q 'changed=0.*failed=0' \\\n    && (printf ${green}'Idempotence test: pass'${neutral}\"\\n\") \\\n    || (printf ${red}'Idempotence test: fail'${neutral}\"\\n\" && exit 1)\nfi\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "fe677147b71ea9cc183b6e67e7ce8ce4dee8e221", "filename": "roles/dns_encryption/handlers/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: daemon reload\n  systemd:\n    daemon_reload: true\n\n- name: restart dnscrypt-proxy\n  systemd:\n    name: dnscrypt-proxy\n    state: restarted\n    daemon_reload: true\n  when: ansible_distribution == 'Ubuntu'\n\n- name: restart dnscrypt-proxy\n  service:\n    name: dnscrypt-proxy\n    state: restarted\n  when: ansible_distribution == 'FreeBSD'\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "579cd7ac67e94f04e279c2a2d28803cfb608f9cc", "filename": "playbooks/libvirt/openshift-cluster/list.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Generate oo_list_hosts group\n  hosts: localhost\n  become: no\n  connection: local\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  tasks:\n  - set_fact: scratch_group=tag_clusterid-{{ cluster_id }}\n    when: cluster_id != ''\n  - set_fact: scratch_group=all\n    when: cluster_id == ''\n  - add_host:\n      name: \"{{ item }}\"\n      groups: oo_list_hosts\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n      oo_public_ipv4: \"\"\n      oo_private_ipv4: \"{{ hostvars[item].libvirt_ip_address }}\"\n    with_items: \"{{ groups[scratch_group] | default([]) | difference(['localhost']) }}\"\n  - debug:\n      msg: \"{{ hostvars | oo_select_keys(groups[scratch_group] | default([])) | oo_pretty_print_cluster }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "a152135fc430d2dd78485fd29f1b43fa71e6fda1", "filename": "playbooks/libvirt/openshift-cluster/update.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: no\n  tasks:\n  - include_vars: vars.yml\n  - include_vars: cluster_hosts.yml\n  - add_host:\n      name: \"{{ item }}\"\n      groups: l_oo_all_hosts\n    with_items: '{{ g_all_hosts }}'\n\n- hosts: l_oo_all_hosts\n  gather_facts: no\n  tasks:\n  - include_vars: vars.yml\n  - include_vars: cluster_hosts.yml\n\n- name: Populate oo_hosts_to_update group\n  hosts: localhost\n  connection: local\n  become: no\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  - cluster_hosts.yml\n  tasks:\n  - name: Evaluate oo_hosts_to_update\n    add_host:\n      name: \"{{ item }}\"\n      groups: oo_hosts_to_update\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    with_items: '{{ g_all_hosts | default([]) }}'\n\n- include: ../../common/openshift-cluster/update_repos_and_packages.yml\n\n- include: config.yml\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "289fcc16bf1b660487a622c49fbd5840f675acf9", "filename": "tasks/create_repo_bower_proxy_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_bower_proxy\n    args: \"{{ _nexus_repos_bower_defaults|combine(item) }}\""}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "1140ca9478535ed68f79a0333f6076541e10f455", "filename": "roles/idmgr/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install idmgr packages\n  package: name={{ item }}\n           state=present\n  with_items:\n   - ds-backup-server\n   - idmgr\n   - xinetd\n   - xs-rsync\n   - incron\n  tags:\n    - download\n\n- name: Configure idmgr\n  template: backup=yes\n            src={{ item.src }}\n            dest={{ item.dest }}\n            owner=root\n            group=root\n            mode={{ item.mode }}\n  with_items:\n    - { src: 'idmgr', dest: '/etc/idmgr.conf', mode: '0644' }\n    - { src: 'idmgr.service.j2', dest: '/etc/systemd/system/idmgr.service', mode: '0644'}\n\n- name: Configure ds-backup\n  command: /etc/sysconfig/olpc-scripts/setup.d/ds-backup\n           creates=/etc/incron.d/ds-backup.conf\n\n- name: Configure idmgr sqlite db\n  command: /etc/sysconfig/olpc-scripts/setup.d/idmgr\n           creates=/home/idmgr/identity.db\n\n- name: Configure xs-rsync\n  command: /etc/sysconfig/olpc-scripts/setup.d/xs-rsync\n           creates=/etc/xinetd.d/xs-rsyncd\n\n- name: Copy idmgr init script\n  command: /bin/cp /etc/init.d/idmgr /usr/libexec/idmgr.init\n           creates=/usr/libexec/idmgr.init\n\n- name: Enable idmgr service\n  service: name={{ item }}\n           enabled=yes\n           state=started\n  with_items:\n    - idmgr\n    - xinetd\n  when: xo_services_enabled\n\n- name: Disable idmgr service\n  service: name={{ item }}\n           enabled=no\n           state=stopped\n  with_items:\n    - idmgr\n    - xinetd\n  when: not xo_services_enabled\n\n#idmgr needs an extra step\n- name: Enable ejabberd service\n  file: src=/etc/systemd/system/idmgr.service\n        dest=/etc/systemd/system/multi-user.target.wants/idmgr.service\n        owner=root\n        group=root\n        state=link\n\n- name: Configure rssh rsync permissions to allow OLPC Backup clients\n  lineinfile: backup=yes\n              dest=/etc/rssh.conf\n             state=present\n             regexp='^#allowrsync'\n             insertafter='^#allowrsync'\n             line=allowrsync\n\n- name: Configure rssh sftp permissions for backup restore clients\n  lineinfile: backup=yes\n              dest=/etc/rssh.conf\n             state=present\n             regexp='^#allowsftp'\n             insertafter='^#allowsftp'\n             line=allowsftp\n\n- name: Add idmgr to service list\n  ini_file: dest='{{ service_filelist }}'\n            section=idmgr\n            option='{{ item.option }}'\n            value='{{ item.value }}'\n  with_items:\n    - option: name\n      value: idmgr\n    - option: description\n      value: '\"Idmgr is an automatic identity manager for XO clients which enables automatic backup\"'\n    - option: enabled\n      value: \"{{ xo_services_enabled }}\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "cfd63063312f55e81b6d2a4f3aa594b39cd6abd6", "filename": "playbooks/roles/zookeeper/defaults/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# defaults file for zookeeper\nwith_zookeeper: true\nenable_zookeeper: true\nmethod: all\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "f191172261f9664fe5ac14c5c61a6d58ce03b7cd", "filename": "ops/playbooks/roles/hpe.openports/vars/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "---\n# vars file for hpe.openports\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "caa2a6568211de9cb4881644cbbda53a3198ccea", "filename": "tasks/nexus_purge.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- name: Make sure nexus is stopped\n  debug:\n    msg: \"trigger nexus stop\"\n  changed_when: true\n  notify:\n    - nexus-service-stop\n\n- meta: flush_handlers\n\n- name: \"Purge Nexus\"\n  file:\n    path: \"{{ item }}\"\n    state: absent\n  with_items:\n    - \"{{ nexus_data_dir }}\"\n    - \"{{ nexus_installation_dir }}/nexus-{{ nexus_version }}\"\n    - \"{{ nexus_restore_log }}\"\n    - \"{{ nexus_installation_dir }}/nexus-latest\"\n    # - \"{{ nexus_backup_dir }}\" # Optional\n\n- name: \"remove nexus package if present\"\n  package:\n    name: nexus\n    state: absent\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "c73c0b997488b41d8c9b01cf19d969beab9f62f7", "filename": "roles/config-vnc-server/tasks/vnc-server.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install additional packages for VNC server\"\n  package:\n    name: \"{{ item }}\"\n    state: latest\n  with_items:\n  - tigervnc-server\n  - policycoreutils-python-utils\n  - checkpolicy \n\n- name: \"Ensure .vnc dir exists\"\n  file:\n    path: \"{{ vnc_home_dir }}/{{ main_user }}/.vnc\"\n    state: directory\n    owner: \"{{ main_user }}\"\n\n- name: \"Check to see if a VNC password already exists\"\n  stat: \n    path: \"{{ vnc_home_dir }}/{{ main_user }}/.vnc/passwd\"\n  register: passwd_info \n\n- name: \"Set a vnc password\"\n  shell: \"echo {{ vnc_password | default('vncpasswd01') }} | vncpasswd -f > {{ vnc_home_dir }}/{{ main_user }}/.vnc/passwd\"\n  when: passwd_info.stat.exists == False\n\n- name: \"Ensure correct ownership of the vnc password file\"\n  file:\n    path: \"{{ vnc_home_dir }}/{{ main_user }}/.vnc/passwd\"\n    owner: \"{{ main_user }}\"\n    mode: 0600\n\n- name: \"Add the xstartup (gnome) configuration to the main user\"\n  copy :\n    src: xstartup-gnome\n    dest: \"{{ vnc_home_dir }}/{{ main_user }}/.vnc/xstartup\"\n    force: no\n    owner: \"{{ main_user }}\"\n    mode: 0755\n  when:\n  - gnome_install|default(False)\n\n- name: \"Add the xstartup (XFCE) configuration to the main user\"\n  copy :\n    src: xstartup-xfce\n    dest: \"{{ vnc_home_dir }}/{{ main_user }}/.vnc/xstartup\"\n    force: no\n    owner: \"{{ main_user }}\"\n    mode: 0755\n  when:\n  - xfce_install|default(False)\n\n- name: \"Add the xstartup (LXDE) configuration to the main user\"\n  copy :\n    src: xstartup-lxde\n    dest: \"{{ vnc_home_dir }}/{{ main_user }}/.vnc/xstartup\"\n    force: no\n    owner: \"{{ main_user }}\"\n    mode: 0755\n  when:\n  - lxde_install|default(False)\n\n- name: \"Add the xstartup (MATE) configuration to the main user\"\n  copy :\n    src: xstartup-mate\n    dest: \"{{ vnc_home_dir }}/{{ main_user }}/.vnc/xstartup\"\n    force: no\n    owner: \"{{ main_user }}\"\n    mode: 0755\n  when:\n  - mate_install|default(False)\n\n- name: \"Copy VNC service file into place\" \n  copy:\n    src: /usr/lib/systemd/system/vncserver@.service\n    dest: \"/etc/systemd/system/vncserver-{{ main_user }}@.service\"\n    remote_src: True\n\n- name: \"Ensure the user config is set for the vnc service\"\n  replace: \n    path: \"/etc/systemd/system/vncserver-{{ main_user }}@.service\"\n    regexp: \"{{ item.0 }}\"\n    replace: \"{{ item.1 }}\"\n  with_together:\n  - ['<USER>', '/home/' ]\n  - [ \"{{ main_user }}\", \"{{ vnc_home_dir }}/\" ]\n\n- name: \"Reload systemctl daemon\"\n  command: systemctl daemon-reload\n\n- name: \"Copy SELinux .te file to the host - used to build the module\"\n  copy:\n    src: SELinuxVNC.te\n    dest: /tmp/SELinuxVNC.te\n  \n- name: \"Build SELinux module (.mod) to allow VNC\"\n  command: checkmodule -M -m -o SELinuxVNC.mod /tmp/SELinuxVNC.te  \n    \n- name: \"Build SELinux module (.pp) to allow VNC\"\n  command: semodule_package -m SELinuxVNC.mod -o SELinuxVNC.pp\n\n- name: \"Load SELinux module to allow VNC\"\n  command: semodule -i SELinuxVNC.pp\n\n- name: \"Enable and start VNC server for user\"\n  service:\n    name: \"vncserver-{{ main_user }}@:1\"\n    enabled: yes\n    state: started\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "978de3138f5d73ece826803688860b5efaacb3c9", "filename": "roles/config-versionlock/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Include prereqs per the type of OS\"\n  include_tasks: \"{{ distro_file }}\"\n  with_first_found:\n  - files:\n    - prereq-{{ ansible_distribution }}.yml\n    skip: true\n  loop_control:\n    loop_var: distro_file\n\n- import_tasks: versionlock.yml\n"}, {"commit_sha": "a10c5f4577e6e74feb1fadec4bcbab039b8b180a", "sha": "73036d7622d107323c2bf05ca99db7aa6cc878f3", "filename": "tasks/configure-docker.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# https://wiki.ubuntu.com/SystemdForUpstartUsers\n# Important! systemd is only fully supported in Ubuntu 15.04 and later releases\n- name: Determine usage of systemd\n  become: true\n  shell: \"ps -p1 | grep systemd 1>/dev/null && echo systemd || echo upstart\"\n  changed_when: no\n  check_mode: no\n  register: _determine_systemd_usage\n  tags:\n    - skip_ansible_lint\n\n- name: Set fact to indicate systemd is used or not\n  set_fact:\n    _docker_systemd_used: \"{{ _determine_systemd_usage is defined and _determine_systemd_usage.stdout == 'systemd' }}\"\n\n- name: Configure systemd service\n  include_tasks: configure-docker/configure-systemd.yml\n  when: _docker_systemd_used | bool\n\n- name: Configure non-systemd service\n  include_tasks: configure-docker/configure-non-systemd.yml\n  when: not _docker_systemd_used | bool\n\n- name: Ensure /etc/docker directory exists\n  become: true\n  file:\n    path: /etc/docker\n    state: directory\n    mode: 0755\n\n- name: Configure Docker daemon (file)\n  become: true\n  copy:\n    src: \"{{ docker_daemon_config_file }}\"\n    dest: /etc/docker/daemon.json\n  notify: restart docker\n  when: docker_daemon_config_file is defined\n\n- name: Fetch Docker daemon status\n  become: true\n  service:\n    name: docker\n  register: _docker_status_check\n  when: docker_plugins | length > 0\n\n- name: Configure Docker daemon (variables)\n  become: true\n  copy:\n    content: \"{{ docker_daemon_config | to_nice_json }}\"\n    dest: /etc/docker/daemon.json\n  notify: restart docker\n  when:\n    - docker_daemon_config_file is not defined\n    - docker_daemon_config is defined\n    - _docker_status_check.status is not defined or\n      (_docker_status_check.status is defined and\n      _docker_status_check.status.SubState is defined and\n      _docker_status_check.status.SubState != \"running\")\n\n- name: Ensure Docker default user namespace is defined in subuid and subgid\n  become: true\n  lineinfile:\n    path: \"{{ item }}\"\n    regexp: '^dockremap'\n    line: 'dockremap:500000:65536'\n  with_items:\n    - /etc/subuid\n    - /etc/subgid\n  when: (_docker_os_dist == \"CentOS\" or _docker_os_dist == \"RedHat\") and\n        ((docker_daemon_config is defined and\n        docker_daemon_config['userns-remap'] is defined and\n        docker_daemon_config['userns-remap'] == 'default') or\n        docker_bug_usermod | bool)\n\n- name: Ensure Docker users are added to the docker group\n  become: true\n  user:\n    name: \"{{ item }}\"\n    groups: docker\n    append: true\n  with_items: \"{{ docker_users }}\"\n\n- name: Ensure devicemapper prerequisites are fulfilled\n  block:\n    - name: Ensure lvm2 is installed\n      become: true\n      package:\n        name: lvm2\n        state: present\n      register: _pkg_result\n      until: _pkg_result|succeeded\n\n    - name: Ensure thin-provisioning-tools is installed\n      become: true\n      package:\n        name: thin-provisioning-tools\n        state: present\n      register: _pkg_result\n      until: _pkg_result|succeeded\n      when: (_docker_os_dist == \"Ubuntu\" or (_docker_os_dist == \"Debian\" and _docker_os_dist_major_version > '7'))\n  when:\n    - docker_daemon_config['storage-driver'] is defined\n    - docker_daemon_config['storage-driver'] == 'devicemapper'\n\n- name: Enable Docker service\n  become: true\n  service:\n    name: docker\n    enabled: yes\n  notify: restart docker\n  register: _docker_service\n\n- name: Trigger start/restart of Docker\n  service:\n    name: docker\n  notify: restart docker\n  changed_when: _docker_service.status.SubState != \"running\"\n  when: _docker_service.status is defined and _docker_service.status.SubState is defined\n\n- name: Install and configure Docker plugins\n  include_tasks: configure-docker/configure-docker-plugins.yml\n  when: docker_plugins | length > 0"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "a83ca90f3eeb0939147c6af8d5fe1605a99d8391", "filename": "tasks/galera/main.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n- name: TEMPLATE | Deploy Galera configuration\n  template:\n    src: etc/mysql/conf.d/09-galera.cnf.j2\n    dest: /etc/mysql/conf.d/09-galera.cnf\n  register: galeraconfig\n\n- name: INCLUDE | Bootstrap first node\n  include: 'bootstrap.yml'\n  when: inventory_hostname == mariadb_galera_primary_node\n\n- name: INCLUDE | Configure other nodes\n  include: 'nodes.yml'\n  when: inventory_hostname != mariadb_galera_primary_node\n\n- name: SERVICE | Restart MariaDB if needed\n  service:\n    name: mysql\n    state: restarted\n  when: >\n    ((galeraconfig or (p is defined and p.changed)) and\n    (bootstrap_run is not defined)) or\n    ((inventory_hostname != mariadb_galera_primary_node) and\n    (mariadb_galera_resetup))\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "777c6873fda17d3cd81d27bbe516e64e196fc230", "filename": "reference-architecture/vmware-ansible/playbooks/roles/heketi-ocp/templates/heketi-secret.yaml.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "apiVersion: v1\nkind: Secret\nmetadata:\n  name: heketi-secret\n  namespace: default\ndata:\n  key: \"{{ heketi_secret }}\" \ntype: kubernetes.io/glusterfs\n\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "ac7cc02fe22fa6e072cdc445281a6201ad450c59", "filename": "tasks/Win32NT/install/zulu_tarball.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Check that the java_folder exists\n  win_stat:\n    path: '{{ java_path }}\\{{ java_folder }}\\bin'\n  register: java_folder_bin\n\n- name: Install java from tarball\n  block:\n  - name: Mkdir for java installation\n    win_file:\n      path: '{{ java_path }}\\{{ java_folder }}'\n      state: directory\n\n  - name: Create temporary directory\n    win_tempfile:\n      state: directory\n    register: temp_dir_path\n\n  - name: Unarchive to temporary directory\n    win_unzip:\n      src: '{{ java_artifact }}'\n      dest: '{{ temp_dir_path }}'\n\n  - name: Find java_folder in temp\n    win_find:\n      paths: '{{ temp_dir_path }}'\n      recurse: false\n      file_type: directory\n    register: java_temp_folder\n\n  - name: Copy from temporary directory\n    win_copy:\n      src: '{{ java_temp_folder.files | map(attribute=\"path\") | list | last }}\\'\n      dest: '{{ java_path }}\\{{ java_folder }}'\n      remote_src: true\n  when: not java_folder_bin.stat.exists | bool\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "d3ca2f1a4072c1e908bcd1914f28e112002db824", "filename": "roles/serverspec/vars/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# vars file for serverspec\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "ba2ebeca076ba1388c64939c67a7b585f26a2f5f", "filename": "roles/lighttpd/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: Enable and restart lighttpd\n  systemd:\n    name: lighttpd\n    state: >-\n      {%- if rock_services | selectattr('name', 'equalto', 'lighttpd') | map(attribute='enabled') or\n      rock_services | selectattr('name', 'equalto', 'docket') | map(attribute='enabled') -%}\n        restarted\n      {%- else -%}\n        stopped\n      {%- endif -%}\n    enabled: >-\n      {%- if rock_services | selectattr('name', 'equalto', 'lighttpd') | map(attribute='enabled') or\n      rock_services | selectattr('name', 'equalto', 'docket') | map(attribute='enabled') -%}\n        True\n      {%- else -%}\n        False\n      {%- endif -%}\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "7119fe6ffa89b8ec0bb5b4d99b907e0646502648", "filename": "roles/static_inventory/tasks/sshconfig.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: set ssh proxy command prefix for accessing nodes via bastion\n  set_fact:\n    ssh_proxy_command: >-\n      ssh {{ ssh_options }}\n      -i {{ private_ssh_key }}\n      {{ ssh_user }}@{{ hostvars['bastion'].ansible_host }}\n\n- name: regenerate ssh config\n  template:\n    src: openstack_ssh_config.j2\n    dest: \"{{ ssh_config_path }}\"\n    mode: 0644\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "ebde76a819069892d27d3db63e1577e80df4a19a", "filename": "dev/playbooks/install_ntp.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: vms\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n    - name: Update chrony.conf\n      template:\n        src: ../templates/chrony.conf.j2\n        dest: /etc/chrony.conf\n        owner: root\n        group: root\n        mode: 0644        \n      notify: Enable and restart chrony service\n\n    - name: use timedatectl\n      command: timedatectl set-ntp true\n\n  handlers:\n    - name: Enable and restart chrony service\n      systemd:\n        name: chronyd\n        enabled: yes\n        state: restarted\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "e04b3d80c719ce8cbb32c36af7e62dcd278a2cc5", "filename": "roles/cloud-gce/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- block:\n    - name: Build python virtual environment\n      import_tasks: venv.yml\n\n    - block:\n      - name: Include prompts\n        import_tasks: prompts.yml\n\n      - name: Network configured\n        gce_net:\n          name: \"algo-net-{{ algo_server_name }}\"\n          fwname: \"algo-net-{{ algo_server_name }}-fw\"\n          allowed: \"udp:500,4500,{{ wireguard_port }};tcp:22\"\n          state: \"present\"\n          mode: auto\n          src_range: 0.0.0.0/0\n          service_account_email: \"{{ service_account_email }}\"\n          credentials_file: \"{{ credentials_file_path  }}\"\n          project_id: \"{{ project_id }}\"\n\n      - block:\n        - name: External IP allocated\n          gce_eip:\n            service_account_email: \"{{ service_account_email }}\"\n            credentials_file: \"{{ credentials_file_path }}\"\n            project_id: \"{{ project_id }}\"\n            name: \"{{ algo_server_name }}\"\n            region: \"{{ algo_region.split('-')[0:2] | join('-') }}\"\n            state: present\n          register: gce_eip\n\n        - name: Set External IP as a fact\n          set_fact:\n            external_ip: \"{{ gce_eip.address }}\"\n        when: cloud_providers.gce.external_static_ip\n\n      - name: \"Creating a new instance...\"\n        gce:\n          instance_names: \"{{ algo_server_name }}\"\n          zone: \"{{ algo_region }}\"\n          external_ip: \"{{ external_ip | default('ephemeral') }}\"\n          machine_type: \"{{ cloud_providers.gce.size }}\"\n          image: \"{{ cloud_providers.gce.image }}\"\n          service_account_email: \"{{ service_account_email }}\"\n          credentials_file: \"{{ credentials_file_path }}\"\n          project_id: \"{{ project_id }}\"\n          metadata: '{\"ssh-keys\":\"ubuntu:{{ ssh_public_key_lookup }}\"}'\n          network: \"algo-net-{{ algo_server_name }}\"\n          tags:\n            - \"environment-algo\"\n        register: google_vm\n\n      - set_fact:\n          cloud_instance_ip: \"{{ google_vm.instance_data[0].public_ip }}\"\n          ansible_ssh_user: ubuntu\n      environment:\n        PYTHONPATH: \"{{ gce_venv }}/lib/python2.7/site-packages/\"\n  rescue:\n    - debug: var=fail_hint\n      tags: always\n    - fail:\n      tags: always\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "5a5a190af8073fa9acadf7f81cb055493e544d80", "filename": "roles/docker/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for docker\n- name: restart docker\n  service:\n   name: docker\n   state: restarted\n  sudo: yes\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "096f210dae612b72859cea6f538e5b950620c154", "filename": "roles/8-mgmt-tools/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# Assessment and Monitoring Tools\n\n- name: ...IS BEGINNING ======================================\n  command: echo\n\n- name: AWSTATS\n  include_role:\n    name: awstats\n  when: awstats_install\n  tags: awstats\n  \n- name: MONIT\n  include_role:\n    name: monit\n  when: monit_install\n  tags: monit\n\n- name: MUNIN\n  include_role:\n    name: munin\n  when: munin_install\n  tags: munin\n\n- name: PHPMYADMIN\n  include_role:\n    name: phpmyadmin\n  when: phpmyadmin_install\n  tags: phpmyadmin\n\n- name: SUGAR-STATS\n  include_role:\n    name: sugar-stats\n  when: sugar_stats_install and ansible_distribution != \"CentOS\"\n  tags: olpc, sugar-stats\n\n- name: TEAMVIEWER\n  include_role:\n    name: teamviewer\n  when: teamviewer_install\n  tags: teamviewer\n\n- name: VNSTAT\n  include_role:\n    name: vnstat\n  when: vnstat_install\n  tags: vnstat\n\n- name: XOVIS\n  include_role:\n    name: xovis\n  when: xovis_install and ansible_distribution != \"CentOS\"\n  tags: xovis\n\n- name: Recording STAGE 8 HAS COMPLETED ======================\n  lineinfile:\n    dest: /etc/iiab/iiab.env\n    regexp: '^STAGE=*'\n    line: 'STAGE=8'\n    state: present\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "a6b66f6994d8dcae54481e02f6bb12bf2f0b4e3b", "filename": "ops/playbooks/install_rsyslog.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- hosts: docker,logger\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - includes/internal_vars.yml\n\n  environment: \"{{ env }}\"\n\n  roles:\n    - role: hpe.openports\n      hpe_openports_ports: \"{{ internal_syslog_ports }}\"\n      when: inventory_hostname in groups.logger\n\n  tasks:\n\n    - name: Install rsyslog\n      yum:\n        name: rsyslog\n        state: latest\n\n    - name: Configure logger server\n      copy: src=../files/rsyslog.conf dest=/etc/rsyslog.conf\n      when: inventory_hostname in groups.logger\n      notify: Restart Rsyslog\n\n#    - name: Allow docker nodes to send logs\n#      template: src=../templates/rsyslog.conf.j2 dest=/etc/rsyslog.conf\n#      when: inventory_hostname in groups.docker\n#      notify: Restart Rsyslog\n\n  handlers:\n    - name: Restart Rsyslog\n      systemd:\n        name: rsyslog\n        enabled: yes\n        state: restarted\n\n"}, {"commit_sha": "9e7bed4aaef56159f148d24e4f7d7e8b53be632b", "sha": "bf37dee844c58fbf44cf1719821ca7176b912e00", "filename": "tasks/main-Fedora.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# tasks file for ansible-role-docker-ce\n\n- name: Add Docker CE repository\n  get_url:\n    url: https://download.docker.com/linux/fedora/docker-ce.repo\n    dest: /etc/yum.repos.d/docker-ce.repo\n    mode: 0644\n  become: true\n  register: dnf_repo\n\n- name: Determine Docker CE Edge repo status\n  shell: dnf config-manager --dump docker-ce-edge | grep enabled\n  args:\n    warn: false\n  ignore_errors: yes\n  changed_when: false\n  register: cmd_docker_ce_edge_enabled\n\n- name: Set current Docker CE Edge repo status fact\n  set_fact:\n    fact_docker_ce_edge_enabled: \"{{ cmd_docker_ce_edge_enabled.stdout == 'enabled = True' }}\"\n\n- name: Enable/Disable Docker CE Edge Repository\n  shell: dnf config-manager --set-{{ (docker_enable_ce_edge == true) | ternary('enabled','disabled') }} docker-ce-edge\n  become: true\n  when: fact_docker_ce_edge_enabled != docker_enable_ce_edge\n\n- name: Update dnf cache\n  shell: dnf makecache fast\n  become: true\n  when: dnf_repo.changed\n\n- name: Install python and deps for ansible modules\n  raw: dnf install -y python2 python2-dnf libselinux-python\n  become: true\n  changed_when: false\n\n- include: main-Storage.yml\n  when: docker_setup_devicemapper == true\n\n- include: main-Generic.yml"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "8ceb48be215b0ca0d31f0cc130f08c691d728bda", "filename": "playbooks/roles/sensor-common/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: force sync time\n  command: >\n    chronyc -a 'burst 3/4'; sleep 5; chronyc -a makestep\n\n- name: configure monitor interfaces\n  shell: >\n    for intf in {{ rock_monifs | join(' ') }}; do\n      /sbin/ifup ${intf};\n    done\n\n- name: sshd restart\n  service: name=sshd state=restarted\n\n...\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "7148b016a4fc68f80ce882a1cf4b6a32ccaa5f74", "filename": "roles/dns-records/tasks/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: \"Generate list of private A records\"\n  set_fact:\n    private_records: \"{{ private_records | default([]) + [ { 'type': 'A', 'hostname': hostvars[item]['ansible_hostname'], 'ip': hostvars[item]['private_v4'] } ] }}\"\n  with_items: \"{{ groups['cluster_hosts'] }}\"\n\n- name: \"Add wildcard records to the private A records for infrahosts\"\n  set_fact:\n    private_records: \"{{ private_records | default([]) + [ { 'type': 'A', 'hostname': '*.' + openshift_app_domain, 'ip': hostvars[item]['private_v4'] } ] }}\"\n  with_items: \"{{ groups['infra_hosts'] }}\"\n\n- name: \"Add public master cluster hostname records to the private A records (single master)\"\n  set_fact:\n    private_records: \"{{ private_records | default([]) + [ { 'type': 'A', 'hostname': (hostvars[groups.masters[0]].openshift_master_cluster_public_hostname | replace(full_dns_domain, ''))[:-1], 'ip': hostvars[groups.masters[0]].private_v4 } ] }}\"\n  when:\n    - hostvars[groups.masters[0]].openshift_master_cluster_public_hostname is defined\n    - openstack_num_masters == 1\n\n- name: \"Add public master cluster hostname records to the private A records (multi-master)\"\n  set_fact:\n    private_records: \"{{ private_records | default([]) + [ { 'type': 'A', 'hostname': (hostvars[groups.masters[0]].openshift_master_cluster_public_hostname | replace(full_dns_domain, ''))[:-1], 'ip': hostvars[groups.lb[0]].private_v4 } ] }}\"\n  when:\n    - hostvars[groups.masters[0]].openshift_master_cluster_public_hostname is defined\n    - openstack_num_masters > 1\n\n- name: \"Set the private DNS server to use the external value (if provided)\"\n  set_fact:\n    nsupdate_server_private: \"{{ external_nsupdate_keys['private']['server'] }}\"\n    nsupdate_key_secret_private: \"{{ external_nsupdate_keys['private']['key_secret'] }}\"\n    nsupdate_key_algorithm_private: \"{{ external_nsupdate_keys['private']['key_algorithm'] }}\"\n    nsupdate_private_key_name: \"{{ external_nsupdate_keys['private']['key_name']|default('private-' + full_dns_domain) }}\"\n  when:\n    - external_nsupdate_keys is defined\n    - external_nsupdate_keys['private'] is defined\n\n- name: \"Set the private DNS server to use the provisioned value\"\n  set_fact:\n    nsupdate_server_private: \"{{ hostvars[groups['dns'][0]].public_v4 }}\"\n    nsupdate_key_secret_private: \"{{ hostvars[groups['dns'][0]].nsupdate_keys['private-' + full_dns_domain].key_secret }}\"\n    nsupdate_key_algorithm_private: \"{{ hostvars[groups['dns'][0]].nsupdate_keys['private-' + full_dns_domain].key_algorithm }}\"\n  when:\n    - nsupdate_server_private is undefined\n\n- name: \"Generate the private Add section for DNS\"\n  set_fact:\n    private_named_records:\n      - view: \"private\"\n        zone: \"{{ full_dns_domain }}\"\n        server: \"{{ nsupdate_server_private }}\"\n        key_name: \"{{ nsupdate_private_key_name|default('private-' + full_dns_domain) }}\"\n        key_secret: \"{{ nsupdate_key_secret_private }}\"\n        key_algorithm: \"{{ nsupdate_key_algorithm_private | lower }}\"\n        entries: \"{{ private_records }}\"\n\n- name: \"Generate list of public A records\"\n  set_fact:\n    public_records: \"{{ public_records | default([]) + [ { 'type': 'A', 'hostname': hostvars[item]['ansible_hostname'], 'ip': hostvars[item]['public_v4'] } ] }}\"\n  with_items: \"{{ groups['cluster_hosts'] }}\"\n  when: hostvars[item]['public_v4'] is defined\n\n- name: \"Add wildcard records to the public A records\"\n  set_fact:\n    public_records: \"{{ public_records | default([]) + [ { 'type': 'A', 'hostname': '*.' + openshift_app_domain, 'ip': hostvars[item]['public_v4'] } ] }}\"\n  with_items: \"{{ groups['infra_hosts'] }}\"\n  when: hostvars[item]['public_v4'] is defined\n\n- name: \"Add public master cluster hostname records to the public A records (single master)\"\n  set_fact:\n    public_records: \"{{ public_records | default([]) + [ { 'type': 'A', 'hostname': (hostvars[groups.masters[0]].openshift_master_cluster_public_hostname | replace(full_dns_domain, ''))[:-1], 'ip': hostvars[groups.masters[0]].public_v4 } ] }}\"\n  when:\n    - hostvars[groups.masters[0]].openshift_master_cluster_public_hostname is defined\n    - openstack_num_masters == 1\n    - not use_bastion|bool\n\n- name: \"Add public master cluster hostname records to the public A records (single master behind a bastion)\"\n  set_fact:\n    public_records: \"{{ public_records | default([]) + [ { 'type': 'A', 'hostname': (hostvars[groups.masters[0]].openshift_master_cluster_public_hostname | replace(full_dns_domain, ''))[:-1], 'ip': hostvars[groups.bastions[0]].public_v4 } ] }}\"\n  when:\n    - hostvars[groups.masters[0]].openshift_master_cluster_public_hostname is defined\n    - openstack_num_masters == 1\n    - use_bastion|bool\n\n- name: \"Add public master cluster hostname records to the public A records (multi-master)\"\n  set_fact:\n    public_records: \"{{ public_records | default([]) + [ { 'type': 'A', 'hostname': (hostvars[groups.masters[0]].openshift_master_cluster_public_hostname | replace(full_dns_domain, ''))[:-1], 'ip': hostvars[groups.lb[0]].public_v4 } ] }}\"\n  when:\n    - hostvars[groups.masters[0]].openshift_master_cluster_public_hostname is defined\n    - openstack_num_masters > 1\n\n- name: \"Set the public DNS server details to use the external value (if provided)\"\n  set_fact:\n    nsupdate_server_public: \"{{ external_nsupdate_keys['public']['server'] }}\"\n    nsupdate_key_secret_public: \"{{ external_nsupdate_keys['public']['key_secret'] }}\"\n    nsupdate_key_algorithm_public: \"{{ external_nsupdate_keys['public']['key_algorithm'] }}\"\n    nsupdate_public_key_name: \"{{ external_nsupdate_keys['public']['key_name']|default('public-' + full_dns_domain) }}\"\n  when:\n    - external_nsupdate_keys is defined\n    - external_nsupdate_keys['public'] is defined\n\n- name: \"Set the public DNS server details to use the provisioned value\"\n  set_fact:\n    nsupdate_server_public: \"{{ hostvars[groups['dns'][0]].public_v4 }}\"\n    nsupdate_key_secret_public: \"{{ hostvars[groups['dns'][0]].nsupdate_keys['public-' + full_dns_domain].key_secret }}\"\n    nsupdate_key_algorithm_public: \"{{ hostvars[groups['dns'][0]].nsupdate_keys['public-' + full_dns_domain].key_algorithm }}\"\n  when:\n    - nsupdate_server_public is undefined\n\n- name: \"Generate the public Add section for DNS\"\n  set_fact:\n    public_named_records:\n      - view: \"public\"\n        zone: \"{{ full_dns_domain }}\"\n        server: \"{{ nsupdate_server_public }}\"\n        key_name: \"{{ nsupdate_public_key_name|default('public-' + full_dns_domain) }}\"\n        key_secret: \"{{ nsupdate_key_secret_public }}\"\n        key_algorithm: \"{{ nsupdate_key_algorithm_public | lower }}\"\n        entries: \"{{ public_records }}\"\n\n- name: \"Generate the final dns_records_add\"\n  set_fact:\n    dns_records_add: \"{{ private_named_records + public_named_records }}\"\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "363fa0430b54b99c96ed099a010428b5026880dc", "filename": "roles/storage-demo/tasks/deprovision.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- set_fact:\n    storage_demo_node_hostname: \"nohost\"\n\n- name: Render storage-demo deployment yaml\n  template:\n    src: \"{{ storage_demo_template_dir }}/storage-demo.yml\"\n    dest: /tmp/storage-demo.yml\n\n- name: Delete storage-demo Resources\n  command: kubectl delete -f /tmp/storage-demo.yml --ignore-not-found\n"}, {"commit_sha": "92dabcd706e72a0dc15ce13086fb9d59f1a8760e", "sha": "c1ed0f871d1093df87ee264f11f715681a895987", "filename": "tasks/nodejs.yml", "repository": "RocketChat/Rocket.Chat.Ansible", "decoded_content": "---\n\n- name: Ensure link /bin/node -> /bin/nodejs exists\n  file:\n    src: /bin/node\n    dest: /bin/nodejs\n    state: link\n  when: (ansible_os_family | lower == \"redhat\")\n\n- name: Check for npm\n  stat:\n    path: \"{{ rocket_chat_npm_dist }}\"\n  register: dist_npm_bin\n\n- name: Ensure node-version-manager (n) is installed (NPM)\n  npm:\n    name: n\n    global: true\n    executable: \"{{ rocket_chat_npm_dist }}\"\n  when: ('stat' in dist_npm_bin)\n         and (dist_npm_bin.stat.exists | bool)\n\n- name: Bootstrap node-version-manager from GitHub\n  get_url:\n    url: https://raw.githubusercontent.com/tj/n/master/bin/n\n    dest: /usr/bin/n\n    force: yes\n    mode: 0755\n    owner: root\n    validate_certs: true\n  become: yes\n  when: ('stat' in dist_npm_bin)\n         and (not (dist_npm_bin.stat.exists | bool))\n\n- name: Check to see if n has installed the required binaries in {{ rocket_chat_node_prefix }}\n  stat:\n    path: \"{{ rocket_chat_node_path }}\"\n  register: n_node_bin\n\n- name: \"Install the supported NodeJS environment via n [Version: {{ rocket_chat_node_version }}]\"\n  shell: n {{ rocket_chat_node_version }}\n  when: ('stat' in n_node_bin)\n         and (not (n_node_bin.stat.exists | bool))\n\n- name: Check to see if the proper npm version has already been installed\n  command: \"{{ rocket_chat_npm_path }} --version\"\n  changed_when: false\n  register: current_npm_version\n\n- name: \"Install the supported NPM version via npm [Version: {{ rocket_chat_npm_version }}]\"\n  npm:\n    name: npm\n    version: \"{{ rocket_chat_npm_version }}\"\n    path: \"{{ rocket_chat_node_prefix }}/lib\"\n    executable: \"{{ rocket_chat_npm_path }}\"\n  environment:\n    PATH: \"{{ rocket_chat_node_prefix }}/bin:{{ ansible_env.PATH }}\"\n  when: (current_npm_version != rocket_chat_npm_version)\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "4ab1ac5e7cd55fe7d23c15f08588ae7eb49de6f0", "filename": "reference-architecture/ansible-tower-integration/tower_config_azure/tower_config_azure/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n\n- name: Install pip\n  yum:\n    name: python2-pip\n    state: present\n  become: true\n\n- name: Install Tower CLI\n  yum:\n    name: ansible-tower-cli\n  become: true\n\n- name: Install npm\n  yum:\n    name: npm\n    state: present\n  become: true\n\n- name: Install npm azure-cli\n  command: npm install -g azure-cli\n  become: true\n\n- name: Azure Login\n  command: azure login -u \"{{ AZURE_USERNAME }}\" -p \"{{ AZURE_PASSWORD }}\"\n\n- name: Create Azure Service Principal\n  command: azure ad sp create -n \"{{ AZURE_SP_NAME }}\" -p \"{{ AZURE_SP_PASSWORD }}\"\n\n- name: Find Azure Service Principal Object ID\n  shell: azure ad sp show -c \"{{ AZURE_SP_NAME }}\" |grep \"Object Id:\" |awk -F\" \" '{print $4}'\n  register: azure_sp_object_id\n\n- debug: var=azure_sp_object_id\n\n- name: Find Azure Service Principal Object ID\n  shell: azure ad sp show -c \"{{ AZURE_SP_NAME }}\" |sed -n '7p' |awk -F\" \" '{print $2}'\n  register: azure_sp_name\n\n- debug: var=azure_sp_name\n\n- name: Find Azure Subscription ID\n  shell: azure account show |grep -v Tenant |grep ID |awk -F\" \" '{print $4}'\n  register: azure_subscription_id\n\n- debug: var=azure_subscription_id\n\n- name: Find Azure Tenant ID\n  shell: azure account show |grep \"Tenant ID\" |awk -F\" \" '{print $5}'\n  register: azure_tenant_id\n\n- debug: var=azure_tenant_id\n\n- name: Wait for Azure to be ready\n  pause:\n    seconds: 10\n\n- name: Grant contributor to Service Principal\n  command: azure role assignment create --objectId {{ azure_sp_object_id.stdout }} -o contributor -c /subscriptions/{{ azure_subscription_id.stdout }}\n\n- name: Add EPEL repository\n  yum_repository:\n    name: epel\n    description: EPEL YUM repo\n    mirrorlist: https://mirrors.fedoraproject.org/mirrorlist?repo=epel-7&arch=$basearch\n    enabled: no\n    gpgcheck: no\n  tags: epel\n  become: true\n\n- name: Install required packages\n  yum:\n    name: \"{{ item }}\"\n    state: present\n    disablerepo: \"epel\"\n  with_items:\n    - ansible\n    - python-devel\n    - openssl-devel\n    - gcc\n  become: true\n\n- name: Install EPEL required packages\n  yum:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - python2-pip\n    - python2-jmespath\n  become: true\n\n- name: Install pip required packages\n  pip:\n    name: \"{{ item }}\"\n  with_items:\n    - packaging\n    - msrestazure\n    - azure==2.0.0rc5\n  become: true\n\n- name: Create tower organization\n  tower_organization:\n    name: \"Default\"\n    description: \"Set to Default since the trial license only allows one organization. You can change it if you have deep pockets\"\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Add tower project for openshift-ansible-contrib\n  tower_project:\n    name: \"openshift-ansible-contrib\"\n    description: \"sync openshift-ansible-contrib\"\n    organization: \"Default\"\n    scm_url: https://github.com/openshift/openshift-ansible-contrib.git\n    scm_type: git\n    scm_branch: master\n    scm_update_on_launch: true\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Sync project openshift-ansible-contrib\n  command: tower-cli project update -n openshift-ansible-contrib\n\n- name: Add tower credential for machine\n  tower_credential:\n    name: azure-privkey\n    kind: ssh\n    become_method: sudo\n    description: azure-privkey\n    organization: \"Default\"\n    state: present\n    ssh_key_data: \"{{ AZURE_MACHINE_SSH_KEY }}\"\n    username: ec2-user\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Add tower credential for Azure\n  tower_credential:\n    name: azure\n    kind: azure_rm\n    description: azure\n    organization: \"Default\"\n    state: present\n    client: \" {{ azure_sp_name.stdout  }}\"\n    subscription: \" {{ azure_subscription_id.stdout }} \"\n    tenant: \" {{ azure_tenant_id.stdout }}\"\n    secret: \" {{ AZURE_SP_PASSWORD }} \"\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Add tower inventory\n  tower_inventory:\n    name: \"azure-inventory\"\n    description: \"Tower inventory for Azure\"\n    organization: \"Default\"\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Add tower group\n  tower_group:\n    name: azure\n    description: \"Tower Group for Azure\"\n    source: azure_rm\n    credential: azure\n    inventory: \"azure-inventory\"\n    source_vars: \"{{ lookup('file', 'tower-group-extravars.yaml') }}\"\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Wait for projects to sync\n  pause:\n    seconds: 10\n\n- name: Create azure-deploy-ocp job template\n  become: true\n  tower_job_template:\n    name: azure-deploy-ocp\n    job_type: run\n    inventory: azure-inventory\n    project: openshift-ansible-contrib\n    playbook: \"reference-architecture/azure-ansible/3.6/ansibledeployocp/playbooks/deploy.yaml\"\n    machine_credential: azure-privkey\n    cloud_credential: azure\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Create azure-destroy-ocp job template\n  become: true\n  tower_job_template:\n    name: azure-destroy-ocp\n    job_type: run\n    inventory: azure-inventory\n    project: openshift-ansible-contrib\n    playbook: \"reference-architecture/azure-ansible/3.6/ansibledeployocp/playbooks/destroy.yaml\"\n    machine_credential: azure-privkey\n    cloud_credential: azure\n    state: present\n    tower_config_file: \"~/.tower_cli.cfg\"\n\n- name: Create workflow-ocp-azure-install\n  command: tower-cli workflow create --name=\"workflow-ocp-azure-install\" --organization=\"Default\" --description=\"A workflow for deploying OCP on Azure\" -e @workflow-ocp-azure-extravars.yaml\n\n- name: Create a schema for workflow-ocp-azure-install\n  command: tower-cli workflow schema workflow-ocp-azure-install @schema-deploy.yaml\n\n- name: Create workflow-ocp-azure-destroy\n  command: tower-cli workflow create --name=\"workflow-ocp-azure-destroy\" --organization=\"Default\" --description=\"A workflow for destroy OCP on Azure\" -e @workflow-ocp-azure-extravars.yaml\n\n- name: Create a schema for workflow-ocp-azure-destroy\n  command: tower-cli workflow schema workflow-ocp-azure-destroy @schema-destroy.yaml\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "445fa7d58dcc39b242b17065c04322ef2b3753ab", "filename": "archive/roles/openshift-common/defaults/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\ndefault_openshift_storage_disk_volume: \"/dev/vdb\"\ndefault_openshift_master_count: 1\ndefault_openshift_node_count: 2\ndefault_openshift_app_domain: \"apps\"\ndefault_openshift_openstack_flavor_name: \"m1.medium\"\ndefault_openshift_openstack_image_name: \"_OS1_rhel-guest-image-7.2-20151102.0.x86_64.qcow2\"\ndefault_openshift_openstack_master_storage_size: 10\ndefault_openshift_openstack_node_storage_size: 10\ndefault_openshift_openstack_master_security_groups:\n  - name: default\n    rules: []\n  - name: ose3_master\n    rules:\n    - name: tcp-22\n      from_port: 22\n      to_port: 22\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: tcp-80\n      from_port: 80\n      to_port: 80\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: tcp-443\n      from_port: 443\n      to_port: 443\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: tcp-8443\n      from_port: 8443\n      to_port: 8443\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: udp-4789\n      from_port: 4789\n      to_port: 4789\n      protocol: udp\n      cidr: 0.0.0.0/0\n    - name: tcp-53\n      from_port: 53\n      to_port: 53\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: udp-53\n      from_port: 53\n      to_port: 53\n      protocol: udp\n      cidr: 0.0.0.0/0\n    - name: tcp-8053\n      from_port: 8053\n      to_port: 8053\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: udp-8053\n      from_port: 8053\n      to_port: 8053\n      protocol: udp\n      cidr: 0.0.0.0/0\n    - name: tcp-2379\n      from_port: 2379\n      to_port: 2379\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: tcp-2380\n      from_port: 2380\n      to_port: 2380\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: tcp-4001\n      from_port: 4001\n      to_port: 4001\n      protocol: tcp\n      cidr: 0.0.0.0/0\ndefault_openshift_openstack_node_security_groups:\n  - name: default\n    rules: []\n  - name: ose3_nodes\n    rules:\n    - name: tcp-22\n      from_port: 22\n      to_port: 22\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: tcp-80\n      from_port: 80\n      to_port: 80\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: tcp-443\n      from_port: 443\n      to_port: 443\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: udp-4789\n      from_port: 4789\n      to_port: 4789\n      protocol: udp\n      cidr: 0.0.0.0/0\n    - name: tcp-10250\n      from_port: 10250\n      to_port: 10250\n      protocol: tcp\n      cidr: 0.0.0.0/0\ndefault_openshift_openstack_dns_security_groups:\n  - name: default\n    rules: []\n  - name: dns\n    rules:\n    - name: ssh\n      from_port: 22\n      to_port: 22\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: dns-tcp-53\n      from_port: 53\n      to_port: 53\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: dns-udp-53\n      from_port: 53\n      to_port: 53\n      protocol: udp\n      cidr: 0.0.0.0/0\ndefault_openshift_openstack_nfs_security_groups:\n  - name: default\n    rules: []\n  - name: nfs\n    rules:\n    - name: ssh\n      from_port: 22\n      to_port: 22\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: nfs-tcp-111\n      from_port: 111\n      to_port: 111\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: nfs-udp-111\n      from_port: 111\n      to_port: 111\n      protocol: udp\n      cidr: 0.0.0.0/0\n    - name: nfs-tcp-2049\n      from_port: 2049\n      to_port: 2049\n      protocol: tcp\n      cidr: 0.0.0.0/0\n    - name: nfs-udp-2049\n      from_port: 2049\n      to_port: 2049\n      protocol: udp\n      cidr: 0.0.0.0/0\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "8f9d52aba792376c0f2119a2839895cabcd9825c", "filename": "roles/vpn/tasks/openssl.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Ensure the pki directory is not exist\n  local_action:\n    module: file\n    dest: configs/{{ IP_subject_alt_name }}/pki\n    state: absent\n  become: no\n  when: easyrsa_reinit_existent == True\n\n- name: Ensure the pki directories are exist\n  local_action:\n    module: file\n    dest: \"configs/{{ IP_subject_alt_name }}/pki/{{ item }}\"\n    state: directory\n    recurse: yes\n  become: no\n  with_items:\n    - ecparams\n    - certs\n    - crl\n    - newcerts\n    - private\n    - reqs\n\n- name: Ensure the files are exist\n  local_action:\n    module: file\n    dest: \"configs/{{ IP_subject_alt_name }}/pki/{{ item }}\"\n    state: touch\n  become: no\n  with_items:\n    - \".rnd\"\n    - \"private/.rnd\"\n    - \"index.txt\"\n    - \"index.txt.attr\"\n    - \"serial\"\n\n- name: Generate the openssl server configs\n  local_action:\n    module: template\n    src: openssl.cnf.j2\n    dest: \"configs/{{ IP_subject_alt_name }}/pki/openssl.cnf\"\n  become: no\n\n\n- name: Build the CA pair\n  local_action: >\n    shell openssl ecparam -name prime256v1 -out ecparams/prime256v1.pem &&\n      openssl req -utf8 -new -newkey {{ algo_params | default('ec:ecparams/prime256v1.pem') }} -config openssl.cnf -keyout private/cakey.pem -out cacert.pem -x509 -days 3650 -batch -passout pass:\"{{ easyrsa_CA_password }}\" &&\n      touch {{ IP_subject_alt_name }}_ca_generated\n  become: no\n  args:\n    chdir: \"configs/{{ IP_subject_alt_name }}/pki/\"\n    creates: \"{{ IP_subject_alt_name }}_ca_generated\"\n  environment:\n    subjectAltName: \"DNS:{{ IP_subject_alt_name }},IP:{{ IP_subject_alt_name }}\"\n\n- name: Copy the CA certificate\n  local_action:\n    module: copy\n    src: \"configs/{{ IP_subject_alt_name }}/pki/cacert.pem\"\n    dest: \"configs/{{ IP_subject_alt_name }}/cacert.pem\"\n    mode: 0600\n  become: no\n\n- name: Generate the serial number\n  local_action: >\n    shell echo 01 > serial &&\n      touch serial_generated\n  become: no\n  args:\n    chdir: \"configs/{{ IP_subject_alt_name }}/pki/\"\n    creates: serial_generated\n\n- name: Build the server pair\n  local_action: >\n    shell openssl req -utf8 -new -newkey {{ algo_params | default('ec:ecparams/prime256v1.pem') }} -config openssl.cnf -keyout private/{{ IP_subject_alt_name }}.key -out reqs/{{ IP_subject_alt_name }}.req -nodes -passin pass:\"{{ easyrsa_CA_password }}\" -subj \"/CN={{ IP_subject_alt_name }}\" -batch &&\n    openssl ca -utf8 -in reqs/{{ IP_subject_alt_name }}.req -out certs/{{ IP_subject_alt_name }}.crt -config openssl.cnf -days 3650 -batch -passin pass:\"{{ easyrsa_CA_password }}\" -subj \"/CN={{ IP_subject_alt_name }}\" &&\n    touch certs/{{ IP_subject_alt_name }}_crt_generated\n  become: no\n  args:\n    chdir: \"configs/{{ IP_subject_alt_name }}/pki/\"\n    creates: certs/{{ IP_subject_alt_name }}_crt_generated\n  environment:\n    subjectAltName: \"DNS:{{ IP_subject_alt_name }},IP:{{ IP_subject_alt_name }}\"\n\n- name: Build the client's pair\n  local_action: >\n   shell openssl req -utf8 -new -newkey {{ algo_params | default('ec:ecparams/prime256v1.pem') }} -config openssl.cnf -keyout private/{{ item }}.key -out reqs/{{ item }}.req -nodes -passin pass:\"{{ easyrsa_CA_password }}\" -subj \"/CN={{ item }}\" -batch &&\n      openssl ca -utf8 -in reqs/{{ item }}.req -out certs/{{ item }}.crt -config openssl.cnf -days 3650 -batch -passin pass:\"{{ easyrsa_CA_password }}\" -subj \"/CN={{ item }}\" &&\n      touch certs/{{ item }}_crt_generated\n  become: no\n  args:\n    chdir: \"configs/{{ IP_subject_alt_name }}/pki/\"\n    creates: certs/{{ item }}_crt_generated\n  environment:\n    subjectAltName: \"DNS:{{ item }}\"\n  with_items: \"{{ users }}\"\n\n- name: Build the client's p12\n  local_action: >\n    shell openssl pkcs12 -in certs/{{ item }}.crt -inkey private/{{ item }}.key -export -name {{ item }} -out private/{{ item }}.p12 -certfile cacert.pem -passout pass:\"{{ easyrsa_p12_export_password }}\"\n  become: no\n  args:\n    chdir: \"configs/{{ IP_subject_alt_name }}/pki/\"\n  with_items: \"{{ users }}\"\n\n- name: Copy the p12 certificates\n  local_action:\n    module: copy\n    src: \"configs/{{ IP_subject_alt_name }}/pki/private/{{ item }}.p12\"\n    dest: \"configs/{{ IP_subject_alt_name }}/{{ item }}.p12\"\n    mode: 0600\n  become: no\n  with_items:\n    - \"{{ users }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "73b763fb2249eaf8c1ac0977c61c1ba90dc1be7d", "filename": "roles/config-iscsi-client/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- include_tasks: iscsi.yml\n  when: \n  - iscsi_target is defined\n  - iscsi_target|trim != ''\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "57eab26bc936dbab3ce462a2fccbc580f240d7cf", "filename": "roles/postgresql/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install PostgreSQL packages\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - postgresql\n  tags:\n    - download\n\n- name: Install postgresql-client (debuntu)\n  package:\n    name: postgresql-client\n  when: is_debuntu\n  tags:\n    - download\n\n- name: Install postgresql-server (OS's other than debuntu)\n  package:\n    name: postgresql-server\n  when: not is_debuntu\n  tags:\n    - download\n\n- name: Create postgresql-iiab systemd service\n  template:\n    src: postgresql-iiab.service\n    dest: /etc/systemd/system/postgresql-iiab.service\n    owner: root\n    group: root\n    mode: 0644\n\n- name: Create postgres data directory\n  file:\n    path: /library/pgsql-iiab\n    owner: postgres\n    group: postgres\n    mode: 0700\n    state: directory\n\n- name: Make sure that the en_US locale is enabled (debuntu)\n  lineinfile:\n    dest: /etc/locale.gen\n    line: \"{{ postgresql_locale }} UTF-8\"\n  when: is_debuntu\n\n- name: Generate the selected locales (debuntu)\n  command: /usr/sbin/locale-gen\n  when: is_debuntu\n\n- name: Initialize the postgres db (debuntu)\n  command: su - postgres -c \"/usr/lib/postgresql/{{ postgresql_version }}/bin/initdb -E 'UTF-8' --locale={{ postgresql_locale }} -D /library/pgsql-iiab\"\n  args:\n    creates: /library/pgsql-iiab/pg_hba.conf\n  when: is_debuntu\n\n- name: Initialize the postgres db (OS's other than debuntu)\n  command: su - postgres -c \"/usr/bin/initdb -E 'UTF-8' --lc-collate={{ postgresql_locale }} --lc-ctype={{ postgresql_locale }} -D /library/pgsql-iiab\"\n  args:\n    creates: /library/pgsql-iiab/pg_hba.conf\n  when: not is_debuntu\n\n- name: Configure PostgreSQL\n  template:\n    backup: yes\n    src: postgresql.conf.j2\n    dest: /library/pgsql-iiab/postgresql.conf\n    owner: postgres\n    group: postgres\n    mode: 0640\n\n- name: Stop postgresql service (debuntu)\n  command: \"/etc/init.d/postgresql stop\"\n  ignore_errors: True\n  when: postgresql_install and is_debuntu\n\n- name: Stop and disable stock postgresql service\n  service:\n    name: postgresql\n    state: stopped\n    enabled: no\n\n- name: Start and enable postgresql-iiab service\n  service:\n    name: postgresql-iiab\n    state: started\n    enabled: yes\n  when: postgresql_enabled\n\n- name: Stop and disable postgresql-iiab service if not postgresql_enabled\n  service:\n    name: postgresql-iiab\n    state: stopped\n    enabled: no\n  when: not postgresql_enabled\n\n- name: Add 'postgresql' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: postgresql\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: PostgreSQL\n    - option: description\n      value: '\"PostgreSQL is a powerful, open source object-relational database system.\"'\n    - option: installed\n      value: \"{{ postgresql_install }}\"\n    - option: enabled\n      value: \"{{ postgresql_enabled }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "561917ab9d1c8420a5f7f561df39bd46ad3145bc", "filename": "reference-architecture/gcp/ansible/playbooks/validation.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: create instance groups\n  hosts: localhost\n  roles:\n  - instance-groups\n\n- include: ../../../../playbooks/post-validation.yaml\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "d0fee87eac230670686998e37a3101546c89b777", "filename": "roles/seed-git-server/meta/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ndependencies:\n- role: git-server\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "4771b61a9b071c831edba19533d443bae6284ef5", "filename": "roles/dokuwiki/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "dokuwiki_url: /wiki\ndokuwiki_install: True\ndokuwiki_enabled: False\ndokuwiki_version: \"dokuwiki-2018-04-22a\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4a50e79b66968be37ee409de15ecab8c520d0c10", "filename": "roles/config-pxe/tasks/kickstart.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install kickstart files\"\n  copy:\n    src: \"{{ ks_files }}\"\n    dest: \"{{ ks_files_destination }}\" \n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "58e4da45f9408b515895d14bb895647009a4f168", "filename": "roles/openshift-applier/tasks/pre-post-step.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n# This is a bit of a \"hack\" to pass the variables in to the role below\n# It would be nice if we could something like `vars: {{ step.vars }}`,\n# but it unfortunately generates an error with Ansible.\n- name: \"Set facts to make variables available to the role\"\n  set_fact:\n    \"{{ var.key }}\": \"{{ var.value }}\"\n  with_dict: \"{{ step.vars | default({}) }}\"\n  loop_control:\n    loop_var: var\n\n- name: \"Include the pre/post step role\"\n  include_role:\n    name: \"{{ tmp_dep_dir }}{{ step.role }}\"\n  when:\n  - step is defined\n  - step.role is defined\n  - step.role|trim != ''\n\n- name: \"Clear facts to ensure that they don't carry over\"\n  set_fact:\n    \"{{ var.key }}\": \"\"\n  with_dict: \"{{ step.vars | default({}) }}\"\n  loop_control:\n    loop_var: var\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "275d092ef2c51a267562d0424d0aa509260b739f", "filename": "roles/docket/tasks/lighttpd.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# - name: docket | configure lighttpd + uwsgi\n#   template:\n#     src: docket_lighttpd_scgi.conf.j2\n#     dest: /etc/lighttpd/conf.d/docket_scgi.conf\n#\n# - name: docket | configure lighttpd TLS listener\n#   template:\n#     src: docket_lighttpd_vhost.conf.j2\n#     dest: /etc/lighttpd/vhosts.d/docket.conf\n#   notify: docket | restart lighttpd\n\n- name: Create lighttpd + uwsgi config\n  template:\n    src: lighttpd-30-docket.conf.j2\n    dest: /etc/lighttpd/vhosts.d/30-docket.conf\n  notify: Restart lighttpd\n\n- name: Create vhost logdir\n  file:\n    state: directory\n    path: \"/var/log/lighttpd/{{ docket_web_server_name }}/\"\n    owner: lighttpd\n    group: lighttpd\n    mode: 0755\n\n- name: Enable lighttpd vhosts\n  lineinfile:\n    path: /etc/lighttpd/lighttpd.conf\n    regexp: '^#?\\s*include.*vhosts\\.d/.*$'\n    line: include \"/etc/lighttpd/vhosts.d/*.conf\"\n  notify: Restart lighttpd\n\n- name: Add lighttpd into docket group\n  user:\n    name: lighttpd\n    append: true\n    groups: \"{{ docket_group }}\"\n  notify: Restart lighttpd\n\n- name: Enable lighttpd service\n  service:\n    name: lighttpd\n    enabled: true\n  notify: Restart lighttpd\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "bf424676dccc6e34054d33bfaabf8c31f22652ed", "filename": "playbooks/provisioning/openstack/provision-openstack.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: True\n  become: False\n  vars_files:\n    - stack_params.yaml\n  pre_tasks:\n    - include: pre_tasks.yml\n  roles:\n    - role: openstack-stack\n    - role: openstack-create-cinder-registry\n      when:\n        - cinder_hosted_registry_name is defined\n        - cinder_hosted_registry_size_gb is defined\n    - role: static_inventory\n      when: openstack_inventory|default('static') == 'static'\n      inventory_path: \"{{ openstack_inventory_path|default(inventory_dir) }}\"\n      private_ssh_key: \"{{ openstack_private_ssh_key|default('') }}\"\n      ssh_config_path: \"{{ openstack_ssh_config_path|default('/tmp/ssh.config.openshift.ansible' + '.' + stack_name) }}\"\n      ssh_user: \"{{ ansible_user }}\"\n\n- name: Refresh Server inventory or exit to apply SSH config\n  hosts: localhost\n  connection: local\n  become: False\n  gather_facts: False\n  tasks:\n    - name: Exit to apply SSH config for a bastion\n      meta: end_play\n      when: openstack_use_bastion|default(False)|bool\n    - name: Refresh Server inventory\n      meta: refresh_inventory\n\n- include: post-provision-openstack.yml\n  when: not openstack_use_bastion|default(False)|bool\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "d60e50803f116d4309300d3e369f911ae46b9677", "filename": "meta/main.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\ngalaxy_info:\n  author: ansible-ThoTeam\n  description: Nexus Repository Manager 3.x (Sonatype)\n  company: ThoTeam\n\n  license: license (GPLv3)\n\n  min_ansible_version: 2.5.4\n\n  github_branch: master\n\n  platforms:\n    - name: EL\n      versions:\n        - 7\n    - name: Ubuntu\n      versions:\n        - xenial\n        - bionic\n    - name: Debian\n      versions:\n        - jessie\n        - stretch\n\n  galaxy_tags:\n    - nexus\n    - nexus3\n    - java\n    - maven\n    - npm\n    - nuget\n    - yum\n    - docker\n    - pypi\n    - web\n\ndependencies: []\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "12aca896108572587a45e4f6aad87caeefe27a12", "filename": "playbooks/roles/docket/defaults/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# defaults file for rocknsm.docket\nansible_cache: /tmp/ansible_cache\n\nrocknsm_conf_dir: /etc/rocknsm\nrocknsm_conf_user: root\nrocknsm_conf_group: root\n\n# How are we going to install docket?\n# Currently supports one of: yumrepo, offline\n# yumrepo sets up the RockNSM repo that hosts docket\n# offline skips repo setup and assumes it's already configured\ndocket_install: yumrepo\ndocket_enable: true\n\n# Application config\ndocket_debug: false\ndocket_testing: false\ndocket_secret: \"{{ lookup('password', '/dev/null chars=letters,digits length=64') }}\"\ndocket_session_cookie: DOCKET_SESSION\ndocket_sendfile: true\ndocket_logger: docket\ndocket_celery_url: redis://localhost:6379\ndocket_spool_dir: /var/spool/docket\ndocket_frontend_dir: /opt/rocknsm/docket/frontend\ndocket_uwsgi_socket: /run/docket/docket.socket\ndocket_no_redis: false\ndocket_long_ago: 24h\n\n# Web server config\ndocket_tls: true\ndocket_user: docket\ndocket_group: docket\n\n# An empty string defaults to all interfaces on IPv4\ndocket_listen_ip: \"0.0.0.0\"\ndocket_listen_port: \"{{ 8443 if docket_tls else 8080 }}\"\n\ndocket_web_server: lighttpd\ndocket_web_pemfile: \"/etc/pki/tls/private/lighttpd_docket.pem\"\ndocket_web_dhparams: \"/etc/pki/tls/misc/lighttpd_dh.pem\"\ndocket_web_server_name: \"{{ansible_fqdn}}\"\ndocket_web_user: lighttpd\n\n# Vars to generate keys/certs\ndocket_x509_dir: /etc/pki/docket/\ndocket_x509_key: \"{{docket_x509_dir}}/docket_{{ ansible_default_ipv4.address }}_key.pem\"\ndocket_x509_cn: \"{{ ansible_hostname }}_docket\"\ndocket_x509_o: Stenographer\ndocket_x509_c: XX\ndocket_x509_user: root\ndocket_x509_group: docket\n\n# These should be overridden by host-specific vars\nsteno_host: \"127.0.0.1\"\nsteno_sensor: \"{{ansible_hostname}}\"\nsteno_port: 1234\nsteno_certs_dir: /etc/stenographer/certs\nsteno_ca_cert: \"{{steno_certs_dir}}/ca_cert.pem\"\nsteno_ca_key:  \"{{steno_certs_dir}}/ca_key.pem\"\n\n# This is used to generate the config for docket on\n# where to connect and how to authenticate\ndocket_steno_instances:\n- { host: \"{{ steno_host}}\", sensor: \"{{ steno_sensor }}\", port: \"{{ steno_port }}\", key: \"{{ docket_x509_key }}\", cert: \"{{docket_x509_dir}}/docket-{{inventory_hostname}}_sensor-{{inventory_hostname}}_cert.pem\", ca: \"{{docket_x509_dir}}/{{inventory_hostname}}_ca_cert.pem\" }\n"}, {"commit_sha": "b11c4477d973b0cc87a296f6b028eaf9abab4686", "sha": "63164f4ce8b7f7d742a42efb9ecb6644ebc8ff78", "filename": "handlers/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# handlers file for ansible-role-docker-ce\n\n- name: restart docker\n  service:\n    name: docker\n    state: restarted\n  become: true"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "6f8908d867bdb853c49e182034a89b9ceb335efd", "filename": "roles/kalite/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# Assessment logic removed 3/1/2017 TFM\n\n# Assume all XOs are F18 and nothing else is\n\n- name: Calc KA Lite db file name (Fedora 18)\n  set_fact:\n    kalite_db_name: \"{{ kalite_root }}/kalite/database/data.sqlite\"\n  when: is_F18\n\n- name: Calc KA Lite db file name (OS's other than Fedora 18)\n  set_fact:\n    kalite_db_name: \"{{ kalite_root }}/database/data.sqlite\"\n  when: not is_F18\n\n- name: See if KA Lite is already configured\n  stat:\n    path: \"{{ kalite_db_name }}\"\n  register: kalite_installed\n\n- include_tasks: install-f18.yml\n  when: not kalite_installed.stat.exists and is_F18\n\n- include_tasks: install.yml\n  when: kalite_installed is defined and not kalite_installed.stat.exists and not is_F18\n\n- name: Ask systemd to reread unit files (daemon-reload)\n  systemd:\n    daemon_reload: yes\n  when: not kalite_installed.stat.exists\n\n- include_tasks: setup-f18.yml\n  when: not kalite_installed.stat.exists and is_F18\n\n- include_tasks: setup.yml\n  when: not kalite_installed.stat.exists and not is_F18\n\n- include_tasks: enable.yml\n\n- name: Add 'kalite' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: kalite\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n  - option: name\n    value: \"KA Lite\"\n  - option: description\n    value: '\"KA Lite is a server to present Khan Academy videos offline and to download them.\"'\n  - option: path\n    value: \"{{ kalite_root }}\"\n  - option: port\n    value: \"{{ kalite_server_port }}\"\n  - option: enabled\n    value: \"{{ kalite_enabled }}\"\n  - option: cron_enabled\n    value: \"{{ kalite_cron_enabled }}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "d2b445360b9713faafbe1b007dff49f1e2584eea", "filename": "roles/pathagar/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "---\n# variables for pathagar\n\npathagar_subpath: /books\n\npathagar_dir: /usr/local/pathagar\npathagar_media: /library/pathagar/media\npathagar_src: '{{ pathagar_dir }}/pathagar'\npathagar_venv: '{{ pathagar_dir }}/venv'\npathagar_collectstatic: '{{ pathagar_dir }}/static'\n\npathagar_db_name: pathagar\npathagar_db_user: pathagar\npathagar_db_password: pathagar\n\npathagar_username: pathagar\npathagar_password: \"pbkdf2_sha256$10000$qkA7MPkQWHTY$AO5j/ByLC68qEt8X1c9QvieArbadYKhiBnlKe8uR6G8=\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "5442a5b080d0b68fb27df48243aaec9042c91610", "filename": "reference-architecture/vmware-ansible/playbooks/prod-ose-crs.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  connection: local\n  gather_facts: yes\n  become: no\n  vars_files:\n  - vars/main.yaml\n  roles:\n  # Group systems\n  - create-vm-crs-prod-ose\n  - instance-groups\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "d771cc8f6524f3b9bf2b6c1dc7270b34c4eeb07a", "filename": "roles/cloud-gce/defaults/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\ngce_venv: \"{{ playbook_dir }}/configs/.venvs/gce\"\n"}, {"commit_sha": "bf6e08dcb2440421477b6536ff6a8d11adc2be17", "sha": "57853e9ca2d3ff026c7d2d95c03d006ce66d5925", "filename": "roles/marathon/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for marathon\n- name: upload marathon template service\n  template:\n    src: marathon.conf.j2\n    dest: /etc/init/marathon.conf\n    mode: 0755\n  notify:\n    - restart marathon\n  sudo: yes\n  tags:\n    - marathon\n\n- name: Set Marathon hostname\n  sudo: yes\n  copy:\n    content: \"{{ marathon_hostname }}\"\n    dest: /etc/marathon/conf/hostname\n    mode: 0644\n  notify:\n    - restart marathon\n  tags:\n    - marathon\n\n- name: remove marathon override\n  sudo: yes\n  file:\n    path: /etc/init/marathon.override\n    state: absent\n  notify:\n    - restart marathon\n  tags:\n    - marathon\n\n- name: install wait script\n  sudo: yes\n  template:\n    src: marathon-wait-for-listen.sh.j2\n    dest: /usr/local/bin/marathon-wait-for-listen.sh\n    mode: 0755\n  notify:\n    - restart marathon\n  tags:\n    - marathon\n\n- name: ensure marathon is running (and enable it at boot)\n  sudo: yes\n  service:\n    name: marathon\n    state: started\n    enabled: yes\n  notify:\n    - wait for marathon to listen\n  tags:\n    - marathon\n\n- meta: flush_handlers\n\n# This is here to workaround an issue where marathon does not receive an\n# acknowledgement correctly from Mesos.\n- name: force restart marathon\n  sudo: yes\n  service:\n    name: marathon\n    state: restarted\n  notify:\n    - wait for marathon to listen\n  tags:\n    - marathon\n\n- meta: flush_handlers\n\n- name: Set Marathon consul service definition\n  sudo: yes\n  template:\n    src: marathon-consul.j2\n    dest: \"{{ consul_dir }}/marathon.json\"\n  notify:\n    - restart consul\n  tags:\n    - marathon\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "0592851722b0b024b1008b93e77dfbde137146f1", "filename": "roles/pathagar/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Remove package Pathagar (in case rpm?)\n  package:\n    name: pathagar\n    state: absent\n\n- name: Install Pathagar prerequisites (all OSs)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - python-virtualenv\n    - python-pip\n    - python-psycopg2\n\n- name: Install Pathagar prerequisites (debuntu)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - libapache2-mod-wsgi\n    - libxml2-dev\n    - libxslt-dev\n  when: is_debuntu\n\n- name: Install Pathagar prerequisites (not debuntu)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - mod_wsgi\n    - libxml2-devel\n    - libxslt-devel\n  when: not is_debuntu\n\n- name: Create destination folder\n  file:\n    path: \"{{ pathagar_src }}\"\n    state: directory\n    owner: root\n    group: root\n    mode: 0755\n\n- name: Create books destination folder\n  file:\n    path: \"{{ pathagar_media }}\"\n    state: directory\n    owner: \"{{ apache_user }}\"\n    group: \"{{ apache_user }}\"\n    mode: 0755\n\n- name: Determine if Pathagar has already been downloaded from git\n  stat:\n    path: \"{{ pathagar_src }}/settings.py\"\n  register: pathagar\n\n- name: Clone Pathagar repo\n  git:\n    repo: https://github.com/PathagarBooks/pathagar.git\n    dest: \"{{ pathagar_src }}\"\n    update: yes\n    version: master\n  when: internet_available and pathagar.stat.exists is defined and not pathagar.stat.exists\n\n- name: Install Pathagar requirements in a virtualenv\n  pip:\n    name: \"{{ item }}\"\n  with_items:\n    - Django==1.4.5\n    - django-tagging==0.3.1\n    - django-sendfile==0.3.6\n    - lxml==3.4.4\n  when: internet_available\n\n- name: Install Pathagar requirements in a virtualenv\n  pip:\n    name: \"{{ item }}\"\n    extra_args: \"--use-wheel\"\n    virtualenv: \"{{ pathagar_venv }}\"\n    virtualenv_site_packages: yes\n  with_items:\n    - django-taggit==0.14\n\n- name: Create Pathagar postgresql user\n  postgresql_user:\n    name: \"{{ pathagar_db_user }}\"\n    password: \"{{ pathagar_db_password }}\"\n    role_attr_flags: NOSUPERUSER,NOCREATEROLE,NOCREATEDB\n    state: present\n  become: yes\n  become_user: postgres\n\n- name: Start postgresql-iiab\n  service:\n    name: postgresql-iiab\n    state: started\n\n- name: Enable Pathagar postgresql user access by md5 method\n  lineinfile:\n    backup: yes\n    dest: /library/pgsql-iiab/pg_hba.conf\n    regexp: '^host\\s+pathagar'\n    line: \"host    pathagar        pathagar     samehost     md5\"\n    state: present\n    insertafter: \"^# IPv4 local connections\"\n    owner: postgres\n    group: postgres\n  register: enable_pathagar_md5_access\n\n- name: Reload postgresql service\n  service:\n    name: postgresql-iiab\n    state: reloaded\n  when: enable_pathagar_md5_access.changed\n\n- name: Create Pathagar postgresql database\n  postgresql_db:\n    name: \"{{ pathagar_db_name }}\"\n    encoding: utf8\n    owner: \"{{ pathagar_db_user }}\"\n    state: present\n    template: template0\n  become: yes\n  become_user: postgres\n\n- name: Install IIAB custom settings for Pathagar\n  template:\n    src: prod_settings.py\n    dest: \"{{ pathagar_src }}/prod_settings.py\"\n    owner: root\n    group: root\n    mode: 0644\n\n- name: Create Pathagar initial db\n  django_manage:\n    app_path: \"{{ pathagar_src }}\"\n    command: syncdb\n    virtualenv: \"{{ pathagar_venv }}\"\n    settings: pathagar.prod_settings\n\n- name: Upload Pathagar admin user\n  template:\n    src: auth.User.json\n    dest: \"{{ pathagar_dir }}/auth.User.json\"\n    owner: root\n    group: root\n    mode: 0600\n\n- name: Load Pathagar admin user\n  django_manage:\n    app_path: \"{{ pathagar_src }}\"\n    command: loaddata\n    virtualenv: \"{{ pathagar_venv }}\"\n    settings: pathagar.prod_settings\n    fixtures: \"{{ pathagar_dir }}/auth.User.json\"\n\n- name: Collect Pathagar static files\n  django_manage:\n    app_path: \"{{ pathagar_src }}\"\n    command: collectstatic\n    virtualenv: \"{{ pathagar_venv }}\"\n    settings: pathagar.prod_settings\n\n- name: Install wsgi.py for Pathagar\n  template:\n    src: wsgi.py\n    dest: \"{{ pathagar_dir }}/wsgi.py\"\n    owner: root\n    group: root\n    mode: 0644\n\n- name: Install httpd conf for Pathagar\n  template:\n    src: pathagar.conf\n    backup: yes\n    dest: \"/etc/{{ apache_config_dir }}/pathagar.conf\"\n    mode: 0644\n\n- name: Enable Pathagar (debuntu)\n  file:\n    path: /etc/apache2/sites-enabled/pathagar.conf\n    src: /etc/apache2/sites-available/pathagar.conf\n    state: link\n  when: pathagar_enabled and is_debuntu\n\n- name: Disable Pathagar (debuntu)\n  file:\n    path: /etc/apache2/sites-enabled/pathagar.conf\n    state: absent\n  when: not pathagar_enabled and is_debuntu\n\n- name: Restart http\n  service:\n    name: \"{{ apache_service }}\"\n    state: reloaded\n\n- name: Add 'pathagar' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: pathagar\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: pathagar\n    - option: description\n      value: '\"Pathagar is a simple bookserver serving OPDS feeds\"'\n    - option: path\n      value: /books\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "c13e86c6f86c615814b86e10ff42e82ae4f97fb4", "filename": "roles/docket/tasks/crypto.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# tasks file for rocknsm.docket\n\n# Install packages\n\n# Ensure `stenographer` and `nginx` groups exists\n\n# Configure docket app settings\n- name: Ensure ansible_cache dir exists\n  tempfile:\n    state: directory\n  changed_when: false\n  register: cachedir\n  run_once: true\n  delegate_to: localhost\n  become: false\n\n- name: Set ansible_cache fact\n  set_fact:\n    ansible_cache: \"{{ cachedir.path }}\"\n\n# TODO: Not sure why this is needed here\n- name: Ensure rock nsm conf dir exists\n  file:\n    path: \"{{ rocknsm_conf_dir }}\"\n    state: directory\n    owner: \"{{ rocknsm_conf_user }}\"\n    group: \"{{ rocknsm_conf_group }}\"\n\n- name: Ensure docket x509 user exists\n  user:\n    name: \"{{ docket_x509_user }}\"\n    state: present\n  when: inventory_hostname in groups['docket']\n\n- name: Ensure docket x509 group exists\n  group:\n    name: \"{{ docket_x509_group }}\"\n    system: true\n    state: present\n  when: inventory_hostname in groups['docket']\n\n- name: Ensure docket x509 dir exists\n  file:\n    path: \"{{ docket_x509_dir }}\"\n    state: directory\n    mode: 0750\n    owner: \"{{ docket_x509_user }}\"\n    group: \"{{ docket_x509_group }}\"\n  when: inventory_hostname in groups['docket']\n\n# Generate/copy x509 client cert/keys and CA certs\n# Use new openssl module in ansible 2.3\n- name: Create docket private key\n  openssl_privatekey:\n    path: \"{{ docket_x509_key }}\"\n    size: 4096\n  when: inventory_hostname in groups['docket']\n\n- name: Set perms on private key\n  file:\n    path: \"{{ docket_x509_key }}\"\n    owner: \"{{ docket_x509_user }}\"\n    group: \"{{ docket_x509_group }}\"\n    mode: \"0644\"\n  when: inventory_hostname in groups['docket']\n\n- name: Check for certificate\n  stat:\n    path: \"{{ docket_x509_dir }}/docket-{{ inventory_hostname_short }}_sensor-{{ item }}_cert.pem\"\n  register: docket_cert\n  changed_when: false\n  loop: \"{{ groups['stenographer'] }}\"\n  when: inventory_hostname in groups['docket']\n\n- debug: var=docket_cert.results\n  when: inventory_hostname in groups['docket']\n\n- name: Create docket csr\n  openssl_csr:\n    path: \"{{ docket_x509_key }}.csr\"\n    privatekey_path: \"{{ docket_x509_key }}\"\n    commonName: \"{{ docket_x509_cn }}\"\n    organizationName: \"{{ docket_x509_o }}\"\n    countryName: \"{{ docket_x509_c }}\"\n    keyUsage: digitalSignature\n    extendedKeyUsage: clientAuth\n  when:\n    - inventory_hostname in groups['docket']\n    - docket_cert|json_query('results[?stat.exists==`false`]')|length\n  register: new_csr\n\n- name: Fetch csr\n  fetch:\n    src: \"{{ docket_x509_key }}.csr\"\n    dest: \"{{ ansible_cache }}/{{ inventory_hostname_short }}.csr\"\n    flat: true\n  when:\n    - inventory_hostname in groups['docket']\n    - not new_csr is skipped\n\n- debug:\n    msg: \"Number of missing certs: {{ hostvars[item].docket_cert|json_query('results[?stat.exists==`false`]')|length }}\"\n  when:\n    - inventory_hostname in groups['stenographer']\n  loop: \"{{ groups['docket'] }}\"\n\n- debug:\n    var: hostvars[item].docket_cert|json_query('results[?item==inventory_hostname_short]')\n  loop: \"{{ groups['docket'] }}\"\n  when:\n    - inventory_hostname in groups['stenographer']\n\n- name: Push csr to stenographer hosts\n  copy:\n    src: \"{{ ansible_cache }}/{{ hostvars[item].inventory_hostname_short }}.csr\"\n    dest: \"{{ steno_certs_dir }}/{{ hostvars[item].inventory_hostname_short }}.csr\"\n  loop: \"{{ groups['docket'] }}\"\n  when:\n    - inventory_hostname in groups['stenographer']\n    - not hostvars[item].new_csr is skipped\n\n- name: Sign certificate signing requests\n  openssl_certificate:\n    path: \"{{ steno_certs_dir }}/docket-{{ hostvars[item].inventory_hostname_short }}_sensor-{{ inventory_hostname_short }}_cert.pem\"\n    csr_path: \"{{ steno_certs_dir }}/{{ hostvars[item].inventory_hostname_short }}.csr\"\n    ownca_privatekey_path: \"{{ steno_ca_key }}\"\n    ownca_path: \"{{ steno_ca_cert }}\"\n    provider: ownca\n  loop: \"{{ groups['docket'] }}\"\n  when:\n    - inventory_hostname in groups['stenographer']\n    - hostvars[item].docket_cert.results|map(attribute=\"stat.exists\")|select(\"equalto\",false)|list|length\n\n- name: Pull certificates back\n  fetch:\n    src: \"{{ steno_certs_dir }}/docket-{{ hostvars[item].inventory_hostname_short }}_sensor-{{ inventory_hostname_short }}_cert.pem\"\n    dest: \"{{ ansible_cache }}/docket-{{ hostvars[item].inventory_hostname_short }}_sensor-{{ inventory_hostname_short }}_cert.pem\"\n    flat: true\n  loop: \"{{ groups['docket'] }}\"\n  when:\n    - inventory_hostname in groups['stenographer']\n    - hostvars[item].docket_cert.results|map(attribute=\"stat.exists\")|select(\"equalto\",false)|list|length\n\n- name: Pull back ca certificates\n  fetch:\n    src: \"{{ steno_ca_cert }}\"\n    dest: \"{{ ansible_cache }}/{{ inventory_hostname_short }}_ca_cert.pem\"\n    flat: true\n  when:\n    - inventory_hostname in groups['stenographer']\n  changed_when: false\n\n- name: Push certificates to docket hosts\n  copy:\n    src: \"{{ ansible_cache }}/docket-{{ inventory_hostname_short }}_sensor-{{ hostvars[item].inventory_hostname_short }}_cert.pem\"\n    dest: \"{{ docket_x509_dir }}/docket-{{ inventory_hostname_short }}_sensor-{{ hostvars[item].inventory_hostname_short }}_cert.pem\"\n    owner: \"{{ docket_x509_user }}\"\n    group: \"{{ docket_x509_group }}\"\n    mode: \"0644\"\n  loop: \"{{ groups['stenographer'] }}\"\n  when:\n    - inventory_hostname in groups['docket']\n    - docket_cert.results|map(attribute=\"stat.exists\")|select(\"equalto\",false)|list|length\n\n- name: Push stenographer ca certs\n  copy:\n    src: \"{{ ansible_cache }}/{{ hostvars[item].inventory_hostname_short }}_ca_cert.pem\"\n    dest: \"{{ docket_x509_dir }}/{{ hostvars[item].inventory_hostname_short }}_ca_cert.pem\"\n    owner: \"{{ docket_x509_user }}\"\n    group: \"{{ docket_x509_group }}\"\n    mode: \"0644\"\n  loop: \"{{ groups['stenographer'] }}\"\n  when:\n    - inventory_hostname in groups['docket']\n\n- name: Cleanup {{ ansible_cache }} dir\n  file:\n    name: \"{{ ansible_cache }}/\"\n    state: absent\n  changed_when: false\n  run_once: true\n  delegate_to: localhost\n  become: false\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "273b8454a8f041552555d52ceda34f3698ff4f84", "filename": "roles/dhcp/tasks/dhcpconfig.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n  - name: \"generate a random local directory in /tmp\"\n    tempfile:\n      path: '{{ dhcp_config_temp_dir }}'\n      state: directory\n      prefix: dhcp\n    register: dhcp_config_dir\n    notify: dhcp-config cleanup temp\n\n  - name: 'Add Read/Execute permissions on directory'\n    file:\n      path: '{{ dhcp_config_dir.path }}'\n      mode: 0755\n      state: directory\n\n  - name: \"Generate local dhcpd.conf file\"\n    template:\n      src: dhcp_conf.j2\n      dest: '{{ dhcp_config_dir.path }}/{{ dhcp_config_temp_file }}'\n    notify: dhcp-config cleanup temp\n\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "4b7b3bf9e10ce32988818cd2f5fb435158cbc97a", "filename": "roles/docker/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for docker\n- name: remove docker override\n  file:\n    path: /etc/init/docker.override\n    state: absent\n  notify:\n    - restart docker\n  tags:\n    - docker\n\n- name: configure docker graph directory\n  sudo: yes\n  lineinfile:\n    dest: /etc/default/docker\n    state: present\n    regexp: ^DOCKER_OPTS=.*--graph.*\n    line: 'DOCKER_OPTS=\\\"$DOCKER_OPTS --graph={{ docker_graph_dir }}\\\"'\n  notify:\n    - restart docker\n\n- name: configure docker temporary directory\n  sudo: yes\n  lineinfile:\n    dest: /etc/default/docker\n    state: present\n    line: 'DOCKER_TMPDIR=\\\"{{ docker_tmp_dir }}\\\"'\n  notify:\n    - restart docker\n\n- name: configure docker proxy\n  sudo: yes\n  lineinfile:\n    dest: /etc/default/docker\n    state: present\n    line: 'export http_proxy=\\\"{{ http_proxy }}\\\"'\n  when: http_proxy is defined and http_proxy != ''\n\n- name: ensure docker is running (and enable it at boot)\n  service:\n    name: docker\n    state: started\n    enabled: yes\n  tags:\n    - docker\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "0a8a46fb271f0befea18d2b99777b2f01e12eb45", "filename": "roles/osp/packstack-install/tasks/packstack-install.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Run packstack with the proper answer file\"\n  command: \"packstack --answer-file=''{{ answer_file }}''\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "ccaafff244e9483f33fae8f9674f6cf9f7bf6d75", "filename": "roles/1-prep/tasks/raspberry_pi.yml", "repository": "iiab/iiab", "decoded_content": "# Setup specific to the Raspberry Pi\n\n- name: Add a udev rule to transfer hwclock to system clock at dev creation\n  template:\n    src: 92-rtc-i2c.rules\n    dest: /etc/udev/rules.d/92-rtc-i2c.rules\n    owner: root\n    group: root\n    mode: 0644\n  when: rtc_id is defined and rtc_id != \"none\"\n\n# RTC requires a change to the device tree (and reboot)\n- name: Check for needing to enable i2c rtc device in config.txt\n  lineinfile:\n    dest: /boot/config.txt\n    line: \"dtoverlay=i2c-rtc,{{ rtc_id }}=on\"\n    state: present\n  register: rpiconfig\n  when: rtc_id != \"none\"\n\n- name: Add a udev rule to transfer hwclock to system clock at dev creation\n  template:\n    src: 92-rtc-i2c.rules\n    dest: /etc/udev/rules.d/92-rtc-i2c.rules\n    owner: root\n    group: root\n    mode: 0644\n  when: rtc_id != \"none\"\n\n- name: Pre-install packages\n  package:\n    name: \"{{ item }}\"\n    state: latest\n  with_items:\n    - ntp\n\n- name: Increase the swap file size, as kalite pip download fails (debuntu)\n  lineinfile:\n    regexp: \"^CONF_SWAPSIZE\"\n    line: CONF_SWAPSIZE=500\n    dest: /etc/dphys-swapfile\n  when: is_debuntu\n\n- name: Restart the swap service (debuntu)\n  command: /etc/init.d/dphys-swapfile restart\n  when: is_debuntu\n\n- name: Add RPi rootfs resizing service\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: root\n    group: root\n    mode: \"{{ item.mode }}\"\n  with_items:\n    - { src: 'iiab-rpi-max-rootfs.sh', dest: '/usr/sbin/iiab-rpi-max-rootfs.sh', mode: '0755'}\n    - { src: 'iiab-rpi-root-resize.service', dest: '/etc/systemd/system/iiab-rpi-root-resize.service', mode: '0644'}\n\n- name: Enable rootfs resizing service\n  service:\n    name: iiab-rpi-root-resize\n    enabled: yes\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "079b719c0b95d50a1d844cd2ef5db058299004a9", "filename": "roles/ipaserver/vars/CentOS-7.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# defaults file for ipaserver\n# vars/rhel.yml\nipaserver_packages: [ \"ipa-server\", \"libselinux-python\" ]\nipaserver_packages_dns: [ \"ipa-server-dns\" ]\nipaserver_packages_adtrust: [ \"ipa-server-trust-ad\" ]"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a1bad8d0caba92e9669f9e4262b4a045253a1f64", "filename": "roles/dhcp/tasks/dhcp.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: 'Copy the dhcp config file to a temp location for validity checking'\n  copy:\n    src: '{{ dhcp_config_dir.path }}/{{ dhcp_config_temp_file }}'\n    dest: '{{ dhcp_config_temp_loc }}'\n  notify: 'cleanup dhcp tmp file'\n\n- name: ' Check the Validity of dhcp.conf file'\n  command: 'dhcpd -t -cf {{ dhcp_config_temp_loc }}'\n\n- name: 'Copy and activate the dhcp config file'\n  copy:\n    remote_src: true\n    src: '{{ dhcp_config_temp_loc }}'\n    dest: '{{ dhcp_config_dest_file }}'\n  notify: 'reload dhcp'\n\n- name: 'Disable dhcp services'\n  service:\n    name: '{{ item }}'\n    enabled: no\n    state: stopped\n  with_items:\n  - dhcpd\n  when: \n  - dhcp_service_enabled|default(True) == False\n\n- name: 'Enable dhcp services'\n  service:\n    name: '{{ item }}'\n    enabled: yes\n    state: started\n  with_items:\n  - dhcpd\n  when: \n  - dhcp_service_enabled|default(True) \n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9f0e7db352db2bd4a0125b1bf414fa249c8bb00f", "filename": "playbooks/manage-confluence-space/README.md", "repository": "redhat-cop/infra-ansible", "decoded_content": "## Confluence Space Playbook\nThis playbook is used to copy confluence space from one location to another.\n\n### Example\nPlease refer to the [roles](../../roles/manage-confluence-space/README.md) directory for information regarding the variables required to run this playbook.\n\n### Running the playbook\n`$ ansible-playbook -i invetory playbook.yaml`\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "d4a297bbb7f16b3c5ac7e0ccff3110d89846cdb1", "filename": "roles/usb-lib/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "usb_lib_install: True\nusb_lib_enabled: True\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "21c708a88654a181a61ab5acdfb0ff613f37bb0d", "filename": "roles/update-host/tasks/reboot-host.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Reboot the host\"\n  shell: sleep 5 && shutdown -r now \"Ansible Reboot of host\"\n  async: 1\n  poll: 0\n  ignore_errors: true\n  when:\n    - host_updated.changed or force_host_reboot\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "98b4c015ca8e5d3c4c007be669a51fa476783354", "filename": "ops/templates/backup_meta.yml.j2", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "backup_node=\"{{ inventory_hostname }}\"\nreplica_id=\"{{ replica_id | default('') }}\"\nbackup_source=\"{{ backup_source | default('') }}\"\nucp_version=\"{{ detected_ucp_version | default ('') }}\"\ndtr_version=\"{{ detected_dtr_version | default ('') }}\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "80d4198134c033805421ab5aa9dcff89bfb2b0a4", "filename": "ops/playbooks/roles/hpe.haproxy/handlers/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n\n  - name: Rebuild haproxy.cfg\n    delegate_to: \"{{ hpe_haproxy_server }}\"\n    assemble:\n      dest: /etc/haproxy/haproxy.cfg\n      src: /etc/haproxy/fragments\n    notify: Restart HA Proxy\n\n  - name: Restart HA Proxy\n    delegate_to: \"{{ hpe_ha_proxy_server }}\"\n    systemd:\n      name: haproxy\n      state: restarted\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "0adfcd9fb43be4df51aa32629d759bea5a62b9cc", "filename": "roles/weave/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for weave\n- name: include all interfaces.d\n  sudo: yes\n  lineinfile: \"dest=/etc/network/interfaces state=present line='source /etc/network/interfaces.d/*.cfg'\"\n  notify:\n    - Restart networking\n  tags:\n    - weave\n\n- name: Start up docker\n  service: name=docker state=started\n  tags:\n    - weave\n\n- name: configure weave\n  sudo: yes\n  template: src=interfaces.j2 dest=/etc/network/interfaces.d/weave.cfg owner=root group=root mode=0644\n  tags:\n    - weave\n\n- name: bring up weave bridge\n  shell: ifup weave\n  sudo: yes\n  tags:\n    - weave\n\n- name: configure weave bridge for docker\n  sudo: yes\n  lineinfile: \"dest=/etc/default/docker state=present regexp=^DOCKER_OPTS= line='DOCKER_OPTS=\\\"{{ weave_docker_opts }}\\\"'\"\n  notify:\n    - Restart docker\n    - Weave launch\n  tags:\n    - weave\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ed05d61ed431f90281bad7bb6a894f6f3f2d4e68", "filename": "playbooks/aws/openshift-cluster/update.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: no\n  tasks:\n  - include_vars: vars.yml\n  - include_vars: cluster_hosts.yml\n  - add_host:\n      name: \"{{ item }}\"\n      groups: l_oo_all_hosts\n    with_items: \"{{ g_all_hosts }}\"\n\n- hosts: l_oo_all_hosts\n  gather_facts: no\n  tasks:\n  - include_vars: vars.yml\n  - include_vars: cluster_hosts.yml\n\n- name: Update - Populate oo_hosts_to_update group\n  hosts: localhost\n  connection: local\n  become: no\n  gather_facts: no\n  tasks:\n  - name: Update - Evaluate oo_hosts_to_update\n    add_host:\n      name: \"{{ item }}\"\n      groups: oo_hosts_to_update\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    with_items: \"{{ g_all_hosts | default([]) }}\"\n\n- include: ../../common/openshift-cluster/update_repos_and_packages.yml\n\n- include: config.yml\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "845601e6e67c1373029bfbcd712543f96db8086d", "filename": "playbooks/roles/zookeeper/tests/test.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- hosts: localhost\n  remote_user: root\n  roles:\n    - zookeeper"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "e8bdd5adefb7a93f9a585c393fdc13c0f4beebdc", "filename": "roles/hostnames/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Setting Hostname Fact\n  set_fact:\n    new_hostname: \"{{ custom_hostname | default(inventory_hostname_short) }}\"\n\n- name: Setting FQDN Fact\n  set_fact:\n    new_fqdn: \"{{ new_hostname }}.{{ full_dns_domain }}\"\n\n# Ansible 2.5 fixed the hostname module for RHEL 7.5:  https://github.com/ansible/ansible/pull/31839\n- name: Checking if this version of Ansible has a fix for the hostname module\n  set_fact:\n    hostname_module_fixed: \"{{ ansible_version['full'] | version_compare('2.5', '>=') }}\"\n\n# The hostname module does not work on RHEL version 7.5 with Ansible versions < 2.5\n- name: Checking if this version of RHEL is affected by the hostname problem\n  set_fact:\n    rhel75: \"{{ (ansible_distribution == 'RedHat') and (ansible_distribution_version | version_compare('7.5', '>=')) }}\"\n\n- name: Setting hostname and DNS domain\n  hostname: name=\"{{ new_fqdn }}\"\n  # Use the hostname module when not on RHEL 7.5 or on the version of Ansible that fixed the hostname module.\n  when: not rhel75 or hostname_module_fixed\n\n- name: Setting hostname and DNS domain using the command module\n  command: \"hostname {{ new_fqdn }}\"\n  # Use the command module when RHEL 7.5 or later, and when Ansible does not contain the hostname module fix.\n  when: rhel75 and not hostname_module_fixed\n\n- name: Check for cloud.cfg\n  stat: path=/etc/cloud/cloud.cfg\n  register: cloud_cfg\n\n- name: Prevent cloud-init updates of hostname/fqdn (if applicable)\n  lineinfile:\n    dest: /etc/cloud/cloud.cfg\n    state: present\n    regexp: \"{{ item.regexp }}\"\n    line: \"{{ item.line }}\"\n  with_items:\n    - { regexp: '^ - set_hostname', line: '# - set_hostname' }\n    - { regexp: '^ - update_hostname', line: '# - update_hostname' }\n  when: cloud_cfg.stat.exists == True\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "b46c33e48b4bf1480d66d65ed5a009702a5c6eee", "filename": "roles/dns/test/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\ndns_records_rm:\n- view: private\n  zone: first.example.com\n  server: \"192.168.48.26\"\n  key_name: \"private-first.example.com\"\n  key_secret: \"EhZfRtlHgy7xTIi2LeVSGsBj99Sb8IGB6K30ovg13dE=\"\n  key_algorithm: \"hmac-sha256\"\n  entries:\n  - type: A\n    hostname: master\n    ip: 172.16.10.19\ndns_records_add:\n- view: private\n  zone: first.example.com\n  server: \"192.168.48.26\"\n  key_name: \"private-first.example.com\"\n  key_secret: \"EhZfRtlHgy7xTIi2LeVSGsBj99Sb8IGB6K30ovg13dE=\"\n  key_algorithm: \"hmac-sha256\"\n  entries:\n  - type: A\n    hostname: master\n    ip: 172.16.10.20\n  - type: A\n    hostname: node1\n    ip: 172.16.10.20\n  - type: A\n    hostname: node2\n    ip: 172.16.10.21\n  - type: A\n    hostname: node3\n    ip: 172.16.10.22\n- view: private\n  zone: second.example.com\n  server: \"192.168.48.26\"\n  key_name: \"private-second.example.com\"\n  key_secret: \"+UYdpSzdQyZ20V9/2Ud9RjHFz9Pouqn4aXP3V9X/gq4=\"\n  key_algorithm: \"hmac-sha256\"\n  entries:\n  - type: A\n    hostname: master\n    ip: 172.17.10.20\n  - type: A\n    hostname: node1\n    ip: 172.17.10.20\n  - type: A \n    hostname: node2\n    ip: 172.17.10.21\n- view: public\n  zone: first.example.com\n  server: \"192.168.48.26\"\n  key_name: \"public-first.example.com\"\n  key_secret: \"5RZv5wMtKS/fZtjtc2bXS2s6L5+cXN2x53jSkEtwNjk=\"\n  key_algorithm: \"hmac-sha256\"\n  entries:\n  - type: A \n    hostname: master\n    ip: 10.9.77.20\n  - type: A \n    hostname: node1\n    ip: 10.9.77.20\n  - type: A\n    hostname: node2\n    ip: 10.9.77.21\n  - type: A\n    hostname: node3\n    ip: 10.9.77.22\n- view: public\n  zone: second.example.com\n  server: \"192.168.48.26\"\n  key_name: \"public-second.example.com\"\n  key_secret: \"7VKvn5iZ64l+s42XT/hllJSxS6CjE3369tOy85vkBk4=\"\n  key_algorithm: \"hmac-sha256\"\n  entries:\n  - type: A\n    hostname: master\n    ip: 10.8.88.20\n  - type: A\n    hostname: node1\n    ip: 10.8.88.20\n  - type: A\n    hostname: node2\n    ip: 10.8.88.21\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "66ad2aa57bee6315451573ae6dd4b818e5a2c286", "filename": "reference-architecture/gcp/ansible/playbooks/openshift-installer-common-vars.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ndebug_level: 2\nopenshift_debug_level: '{{ debug_level }}'\nopenshift_node_debug_level: '{{ debug_level }}'\nopenshift_master_debug_level: '{{ debug_level }}'\nosm_cluster_network_cidr: 172.16.0.0/16\nosm_default_node_selector: 'role=app'\nosm_default_subdomain: '{{ wildcard_zone }}'\nosm_use_cockpit: true\nopenshift_cloudprovider_kind: gce\nopenshift_master_api_port: '{{ console_port }}'\nopenshift_master_console_port: '{{ console_port }}'\nopenshift_master_cluster_method: native\nopenshift_master_default_subdomain: '{{ osm_default_subdomain }}'\nopenshift_master_access_token_max_seconds: 2419200\nopenshift_hosted_router_selector: 'role=infra'\nopenshift_hosted_registry_selector: 'role=infra'\nopenshift_hosted_registry_storage_kind: object\nopenshift_hosted_registry_storage_provider: gcs\nopenshift_hosted_registry_storage_gcs_bucket: '{{ gcs_registry_bucket }}'\nopenshift_hosted_registry_storage_gcs_rootdirectory: /registry\nopenshift_hosted_registry_pullthrough: true\nopenshift_hosted_registry_acceptschema2: true\nopenshift_hosted_registry_enforcequota: true\nopenshift_hosted_metrics_storage_kind: dynamic\nopenshift_metrics_hawkular_nodeselector: { 'role': 'infra' }\nopenshift_metrics_cassandra_nodeselector: { 'role': 'infra' }\nopenshift_metrics_heapster_nodeselector: { 'role': 'infra' }\nopenshift_disable_check: docker_storage,memory_availability\n"}, {"commit_sha": "3c8d04f3e0875a9baf1f1282f6665b2e7d6871a8", "sha": "f725c8437ee5e8b55df838c109705fa387410ae3", "filename": "tasks/fail2ban-RedHat.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\n- name: Install fail2ban.\n  package: name=fail2ban state=present enablerepo=epel\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "279772037e8567e7431c518d89a33dd9f8eddf43", "filename": "roles/cloud-ec2/tasks/cloudformation.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Deploy the template\n  cloudformation:\n    aws_access_key: \"{{ access_key }}\"\n    aws_secret_key: \"{{ secret_key }}\"\n    stack_name: \"{{ stack_name }}\"\n    state: \"present\"\n    region: \"{{ algo_region }}\"\n    template: roles/cloud-ec2/files/stack.yml\n    template_parameters:\n      InstanceTypeParameter: \"{{ cloud_providers.ec2.size }}\"\n      PublicSSHKeyParameter: \"{{ lookup('file', SSH_keys.public) }}\"\n      ImageIdParameter: \"{{ ami_image }}\"\n      WireGuardPort: \"{{ wireguard_port }}\"\n    tags:\n      Environment: Algo\n  register: stack\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "7c441ec7caa21149806b69cd70618b1ee39f5409", "filename": "ops/playbooks/create_vms.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- name: Deploy Linux VMs\n  hosts: vms\n  gather_facts: false\n  connection: local\n  user: remote\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n    - name: Create VMs\n      vmware_guest:\n        hostname: \"{{ vcenter_hostname }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        validate_certs: \"{{ vcenter_validate_certs }}\"\n        esxi_hostname: \"{{ esxi_host }}\"\n        datacenter: \"{{ datacenter }}\"\n        folder: \"{{ datacenter }}/vm{{ folder_name }}\"\n        name: \"{{ inventory_hostname }}\"\n        template: \"{{ vm_template }}\"\n        state: poweredon\n        networks: \"{{ networks }}\"\n        customization:\n          dns_servers: \"{{ dns }}\"\n          domain: \"{{ domain_name }}\"\n        disk: \"{{ disks_specs }}\"\n            \n        hardware:\n          memory_mb: \"{{ ram }}\"\n          num_cpus: \"{{ cpus }}\"\n      notify: CreateVMsWait\n\n  handlers:\n    - name: CreateVMsWait\n      wait_for_connection:\n        delay: 120\n        timeout: 300\n\n\n#######################################################################\n#\n# play 2: deploy windows workers\n#\n#######################################################################\n- name: Deploy Windows VMs\n  hosts: win_worker\n  gather_facts: false\n  connection: local\n  user: remote\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n#  environment: \"{{ env }}\"\n\n  tasks:\n\n  - name: Create VMs \n    vmware_guest:\n      hostname: \"{{ vcenter_hostname }}\"\n      username: \"{{ vcenter_username }}\"\n      password: \"{{ vcenter_password }}\"\n      validate_certs: \"{{ vcenter_validate_certs }}\"\n      esxi_hostname: \"{{ esxi_host }}\"\n      datacenter: \"{{ datacenter }}\"\n      folder: \"{{ datacenter }}/vm{{ folder_name }}\"\n      name: \"{{ inventory_hostname }}\"\n      template: \"{{ win_vm_template }}\"\n      state: poweredon\n      networks: \"{{ networks }}\"\n      disk: \"{{ disks_specs }}\"\n      customization:\n        autologon: true\n        autologoncount: 1\n        domain: \"{{ domain_name }}\"\n        dns_servers: \"{{ dns }}\"\n        password: \"{{ ansible_password }}\"\n        timezone: \"{{ windows_timezone }}\"\n        runonce: \n        - powershell -NoProfile -ExecutionPolicy Bypass -Command \"iex ((new-object net.webclient).DownloadString('{{ windows_winrm_script }}'))\"\n      hardware:\n        memory_mb: \"{{ ram }}\"\n        num_cpus: \"{{ cpus }}\"\n      wait_for_ip_address: yes\n    vars:\n      ansible_connection: local\n      datastore: \"{{ datastores | random }}\"\n    notify: CreateVMsWait\n\n\n  handlers:\n    - name: CreateVMsWait\n      wait_for_connection:\n        delay: 240\n        sleep: 15\n        timeout: 900\n      vars:\n        ansible_connection: local\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "172c840373d96f55f0d89e1084d8889b3fc45bff", "filename": "tasks/replication/slave/debiancnf.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n- name: FETCH | Get /etc/mysql/debian.cnf on master\n  fetch:\n    src: /etc/mysql/debian.cnf\n    dest: /tmp/{{ mariadb_slave_import_from }}/debian.cnf\n    flat: yes\n  changed_when: false\n  delegate_to: \"{{ mariadb_slave_import_from }}\"\n\n- name: LOCAL_ACTION FILE | Secure fetched file\n  local_action: file path=/tmp/{{ mariadb_slave_import_from }}/debian.cnf mode=0600\n  become: no\n\n- name: COPY | Fetched file to /etc/mysql/debian.cnf\n  copy:\n    src: \"/tmp/{{ mariadb_slave_import_from }}/debian.cnf\"\n    dest: /etc/mysql/debian.cnf\n    owner: root\n    group: root\n    mode: 0600\n  notify: restart mariadb\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4c6fd9823d4bae389d666e42faf19e2cd9ce7ed4", "filename": "roles/config-nagios-target/tasks/nrpe_dns.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Copy in additional Nagios service plugin\n  copy: \n    src: plugins/check_service.sh\n    dest: /usr/lib64/nagios/plugins/check_service.sh\n    owner: root\n    group: root\n    mode: 0755\n\n- name: Copy nrpe.d DNS configuration files\n  copy: \n    src: nrpe.d/check_dns.cfg\n    dest: /etc/nrpe.d/check_dns.cfg\n    owner: root\n    group: root\n    mode: 0644\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "3188a66b8b8f4eeaceaf695e6f5ce384c3f1af1f", "filename": "reference-architecture/aws-ansible/playbooks/roles/ssh-key/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: OSE ec2 key\n  ec2_key:\n    name: \"{{ keypair }}\"\n    region: \"{{ region }}\"\n    key_material: \"{{ item }}\"\n  with_file: \"{{ key_path }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "474c9c80372eac68d4e8b0ff303848ba11dfac09", "filename": "playbooks/provisioning/openstack/provision.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- include: \"prerequisites.yml\"\n\n- include: \"provision-openstack.yml\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f1c0e2f81645325f18de87e4c4a67f91757fefe9", "filename": "roles/config-timezone/test/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\ntimezone: America/Denver\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "48daa4e702a76c98184378471d54e187b4b958d3", "filename": "roles/cdi/defaults/main.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "cdi_namespace: \"{{ cdi_image_namespace | default('golden-images') }}\"\ncdi_storageclass: \"{{ cdi_kubevirt_storageclass | default('kubevirt') }}\"\naction: \"provision\"\nrepo_tag: \"{{ cdi_repo_tag | default('jcoperh') }}\"\nrelease_tag: \"{{ cdi_release_tag | default('latest') }}\"\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "b3a445005238a4e20f25aea7358149d25857e068", "filename": "tasks/checks/distribution-checks-RedHat.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Fail if unsupported RedHat version\n  fail:\n    msg: \"RedHat 7 or later is required!\"\n  when: _docker_os_dist_major_version | int < 7\n\n# https://github.com/haxorof/ansible-role-docker-ce/issues/97\n- name: Fail if Docker SDK/Stack/Compose shall be installed using PiP on RedHat\n  fail:\n    msg: \"This role does not handle packages coming from PiP!\"\n  when: (docker_sdk | bool) or (docker_stack | bool)\n    or (docker_compose | bool and not docker_compose_no_pip | bool)\n"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "c44b4ff8b31823598181df145846b16f1af7c74f", "filename": "tasks/fetch/local.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Artifact stored localy\"\n  set_fact:\n    oracle_artifact: \"{{ transport_local }}\"\n    oracle_artifact_basename: \"{{ transport_local | basename }}\"\n\n- name: \"Getting java version variables\"\n  set_fact:\n    java_package: \"{{ oracle_artifact_basename.split('-')[0] }}\"\n    java_major_version: \"{{ oracle_artifact_basename.split('-')[1].split('u')[0] }}\"\n    java_minor_version: \"{{ oracle_artifact_basename.split('-')[1].split('u')[1] }}\"\n    java_arch: \"{{ oracle_artifact_basename.split('-')[3] }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "82329eac1ed4b6566e3619f204592dc08ae33dd9", "filename": "playbooks/openstack/openshift-cluster/files/heat_stack.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "heat_template_version: 2014-10-16\n\ndescription: OpenShift cluster\n\nparameters:\n\n  cluster_env:\n    type: string\n    label: Cluster environment\n    description: Environment of the cluster\n\n  cluster_id:\n    type: string\n    label: Cluster ID\n    description: Identifier of the cluster\n\n  subnet_24_prefix:\n    type: string\n    label: subnet /24 prefix\n    description: /24 subnet prefix of the network of the cluster (dot separated number triplet)\n\n  dns_nameservers:\n    type: comma_delimited_list\n    label: DNS nameservers list\n    description: List of DNS nameservers\n\n  external_net:\n    type: string\n    label: External network\n    description: Name of the external network\n    default: external\n\n  ssh_public_key:\n    type: string\n    label: SSH public key\n    description: SSH public key\n    hidden: true\n\n  ssh_incoming:\n    type: string\n    label: Source of ssh connections\n    description: Source of legitimate ssh connections\n    default: 0.0.0.0/0\n\n  node_port_incoming:\n    type: string\n    label: Source of node port connections\n    description: Authorized sources targeting node ports\n    default: 0.0.0.0/0\n\n  num_etcd:\n    type: number\n    label: Number of etcd nodes\n    description: Number of etcd nodes\n\n  num_masters:\n    type: number\n    label: Number of masters\n    description: Number of masters\n\n  num_nodes:\n    type: number\n    label: Number of compute nodes\n    description: Number of compute nodes\n\n  num_infra:\n    type: number\n    label: Number of infrastructure nodes\n    description: Number of infrastructure nodes\n\n  etcd_image:\n    type: string\n    label: Etcd image\n    description: Name of the image for the etcd servers\n\n  master_image:\n    type: string\n    label: Master image\n    description: Name of the image for the master servers\n\n  node_image:\n    type: string\n    label: Node image\n    description: Name of the image for the compute node servers\n\n  infra_image:\n    type: string\n    label: Infra image\n    description: Name of the image for the infra node servers\n\n  etcd_flavor:\n    type: string\n    label: Etcd flavor\n    description: Flavor of the etcd servers\n\n  master_flavor:\n    type: string\n    label: Master flavor\n    description: Flavor of the master servers\n\n  node_flavor:\n    type: string\n    label: Node flavor\n    description: Flavor of the compute node servers\n\n  infra_flavor:\n    type: string\n    label: Infra flavor\n    description: Flavor of the infra node servers\n\noutputs:\n\n  etcd_names:\n    description: Name of the etcds\n    value: { get_attr: [ etcd, name ] }\n\n  etcd_ips:\n    description: IPs of the etcds\n    value: { get_attr: [ etcd, private_ip ] }\n\n  etcd_floating_ips:\n    description: Floating IPs of the etcds\n    value: { get_attr: [ etcd, floating_ip ] }\n\n  master_names:\n    description: Name of the masters\n    value: { get_attr: [ masters, name ] }\n\n  master_ips:\n    description: IPs of the masters\n    value: { get_attr: [ masters, private_ip ] }\n\n  master_floating_ips:\n    description: Floating IPs of the masters\n    value: { get_attr: [ masters, floating_ip ] }\n\n  node_names:\n    description: Name of the nodes\n    value: { get_attr: [ compute_nodes, name ] }\n\n  node_ips:\n    description: IPs of the nodes\n    value: { get_attr: [ compute_nodes, private_ip ] }\n\n  node_floating_ips:\n    description: Floating IPs of the nodes\n    value: { get_attr: [ compute_nodes, floating_ip ] }\n\n  infra_names:\n    description: Name of the nodes\n    value: { get_attr: [ infra_nodes, name ] }\n\n  infra_ips:\n    description: IPs of the nodes\n    value: { get_attr: [ infra_nodes, private_ip ] }\n\n  infra_floating_ips:\n    description: Floating IPs of the nodes\n    value: { get_attr: [ infra_nodes, floating_ip ] }\n\nresources:\n\n  net:\n    type: OS::Neutron::Net\n    properties:\n      name:\n        str_replace:\n          template: openshift-ansible-cluster_id-net\n          params:\n            cluster_id: { get_param: cluster_id }\n\n  subnet:\n    type: OS::Neutron::Subnet\n    properties:\n      name:\n        str_replace:\n          template: openshift-ansible-cluster_id-subnet\n          params:\n            cluster_id: { get_param: cluster_id }\n      network: { get_resource: net }\n      cidr:\n        str_replace:\n          template: subnet_24_prefix.0/24\n          params:\n            subnet_24_prefix: { get_param: subnet_24_prefix }\n      dns_nameservers: { get_param: dns_nameservers }\n\n  router:\n    type: OS::Neutron::Router\n    properties:\n      name:\n        str_replace:\n          template: openshift-ansible-cluster_id-router\n          params:\n            cluster_id: { get_param: cluster_id }\n      external_gateway_info:\n        network: { get_param: external_net }\n\n  interface:\n    type: OS::Neutron::RouterInterface\n    properties:\n      router_id: { get_resource: router }\n      subnet_id: { get_resource: subnet }\n\n  keypair:\n    type: OS::Nova::KeyPair\n    properties:\n      name:\n        str_replace:\n          template: openshift-ansible-cluster_id-keypair\n          params:\n            cluster_id: { get_param: cluster_id }\n      public_key: { get_param: ssh_public_key }\n\n  master-secgrp:\n    type: OS::Neutron::SecurityGroup\n    properties:\n      name:\n        str_replace:\n          template: openshift-ansible-cluster_id-master-secgrp\n          params:\n            cluster_id: { get_param: cluster_id }\n      description:\n        str_replace:\n          template: Security group for cluster_id OpenShift cluster master\n          params:\n            cluster_id: { get_param: cluster_id }\n      rules:\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 22\n          port_range_max: 22\n          remote_ip_prefix: { get_param: ssh_incoming }\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 4001\n          port_range_max: 4001\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 8443\n          port_range_max: 8443\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 8444\n          port_range_max: 8444\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 53\n          port_range_max: 53\n        - direction: ingress\n          protocol: udp\n          port_range_min: 53\n          port_range_max: 53\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 8053\n          port_range_max: 8053\n        - direction: ingress\n          protocol: udp\n          port_range_min: 8053\n          port_range_max: 8053\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 24224\n          port_range_max: 24224\n        - direction: ingress\n          protocol: udp\n          port_range_min: 24224\n          port_range_max: 24224\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 2224\n          port_range_max: 2224\n        - direction: ingress\n          protocol: udp\n          port_range_min: 5404\n          port_range_max: 5404\n        - direction: ingress\n          protocol: udp\n          port_range_min: 5405\n          port_range_max: 5405\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 9090\n          port_range_max: 9090\n\n  etcd-secgrp:\n    type: OS::Neutron::SecurityGroup\n    properties:\n      name:\n        str_replace:\n          template: openshift-ansible-cluster_id-etcd-secgrp\n          params:\n            cluster_id: { get_param: cluster_id }\n      description:\n        str_replace:\n          template: Security group for cluster_id etcd cluster\n          params:\n            cluster_id: { get_param: cluster_id }\n      rules:\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 22\n          port_range_max: 22\n          remote_ip_prefix: { get_param: ssh_incoming }\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 2379\n          port_range_max: 2379\n          remote_mode: remote_group_id\n          remote_group_id: { get_resource: master-secgrp }\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 2380\n          port_range_max: 2380\n          remote_mode: remote_group_id\n\n  node-secgrp:\n    type: OS::Neutron::SecurityGroup\n    properties:\n      name:\n        str_replace:\n          template: openshift-ansible-cluster_id-node-secgrp\n          params:\n            cluster_id: { get_param: cluster_id }\n      description:\n        str_replace:\n          template: Security group for cluster_id OpenShift cluster nodes\n          params:\n            cluster_id: { get_param: cluster_id }\n      rules:\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 22\n          port_range_max: 22\n          remote_ip_prefix: { get_param: ssh_incoming }\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 10250\n          port_range_max: 10250\n          remote_mode: remote_group_id\n        - direction: ingress\n          protocol: udp\n          port_range_min: 4789\n          port_range_max: 4789\n          remote_mode: remote_group_id\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 30000\n          port_range_max: 32767\n          remote_ip_prefix: { get_param: node_port_incoming }\n\n  infra-secgrp:\n    type: OS::Neutron::SecurityGroup\n    properties:\n      name:\n        str_replace:\n          template: openshift-ansible-cluster_id-infra-secgrp\n          params:\n            cluster_id: { get_param: cluster_id }\n      description:\n        str_replace:\n          template: Security group for cluster_id OpenShift infrastructure cluster nodes\n          params:\n            cluster_id: { get_param: cluster_id }\n      rules:\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 80\n          port_range_max: 80\n        - direction: ingress\n          protocol: tcp\n          port_range_min: 443\n          port_range_max: 443\n\n  etcd:\n    type: OS::Heat::ResourceGroup\n    properties:\n      count: { get_param: num_etcd }\n      resource_def:\n        type: heat_stack_server.yaml\n        properties:\n          name:\n            str_replace:\n              template: cluster_id-k8s_type-%index%\n              params:\n                cluster_id: { get_param: cluster_id }\n                k8s_type: etcd\n          cluster_env: { get_param: cluster_env }\n          cluster_id:  { get_param: cluster_id }\n          type:        etcd\n          image:       { get_param: etcd_image }\n          flavor:      { get_param: etcd_flavor }\n          key_name:    { get_resource: keypair }\n          net:         { get_resource: net }\n          subnet:      { get_resource: subnet }\n          secgrp:\n            - { get_resource: etcd-secgrp }\n          floating_network: { get_param: external_net }\n          net_name:\n            str_replace:\n              template: openshift-ansible-cluster_id-net\n              params:\n                cluster_id: { get_param: cluster_id }\n    depends_on:\n      - interface\n\n  masters:\n    type: OS::Heat::ResourceGroup\n    properties:\n      count: { get_param: num_masters }\n      resource_def:\n        type: heat_stack_server.yaml\n        properties:\n          name:\n            str_replace:\n              template: cluster_id-k8s_type-%index%\n              params:\n                cluster_id: { get_param: cluster_id }\n                k8s_type: master\n          cluster_env: { get_param: cluster_env }\n          cluster_id:  { get_param: cluster_id }\n          type:        master\n          image:       { get_param: master_image }\n          flavor:      { get_param: master_flavor }\n          key_name:    { get_resource: keypair }\n          net:         { get_resource: net }\n          subnet:      { get_resource: subnet }\n          secgrp:\n            - { get_resource: master-secgrp }\n            - { get_resource: node-secgrp }\n          floating_network: { get_param: external_net }\n          net_name:\n            str_replace:\n              template: openshift-ansible-cluster_id-net\n              params:\n                cluster_id: { get_param: cluster_id }\n    depends_on:\n      - interface\n\n  compute_nodes:\n    type: OS::Heat::ResourceGroup\n    properties:\n      count: { get_param: num_nodes }\n      resource_def:\n        type: heat_stack_server.yaml\n        properties:\n          name:\n            str_replace:\n              template: cluster_id-k8s_type-sub_host_type-%index%\n              params:\n                cluster_id: { get_param: cluster_id }\n                k8s_type: node\n                sub_host_type: compute\n          cluster_env: { get_param: cluster_env }\n          cluster_id:  { get_param: cluster_id }\n          type:        node\n          subtype:     compute\n          image:       { get_param: node_image }\n          flavor:      { get_param: node_flavor }\n          key_name:    { get_resource: keypair }\n          net:         { get_resource: net }\n          subnet:      { get_resource: subnet }\n          secgrp:\n            - { get_resource: node-secgrp }\n          floating_network: { get_param: external_net }\n          net_name:\n            str_replace:\n              template: openshift-ansible-cluster_id-net\n              params:\n                cluster_id: { get_param: cluster_id }\n    depends_on:\n      - interface\n\n  infra_nodes:\n    type: OS::Heat::ResourceGroup\n    properties:\n      count: { get_param: num_infra }\n      resource_def:\n        type: heat_stack_server.yaml\n        properties:\n          name:\n            str_replace:\n              template: cluster_id-k8s_type-sub_host_type-%index%\n              params:\n                cluster_id: { get_param: cluster_id }\n                k8s_type: node\n                sub_host_type: infra\n          cluster_env: { get_param: cluster_env }\n          cluster_id:  { get_param: cluster_id }\n          type:        node\n          subtype:     infra\n          image:       { get_param: infra_image }\n          flavor:      { get_param: infra_flavor }\n          key_name:    { get_resource: keypair }\n          net:         { get_resource: net }\n          subnet:      { get_resource: subnet }\n          secgrp:\n            - { get_resource: node-secgrp }\n            - { get_resource: infra-secgrp }\n          floating_network: { get_param: external_net }\n          net_name:\n            str_replace:\n              template: openshift-ansible-cluster_id-net\n              params:\n                cluster_id: { get_param: cluster_id }\n    depends_on:\n      - interface\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "bd0882e314f12ec0e4b0e68f5ea08c265c76ea42", "filename": "tasks/remove-pre-docker-ce.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Reset fact related to removal of old Docker\n  set_fact:\n    _remove_old_docker: false\n\n- name: Determine Docker version\n  shell: \"docker version --format='{{ '{{' }} .Client.Version {{ '}}' }}' 2>/dev/null\"\n  register: _cmd_docker_version\n  changed_when: false\n  failed_when: false\n  check_mode: no\n\n- name: Set fact if old Docker installation shall be removed\n  set_fact:\n    _remove_old_docker: true\n  when:\n    - _cmd_docker_version.stdout_lines is defined\n    - _cmd_docker_version.stdout_lines[0] is defined\n    - _cmd_docker_version.stdout_lines[0] | int is version(17, '<')\n\n- name: Check if Docker is running\n  become: true\n  systemd:\n    name: docker\n  ignore_errors: yes\n  register: _service_docker_status\n  check_mode: no\n  when: _remove_old_docker | bool\n\n- name: Stop Docker service\n  service:\n    name: docker\n    state: stopped\n  when: \"_service_docker_status.rc | default(1) == 0\"\n\n- name: Remove old Docker installation before Docker CE\n  become: true\n  package:\n    name: \"{{ item }}\"\n    state: absent\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when: _remove_old_docker | bool\n  loop: \"{{ docker_old_packages[_docker_os_dist] }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "44584c266c43526ca41c9944dec3064a1a3087b9", "filename": "roles/config-iscsi-client/tasks/multipath-config.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Configure MultiPath configuration\"\n  blockinfile: \n    path: /etc/multipath.conf\n    block: |\n      blacklist {\n        devnode \"{{ multipath_blacklist_node | default('^sda') }}\"\n      }\n    create: yes\n    owner: root\n    group: root\n    mode: 0600\n    state: present\n  register: multipathconf\n\n- name: 'restart multipathd'\n  service:\n    name: multipathd\n    state: restarted\n  when: \n  - multipathconf.changed \n\n\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b19fe708a5628615d1e401d786258db80a759188", "filename": "roles/user-management/manage-local-user-password/test/playbook.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Add a test user\"\n  hosts: all\n  tasks:\n  - user:\n      name: \"{{ user_name }}\"\n      comment: \"Test User\"\n    when:\n      - user_name != \"root\"\n\n# Test the role to update the password\n- name: \"Update {{ user_name }} password\"\n  hosts: all\n  roles:\n    - role: manage-local-user-password\n\n# Test the password access by running a remote command on machine\n#\n\n- name: \"Test password update\" \n  hosts: all\n  tasks:\n\n  - name: \"Test password for {{ user_name }} on {{ ansible_host }}\"\n    raw: \"sshpass -p \\\"{{ clear_text_password }}\\\" ssh {{ user_name }}@{{ansible_host }} /bin/true\"\n    delegate_to: localhost\n    become: False\n    register: result\n    changed_when: False\n    failed_when:\n      result.rc != 0\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "73e6783f0cc12d9fde13440d3b9fd7863353d239", "filename": "roles/common/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- block:\n  - name: Check the system\n    raw: uname -a\n    register: OS\n\n  - include_tasks: ubuntu.yml\n    when: '\"Ubuntu\" in OS.stdout or \"Linux\" in OS.stdout'\n\n  - include_tasks: freebsd.yml\n    when: '\"FreeBSD\" in OS.stdout'\n\n  - name: Gather additional facts\n    import_tasks: facts.yml\n\n  - name: Sysctl tuning\n    sysctl: name=\"{{ item.item }}\" value=\"{{ item.value }}\"\n    with_items:\n      - \"{{ sysctl|default([]) }}\"\n    tags:\n      - always\n\n  - meta: flush_handlers\n  rescue:\n    - debug: var=fail_hint\n      tags: always\n    - fail:\n      tags: always\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "591f440b12e7b40a59f3ff92e8eb1e91751160a0", "filename": "roles/config-hostname/tasks/prep.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - libselinux-python\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9a2249d35a7a3a140022248d423390f4a26f6122", "filename": "roles/idm-host-cert/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- hosts: localhost\n  vars:\n    idm_fqdn: \"idm.example.com\"\n    idm_user: \"admin\"\n    idm_password: \"admin!\"\n    csr_country: \"US\"\n    csr_state: \"North Carolina\"\n    csr_location: \"Raleigh\"\n    csr_org_name: \"Red Hat, Inc.\"\n    csr_org_unit: \"Open Innovation Labs\"\n    csr_email: \"myemail@example.com\"\n    host_name: \"host-1.example.com\"\n    host_realm: \"EXAMPLE.COM\"\n    host_description: \"Testing My Host Cert\"\n    target_host_cert_file: \"/tmp/{{ host_name }}.pem\"\n    target_host_key_file: \"/tmp/{{ host_name }}.key\"\n    target_ca_cert_file: \"/tmp/ca.pem\"\n  roles:\n  - idm-host-cert\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "ca9811c574368b7c6421651fe5efc861ce97ecda", "filename": "roles/ovirt-engine-remote-db/meta/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\ngalaxy_info:\n  author: \"Petr Kubica\"\n  description: \"oVirt DB installer\"\n  company: \"Red Hat\"\n  license: \"GPLv3\"\n  min_ansible_version: 1.9\n  platforms:\n  - name: EL\n    versions:\n    - all\n  galaxy_tags:\n    - installer\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "f367a8610bfd71f21c746ba286eeb9d8666d96e6", "filename": "playbooks/roles/docket/tests/inventory", "repository": "rocknsm/rock", "decoded_content": "docket01    ansible_connection=local\n\n[docket]\ndocket01\n\n[stenographer]\ndocket01\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "fd4d18a1fd3ae8561ae148921e42c5ab8874284b", "filename": "playbooks/openshift/pre-tasks.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: Generate Environment ID\n  set_fact:\n    env_random_id: \"{{ ansible_date_time.epoch }}\"\n  run_once: true\n  delegate_to: localhost\n\n- name: Set default Environment ID\n  set_fact:\n    default_env_id: \"casl-{{ lookup('env','OS_USERNAME') }}-{{ env_random_id }}\"\n  delegate_to: localhost\n\n- name: Setting Common Facts\n  set_fact:\n    env_id: \"{{ env_id | default(default_env_id) }}\"\n  delegate_to: localhost\n\n- name: Set Dynamic Inventory Filters\n  shell: >\n    export OS_INV_FILTER_KEY=clusterid && OS_INV_FILTER_VALUE={{ env_id }}\n  delegate_to: localhost\n\n- name: Updating DNS configurations\n  set_fact:\n    full_dns_domain: \"{{ (env_id|trim == '') | ternary(dns_domain, env_id + '.' + dns_domain) }}\"\n    openshift_app_domain: \"{{ openshift_app_domain | default('apps') }}\"\n    public_dns_domain: \"{{ dns_domain }}\"\n  delegate_to: localhost\n\n- name: Set the default app domain for routing purposes\n  set_fact:\n    openshift_master_default_subdomain: \"{{ openshift_app_domain }}.{{ full_dns_domain }}\"\n  delegate_to: localhost\n  when:\n  - openshift_master_default_subdomain is undefined\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "cc01b58bff011a64745b7f2a210a5a3fa0045764", "filename": "roles/manage-jira/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- include_tasks: prepare_vars.yml \n\n- include_tasks: create_project_category.yml\n \n- include_tasks: create_permission_scheme.yml\n\n- include_tasks: create_project.yml\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "fa8e9025b23b8aa63ca6c3daa9c133accc7856a6", "filename": "roles/bro/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# handlers file for Bro\n\n- name: Configure monitor interfaces\n  shell: >\n    for intf in {{ rock_monifs | join(' ') }}; do\n      /sbin/ifup ${intf};\n    done\n\n- name: Reload bro\n  service:\n    name: bro\n    state: \"{{ 'started' if rock_services | selectattr('name', 'equalto', 'bro') | map(attribute='enabled') | bool else 'stopped' }}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "a66a13a838611cc92091f10156b956da8db70417", "filename": "roles/phpmyadmin/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Download phpMyAdmin software\n  get_url:\n    url: \"{{ iiab_download_url }}/{{ phpmyadmin_name_zip }}\"\n    dest: \"{{ downloads_dir }}\"\n    timeout: \"{{ download_timeout }}\"\n  #register: phpmyadmin_dl_output\n  when: internet_available\n\n- name: Check if /opt/iiab/downloads/{{ phpmyadmin_name_zip }} exists\n  stat:\n    path: \"{{ downloads_dir }}/{{ phpmyadmin_name_zip }}\"\n  register: phpmyadmin_dl\n\n- name: FAIL (force Ansible to exit) IF /opt/iiab/downloads/{{ phpmyadmin_name_zip }} doesn't exist\n  fail:\n    msg: \"{{ downloads_dir }}/{{ phpmyadmin_name_zip }} is REQUIRED in order to install phpMyAdmin.\"\n  when: not phpmyadmin_dl.stat.exists\n\n- name: Unzip to permanent location /opt/{{ phpmyadmin_name }}\n  unarchive:\n    src: \"{{ downloads_dir }}/{{ phpmyadmin_name_zip }}\"\n    dest: /opt\n    owner: \"{{ apache_user }}\"\n\n- name: Create symbolic link /opt/phpmyadmin to phpMyAdmin folder above\n  file:\n    src: \"{{ phpmyadmin_name }}\"\n    dest: /opt/phpmyadmin\n    owner: \"{{ apache_user }}\"\n    state: link\n\n- name: Copy phpMyAdmin's config file into place\n  template:\n    src: config.inc.php\n    dest: /opt/phpmyadmin/config.inc.php\n    owner: \"{{ apache_user }}\"\n\n# Above 3 stanzas set link/tree/contents ownership to {{ apache_user }}:root\n# OOPS: CHOWN BELOW CHANGED LINK ALONE (TREE/CONTENTS REMAINED root:root)\n\n# - name: Change the owner of the PHP tree to Apache\n#   shell: \"chown -R {{ apache_user }} /opt/phpmyadmin\"\n#   #file:\n#   #  path: \"/opt/{{ phpmyadmin_name_zip }}\"\n#   #  owner: \"{{ apache_user }}\"\n#   #  recurse: yes\n#   #  state: directory\n\n- name: Put the alias into Apache config when enabled\n  template:\n    src: phpmyadmin.j2\n    dest: \"/etc/{{ apache_config_dir }}/phpmyadmin.conf\"\n  when: phpmyadmin_enabled\n\n- name: Enable phpMyAdmin\n  file:\n    src: /etc/apache2/sites-available/phpmyadmin.conf\n    dest: /etc/apache2/sites-enabled/phpmyadmin.conf\n    state: link\n  when: phpmyadmin_enabled and is_debuntu\n\n- name: Remove the alias into Apache config when not enabled\n  file:\n    path: /etc/apache2/sites-enabled/phpmyadmin.conf\n    state: absent\n  when: not phpmyadmin_enabled and is_debuntu\n\n- name: Add 'phpmyadmin' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: phpmyadmin\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: phpMyAdmin\n    - option: description\n      value: '\"phpMyAdmin is an interface with a MySQL database written in PHP, and available to administer the database engine locally or across the network.\"'\n    - option: path\n      value: /opt/phpmyadmin\n    - option: enabled\n      value: \"{{ phpmyadmin_enabled }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4104060bd8683bb09bab354be131ac92334938f1", "filename": "playbooks/provision-dns-server/subscribe-rhel.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Subscribe RHEL based instances\" \n  vars:\n    rhsm_username: \"{{ hostvars['localhost'].rhsm_username|default(omit) }}\"\n    rhsm_password: \"{{ hostvars['localhost'].rhsm_password|default(omit) }}\"\n  include_role: \n    name: rhsm\n\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "a31941bae05205dc3d3f948fe23a3514ba8c7c64", "filename": "roles/proxy/handlers/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: restart privoxy\n  service: name=privoxy state=restarted\n\n- name: daemon-reload\n  shell: systemctl daemon-reload\n\n- name: restart apparmor\n  service: name=apparmor state=restarted\n\n- name: restart apache2\n  service: name=apache2 state=restarted\n\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "731f59bf659097dcdf3c23325abec0a73b1e7f4b", "filename": "archive/roles/cicd-common/defaults/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\ndefault_cicd_storage_disk_volume: \"/dev/vdb\"\n\ndefault_cicd_openstack_security_groups: \"CI-CD\"\ndefault_cicd_openstack_flavor_name: \"m1.medium\"\ndefault_cicd_openstack_image_name: \"ose3_1-base\"\ndefault_cicd_openstack_storage_size: 10\ndefault_cicd_instance_count: 1"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "605ed3ae217ccf581c9460b0fa21872b9ec01be6", "filename": "reference-architecture/aws-ansible/playbooks/roles/host-up/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: check to see if host is available\n  wait_for:\n    port: 22\n    host: \"bastion.{{ public_hosted_zone }}\"\n    state: started\n    delay: \"{{ host_up_time }}\"\n  when: byo_bastion == \"no\"\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "b2f769b9cc2c430c4a67875f56dc3e57c21e6b97", "filename": "roles/common/tasks/deploy.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# deploy.yml - Common tasks for ROCK\n- import_tasks: configure.yml\n- import_tasks: configure-time.yml\n- import_tasks: configure-pipelining.yml\n...\n"}, {"commit_sha": "6d10af54bdbf8e81c3d90a70ffea87b4d2c20eb2", "sha": "77eaddcc228fe8d7d5ae366a4ed216e7bbc740c0", "filename": "tasks/wp-cli.yml", "repository": "Oefenweb/ansible-wordpress", "decoded_content": "# tasks file for wordpress, wp-cli\n---\n- name: install (wp-cli)\n  get_url:\n    url: https://raw.githubusercontent.com/wp-cli/builds/gh-pages/phar/wp-cli.phar\n    dest: \"{{ wordpress_wp_cli_install_dir }}/wp-cli\"\n    force: true\n    owner: root\n    group: root\n    mode: 0755\n  tags: [configuration, wordpress, wordpress-wp-cli, wordpress-wp-cli-install]\n\n- name: check (wp-cli)\n  command: \"wp-cli --allow-root --no-color --info\"\n  register: check_info\n  failed_when: \"'WP-CLI' not in check_info.stdout\"\n  changed_when: False\n  tags: [configuration, wordpress, wordpress-wp-cli, wordpress-wp-cli-check]\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "20e1017dc4c3dc6be1a6780f70739f78a9c286a0", "filename": "tasks/Linux/install/tarball.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Mkdir for java installation\n  file:\n    path: '{{ java_path }}/{{ java_folder }}'\n    state: directory\n    owner: root\n    group: root\n    mode: 0755\n\n- name: 'Install java {{ java_full_version }}'\n  unarchive:\n    src: '{{ java_artifact }}'\n    dest: '{{ java_path }}/{{ java_folder }}'\n    remote_src: true\n    owner: root\n    group: root\n    mode: 0755\n    extra_opts: [--strip-components=1]\n    creates: '{{ java_path }}/{{ java_folder }}/bin/'\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "f3816a6f8a74d0136d38c0eaf9a58de16596071c", "filename": "archive/roles/openstack-create/pre_tasks/pre_tasks.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: \"Validate OpenStack SSH key is defined\"\n  fail: msg=\"Required 'openstack_key_name' is not defined!\"\n  when: openstack_key_name is undefined or openstack_key_name is none or openstack_key_name|trim == ''\n\n- name: \"Verify connectivity to OpenStack\"\n  command: \"nova credentials\"\n  register: nova_result\n  failed_when: nova_result.rc != 0\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "85511d4728aea432701e808fe3f69ec4e4f348bb", "filename": "tasks/nexus_install.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- name: Download nexus_package\n  get_url:\n    url: \"http://download.sonatype.com/nexus/3/{{ nexus_package }}\"\n    dest: \"{{ nexus_download_dir }}/{{ nexus_package }}\"\n    force: no\n  notify:\n    - nexus-service-stop\n\n- name: Ensure Nexus o/s group exists\n  group:\n    name: \"{{ nexus_os_group }}\"\n    state: present\n\n- name: Ensure Nexus o/s user exists\n  user:\n    name: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n    shell: \"/bin/bash\"\n    state: present\n\n- name: Ensure Nexus installation directory exists\n  file:\n    path: \"{{ nexus_installation_dir }}\"\n    state: \"directory\"\n\n- name: Unpack Nexus download\n  unarchive:\n    src: \"{{ nexus_download_dir }}/{{ nexus_package }}\"\n    dest: \"{{ nexus_installation_dir }}\"\n    creates: \"{{ nexus_installation_dir }}/nexus-{{ nexus_version }}\"\n    force: no\n    copy: false\n  notify:\n    - nexus-service-stop\n\n- meta: flush_handlers\n\n- name: Update symlink nexus-latest\n  file:\n    path: \"{{ nexus_installation_dir }}/nexus-latest\"\n    src: \"{{ nexus_installation_dir }}/nexus-{{ nexus_version }}\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n    state: link\n  register: nexus_latest_version\n\n- name: Delete unpacked data directory\n  file:\n    path: \"{{ nexus_installation_dir }}/nexus-latest/data\"\n    state: absent\n\n- name: Get path to default settings\n  set_fact:\n    nexus_default_settings_file: \"{{ nexus_installation_dir }}/nexus-latest/etc/org.sonatype.nexus.cfg\"\n  when: nexus_version < '3.1.0'\n\n- name: Get path to default settings\n  set_fact:\n    nexus_default_settings_file: \"{{ nexus_installation_dir }}/nexus-latest/etc/nexus-default.properties\"\n  when: nexus_version >= '3.1.0'\n\n- name: Get application settings directories\n  set_fact:\n    nexus_app_dir_settings_dirs:\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc\"\n  when: nexus_version < '3.1.0'\n\n- name: Get application settings directories\n  set_fact:\n    nexus_app_dir_settings_dirs:\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/karaf\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/jetty\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/fabric\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/logback\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/scripts\"\n  when: nexus_version >= '3.1.0'\n\n- name: Allow nexus to create first-time install configuration files in  {{ nexus_installation_dir }}/nexus-latest/etc\n  file:\n    path: \"{{ item }}\"\n    state: \"directory\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n    mode: \"0755\"\n    recurse: false\n  with_items: \"{{ nexus_app_dir_settings_dirs }}\"\n  when: nexus_latest_version.changed\n  register: chown_config_first_time\n  tags:\n    # hard to run as a handler for time being\n    - skip_ansible_lint\n\n- name: Create Nexus data directory\n  file:\n    path: \"{{ nexus_data_dir }}\"\n    state: \"directory\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n\n- name: Setup Nexus data directory\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Dkaraf.data=.*\"\n    line: \"-Dkaraf.data={{ nexus_data_dir }}\"\n\n- name: Setup JVM logfile directory\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-XX:LogFile=.*\"\n    line: \"-XX:LogFile={{ nexus_data_dir }}/log/jvm.log\"\n\n- name: Setup Nexus default timezone\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Duser.timezone=.*\"\n    line: \"-Duser.timezone={{ nexus_timezone }}\"\n\n- name: Create Nexus tmp/backup directory\n  file:\n    path: \"{{ item }}\"\n    state: \"directory\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n  with_items:\n    - \"{{ nexus_tmp_dir }}\"\n    - \"{{ nexus_backup_dir }}\"\n\n- name: Setup Nexus tmp directory\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Djava.io.tmpdir=.*\"\n    line: \"-Djava.io.tmpdir={{ nexus_tmp_dir }}\"\n\n- name: Set NEXUS_HOME for the service user\n  lineinfile:\n    dest: \"/home/{{ nexus_os_user }}/.bashrc\"\n    regexp: \"^export NEXUS_HOME=.*\"\n    line: \"export NEXUS_HOME={{ nexus_installation_dir }}/nexus-latest\"\n\n- name: Set nexus user\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.rc\"\n    regexp: \".*run_as_user=.*\"\n    line: \"run_as_user=\\\"{{ nexus_os_user }}\\\"\"\n\n- name: Set nexus port\n  lineinfile:\n    dest: \"{{ nexus_default_settings_file }}\"\n    regexp: \"^application-port=.*\"\n    line: \"application-port={{ nexus_default_port }}\"\n\n- name: Set nexus context path\n  lineinfile:\n    dest: \"{{ nexus_default_settings_file }}\"\n    regexp: \"^nexus-context-path=.*\"\n    line: \"nexus-context-path={{ nexus_default_context_path }}\"\n\n- name: Bind nexus service to 127.0.0.1 only\n  lineinfile:\n    dest: \"{{ nexus_default_settings_file }}\"\n    regexp: \"^application-host=.*\"\n    line: \"application-host=127.0.0.1\"\n  when: httpd_setup_enable\n\n- name: Create systemd service configuration\n  template:\n    src: \"nexus.service\"\n    dest: \"/etc/systemd/system\"\n  notify:\n    - systemd-reload\n\n- block:\n    - name: \"Deploy backup restore script\"\n      template:\n        src: \"nexus-blob-restore.sh.j2\"\n        dest: \"{{ nexus_script_dir }}/nexus-blob-restore.sh\"\n        mode: 0755\n    - name: \"Symlink backup restore script to /sbin\"\n      file:\n        src: \"{{ nexus_script_dir }}/nexus-blob-restore.sh\"\n        dest: \"/sbin/nexus-blob-restore.sh\"\n        state: link\n  when: nexus_backup_configure | bool\n\n- name: 'Check if data directory is empty (first-time install)'\n  command: \"ls {{ nexus_data_dir }}\"\n  register: nexus_data_dir_contents\n  check_mode: no\n  changed_when: false\n\n- name: Clean cache for upgrade process\n  file:\n    path: \"{{ nexus_data_dir }}/clean_cache\"\n    state: touch\n  when: nexus_latest_version.changed and nexus_data_dir_contents.stdout != \"\"\n  tags:\n    # hard to run as a handler for time being\n    - skip_ansible_lint\n\n- meta: flush_handlers\n\n- name: Enable nexus service and make sure it is started\n  systemd:\n    name: nexus.service\n    enabled: yes\n    state: started\n  notify:\n    - wait-for-nexus\n    - wait-for-nexus-port\n\n- meta: flush_handlers\n\n- name: Chown configuration files from {{ nexus_installation_dir }}/nexus-latest/etc back to root\n  file:\n    path: \"{{ nexus_installation_dir }}/nexus-latest/etc\"\n    owner: \"root\"\n    group: \"root\"\n    mode: a=rX,u+w\n    recurse: true\n  when: chown_config_first_time.changed\n  tags:\n    # hard to run as a handler for time being\n    - skip_ansible_lint\n\n- name: Prevent nexus to create any new configuration files in  {{ nexus_installation_dir }}/nexus-latest/etc\n  file:\n    path: \"{{ item }}\"\n    state: \"directory\"\n    owner: \"root\"\n    group: \"root\"\n    mode: \"0755\"\n    recurse: false\n  with_items: \"{{ nexus_app_dir_settings_dirs }}\"\n\n- name: First-time install admin password\n  set_fact:\n    current_nexus_admin_password: 'admin123'\n  when: nexus_data_dir_contents.stdout == \"\"\n\n- name: Subsequent re-provision admin password\n  set_fact:\n    current_nexus_admin_password: \"{{ nexus_admin_password }}\"\n  when: nexus_data_dir_contents.stdout != \"\"\n  no_log: true\n\n- name: Create directory to hold current groovy scripts for reference\n  file:\n    path: \"{{ nexus_data_dir }}/groovy-raw-scripts/current\"\n    state: directory\n    owner: root\n    group: root\n\n- name: Upload new scripts\n  synchronize:\n    archive: no\n    checksum: yes\n    recursive: yes\n    delete: yes\n    mode: push\n    src: \"files/groovy/\"\n    dest: \"{{ nexus_data_dir }}/groovy-raw-scripts/new/\"\n\n- name: Sync new scripts to old and get differences\n  shell: 'rsync -ric {{ nexus_data_dir }}/groovy-raw-scripts/new/ {{ nexus_data_dir }}/groovy-raw-scripts/current/ | cut -d\" \" -f 2 | sed \"s/\\.groovy//g\"'\n  register: nexus_groovy_files_changed\n  check_mode: no\n  changed_when: false\n  # simple check on changed files kept on host\n  # skip ansible lint (we don't want to use synchronize module for this)\n  args:\n    warn: false\n\n- name: Declare new or changed groovy scripts in nexus\n  include: declare_script_each.yml\n  with_items: \"{{ nexus_groovy_files_changed.stdout_lines}}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f5eff8d4cf880e13bb74b04f3210d54fe8c3147e", "filename": "roles/user-management/manage-local-user-password/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nuser_name: \"\"\nclear_text_password: \"\"\n\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "735862c7cc86bee6b5f2398cdab3f7d57e5da2d4", "filename": "ops/playbooks/roles/worker/tasks/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n\n    - name: Retrieve a token for the UCP API\n      uri:\n        url: \"https://{{ ARG_UCP_IP }}/auth/login\"\n        headers:\n          Content-Type: application/json\n        method: POST\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        body: '{\"username\":\"{{ ARG_UCP_USER }}\",\"password\":\"{{ ARG_UCP_PASSWORD }}\"}'\n      delegate_to: localhost\n      register: resp\n      until: resp.status == 200\n      retries: 20\n      delay: 5\n\n    - name: Remember the API's token\n      set_fact:\n        auth_token:  \"{{resp.json.auth_token}}\"\n\n    - name: Is the node already in the swarm\n      uri:\n        url: 'https://{{ ARG_UCP_IP }}/nodes?filters={\"name\":{\"{{ inventory_hostname }}.{{ domain_name }}\":true}}'\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        validate_certs: no\n      delegate_to: localhost\n      register: resp\n\n    - set_fact:\n        swarm_member: \"{% if resp.json[0] is defined %}true{% else %}false{% endif %}\"  \n\n    - debug:\n        var: swarm_member\n      when: _debug is defined\n\n    - name: Retrieve a worker token\n      uri:\n        url: \"https://{{ ARG_UCP_IP }}/swarm\"\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      delegate_to: localhost\n      register: resp\n      when: swarm_member == false\n\n    - name: Memorize the swarm's token\n      set_fact:\n        token:  \"{{ resp.json.JoinTokens.Worker }}\"\n      when: swarm_member == false\n\n    - name: Add node to the swarm\n      command: \"docker swarm join --token {{ token }} {{ ARG_ADVERTIZE_IP }}\"\n      when: swarm_member == false\n\n#    - name: Join the swarm\n#      uri:\n#        url: 'https://{{ ARG_UCP_IP }}/swarm/join'\n#        headers:\n#          Content-Type: application/json\n#          Authorization: Bearer {{ auth_token }}\n#        method: POST\n#        status_code: 200\n#        body_format: json\n#        body: '{\"AdvertiseAddr\": \"{{ advaddr }}:2377\",\"RemoteAddrs\": [\"{{ ARG_ADVERTIZE_IP }}],\"ListenAddr\": \"0.0.0.0:2377\",\"JoinToken\": \"{{ token }}\"}'\n#        validate_certs: no\n#      vars:\n#         advaddr:  \"{{ ip_addr | ipaddr('address') }}\"\n#      register: resp\n#      when: swarm_member == false\n\n    - name: Poll the status of the node\n      uri:\n        url: 'https://{{ ARG_UCP_IP }}/nodes/{{ inventory_hostname }}.{{ domain_name }}'\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200,404\n        body_format: json\n        validate_certs: no\n      delegate_to: localhost\n      register: resp\n      until: resp.status == 200 and resp.json.Spec.Role == \"worker\" and resp.json.Status.State == \"ready\"\n      delay: 10\n      retries:  \"{{ ( 1 + worker_join_delay  / 10 ) | int }}\"\n\n    - debug: msg=\"Availability={{resp.json.Spec.Availability}} Role={{resp.json.Spec.Role}} State={{resp.json.Status.State}}\"\n      when: _debug is defined\n\n#\n# set orchestrator type\n#\n    - name: Clear Orchestrator type \"swarm\" \n      command: \"docker node update --label-rm com.docker.ucp.orchestrator.swarm {{ inventory_hostname }}.{{ domain_name }}\" \n      when: orchestrator is defined and orchestrator == \"kubernetes\"\n      failed_when: false\n      delegate_to: \"{{ ucp_instance }}\"\n\n    - name: Set Ochestrator type Kubernetes\n      command: \"docker node update --label-add com.docker.ucp.orchestrator.kubernetes=true {{ inventory_hostname }}.{{ domain_name }}\"\n      when: orchestrator is defined and orchestrator == \"kubernetes\"\n      delegate_to: \"{{ ucp_instance }}\"\n\n    - name: Clear Orchestrator type \"Kubernetes\"\n      command: docker node update --label-rm com.docker.ucp.orchestrator.kubernetes {{ inventory_hostname }}.{{ domain_name }}\n      when: orchestrator is defined and orchestrator == \"swarm\"\n      failed_when: false\n      delegate_to: \"{{ ucp_instance }}\"\n\n    - name: Set Orchestrator type \"swarm\" \n      command: \"docker node update --label-add com.docker.ucp.orchestrator.swarm=true {{ inventory_hostname }}.{{ domain_name }}\" \n      when: orchestrator is defined and orchestrator == \"swarm\"\n      delegate_to: \"{{ ucp_instance }}\"\n\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "4a101becf2f526dcd23fde074a7265636450066c", "filename": "playbooks/openshift/post-install.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- hosts: cluster_hosts\n  roles:\n    - role: sync-keys\n      key_url: \"{{ openshift_authorized_key_url | default('') }}\"\n\n- hosts: masters\n  roles:\n  - { role: create_users }\n\n- import_playbook: ../openshift-cluster-seed.yml\n  when:\n  - openshift_cluster_content is defined\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "1b73921a5def25535c8522df073862808187caaa", "filename": "roles/dns_adblocking/tasks/freebsd.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: FreeBSD / HardenedBSD | Enable dnsmasq\n  lineinfile: dest=/etc/rc.conf regexp=^dnsmasq_enable= line='dnsmasq_enable=\"YES\"'\n\n- name: The dnsmasq additional directories created\n  file:\n    dest: \"{{ item }}\"\n    state: directory\n    mode: '0755'\n  with_items:\n    - \"{{ config_prefix|default('/') }}etc/dnsmasq.d\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "21e0696a090202f7ffef057d8cc8c6d091508f2b", "filename": "roles/setup-slack/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Initialise channels name to id mapping\n  set_fact:\n    channel_mapping: {}\n\n- name: Create channels\n  include_tasks: create_channels.yml\n  with_items: \"{{ slack_channels }}\"\n  loop_control:\n    loop_var: channel\n\n- name: Invite users\n  include_tasks: invite_users.yml\n  with_items: \"{{ slack_users }}\"\n  loop_control:\n    loop_var: user\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "6762e5b43fac888dffc60ddc48a20279a5d4c40e", "filename": "roles/iiab-admin/tasks/access.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install textmode remote access packages\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - screen\n    - lynx\n  tags:\n    - download\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "a62ac42e3b09327d66f112de2409b043228db266", "filename": "playbooks/roles/kafka/tests/test.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- hosts: localhost\n  remote_user: root\n  roles:\n    - kafka"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "dfac6ae9cd375d2ad8bbc0233daeddc3f0b12e03", "filename": "playbooks/debug.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- debug: msg=\"Dumping variables for debug\"\n- debug: var=rock_debug\n- debug: var=rock_online_install\n- debug: var=rock_data_dir\n- debug: var=bro_data_dir\n- debug: var=suricata_data_dir\n- debug: var=stenographer_data_dir\n- debug: var=rock_data_user\n- debug: var=es_mem\n- debug: var=bro_cpu\n- debug: var=rock_monifs\n- debug: var=rock_hostname\n- debug: var=rock_fqdn\n- debug: var=epel_baseurl\n- debug: var=epel_gpgurl\n- debug: var=elastic_baseurl\n- debug: var=elastic_gpgurl\n- debug: var=rocknsm_baseurl\n- debug: var=rocknsm_gpgurl\n- debug: var=rocknsm_local_baseurl\n- debug: var=http_proxy\n- debug: var=https_proxy\n"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "44ab7ee0ff3a79bda2a2913ea9401799514ac9ac", "filename": "tasks/security_policy_apply/Win32NT.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Copy patch file\"\n  win_copy:\n    src: \"{{ security_policy_oracle_artifact }}\"\n    dest: \"C:\\Program Files\\\\Java\\\\{{ security_policy_oracle_artifact|basename }}\"\n    force: True\n  when:\n    - (java_major_version|int == 8 and java_minor_version|int < 151) or (java_major_version|int < 8)\n\n- name: \"Unzip patch file\"\n  win_unzip:\n    src: \"C:\\Program Files\\\\Java\\\\{{ security_policy_oracle_artifact|basename }}\"\n    dest: \"C:\\Program Files\\\\Java\\\\{{ security_policy_oracle_artifact|basename }}\\\\\\\n      jre\\\\lib\\\\security\"\n  when:\n    - (java_major_version|int == 8 and java_minor_version|int < 151) or (java_major_version|int < 8)\n\n- name: \"Apply patch file\"\n  win_command:\n    \"xcopy -r C:\\Program Files\\\\Java\\\\jre\\\\lib\\\\security\\\\\\\n    {{ security_patch_folders[java_major_version|int] }}/*.jar \\\n    C:\\Program Files\\\\Java\\\\jre\\\\lib\\\\security\\\\\"\n  args:\n    removes: \"C:\\Program Files\\\\Java\\\\jre\\\\lib\\\\security\\\\\\\n      {{ security_patch_folders[java_major_version|int] }}\\\\local_policy.jar\"\n  when:\n    - (java_major_version|int == 8 and java_minor_version|int < 151) or (java_major_version|int < 8)\n\n- name: \"Apply setting\"\n  win_lineinfile:\n    path: \"C:\\Program Files\\\\Java\\\\jre\\\\lib\\\\security\\\\java.security\"\n    regexp: \"#crypto.policy=unlimited\"\n    line: \"crypto.policy=unlimited\"\n  when:\n    - java_major_version == \"8\"\n    - java_minor_version|int >= 151\n"}, {"commit_sha": "6d10af54bdbf8e81c3d90a70ffea87b4d2c20eb2", "sha": "0657fc1dc99a0d51fcaab3031127ac775e0b8aaa", "filename": "tasks/core.yml", "repository": "Oefenweb/ansible-wordpress", "decoded_content": "# tasks file for wordpress, core\n---\n- name: check download\n  shell: \"ls {{ item.path }} | grep -q 'wp-'\"\n  register: check_download\n  failed_when: False\n  changed_when: False\n  with_items: wordpress_installs\n  tags: [configuration, wordpress, wordpress-core, wordpress-is-downloaded]\n\n- name: download\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.item.path }}' core download\"\n  with_items: check_download.results\n  when: item.rc != 0\n  tags: [configuration, wordpress, wordpress-core, wordpress-downloaded]\n\n- name: configure\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.path }}' core config --dbname='{{ item.dbname }}' --dbuser='{{ item.dbuser }}' --dbpass='{{ item.dbpass }}' --dbhost='{{ item.dbhost | default('localhost') }}'\"\n  args:\n    creates: \"{{ item.path }}/wp-config.php\"\n  with_items: wordpress_installs\n  tags: [configuration, wordpress, wordpress-core, wordpress-configure]\n\n- name: identify installation\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.path }}' core is-installed\"\n  register: check_installation\n  failed_when: False\n  changed_when: False\n  with_items: wordpress_installs\n  tags: [configuration, wordpress, wordpress-core, wordpress-is-installed]\n\n- name: install\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.item.path }}' core install --url='{{ item.item.url }}' --title='{{ item.item.title }}' --admin_name='{{ item.item.admin_name | default('admin') }}' --admin_email='{{ item.item.admin_email }}' --admin_password='{{ item.item.admin_password }}'\"\n  with_items: check_installation.results\n  when: item.rc != 0\n  tags: [configuration, wordpress, wordpress-core, wordpress-install]\n\n- name: check install\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.path }}' core is-installed\"\n  changed_when: False\n  with_items: wordpress_installs\n  tags: [configuration, wordpress, wordpress-core, wordpress-install-check]\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9e892972b152d96001df29e37c1e06022a56aaea", "filename": "roles/config-vlans/tasks/interfaces.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Configure VLAN interfaces\"\n  template:\n    src: vlan.j2 \n    dest: /etc/sysconfig/network-scripts/ifcfg-{{ ifcfg.device }}\n  with_items:\n  - '{{ vlans }}'\n  loop_control:\n    loop_var: ifcfg\n\n- name: \"Attempt to Bring Up VLAN interfaces\"\n  command: ifup {{ ifcfg.device }}\n  with_items:\n  - '{{ vlans }}'\n  loop_control:\n    loop_var: ifcfg\n  ignore_errors: yes\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "c4c5cafdede6f786851f4a2a18d5416583dac0e6", "filename": "roles/mongodb/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install MongoDB required packages\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - mongodb-server\n    - mongodb\n  when: internet_available\n  tags:\n    - download\n\n- name: Create the data directory for MongoDB\n  file:\n    state: directory\n    path: \"{{ item.path }}\"\n    owner: mongodb\n  with_items:\n    - { path: '/var/run/mongodb' }\n    - { path: \"{{ mongodb_db_path }}\" }    # == /library/dbdata/mongodb/\n    - { path: '/var/log/mongodb' }\n\n- name: Populate & position /etc/mongod.conf, mongodb.service\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: root\n    group: root\n    mode: 0644\n  with_items:\n    - { src: 'mongod.conf.j2', dest: \"{{ mongodb_conf }}\" }\n    - { src: 'mongodb.service.j2', dest: '/etc/systemd/system/mongodb.service' }\n\n- name: Enable+restart systemd service if mongodb_enabled, with \"systemctl daemon-reload\" (in case mongodb.service changed?)\n  systemd:\n    name: mongodb\n    enabled: yes\n    state: restarted\n    daemon_reload: yes\n  when: mongodb_enabled\n\n- name: 'Disable+stop systemd service if mongodb_enabled: False'\n  systemd:\n    name: mongodb\n    enabled: no\n    state: stopped\n  when: not mongodb_enabled\n\n#- name: Restart service if enabled\n#  service:\n#    name: mongodb\n#    enabled: yes\n#    state: restarted\n#  when: mongodb_enabled\n\n#- name: Stop service if not enabled\n#  service:\n#    name: mongodb\n#    enabled: no\n#    state: stopped\n#  when: not mongodb_enabled\n\n- name: Add 'mongodb' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: mongodb\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: MongoDB\n    - option: description\n      value: '\"MongoDB is an open-source document database that provides high performance, high availability, and automatic scaling.\"'\n    - option: enabled\n      value: \"{{ mongodb_enabled }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "be2c937d34edaabbf4a3102d273db1441b1a0fda", "filename": "roles/install-mongodb/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: 'Check if mongodb is installed'\n  command: which mongod\n  register: mongod_check\n  ignore_errors: yes\n\n- name: 'Install mongodb if not installed'\n  include_tasks: 'install_mongodb.yml'\n  when: mongod_check != '/usr/bin/mongod'\n\n- name: 'Start MongoDB'\n  service:\n    name: mongod\n    state: started\n\n- name: 'Check if pip is installed'\n  command: which pip\n  register: pip_check\n  ignore_errors: yes\n\n- name: 'Install pip if not installed'\n  command: easy_install pip\n  when: pip_check != '/usr/bin/pip'\n\n- name: 'Install pymongo'\n  command: pip install pymongo\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "57dc4e20360324afec0743f3ab4ce0f7cb28361a", "filename": "roles/config-nagios-target/tasks/nrpe_openshift_master.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Copy in additional Nagios service plugin\n  copy: \n    src: plugins/check_service.sh\n    dest: /usr/lib64/nagios/plugins/check_service.sh\n    owner: root\n    group: root\n    mode: 0755\n\n- name: Copy nrpe.d OpenShift node configuration files\n  copy: \n    src: nrpe.d/check_openshift_master.cfg\n    dest: /etc/nrpe.d/check_openshift_master.cfg\n    owner: root\n    group: root\n    mode: 0644\n\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "9b6eb131b653db94f34fb83a0b6cbabc1334338e", "filename": "playbooks/setup-deploy-host.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n- hosts: localhost\n  gather_facts: false\n  tasks:\n    - name: Install RockNSM GPG keys\n      copy:\n        src: \"{{ item }}\"\n        dest: \"/etc/pki/rpm-gpg/{{ item }}\"\n        mode: 0644\n        owner: root\n        group: root\n      with_items:\n        - RPM-GPG-KEY-RockNSM-2\n        - RPM-GPG-KEY-RockNSM-Testing\n        - RPM-GPG-KEY-RockNSM-pkgcloud-2_4\n\n    - name: Trust RockNSM GPG keys\n      rpm_key:\n        state: present\n        key: \"{{ item.path }}\"\n      with_items:\n        - { repoid: \"rocknsm_2_4\", path: \"/etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-2\" }\n        - { repoid: \"rocknsm_2_4\", path: \"/etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-pkgcloud-2_4\" }\n      register: registered_keys\n\n    - name: Configure RockNSM online repos\n      yum_repository:\n        file: rocknsm\n        name: \"{{ item.name }}\"\n        enabled: \"{{ rock_online_install }}\"\n        description: \"{{ item.name }}\"\n        baseurl: \"{{ item.baseurl }}\"\n        repo_gpgcheck: 1\n        gpgcheck: \"{{ item.gpgcheck }}\"\n        gpgkey:\n          - file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-pkgcloud-2_4\n          - file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-2\n        sslverify: 1\n        sslcacert: /etc/pki/tls/certs/ca-bundle.crt\n        metadata_expire: 300\n        cost: 750\n        state: present\n      with_items:\n        - { name: \"rocknsm_2_4\", gpgcheck: true, baseurl: \"{{ rocknsm_baseurl }}\" }\n        - { name: \"rocknsm_2_4-source\", gpgcheck: false, baseurl: \"{{ rocknsm_srpm_baseurl }}\" }\n\n    - name: Trust RockNSM GPG keys in yum\n      command: \"yum -q makecache -y --disablerepo='*' --enablerepo='{{ item.repoid }}'\"\n      with_items:\n        - { repoid: \"rocknsm_2_4\", test: \"{{ rock_online_install }}\" }\n        - { repoid: \"rocknsm_2_4-source\", test: \"{{ rock_online_install }}\" }\n      when: item.test | bool\n      changed_when: false\n      # TODO: Fix this ^^\n\n    - name: Install support packages\n      yum:\n        name:\n          - python2-jinja2\n          - python2-markupsafe\n        state: latest\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "8f9053de55e854929b5646f10592e4c1c01a20cf", "filename": "roles/kibana/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n- name: Install packages\n  yum:\n    name: kibana\n    state: present\n\n- name: Update kibana config\n  template:\n    src: kibana.yml.j2\n    dest: /etc/kibana/kibana.yml\n  notify: Restart kibana\n\n- name: Flush handlers\n  meta: flush_handlers\n\n- name: Enable and start kibana\n  service:\n    name: kibana\n    state: \"started\"\n    enabled: \"{{ 'True' if rock_services | selectattr('name', 'equalto', 'kibana') | map(attribute='enabled') | bool else 'False' }}\"\n\n- name: \"Wait for Kibana to be available\"\n  uri:\n    url: \"{{ kibana_url }}/api/kibana/settings\"\n    status_code: 200\n  register: result\n  until: result.status == 200\n  retries: 60\n  delay: 1\n\n- name: Blanket install/update kibana saved objects\n  command: ./import-saved-items.sh \"{{ kibana_url }}\"\n  args:\n    chdir: \"{{ rock_module_dir }}/configuration/kibana\"\n  changed_when: false\n  tags:\n    - saved_objects\n    # TODO: Fix this ^^\n\n- name: Configure kibana templates\n  uri:\n    method: PUT\n    url: \"{{ es_url }}/_template/kibana-config\"\n    body: >\n      { \"order\" : 0, \"template\" : \".kibana\",\n        \"settings\" :\n          { \"index.number_of_replicas\" : \"0\",\n            \"index.number_of_shards\" : \"1\" },\n        \"mappings\" : { }, \"aliases\" : { } }\n    body_format: json\n    status_code: 200,201\n\n- name: Add the kibanapw shell function\n  copy:\n    src: profile.d-kibanapw.sh\n    dest: /etc/profile.d/kibanapw.sh\n    mode: 0644\n    owner: root\n    group: root\n\n- name: Download RockNSM elastic configs\n  get_url:\n    url: \"{{ rock_dashboards_url }}\"\n    dest: \"{{ rock_cache_dir }}/{{ rock_dashboards_filename }}\"\n    mode: 0644\n  when:\n    rock_services | selectattr('name', 'equalto', 'elasticsearch') | map(attribute='installed') or\n    rock_services | selectattr('name', 'equalto', 'logstash') | map(attribute='installed') and\n    rock_online_install\n\n- name: Extract RockNSM elastic configs\n  unarchive:\n    src: \"{{ rock_cache_dir }}/{{ rock_dashboards_filename }}\"\n    dest: /opt/rocknsm\n    owner: root\n    group: root\n    creates: \"{{ rock_module_dir }}\"\n    remote_src: true\n  when:\n    rock_services | selectattr('name', 'equalto', 'elasticsearch') | map(attribute='installed') or\n    rock_services | selectattr('name', 'equalto', 'logstash') | map(attribute='installed')\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "0ac050f5db9f98b4541ad13cf8ae69a2521e7586", "filename": "roles/config-linux-desktop/config-lxde/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install, configure and enable LXDE\"\n  include_tasks: \"{{ distro_file }}\"\n  with_first_found:\n  - files:\n    - lxde-{{ ansible_distribution }}.yml\n    skip: true\n  loop_control:\n    loop_var: distro_file\n  when:\n  - lxde_install|default(False)\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "0a28c4e791476c064047d2c7e2828f7df3dedcce", "filename": "roles/config-chrony/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Reload chrony\" \n  service:\n    name: 'chronyd'\n    state: restarted\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "14d71dfee90aa243bbd7637e7ba18045f4dc7535", "filename": "roles/manage-jira/tasks/create_permission_scheme.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Create Jira Permission Scheme\n  uri:\n    url: \"{{ jira_url }}/rest/api/2/permissionscheme\"\n    method: POST\n    user: \"{{ jira_username }}\"\n    password: \"{{ jira_password }}\"\n    return_content: yes\n    force_basic_auth: yes\n    body_format: json\n    header:\n      - Accept: 'application/json'\n      - Content-Type: 'application/json'\n    body: \"{{ lookup('template','permissionScheme.json.j2') }}\"\n    status_code: 201\n  register: permissionScheme\n\n- name: Set fact for Permission Scheme ID\n  set_fact:\n    PermissionScheme: \"{{ permissionScheme.json.id }}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "6becb81cfe86c26eb0fc5280df50a9376a602932", "filename": "roles/munin/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install Munin package (debuntu)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - munin\n    - munin-node\n    - munin-plugins-extra\n    - libcgi-fast-perl\n    - libapache2-mod-fcgid\n  tags:\n    - download\n  when: is_debuntu\n\n- name: Install Munin package (OS's other than debuntu)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - munin\n    - munin-node\n  tags:\n    - download\n  when: not is_debuntu\n\n- name: Copy Munin config file\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: root\n    group: root\n    mode: 0644\n  with_items:\n    - { src: 'munin.conf.j2', dest: '/etc/munin/munin.conf' }\n    - { src: 'munin24.conf.j2', dest: '/etc/{{ apache_config_dir }}/munin24.conf' }\n\n- name: Create admin user\n  htpasswd:\n    path: /etc/munin/munin-htpasswd\n    name: Admin\n    password: changeme\n    create: yes\n    state: present\n\n- name: Enable munin-node service\n  service:\n    name: munin-node\n    enabled: yes\n    state: started\n  when: munin_enabled\n\n- name: Enable Apache lookup (debuntu)\n  file:\n    src: /etc/apache2/sites-available/munin24.conf\n    dest: /etc/apache2/sites-enabled/munin24.conf\n    state: link\n  when: munin_enabled and is_debuntu\n\n- name: Disable Apache lookup (debuntu)\n  file:\n    src: /etc/apache2/sites-available/munin24.conf\n    dest: /etc/apache2/sites-enabled/munin24.conf\n    state: absent\n  when: not munin_enabled and is_debuntu\n\n- name: Disable munin-node service when it becomes disabled\n  service:\n    name: munin-node\n    enabled: no\n    state: stopped\n  when: not munin_enabled\n\n- name: If MySQL is enabled, let Munin monitor it\n  copy:\n    src: \"{{ item }}\"\n    dest: /etc/munin/plugins/\n  with_items:\n    - /usr/share/munin/plugins/mysql_\n    - /usr/share/munin/plugins/mysql_bytes\n    - /usr/share/munin/plugins/mysql_innodb\n    - /usr/share/munin/plugins/mysql_isam_space_\n    - /usr/share/munin/plugins/mysql_queries\n    - /usr/share/munin/plugins/mysql_slowqueries\n    - /usr/share/munin/plugins/mysql_threads\n  when: mysql_enabled\n\n- name: Add 'munin' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: munin\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: Munin\n    - option: description\n      value: '\"Munin is a networked resource monitoring tool that can help analyze resource trends and \\\"what just happened to kill our performance?\\\" problems.\"'\n    - option: installed\n      value: \"{{ munin_install }}\"\n    - option: enabled\n      value: \"{{ munin_enabled }}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "50f50dd1028ae2963d937181d035889b81f34889", "filename": "roles/owncloud/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "owncloud_install: True\nowncloud_enabled: False\n\nowncloud_url: /owncloud\nowncloud_prefix: /opt\nowncloud_data_dir: /library/owncloud/data\nowncloud_dl_url: https://download.owncloud.org/community/\nowncloud_src_file: owncloud-9.0.2.tar.bz2\n\n# we install on mysql with these setting or those from default_vars, etc.\nowncloud_dbname: owncloud\nowncloud_dbhost: localhost\nowncloud_dbuser: owncloud\nowncloud_dbpassword: owncloudmysql\n\nowncloud_admin_user: 'Admin'\nowncloud_admin_password: 'changeme'\n\nowncloud_required_ip: 10.0.0.0/8 192.168.0.0/16\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "9cf546442c04ec3b796302edf521e817f7c6a21f", "filename": "roles/haproxy/vars/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# vars file for haproxy\n"}, {"commit_sha": "01c4359d8ad17ba10149ac898663e598069b9055", "sha": "238c1a28690d4fbd384f8b5c2f19008606f41d63", "filename": "tasks/fail2ban-Debian.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\n- name: Install fail2ban.\n  apt: name=fail2ban state=present\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "5a4ada7bab01b729f3745c4c5e706f4d2d147b4d", "filename": "dev/playbooks/install_worker_nodes.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: worker\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Open required ports for Workers\n      command: firewall-cmd --permanent --zone=public --add-port=443/tcp --add-port=2377/tcp --add-port=4789/tcp --add-port=4789/udp --add-port=7946/tcp --add-port=7946/udp --add-port=12376/tcp\n\n    - name: Reload firewalld configuration\n      command: firewall-cmd --reload\n\n    - name: Get worker token\n      include_vars:\n        file: /tmp/worker_token\n        name: add_worker\n\n    - name: Check if node already belongs to the swarm\n      shell: docker info | grep \"Swarm{{':'}} inactive\" | wc -l\n      register: swarm_inactive\n\n    - name: Add Worker nodes to the swarm\n      command: \"{{ add_worker.token }}\"\n      when: swarm_inactive.stdout == \"1\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "2a4250a497171ff4a679b56974ca9540ed5a7fdb", "filename": "roles/config-pxe/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\ntftpserver_root_dir: \"/var/lib/tftpboot\"\n\ndefault_pxe_menu_title: \"PXE Boot Menu\"\ndefault_pxe_timeout: 300\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "f226cdd563c28a00125bd8f30adadae337f0af02", "filename": "roles/filebeat/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n- name: Restart filebeat\n  systemd:\n    name: filebeat\n    state: restarted\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "707a73515a4fc2986a4b4cfafd7c64f75b42e9e5", "filename": "reference-architecture/ansible-tower-integration/create_httpd_file/create_httpd_file/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Install httpd-tools\n  yum:\n    name: httpd-tools\n    state: present\n  become: true\n\n- name: create httpd password\n  command: htpasswd -b /etc/origin/master/htpasswd {{ ocp_username }} {{ ocp_password }}\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "54dcd1825f2a8b663f525c7f360729d1d819586e", "filename": "roles/storage-demo/defaults/main.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "namespace: \"kube-system\"\ncluster: \"openshift\"\ncinder_provisioner_repo: \"quay.io/aglitke\"\ncinder_provisioner_release: \"sprint4\"\naction: \"provision\"\n\nstorage_demo_template_dir: \"{{ role_path }}/templates\""}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "d07965e78c474fd528110d205434e9bcba91611f", "filename": "roles/mysql/tasks/centos.yml", "repository": "iiab/iiab", "decoded_content": "    - name: Install MySQL\n      package: name={{ item }}\n               state=present\n      with_items:\n        - mysql-connector-python\n        - mariadb-server\n        - mariadb\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ee60adcab1a757846e6a592105340e329191ef1f", "filename": "reference-architecture/gcp/ansible/playbooks/roles/master-http-proxy/handlers/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: restart haproxy\n  service:\n    name: haproxy\n    state: restarted\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "d17c1e3357350e5d8f91d4f30ea3b91e802051bd", "filename": "playbooks/provisioning/openstack/custom-actions/add-rhn-pools.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: cluster_hosts\n  vars:\n    rhn_pools: []\n  tasks:\n  - name: Attach additional RHN pools\n    become: true\n    with_items: \"{{ rhn_pools }}\"\n    command: \"/usr/bin/subscription-manager attach --pool={{ item }}\"\n    register: attach_rhn_pools_result\n    until: attach_rhn_pools_result.rc == 0\n    retries: 10\n    delay: 1\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e896b660944daf12f17513f66948f560e7610ec5", "filename": "roles/config-pxe/tasks/prep.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - tftp-server\n  - syslinux\n  - firewalld\n  - python-firewall\n\n- name: 'Ensure firewalld is running'\n  service:\n    name: firewalld\n    state: started \n    enabled: yes\n\n- name: 'Ensure tftp-server is running'\n  service:\n    name: tftp\n    state: started \n    enabled: yes\n\n- name: 'Open Firewall for PXE/TFTP use'\n  firewalld: \n    port: \"{{ item }}\"\n    permanent: yes\n    state: enabled\n    immediate: yes\n  with_items:\n  - 69/udp\n\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "f105677257283445daa31476bb11efbd24a7a841", "filename": "ops/playbooks/restore_dtr_metadata.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- name: Restore DTR Metadata\n  hosts: dtr_main\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - includes/internal_vars.yml\n    - ../group_vars/backups\n\n  vars:\n    http_proxy_switch:  \"{% if  env.http_proxy is defined %} --http-proxy {{ env.http_proxy }} {% endif %}\"\n    https_proxy_switch:  \"{% if  env.https_proxy is defined %} --https-proxy {{ env.https_proxy }} {% endif %}\"\n    no_proxy_switch:  \"{% if  env.no_proxy is defined %} --no-proxy '{{ env.no_proxy }}' {% endif %}\"\n\n  pre_tasks:\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n    - fail:\n      when: ucp_instance == \".none.\"\n\n  roles:\n    - role: get-dtr-replica\n      ARG_UCP_IP:        \"{{ ucp_instance }}.{{domain_name}}\"\n      ARG_UCP_USER:      \"{{ ucp_username }}\"\n      ARG_UCP_PASSWORD:  \"{{ ucp_password }}\"\n\n    - role: worker\n      ARG_UCP_IP:         \"{{ ucp_instance }}.{{ domain_name }}\"\n      ARG_UCP_USER:       \"{{ ucp_username }}\"\n      ARG_UCP_PASSWORD:   \"{{ ucp_password }}\"\n      ARG_ADVERTIZE_IP:   \"{{ ucp_instance }}.{{ domain_name }}:2377\"\n      worker_role_ports:  \"{{ internal_dtr_ports }}\"\n      worker_join_delay:  180\n\n\n  tasks:\n#\n# exit if there is an existing DTR replica\n#\n    - debug: var=existing_dtr_replica_id\n      when: _debug is defined\n\n#\n# configure passwordless ssh to our ansible box\n#\n    - name: Register key\n      stat: path=/root/.ssh/id_rsa\n      register: key\n    - name: Create keypairs\n      shell: ssh-keygen -t rsa -f /root/.ssh/id_rsa -N ''\n      when: key.stat.exists == False\n    - name: Fetch all public ssh keys\n      shell: cat ~/.ssh/id_rsa.pub\n      register: ssh_keys\n    - name: Deploy keys on localhost\n      authorized_key: user=root key=\"{{ item }}\"\n      delegate_to: localhost\n      with_items:\n        - \"{{ ssh_keys.stdout }}\"\n\n    - name: Create script directory\n      file:\n        path: /root/scripts\n        state: directory\n    - name: Copy UCP root CA Certificate if one is specified\n      copy:\n        src: \"{{ ucp_certs_dir }}/ca.pem\"\n        dest: /root/scripts/ca.pem\n      when: ucp_certs_dir is defined\n    - name: Copy restore script to target\n      template: src=../templates/restore_dtr_metadata.sh.j2 dest=/root/scripts/restore_dtr_metadata.sh\n    - file:\n        path: /root/scripts/restore_dtr_metadata.sh\n        mode: 0744\n\n    - name: restore DTR\n      shell: /root/scripts/restore_dtr_metadata.sh {{ backup_dtr_id }}\n      environment:\n        UCP_USERNAME: \"{{ucp_username}}\"\n        UCP_PASSWORD: \"{{ucp_password}}\"\n      register: res\n      when: existing_dtr_replica_id == \"0\"\n\n\n    - debug:\n        msg: \"Existing DTR replica detected with id {{ existing_dtr_replica_id }} . Did not restore DTR\"\n      when: existing_dtr_replica_id != \"0\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b4fc2c4a2b32f6d0188224ea7f8972dfea3eb361", "filename": "roles/manage-sshd-config/test/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n# needed to setup for the test conditions\nuser_name: root\nauthorized_keyfile: \"{{ inventory_dir }}/../authorized_keys\"\nkey_url: \"{{ lookup('file', authorized_keyfile) }}\"\n\nreset_keyfile: yes\nclear_text_password: test1234\n\n#needed to test the role\nupdate_sshd_config:\n  PermitRootLogin: \"without-password\"\n\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "e83099390050e5ccc3d8a67b2af2a2ad3b4062ba", "filename": "roles/common/tasks/ubuntu.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Install software updates\n  apt: update_cache=yes upgrade=dist\n  tags:\n    - cloud\n\n- name: Check if reboot is required\n  shell: >\n    if [[ -e /var/run/reboot-required ]]; then echo \"required\"; else echo \"no\"; fi\n  args:\n    executable: /bin/bash\n  register: reboot_required\n  tags:\n    - cloud\n\n- name: Reboot\n  shell: sleep 2 && shutdown -r now \"Ansible updates triggered\"\n  async: 1\n  poll: 0\n  when: reboot_required is defined and reboot_required.stdout == 'required'\n  ignore_errors: true\n  tags:\n    - cloud\n\n- name: Wait until SSH becomes ready...\n  local_action:\n    module: wait_for\n    port: 22\n    host: \"{{ inventory_hostname }}\"\n    search_regex: OpenSSH\n    delay: 10\n    timeout: 320\n  when: reboot_required is defined and reboot_required.stdout == 'required'\n  become: false\n  tags:\n    - cloud\n\n- name: Disable MOTD on login and SSHD\n  replace: dest=\"{{ item.file }}\" regexp=\"{{ item.regexp }}\" replace=\"{{ item.line }}\"\n  with_items:\n    - { regexp: '^session.*optional.*pam_motd.so.*', line: '# MOTD DISABLED', file: '/etc/pam.d/login' }\n    - { regexp: '^session.*optional.*pam_motd.so.*', line: '# MOTD DISABLED', file: '/etc/pam.d/sshd' }\n  tags:\n    - cloud\n\n- name: Loopback for services configured\n  template: src=10-loopback-services.cfg.j2 dest=/etc/network/interfaces.d/10-loopback-services.cfg\n  notify:\n    - restart loopback\n  tags:\n    - always\n\n- name: Loopback included into the network config\n  lineinfile: dest=/etc/network/interfaces line='source /etc/network/interfaces.d/10-loopback-services.cfg' state=present\n  notify:\n    - restart loopback\n  tags:\n    - always\n\n- meta: flush_handlers\n  tags:\n    - always\n\n- name: Check apparmor support\n  shell: apparmor_status\n  ignore_errors: yes\n  register: apparmor_status\n\n- set_fact:\n    apparmor_enabled: true\n  when: '\"profiles are in enforce mode\" in apparmor_status.stdout'\n\n- set_fact:\n    tools:\n      - git\n      - screen\n      - apparmor-utils\n      - uuid-runtime\n      - coreutils\n      - sendmail\n      - iptables-persistent\n      - cgroup-tools\n      - openssl\n    sysctl:\n      forwarding:\n        - net.ipv4.ip_forward\n        - net.ipv4.conf.all.forwarding\n        - net.ipv6.conf.all.forwarding\n  tags:\n    - always\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "99698b4d0e5b02e52a5d8ffc4eac424b70c3a149", "filename": "playbooks/aws/README.md", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "# AWS playbooks\n\nThis playbook directory is meant to be driven by [`bin/cluster`](../../bin),\nwhich is community supported and most use is considered deprecated.\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "55923c8c18b25e50168dbd8b1344a670d61c4c97", "filename": "ops/playbooks/includes/svt_wait_for_tasks.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n#\n# we should query the task's status and wait until they are all finished, but for now, we just wait 2 secnds\n#\n- wait_for: timeout=2\n  delegate_to: localhost\n"}, {"commit_sha": "45971be8249cc4627ef8ddfacf55a661b7fc13ca", "sha": "367499251edb795a54503d5308afa65d9e01900e", "filename": "tasks/compatibility-checks.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# https://github.com/moby/moby/issues/35873\n# https://access.redhat.com/solutions/2991041\n- name: Compatibility check - Fail if both MountFlags=slave and live-restore are set\n  fail:\n    msg: >\n      Setting both `MountFlags=slave` (docker_enable_mount_flag_fix: true)\n      and `live-restore=true` (docker_daemon_config['live-restore']: true)\n      triggers a bug (https://github.com/moby/moby/issues/35873). For now,\n      don't use both.\n  when: docker_enable_mount_flag_fix\n        and (docker_daemon_config['live-restore'] is defined\n        and docker_daemon_config['live-restore'])\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "5b707fce5ce9647c56dc07cb3705a34cde7b0257", "filename": "ops/playbooks/roles/hpe.haproxy/tests/test.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "---\n- hosts: localhost\n  remote_user: root\n  roles:\n    - hpe.haproxy\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "319d5e29b6a4e55ac707aad93bf21ea323321707", "filename": "roles/stenographer/tasks/install.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: Install packages\n  yum:\n    name: \"{{ stenographer_packagename }}\"\n    state: \"present\"\n  tags:\n    - yum\n    - stenographer\n    - install\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "d4aaeac23c9ee2e8805cc4b4eaab4d186cffe6d3", "filename": "roles/config-redis/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "- name: Restart Redis Service\n  systemd:\n    name: \"{{ redis_name }}\"\n    enabled: yes\n    state: restarted\n    daemon_reload: yes\n\n- name: restart firewalld\n  service:\n    name: firewalld\n    state: restarted\n\n- name: restart iptables\n  service:\n    name: iptables\n    state: restarted"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "9a3f717c8fae226b9a9e8adf524b721e5c7622fe", "filename": "tasks/Linux/fetch/security-fetch/security-fetch-local.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Copy security policy artifact to destination\n  copy:\n    src: '{{ java_unlimited_policy_transport_local }}'\n    dest: '{{ java_download_path }}'\n  register: policy_file_downloaded\n  retries: 5\n  delay: 2\n  until: policy_file_downloaded is succeeded\n\n- name: Downloaded artifact\n  set_fact:\n    security_policy_java_artifact: '{{ policy_file_downloaded.dest }}'\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "eded206ee8d35f0245656b85fbc8b479541885f1", "filename": "roles/requirements.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "../casl-requirements.yml"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "3898ce0279b607c6ebcbd117d3f7b68908ad6786", "filename": "tasks/Win32NT/system.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: 'Perform {{ java_binary_type }} install'\n  include_tasks: '{{ install_task }}'\n  with_first_found:\n    - 'install/{{ java_distribution }}_{{ java_binary_type }}.yml'\n    - 'install/{{ java_binary_type }}.yml'\n  loop_control:\n    loop_var: install_task\n\n- name: Find java_folder\n  win_find:\n    paths: '{{ java_path }}'\n    recurse: false\n    file_type: directory\n    patterns: '{{ java_folder }}'\n    use_regex: true\n  register: java_dir\n\n- name: Set actual java directory\n  set_fact:\n    java_act_path: \"{{ java_dir.files | map(attribute='path') | list | last }}\"\n\n- name: Set java environment variable\n  win_environment:\n    name: JAVA_HOME\n    state: present\n    value: '{{ java_act_path }}'\n    level: machine\n\n- name: Ensure that 'JAVA_HOME\\bin' present in 'Path' variable\n  win_path:\n    elements: '{{ java_act_path }}\\bin'\n    state: present\n    scope: machine\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "0ce2a83424d088dced24fd1c58672138a2d0a80c", "filename": "playbooks/libvirt/openshift-cluster/templates/network.xml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "<network>\n  <name>{{ libvirt_network }}</name>\n  <forward mode='nat'>\n    <nat>\n      <port start='1024' end='65535'/>\n    </nat>\n  </forward>\n  <!-- TODO: query for first available virbr interface available -->\n  <bridge name='virbr3' stp='on' delay='0'/>\n  <!-- TODO: make overridable -->\n  <domain name='example.com' localOnly='yes' />\n  <dns>\n    <!-- TODO: automatically add host entries -->\n  </dns>\n  <!-- TODO: query for available address space -->\n  <ip address='192.168.55.1' netmask='255.255.255.0'>\n    <dhcp>\n      <range start='192.168.55.2' end='192.168.55.254'/>\n      <!-- TODO: add static entries addresses for the hosts to be created -->\n    </dhcp>\n  </ip>\n</network>\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "40345b19e5c1ce2001e80b0b18ce6d39e69209a6", "filename": "roles/ansible/tower/manage-projects/tasks/process-project.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Get the org id based on the org name\"\n  set_fact:\n    org_id: \"{{ item.id }}\"\n  when:\n  - item.name|trim == project.organization|trim\n  with_items:\n  - \"{{ existing_organizations_output.rest_output }}\"\n\n- name: \"Load up the project\"\n  uri:\n    url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/projects/\"\n    user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n    password: \"{{ ansible_tower.admin_password }}\"\n    force_basic_auth: yes\n    method: POST\n    body: \"{{ lookup('template', 'project.j2') }}\"\n    body_format: 'json'\n    headers:\n      Content-Type: \"application/json\"\n      Accept: \"application/json\"\n    validate_certs: no\n    status_code: 200,201,400\n  register: project_output\n\n- name: \"Clear/Update facts\"\n  set_fact:\n    org_id: ''\n    processed_projects: \"{{ processed_projects + [ { 'name': project.name } ] }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6a1b74b3d2b0f317d809ed717d6af620993e17ea", "filename": "playbooks/provisioning/openstack/sample-inventory/inventory.py", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nimport json\n\nimport shade\n\n\nif __name__ == '__main__':\n    cloud = shade.openstack_cloud()\n\n    inventory = {}\n\n    # TODO(shadower): filter the servers based on the `OPENSHIFT_CLUSTER`\n    # environment variable.\n    cluster_hosts = [\n        server for server in cloud.list_servers()\n        if 'metadata' in server and 'clusterid' in server.metadata]\n\n    masters = [server.name for server in cluster_hosts\n               if server.metadata['host-type'] == 'master']\n\n    etcd = [server.name for server in cluster_hosts\n            if server.metadata['host-type'] == 'etcd']\n    if not etcd:\n        etcd = masters\n\n    infra_hosts = [server.name for server in cluster_hosts\n                   if server.metadata['host-type'] == 'node' and\n                   server.metadata['sub-host-type'] == 'infra']\n\n    app = [server.name for server in cluster_hosts\n           if server.metadata['host-type'] == 'node' and\n           server.metadata['sub-host-type'] == 'app']\n\n    nodes = list(set(masters + infra_hosts + app))\n\n    dns = [server.name for server in cluster_hosts\n           if server.metadata['host-type'] == 'dns']\n\n    lb = [server.name for server in cluster_hosts\n          if server.metadata['host-type'] == 'lb']\n\n    osev3 = list(set(nodes + etcd + lb))\n\n    groups = [server.metadata.group for server in cluster_hosts\n              if 'group' in server.metadata]\n\n    inventory['cluster_hosts'] = {'hosts': [s.name for s in cluster_hosts]}\n    inventory['OSEv3'] = {'hosts': osev3}\n    inventory['masters'] = {'hosts': masters}\n    inventory['etcd'] = {'hosts': etcd}\n    inventory['nodes'] = {'hosts': nodes}\n    inventory['infra_hosts'] = {'hosts': infra_hosts}\n    inventory['app'] = {'hosts': app}\n    inventory['dns'] = {'hosts': dns}\n    inventory['lb'] = {'hosts': lb}\n\n    for server in cluster_hosts:\n        if 'group' in server.metadata:\n            group = server.metadata.group\n            if group not in inventory:\n                inventory[group] = {'hosts': []}\n            inventory[group]['hosts'].append(server.name)\n\n    inventory['_meta'] = {'hostvars': {}}\n\n    for server in cluster_hosts:\n        ssh_ip_address = server.public_v4 or server.private_v4\n        vars = {\n            'ansible_host': ssh_ip_address\n        }\n\n        public_v4 = server.public_v4 or server.private_v4\n        if public_v4:\n            vars['public_v4'] = public_v4\n        # TODO(shadower): what about multiple networks?\n        if server.private_v4:\n            vars['private_v4'] = server.private_v4\n\n        node_labels = server.metadata.get('node_labels')\n        if node_labels:\n            vars['openshift_node_labels'] = node_labels\n\n        inventory['_meta']['hostvars'][server.name] = vars\n\n    print(json.dumps(inventory, indent=4, sort_keys=True))\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "2074ad2c9ff30b0ece0d221c970bdd8abdf8f95a", "filename": "tasks/process_repos_list.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n\n# Process a single _nexus_repos_global_list for configured formats and apply default values for type.\n\n# @todo: refactor with easier syntax once the 'flip' filter is released (possibly in ansible 2.8)\n# See the following related PRs/issues.\n# - https://github.com/ansible/ansible/pull/46340\n# - https://github.com/ansible/ansible/pull/46255\n# - https://github.com/ansible/ansible/issues/46215\n# - https://github.com/pallets/jinja/pull/906\n\n- name: apply defaults to maven proxy repos\n  set_fact:\n    nexus_repos_maven_proxy: >-\n      {%- set result=[] -%}\n      {%- for repo in nexus_repos_maven_proxy -%}\n        {{ result.append(_nexus_repos_maven_defaults | combine(repo)) }}\n      {%- endfor -%}\n      {{ result | to_json | from_json }}\n\n- name: apply defaults to maven hosted repos\n  set_fact:\n    nexus_repos_maven_hosted: >-\n      {%- set result=[] -%}\n      {%- for repo in nexus_repos_maven_hosted -%}\n        {{ result.append(_nexus_repos_maven_defaults | combine(repo)) }}\n      {%- endfor -%}\n      {{ result | to_json | from_json }}\n\n- name: apply defaults to maven group repos\n  set_fact:\n    nexus_repos_maven_group: >-\n      {%- set result=[] -%}\n      {%- for repo in nexus_repos_maven_group -%}\n        {{ result.append(_nexus_repos_maven_defaults | combine(repo)) }}\n      {%- endfor -%}\n      {{ result | to_json | from_json }}\n\n- name: Add maven repositories to global repos list\n  set_fact:\n    _nexus_repos_global_list: >-\n      {{\n        _nexus_repos_global_list | default([])\n        +\n        (nexus_repos_maven_proxy | map('combine', {\"format\": \"maven2\", \"type\": \"proxy\"}) | list)\n        +\n        (nexus_repos_maven_hosted | map('combine', {\"format\": \"maven2\", \"type\": \"hosted\"}) | list)\n        +\n        (nexus_repos_maven_group | map('combine', {\"format\": \"maven2\", \"type\": \"group\"}) | list)\n      }}\n\n- name: Process definitions for docker\n  when: nexus_config_docker | bool\n  block:\n\n    - name: apply defaults to docker proxy repos\n      set_fact:\n        nexus_repos_docker_proxy: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_docker_proxy -%}\n            {{ result.append(_nexus_repos_docker_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to docker hosted repos\n      set_fact:\n        nexus_repos_docker_hosted: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_docker_hosted -%}\n            {{ result.append(_nexus_repos_docker_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to docker group repos\n      set_fact:\n        nexus_repos_docker_group: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_docker_group -%}\n            {{ result.append(_nexus_repos_docker_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: Add docker repositories to global repos list\n      set_fact:\n        _nexus_repos_global_list: >-\n          {{\n            _nexus_repos_global_list | default([])\n            +\n            (nexus_repos_docker_proxy | map('combine', {\"format\": \"docker\", \"type\": \"proxy\"}) | list)\n            +\n            (nexus_repos_docker_hosted | map('combine', {\"format\": \"docker\", \"type\": \"hosted\"}) | list)\n            +\n            (nexus_repos_docker_group | map('combine', {\"format\": \"docker\", \"type\": \"group\"}) | list)\n          }}\n\n- name: Process definitions for pypi\n  when: nexus_config_pypi | bool\n  block:\n\n    - name: apply defaults to pypi proxy repos\n      set_fact:\n        nexus_repos_pypi_proxy: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_pypi_proxy -%}\n            {{ result.append(_nexus_repos_pypi_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to pypi hosted repos\n      set_fact:\n        nexus_repos_pypi_hosted: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_pypi_hosted -%}\n            {{ result.append(_nexus_repos_pypi_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to pypi group repos\n      set_fact:\n        nexus_repos_pypi_group: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_pypi_group -%}\n            {{ result.append(_nexus_repos_pypi_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: Add pypi repositories to global repos list\n      set_fact:\n        _nexus_repos_global_list: >-\n          {{\n            _nexus_repos_global_list | default([])\n            +\n            (nexus_repos_pypi_proxy | map('combine', {\"format\": \"pypi\", \"type\": \"proxy\"}) | list)\n            +\n            (nexus_repos_pypi_hosted | map('combine', {\"format\": \"pypi\", \"type\": \"hosted\"}) | list)\n            +\n            (nexus_repos_pypi_group | map('combine', {\"format\": \"pypi\", \"type\": \"group\"}) | list)\n          }}\n\n- name: Process definitions for raw repositories\n  when: nexus_config_raw | bool\n  block:\n\n    - name: apply defaults to raw proxy repos\n      set_fact:\n        nexus_repos_raw_proxy: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_raw_proxy -%}\n            {{ result.append(_nexus_repos_raw_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to raw hosted repos\n      set_fact:\n        nexus_repos_raw_hosted: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_raw_hosted -%}\n            {{ result.append(_nexus_repos_raw_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to raw group repos\n      set_fact:\n        nexus_repos_raw_group: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_raw_group -%}\n            {{ result.append(_nexus_repos_raw_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: Add raw repositories to global repos list\n      set_fact:\n        _nexus_repos_global_list: >-\n          {{\n            _nexus_repos_global_list | default([])\n            +\n            (nexus_repos_raw_proxy | map('combine', {\"format\": \"raw\", \"type\": \"proxy\"}) | list)\n            +\n            (nexus_repos_raw_hosted | map('combine', {\"format\": \"raw\", \"type\": \"hosted\"}) | list)\n            +\n            (nexus_repos_raw_group | map('combine', {\"format\": \"raw\", \"type\": \"group\"}) | list)\n          }}\n\n- name: Process definitions for rubygems repositories\n  when: nexus_config_rubygems | bool\n  block:\n\n    - name: apply defaults to rubygems proxy repos\n      set_fact:\n        nexus_repos_rubygems_proxy: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_rubygems_proxy -%}\n            {{ result.append(_nexus_repos_rubygems_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to rubygems hosted repos\n      set_fact:\n        nexus_repos_rubygems_hosted: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_rubygems_hosted -%}\n            {{ result.append(_nexus_repos_rubygems_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to rubygems group repos\n      set_fact:\n        nexus_repos_rubygems_group: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_rubygems_group -%}\n            {{ result.append(_nexus_repos_rubygems_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: Add rubygems repositories to global repos list\n      set_fact:\n        _nexus_repos_global_list: >-\n          {{\n            _nexus_repos_global_list | default([])\n            +\n            (nexus_repos_rubygems_proxy | map('combine', {\"format\": \"rubygems\", \"type\": \"proxy\"}) | list)\n            +\n            (nexus_repos_rubygems_hosted | map('combine', {\"format\": \"rubygems\", \"type\": \"hosted\"}) | list)\n            +\n            (nexus_repos_rubygems_group | map('combine', {\"format\": \"rubygems\", \"type\": \"group\"}) | list)\n          }}\n\n- name: Process definitions for bower repositories\n  when: nexus_config_bower | bool\n  block:\n\n    - name: apply defaults to bower proxy repos\n      set_fact:\n        nexus_repos_bower_proxy: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_bower_proxy -%}\n            {{ result.append(_nexus_repos_bower_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to bower hosted repos\n      set_fact:\n        nexus_repos_bower_hosted: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_bower_hosted -%}\n            {{ result.append(_nexus_repos_bower_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to bower group repos\n      set_fact:\n        nexus_repos_bower_group: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_bower_group -%}\n            {{ result.append(_nexus_repos_bower_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: Add bower repositories to global repos list\n      set_fact:\n        _nexus_repos_global_list: >-\n          {{\n            _nexus_repos_global_list | default([])\n            +\n            (nexus_repos_bower_proxy | map('combine', {\"format\": \"bower\", \"type\": \"proxy\"}) | list)\n            +\n            (nexus_repos_bower_hosted | map('combine', {\"format\": \"bower\", \"type\": \"hosted\"}) | list)\n            +\n            (nexus_repos_bower_group | map('combine', {\"format\": \"bower\", \"type\": \"group\"}) | list)\n          }}\n\n- name: Process definitions for npm repositories\n  when: nexus_config_npm | bool\n  block:\n\n    - name: apply defaults to npm proxy repos\n      set_fact:\n        nexus_repos_npm_proxy: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_npm_proxy -%}\n            {{ result.append(_nexus_repos_npm_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to npm hosted repos\n      set_fact:\n        nexus_repos_npm_hosted: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_npm_hosted -%}\n            {{ result.append(_nexus_repos_npm_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to npm group repos\n      set_fact:\n        nexus_repos_npm_group: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_npm_group -%}\n            {{ result.append(_nexus_repos_npm_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: Add npm repositories to global repos list\n      set_fact:\n        _nexus_repos_global_list: >-\n          {{\n            _nexus_repos_global_list | default([])\n            +\n            (nexus_repos_npm_proxy | map('combine', {\"format\": \"npm\", \"type\": \"proxy\"}) | list)\n            +\n            (nexus_repos_npm_hosted | map('combine', {\"format\": \"npm\", \"type\": \"hosted\"}) | list)\n            +\n            (nexus_repos_npm_group | map('combine', {\"format\": \"npm\", \"type\": \"group\"}) | list)\n          }}\n\n- name: Process definitions for nuget repositories\n  when: nexus_config_nuget | bool\n  block:\n\n    - name: apply defaults to nuget proxy repos\n      set_fact:\n        nexus_repos_nuget_proxy: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_nuget_proxy -%}\n            {{ result.append(_nexus_repos_nuget_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to nuget hosted repos\n      set_fact:\n        nexus_repos_nuget_hosted: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_nuget_hosted -%}\n            {{ result.append(_nexus_repos_nuget_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to nuget group repos\n      set_fact:\n        nexus_repos_nuget_group: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_nuget_group -%}\n            {{ result.append(_nexus_repos_nuget_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: Add nuget repositories to global repos list\n      set_fact:\n        _nexus_repos_global_list: >-\n          {{\n            _nexus_repos_global_list | default([])\n            +\n            (nexus_repos_nuget_proxy | map('combine', {\"format\": \"nuget\", \"type\": \"proxy\"}) | list)\n            +\n            (nexus_repos_nuget_hosted | map('combine', {\"format\": \"nuget\", \"type\": \"hosted\"}) | list)\n            +\n            (nexus_repos_nuget_group | map('combine', {\"format\": \"nuget\", \"type\": \"group\"}) | list)\n          }}\n\n- name: Process definitions for gitlfs repositories\n  when: nexus_config_gitlfs | bool\n  block:\n\n    - name: apply defaults to gitlfs hosted repos\n      set_fact:\n        nexus_repos_gitlfs_hosted: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_gitlfs_hosted -%}\n            {{ result.append(_nexus_repos_gitlfs_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: Add gitlfs repositories to global repos list\n      set_fact:\n        _nexus_repos_global_list: >-\n          {{\n            _nexus_repos_global_list | default([])\n            +\n            (nexus_repos_gitlfs_hosted | map('combine', {\"format\": \"gitlfs\", \"type\": \"hosted\"}) | list)\n          }}\n\n- name: Process definitions for yum repositories\n  when: nexus_config_yum | bool\n  block:\n\n    - name: apply defaults to yum proxy repos\n      set_fact:\n        nexus_repos_yum_proxy: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_yum_proxy -%}\n            {{ result.append(_nexus_repos_yum_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to yum hosted repos\n      set_fact:\n        nexus_repos_yum_hosted: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_yum_hosted -%}\n            {{ result.append(_nexus_repos_yum_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to yum group repos\n      set_fact:\n        nexus_repos_yum_group: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_yum_group -%}\n            {{ result.append(_nexus_repos_yum_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: Add yum repositories to global repos list\n      set_fact:\n        _nexus_repos_global_list: >-\n          {{\n            _nexus_repos_global_list | default([])\n            +\n            (nexus_repos_yum_proxy | map('combine', {\"format\": \"yum\", \"type\": \"proxy\"}) | list)\n            +\n            (nexus_repos_yum_hosted | map('combine', {\"format\": \"yum\", \"type\": \"hosted\"}) | list)\n            +\n            (nexus_repos_yum_group | map('combine', {\"format\": \"yum\", \"type\": \"group\"}) | list)\n          }}\n\n- name: Process definitions for apt repositories\n  when: nexus_config_apt | bool\n  block:\n\n    - name: apply defaults to apt proxy repos\n      set_fact:\n        nexus_repos_apt_proxy: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_apt_proxy -%}\n            {{ result.append(_nexus_repos_apt_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: apply defaults to apt hosted repos\n      set_fact:\n        nexus_repos_apt_hosted: >-\n          {%- set result=[] -%}\n          {%- for repo in nexus_repos_apt_hosted -%}\n            {{ result.append(_nexus_repos_apt_defaults | combine(repo)) }}\n          {%- endfor -%}\n          {{ result | to_json | from_json }}\n\n    - name: Add apt repositories to global repos list\n      set_fact:\n        _nexus_repos_global_list: >-\n          {{\n            _nexus_repos_global_list | default([])\n            +\n            (nexus_repos_apt_proxy | map('combine', {\"format\": \"apt\", \"type\": \"proxy\"}) | list)\n            +\n            (nexus_repos_apt_hosted | map('combine', {\"format\": \"apt\", \"type\": \"hosted\"}) | list)\n          }}\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "bbbfafd0812e17e58e7fbe3faa77144f1c5852e5", "filename": "tasks/Win32NT/fetch/zulu-fallback.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: 'Fetch download page'\n  win_uri:\n    url: \"{{ zulu_api_page }}\\\n      /bundles/latest/\\\n      ?version={{ java_major_version }}\\\n      &ext=zip&os=win&\\\n      arch={{ (java_arch == 'x64') | ternary('x64', 'x86') }}\"\n    return_content: true\n    follow_redirects: all\n  register: root_page\n\n- name: Find release url and checksum\n  set_fact:\n    release_url: >-\n      {{ [(root_page.content | from_json).url] + [(root_page.content | from_json).md5_hash] }}\n\n- name: 'Get artifact checksum {{ release_url[1] }}'\n  set_fact:\n    artifact_checksum:\n      content: >-\n        {{ release_url[1] }}\n\n- name: Exit if Zulu version is not found\n  fail:\n    msg: 'Zulu version {{ java_major_version }} not found'\n  when: release_url is not defined\n\n- name: 'Download artifact from {{ release_url[0] }}'\n  win_get_url:\n    url: '{{ release_url }}'\n    dest: '{{ java_download_path }}'\n    force: true\n    checksum: '{{ artifact_checksum.content }}'\n    checksum_algorithm: '{{ checksum_alg }}'\n  register: file_downloaded\n  retries: 20\n  delay: 5\n  until: file_downloaded is succeeded\n  when: ansible_version.full is version('2.8.0', '>=')\n\n- name: Old fetch (Ansible < 2.8)\n  include_tasks: fetch_fallback_old.yml\n  when: ansible_version.full is version('2.8.0', '<')\n"}, {"commit_sha": "bf6e08dcb2440421477b6536ff6a8d11adc2be17", "sha": "b88a3a1133ad6c52a0c52427bb99a23702ad190b", "filename": "roles/zookeeper/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "- name: ensure zookeeper is running (and enable it at boot)\n  sudo: yes\n  service:\n    name: zookeeper\n    state: started\n    enabled: yes\n  tags:\n    - zookeeper\n\n- name: Create zookeeper config file\n  template:\n    src: zoo.cfg.j2\n    dest: /etc/zookeeper/conf/zoo.cfg\n  sudo: yes\n  notify:\n    - restart zookeeper\n  tags:\n    - zookeeper\n\n- name: Create zookeeper myid file\n  copy:\n    content: \"{{zookeeper_id}}\"\n    dest: /etc/zookeeper/conf/myid\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart zookeeper\n  tags:\n    - zookeeper\n\n- name: Set Zookeeper consul service definition\n  sudo: yes\n  template:\n    src: zookeeper-consul.j2\n    dest: \"{{ consul_dir }}/zookeeper.json\"\n  notify:\n    - restart consul\n  tags:\n    - zookeeper\n"}, {"commit_sha": "bf6e08dcb2440421477b6536ff6a8d11adc2be17", "sha": "70bcee16de04a4e151434138160d7a05ca816b12", "filename": "roles/haproxy/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for haproxy\n- name: \"assures {{ consul_template_dir }} dirs exists\"\n  file:\n    path: \"{{ consul_template_dir }}/{{ item.path }}\"\n    state: directory\n  with_items:\n    - { path: 'config' }\n    - { path: 'templates' }\n  tags:\n    - haproxy\n\n- name: upload template config files\n  template:\n    src: consul.cfg.j2\n    dest: \"{{ consul_template_dir }}/config/consul.cfg\"\n    mode: 0644\n  sudo: yes\n  tags:\n    - haproxy\n\n- name: upload static config files\n  copy:\n    src: \"{{ item.src }}\"\n    dest: \"{{ consul_template_dir }}/{{ item.dst }}\"\n    mode: 0644\n  sudo: yes\n  with_items:\n    - { src: haproxy.cfg, dst: 'config/haproxy.cfg' }\n    - { src: haproxy.tmpl, dst: 'templates/haproxy.tmpl' }\n  tags:\n    - haproxy\n\n- name: run haproxy container\n  docker:\n    name: haproxy\n    image: \"{{ haproxy_image }}\"\n    state: started\n    net: host\n    restart_policy: always\n    ports:\n      - \"80:80\"\n    env:\n      HAPROXY_DOMAIN: \"{{ haproxy_domain }}\"\n      CONSUL_TEMPLATE_VERSION: \"{{ consul_template_version }}\"\n      CONSUL_LOGLEVEL: \"{{ consul_template_loglevel }}\"\n      CONSUL_CONNECT: \"{{ consul_backend }}\"\n      CONSUL_CONFIG: \"/config\"\n      SERVICE_NAME: haproxy\n    volumes:\n    - \"{{ consul_template_dir }}/config:/config\"\n    - \"{{ consul_template_dir }}/templates:/templates\"\n  tags:\n    - haproxy\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c7cf4e1078d9ed912441d563bb36e1dca8e31f2c", "filename": "roles/config-nagios-target/tasks/nrpe_docker.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Ensure docker group exists\n  group: \n    name: docker\n    state: present\n  notify: \n  - restart docker\n\n- name: Ensure the nrpe user belongs to the docker group\n  user: \n    name: nrpe\n    groups: docker\n    append: yes\n\n- name: Copy in additional Nagios Docker plugin\n  copy: \n    src: plugins/check_docker_storage\n    dest: /usr/lib64/nagios/plugins/check_docker_storage\n    owner: root\n    group: root\n    mode: 0755\n\n- name: Copy nrpe.d Docker configuration files\n  copy: \n    src: nrpe.d/check_docker.cfg\n    dest: /etc/nrpe.d/check_docker.cfg\n    owner: root\n    group: root\n    mode: 0644\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "226d7ccd3e881e1278fd43af0f17fdceac0485e9", "filename": "playbooks/osp/update-osp-cluster-admin.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: infra_osp_hosts\n  roles:\n  - { role: config-software-src, when: '\"controller\" in osp_roles' }\n  - { role: osp/admin-keystone-domain, when: '\"controller\" in osp_roles' }\n  - { role: osp/admin-volume-type, when: '\"controller\" in osp_roles' }\n  - { role: osp/admin-nova-service, when: '\"controller\" in osp_roles' }\n  - { role: osp/admin-nova-flavor, when: '\"controller\" in osp_roles' }\n  - { role: osp/admin-image, when: '\"controller\" in osp_roles' }\n  - { role: osp/admin-project, when: '\"controller\" in osp_roles' }\n  - { role: osp/admin-network, when: '\"controller\" in osp_roles' }\n  - { role: osp/admin-user, when: '\"controller\" in osp_roles' }\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "c9f8248af32ed9670199c3eda497cda3dfea890f", "filename": "roles/dns-views/defaults/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\nexternal_nsupdate_keys: {}\nnamed_private_recursion: 'yes'\nnamed_public_recursion: 'no'\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "f9ddb9469bec10bc4c1bf14da5f763ed85279869", "filename": "playbooks/openstack/openshift-cluster/config.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: no\n  tasks:\n  - include_vars: vars.yml\n  - include_vars: cluster_hosts.yml\n  - add_host:\n      name: \"{{ item }}\"\n      groups: l_oo_all_hosts\n    with_items: \"{{ g_all_hosts | default([]) }}\"\n\n- hosts: l_oo_all_hosts\n  gather_facts: no\n  tasks:\n  - include_vars: vars.yml\n  - include_vars: cluster_hosts.yml\n\n- include: ../../common/openshift-cluster/config.yml\n  vars:\n    g_nodeonmaster: true\n    g_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n    g_sudo: \"{{ deployment_vars[deployment_type].become }}\"\n    openshift_cluster_id: \"{{ cluster_id }}\"\n    openshift_debug_level: \"{{ debug_level }}\"\n    openshift_deployment_type: \"{{ deployment_type }}\"\n    openshift_hosted_registry_selector: 'type=infra'\n    openshift_hosted_router_selector: 'type=infra'\n    openshift_master_cluster_method: 'native'\n    openshift_use_openshift_sdn: \"{{ lookup('oo_option', 'use_openshift_sdn') }}\"\n    os_sdn_network_plugin_name: \"{{ lookup('oo_option', 'sdn_network_plugin_name') }}\"\n    openshift_use_flannel: \"{{ lookup('oo_option', 'use_flannel') }}\"\n    openshift_use_calico: \"{{ lookup('oo_option', 'use_calico') }}\"\n    openshift_use_fluentd: \"{{ lookup('oo_option', 'use_fluentd') }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "9fba1d922c79f048dd7a39263553b535ccfc5e3c", "filename": "reference-architecture/aws-ansible/playbooks/roles/cloudformation-infra/files/brownfield-byo-bastion.json.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "{\n  \"AWSTemplateFormatVersion\": \"2010-09-09\",\n  \"Parameters\": {\n    \"VpcName\": {\n      \"Type\": \"String\",\n      \"Default\": \"ose-on-aws\"\n    },\n    \"S3BucketName\": {\n      \"Type\": \"String\"\n    },\n    \"S3User\": {\n      \"Type\": \"String\"\n    },\n    \"MasterApiPort\": {\n      \"Type\": \"Number\"\n    },\n    \"MasterHealthTarget\": {\n      \"Type\": \"String\"\n    },\n    \"Route53HostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"PublicHostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"MasterClusterPublicHostname\": {\n      \"Type\": \"String\"\n    },\n    \"MasterClusterHostname\": {\n      \"Type\": \"String\"\n    },\n    \"AppWildcardDomain\": {\n      \"Type\": \"String\"\n    },\n    \"KeyName\": {\n      \"Type\": \"AWS::EC2::KeyPair::KeyName\"\n    },\n    \"MasterInstanceType\": {\n      \"Type\": \"String\",\n      \"Default\": \"t2.medium\"\n    },\n    \"AmiId\": {\n      \"Type\": \"AWS::EC2::Image::Id\"\n    },\n    \"MasterRootVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"50\"\n    },\n    \"MasterDockerVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"MasterUserData\": {\n      \"Type\": \"String\"\n    },\n    \"MasterEtcdVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"MasterEmptyVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"5\"\n    },\n    \"MasterEtcdVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"MasterEmptyVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"MasterDockerVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"MasterRootVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"InfraInstanceType\": {\n      \"Type\": \"String\",\n      \"Default\": \"t2.medium\"\n    },\n    \"InfraRootVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"50\"\n    },\n    \"InfraDockerVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"InfraDockerVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"InfraRootVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"AppNodeInstanceType\": {\n      \"Type\": \"String\",\n      \"Default\": \"t2.medium\"\n    },\n    \"NodeRootVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"50\"\n    },\n    \"NodeDockerVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"NodeDockerVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"NodeUserData\": {\n      \"Type\": \"String\"\n    },\n    \"NodeEmptyVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"NodeEmptyVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"NodeRootVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"Vpc\": {\n      \"Type\": \"String\"\n    },\n    \"PublicSubnet1\": {\n      \"Type\": \"String\"\n    },\n    \"PublicSubnet2\": {\n      \"Type\": \"String\"\n    },\n    \"PublicSubnet3\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet1\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet2\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet3\": {\n      \"Type\": \"String\"\n    },\n    \"BastionSg\": {\n      \"Type\": \"String\"\n    }\n  },\n  \"Resources\": {\n    \"EtcdSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"etcd\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_etcd_sg\"} ]\n      }\n    },\n    \"InfraElbSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Infra Load Balancer\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_router_sg\"} ],\n        \"SecurityGroupIngress\": [\n          {\n            \"IpProtocol\": \"tcp\",\n            \"FromPort\": \"80\",\n            \"ToPort\": \"80\",\n            \"CidrIp\": \"0.0.0.0/0\"\n          },\n          {\n            \"IpProtocol\": \"tcp\",\n            \"FromPort\": \"443\",\n            \"ToPort\": \"443\",\n            \"CidrIp\": \"0.0.0.0/0\"\n          }\n        ]\n      }\n    },\n    \"MasterExtElbSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Master External Load Balancer\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_elb_master_sg\"} ],\n        \"SecurityGroupIngress\": [\n          {\n            \"IpProtocol\": \"tcp\",\n            \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n            \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n            \"CidrIp\": \"0.0.0.0/0\"\n          }\n        ]\n      }\n    },\n    \"MasterIntElbSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Master Internal Load Balancer\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_internal_elb_master_sg\"} ]\n      }\n    },\n    \"InfraSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Infra\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_infra_node_sg\"} ]\n      }\n    },\n    \"NodeSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Node\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_node_sg\"}, {\"Key\": \"KubernetesCluster\", \"Value\": { \"Ref\": \"AWS::StackName\" }}]\n      }\n    },\n    \"MasterSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"Master\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_master_sg\"} ]\n      }\n    },\n    \"InfraElbEgressHTTP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"80\",\n        \"ToPort\": \"80\",\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] }\n      }\n    },\n    \"InfraElbEgressHTTPS\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"443\",\n        \"ToPort\": \"443\",\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] }\n      }\n    },\n    \"ElasticApi\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"9200\",\n        \"ToPort\": \"9200\",\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] }\n      }\n    },\n    \"ElasticCluster\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"9300\",\n        \"ToPort\": \"9300\",\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterExtElbEgressAPI\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterExtElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIntElbEgressAPI\": {\n      \"Type\": \"AWS::EC2::SecurityGroupEgress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterIntElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"DestinationSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIntElbIngressMasters\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterIntElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIntElbIngressNodes\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterIntElbSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"InfraIngressHTTP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"80\",\n        \"ToPort\": \"80\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] }\n      }\n    },\n    \"InfraIngressHTTPS\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"443\",\n        \"ToPort\": \"443\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"InfraElbSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterDaemon\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24007\",\n        \"ToPort\": \"24007\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterManagement\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24008\",\n        \"ToPort\": \"24008\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterSsh\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"2222\",\n        \"ToPort\": \"2222\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterNfs\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"49152\",\n        \"ToPort\": \"49664\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"NodeIngressMasterKubelet\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"10250\",\n        \"ToPort\": \"10250\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"NodeIngressNodeKubelet\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"10250\",\n        \"ToPort\": \"10250\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"NodeIngressNodeVXLAN\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"udp\",\n        \"FromPort\": \"4789\",\n        \"ToPort\": \"4789\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"NodeIngressSsh\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"22\",\n        \"ToPort\": \"22\",\n        \"SourceSecurityGroupId\": {\"Ref\": \"BastionSg\"}\n      }\n    },\n    \"MasterIngressIntLB\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterIntElbSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressExtLB\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterExtElbSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressNodesDNSUDP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"udp\",\n        \"FromPort\": \"8053\",\n        \"ToPort\": \"8053\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressNodesDNSTCP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"8053\",\n        \"ToPort\": \"8053\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressNodesAPITCP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"LoggingTCP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24224\",\n        \"ToPort\": \"24224\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"LoggingUDP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"udp\",\n        \"FromPort\": \"24224\",\n        \"ToPort\": \"24224\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIngressMastersAPITCP\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": { \"Ref\": \"MasterApiPort\" },\n        \"ToPort\": { \"Ref\": \"MasterApiPort\" },\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"EtcdIngressEtcd\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"EtcdSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"2379\",\n        \"ToPort\": \"2379\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"EtcdSG\", \"GroupId\" ] }\n      }\n    },\n    \"EtcdIngressMasters\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"EtcdSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"2379\",\n        \"ToPort\": \"2379\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"MasterSG\", \"GroupId\" ] }\n      }\n    },\n    \"EtcdIngressEtcdPeer\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"EtcdSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"2380\",\n        \"ToPort\": \"2380\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"EtcdSG\", \"GroupId\" ] }\n      }\n    },\n    \"MasterIntElb\": {\n      \"Type\": \"AWS::ElasticLoadBalancing::LoadBalancer\",\n      \"Properties\": {\n        \"CrossZone\": \"true\",\n        \"ConnectionSettings\": {\"IdleTimeout\" : 300},\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_internal_master_elb\"} ],\n        \"HealthCheck\": {\n          \"HealthyThreshold\" : \"2\",\n          \"Interval\" : \"5\",\n          \"Target\" : { \"Ref\": \"MasterHealthTarget\" },\n          \"Timeout\" : \"2\",\n          \"UnhealthyThreshold\" : \"2\"\n        },\n        \"Listeners\":[\n          {\n            \"InstancePort\": { \"Ref\" : \"MasterApiPort\" },\n            \"InstanceProtocol\": \"TCP\",\n            \"LoadBalancerPort\": { \"Ref\" : \"MasterApiPort\" },\n            \"Protocol\": \"TCP\"\n          }\n        ],\n        \"Scheme\": \"internal\",\n        \"SecurityGroups\": [ { \"Ref\": \"MasterIntElbSG\" } ],\n        \"Subnets\": [\n          {\"Ref\": \"PrivateSubnet1\"},\n          {\"Ref\": \"PrivateSubnet2\"},\n          {\"Ref\": \"PrivateSubnet3\"}\n            ],\n        \"Instances\": [\n          {\"Ref\": \"Master01\"},\n          {\"Ref\": \"Master02\"},\n          {\"Ref\": \"Master03\"}\n            ]\n        }\n    },\n    \"MasterExtElb\": {\n      \"Type\": \"AWS::ElasticLoadBalancing::LoadBalancer\",\n      \"Properties\": {\n        \"CrossZone\": \"true\",\n        \"ConnectionSettings\": {\"IdleTimeout\" : 300},\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_master_elb\"} ],\n        \"HealthCheck\": {\n          \"HealthyThreshold\" : \"2\",\n          \"Interval\" : \"5\",\n          \"Target\" : { \"Ref\": \"MasterHealthTarget\" },\n          \"Timeout\" : \"2\",\n          \"UnhealthyThreshold\" : \"2\"\n        },\n        \"Listeners\":[\n          {\n            \"InstancePort\": { \"Ref\" : \"MasterApiPort\" },\n            \"InstanceProtocol\": \"TCP\",\n            \"LoadBalancerPort\": { \"Ref\" : \"MasterApiPort\" },\n            \"Protocol\": \"TCP\"\n          }\n        ],\n        \"SecurityGroups\": [{\"Ref\": \"MasterExtElbSG\"}],\n        \"Subnets\": [\n          {\"Ref\": \"PublicSubnet1\"},\n          {\"Ref\": \"PublicSubnet2\"},\n          {\"Ref\": \"PublicSubnet3\"}\n            ],\n        \"Instances\": [\n          {\"Ref\": \"Master01\"},\n          {\"Ref\": \"Master02\"},\n          {\"Ref\": \"Master03\"}\n            ]\n      }\n    },\n    \"InfraElb\": {\n      \"Type\": \"AWS::ElasticLoadBalancing::LoadBalancer\",\n      \"Properties\": {\n        \"CrossZone\": \"true\",\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"ose_router_elb\"} ],\n        \"HealthCheck\": {\n          \"HealthyThreshold\" : \"2\",\n          \"Interval\" : \"5\",\n          \"Target\" : \"TCP:443\",\n          \"Timeout\" : \"2\",\n          \"UnhealthyThreshold\" : \"2\"\n        },\n        \"Listeners\":[\n          {\n            \"InstancePort\": \"443\",\n            \"InstanceProtocol\": \"TCP\",\n            \"LoadBalancerPort\": \"443\",\n            \"Protocol\": \"TCP\"\n          },\n          {\n            \"InstancePort\": \"80\",\n            \"InstanceProtocol\": \"TCP\",\n            \"LoadBalancerPort\": \"80\",\n            \"Protocol\": \"TCP\"\n          }\n        ],\n        \"SecurityGroups\": [ { \"Ref\": \"InfraElbSG\" } ],\n        \"Subnets\": [\n          {\"Ref\": \"PublicSubnet1\"},\n          {\"Ref\": \"PublicSubnet2\"},\n          {\"Ref\": \"PublicSubnet3\"}\n        \t],\n        \"Instances\": [\n          {\"Ref\": \"InfraNode01\"},\n          {\"Ref\": \"InfraNode02\"},\n          {\"Ref\": \"InfraNode03\"}\n            ]\n      }\n    },\n    \"NodePolicy\": {\n      \"Type\": \"AWS::IAM::Role\",\n      \"Properties\": {\n        \"AssumeRolePolicyDocument\": {\n          \"Version\": \"2012-10-17\",\n          \"Statement\": [\n            {\n              \"Effect\": \"Allow\",\n              \"Principal\": { \"Service\": [ \"ec2.amazonaws.com\" ] },\n              \"Action\": [ \"sts:AssumeRole\" ]\n            }\n          ]\n        },\n        \"Policies\": [\n          {\n            \"PolicyName\": \"node-describe\",\n            \"PolicyDocument\": {\n              \"Version\" : \"2012-10-17\",\n              \"Statement\": [\n                {\n                  \"Effect\": \"Allow\",\n                  \"Action\": [\n                     \"ec2:DescribeInstance*\"\n                  ], \n                  \"Resource\": \"*\"\n                }\n              ]\n            }\n          }\n        ]\n      }\n    },\n    \"MasterPolicy\": {\n      \"Type\": \"AWS::IAM::Role\",\n      \"Properties\": {\n        \"AssumeRolePolicyDocument\": {\n          \"Version\": \"2012-10-17\",\n          \"Statement\": [\n            {\n              \"Effect\": \"Allow\",\n              \"Principal\": { \"Service\": [ \"ec2.amazonaws.com\" ] },\n              \"Action\": [ \"sts:AssumeRole\" ]\n            }\n          ]\n        },\n        \"Policies\": [\n          {\n            \"PolicyName\": \"master-ec2-all\",\n            \"PolicyDocument\": {\n              \"Version\" : \"2012-10-17\",\n              \"Statement\": [\n                {\n                  \"Effect\": \"Allow\",\n                  \"Action\": [\n                     \"ec2:DescribeVolume*\",\n                     \"ec2:CreateVolume\",\n                     \"ec2:CreateTags\",\n                     \"ec2:DescribeInstance*\",\n                     \"ec2:AttachVolume\",\n                     \"ec2:DetachVolume\",\n                     \"ec2:DeleteVolume\",\n                     \"ec2:DescribeSubnets\",\n                     \"ec2:CreateSecurityGroup\",\n                     \"ec2:DescribeSecurityGroups\",\n                     \"elasticloadbalancing:DescribeTags\",\n                     \"elasticloadbalancing:CreateLoadBalancerListeners\",\n                     \"ec2:DescribeRouteTables\",\n                     \"elasticloadbalancing:ConfigureHealthCheck\",\n                     \"ec2:AuthorizeSecurityGroupIngress\",\n                     \"elasticloadbalancing:DeleteLoadBalancerListeners\",\n                     \"elasticloadbalancing:RegisterInstancesWithLoadBalancer\",\n                     \"elasticloadbalancing:DescribeLoadBalancers\",\n                     \"elasticloadbalancing:CreateLoadBalancer\",\n                     \"elasticloadbalancing:DeleteLoadBalancer\",\n                     \"elasticloadbalancing:ModifyLoadBalancerAttributes\",\n                     \"elasticloadbalancing:DescribeLoadBalancerAttributes\"\n                  ],\n                  \"Resource\": \"*\"\n                }\n              ]\n            }\n          }\n        ]\n      }\n    },\n    \"MasterInstanceProfile\": {\n      \"Type\": \"AWS::IAM::InstanceProfile\",\n      \"Properties\": {\n        \"Roles\": [ { \"Ref\": \"MasterPolicy\" } ]\n      }\n    },\n    \"NodeInstanceProfile\": {\n      \"Type\": \"AWS::IAM::InstanceProfile\",\n      \"Properties\": {\n        \"Roles\": [ { \"Ref\": \"NodePolicy\" } ]\n      }\n    },\n    \"RegistryBucket\": {\n    \"Type\": \"AWS::S3::Bucket\",\n    \"Properties\" : {\n       \"BucketName\": { \"Ref\": \"S3BucketName\"}\n                   }\n     },\n    \"Route53Records\": {\n      \"Type\": \"AWS::Route53::RecordSetGroup\",\n      \"DependsOn\": [\n        \"InfraElb\",\n        \"MasterIntElb\",\n        \"Master01\",\n        \"Master02\",\n        \"Master03\",\n        \"MasterExtElb\"\n      ],\n      \"Properties\": {\n        \"HostedZoneName\": { \"Ref\": \"Route53HostedZone\" },\n        \"RecordSets\": [\n          {\n            \"Name\":  { \"Ref\": \"MasterClusterPublicHostname\" },\n            \"Type\": \"A\",\n            \"AliasTarget\": {\n                \"HostedZoneId\": { \"Fn::GetAtt\" : [\"MasterExtElb\", \"CanonicalHostedZoneNameID\"] },\n                \"DNSName\": { \"Fn::GetAtt\" : [\"MasterExtElb\",\"CanonicalHostedZoneName\"] }\n            }\n          },\n          {\n            \"Name\": { \"Ref\": \"MasterClusterHostname\" },\n            \"Type\": \"A\",\n            \"AliasTarget\": {\n                \"HostedZoneId\": { \"Fn::GetAtt\" : [\"MasterIntElb\", \"CanonicalHostedZoneNameID\"] },\n                \"DNSName\": { \"Fn::GetAtt\" : [\"MasterIntElb\",\"DNSName\"] }\n            }\n          },\n          {\n            \"Name\": { \"Ref\": \"AppWildcardDomain\" },\n            \"Type\": \"A\",\n            \"AliasTarget\": {\n                \"HostedZoneId\": { \"Fn::GetAtt\" : [\"InfraElb\", \"CanonicalHostedZoneNameID\"] },\n                \"DNSName\": { \"Fn::GetAtt\" : [\"InfraElb\",\"CanonicalHostedZoneName\"] }\n            }\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-master01\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"Master01\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-master02\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"Master02\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-master03\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"Master03\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-infra-node01\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"InfraNode01\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-infra-node02\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t\t\t\"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"InfraNode02\", \"PrivateIp\"] }]\n          },\n{% for idx in range(1, app_node_count|int + 1) %}\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-app-node{{ '%02d' % idx }}\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n             \"TTL\": \"300\",\n            \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"AppNode{{ '%02d' % idx }}\", \"PrivateIp\"] }]\n          },\n{% endfor %}\n          {\n            \"Name\": {\"Fn::Join\": [\".\", [\"ose-infra-node03\",{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n                        \"TTL\": \"300\",\n                    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"InfraNode03\", \"PrivateIp\"] }]\n          }\n        ]\n      }\n    },\n    \"Master01\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"MasterUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n          \"InstanceType\": {\"Ref\": \"MasterInstanceType\"},\n          \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"MasterSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"EtcdSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet1\"},\n          \"IamInstanceProfile\": { \"Ref\": \"MasterInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-master01\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"master\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"false\",\n              \"VolumeSize\": {\"Ref\": \"MasterEtcdVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterEtcdVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdd\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterEmptyVolType\"}\n            }\n          }\n         ]\n     }\n  },\n    \"Master02\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"MasterUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"MasterInstanceType\"},\n          \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"MasterSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"EtcdSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet2\"},\n          \"IamInstanceProfile\": { \"Ref\": \"MasterInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-master02\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"master\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"false\",\n              \"VolumeSize\": {\"Ref\": \"MasterEtcdVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterEtcdVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdd\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterEmptyVolType\"}\n            }\n          }\n         ]\n     }\n   },\n    \"Master03\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"MasterUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t      \"InstanceType\": {\"Ref\": \"MasterInstanceType\"},\n          \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"MasterSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"EtcdSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet3\"},\n          \"IamInstanceProfile\": { \"Ref\": \"MasterInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-master03\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"master\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"false\",\n              \"VolumeSize\": {\"Ref\": \"MasterEtcdVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterEtcdVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdd\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"MasterEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"MasterEmptyVolType\"}\n            }\n          }\n         ]\n     }\n   },\n    \"InfraNode01\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t      \"InstanceType\": {\"Ref\": \"InfraInstanceType\"},\n\t\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"InfraSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet1\"},\n\t\t  \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-infra-node01\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"infra\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          }\n         ]\n     }\n    },\n    \"InfraNode02\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InfraInstanceType\"},\n\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"InfraSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet2\"},\n\t  \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-infra-node02\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"infra\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          }\n         ]\n     }\n },\n    \"InfraNode03\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n          \"InstanceType\": {\"Ref\": \"InfraInstanceType\"},\n\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }, { \"Fn::GetAtt\" : [\"InfraSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet3\"},\n\t  \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-infra-node03\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"infra\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          }\n         ]\n     }\n },\n{% set rotator = 1 %}\n{% for idx in range(1, app_node_count|int + 1) %}\n{% if rotator == 4 %}\n  {% set rotator = 1 %}\n{% endif %}\n    \"AppNode{{ '%02d' % idx }}\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"DependsOn\": [\"NodeInstanceProfile\"],\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n          \"InstanceType\": {\"Ref\": \"AppNodeInstanceType\"},\n          \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"NodeSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet{{ rotator }}\"},\n          \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [\"ose-app-node{{ '%02d' % idx }}\",{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"KubernetesCluster\",\n              \"Value\": { \"Ref\": \"AWS::StackName\" }\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"app\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          }\n         ]\n     }\n   },\n{% set rotator = rotator + 1 %}\n{% endfor %}\n    \"S3UserName\" : {\n      \"Type\" : \"AWS::IAM::User\",\n      \"Properties\" : {\n        \"Path\" : \"/\",\n        \"UserName\": { \"Ref\": \"S3User\" },\n        \"Policies\" : [ {\n          \"PolicyName\" : \"accessalls3\",\n          \"PolicyDocument\" : {\n            \"Version\": \"2012-10-17\",\n            \"Statement\" : [ {\n              \"Effect\" : \"Allow\",\n              \"Action\" : [ \"s3:*\" ],\n              \"Resource\": \"*\"\n            } ]\n          }\n        } ]\n      }\n    },\n    \"CFNKeys\" : {\n      \"Type\" : \"AWS::IAM::AccessKey\",\n      \"Properties\" : {\n        \"UserName\" : { \"Ref\": \"S3UserName\" }\n      }\n    }\n  },\n  \"Outputs\" : {\n    \"StackVpc\" : {\n      \"Value\" : { \"Ref\" : \"Vpc\" },\n      \"Description\" : \"VPC that was created\"\n    },\n    \"PrivateSubnet1\" : {\n      \"Value\" : { \"Ref\" : \"PrivateSubnet1\" },\n      \"Description\" : \"Private Subnet 1\"\n    },\n    \"PrivateSubnet2\" : {\n      \"Value\" : { \"Ref\" : \"PrivateSubnet2\" },\n      \"Description\" : \"Private Subnet 2\"\n    },\n    \"PrivateSubnet3\" : {\n      \"Value\" : { \"Ref\" : \"PrivateSubnet3\" },\n      \"Description\" : \"Private Subnet 3\"\n    },\n    \"NodeSGId\" : {\n      \"Value\" : { \"Fn::GetAtt\": [ \"NodeSG\", \"GroupId\" ]},\n      \"Description\" : \"Node SG id\"\n    },\n    \"InfraSGId\" : {\n      \"Value\" : { \"Fn::GetAtt\": [ \"InfraSG\", \"GroupId\" ]},\n      \"Description\" : \"Infra Node SG id\"\n    },\n    \"BastionSGId\" : {\n      \"Value\" : { \"Ref\" : \"BastionSg\"},\n      \"Description\" : \"Bastion SG id\"\n    },\n    \"NodeARN\" : {\n      \"Value\" : { \"Ref\": \"NodeInstanceProfile\" },\n      \"Description\": \"ARN for the Node instance profile\"\n    },\n    \"S3UserAccessId\" : {\n      \"Value\" : { \"Ref\" : \"CFNKeys\" },\n      \"Description\" : \"AWSAccessKeyId of user\"\n    },\n    \"S3UserSecretKey\" : {\n      \"Value\" : { \"Fn::GetAtt\" : [\"CFNKeys\", \"SecretAccessKey\"]},\n      \"Description\" : \"AWSSecretKey of new S3\"\n    },\n    \"S3Bucket\" : {\n      \"Value\" : { \"Ref\" : \"RegistryBucket\"},\n      \"Description\" : \"Name of S3 bucket\"\n    }\n }\n}\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "0c0ef30fd2fe76f395ccd5073d7068f06cfd2b78", "filename": "roles/config-software-src/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: prep.yml\n- import_tasks: mount-software.yml\n\n\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "940368af1e31a6fb5daa87064c486ec415e59fef", "filename": "roles/registrator/vars/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# vars file for registrator\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "451d688741d3653b638044fd5a6264b055987734", "filename": "roles/manage-aws-infra/tasks/create-security-group.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "# Create Security Group (SG)\n---\n\n- name: \"Create Security Group for {{ sg_name }}\"\n  ec2_group:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    name: \"{{ sg_name }}\"\n    description: \"{{ sg_description }}\"\n    vpc_id: \"{{ aws_vpc_id }}\"\n    region: \"{{ aws_region }}\"\n    state: \"present\"\n    tags:\n      env_id: \"{{ env_id }}\"\n    rules: \"{{ sg_rules }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "7384f58ee13b686ac0a7e1af8deb48693497761b", "filename": "reference-architecture/vmware-ansible/playbooks/roles/heketi-install/handlers/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: restart heketi\n  service: name=heketi state=restarted\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "63cb001ebf448b67e9c9e9f67e5c683526add7fc", "filename": "roles/user-management/manage-local-user-ssh-authkeys/test/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nuser_name: user1\nclear_text_password: test123\n\nauthorized_keyfile: \"{{ inventory_dir }}/../authorized_keys\"\nreset_keyfile: yes\n\n\nkey_url: \"{{ lookup('file', authorized_keyfile) }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "968e9edab02677c21098852c09986e5b887f87a0", "filename": "reference-architecture/gcp/ansible/playbooks/roles/ssl-certificate/defaults/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\nssl_lb_cert: master-https-lb-cert\nssl_lb_cert_with_prefix: '{{ prefix }}-{{ ssl_lb_cert }}'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "6efa54b2a40a07a90a8c723c99ae01ca3b0b04a6", "filename": "playbooks/notifications/email-notify-single-user.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- include_vars:\n    file: \"{{ email_content_file }}\"\n- set_fact:\n    markdown_content: \"{{ body }}\"\n- include_role:\n    name: notifications/md-to-html\n- set_fact: \n    mail: \"{{ mail | combine( {'subject': title, 'body': md_to_html.html_body_message, 'to': email_to} ) }}\"\n- include_role:\n    name: notifications/send-email\n\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "967e274d47364309b2d3a1744fd5185780ea137a", "filename": "roles/cloud-ec2/tasks/encrypt_image.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Check if the encrypted image already exist\n  ec2_ami_facts:\n    aws_access_key: \"{{ access_key }}\"\n    aws_secret_key: \"{{ secret_key }}\"\n    owners: self\n    region: \"{{ algo_region }}\"\n    filters:\n      state: available\n      \"tag:Algo\": encrypted\n  register: search_crypt\n\n- name: Copy to an encrypted image\n  ec2_ami_copy:\n    aws_access_key: \"{{ access_key }}\"\n    aws_secret_key: \"{{ secret_key }}\"\n    encrypted: yes\n    name: algo\n    kms_key_id: \"{{ kms_key_id | default(omit) }}\"\n    region: \"{{ algo_region }}\"\n    source_image_id: \"{{ (ami_search.images | sort(attribute='creation_date') | last)['image_id'] }}\"\n    source_region: \"{{ algo_region }}\"\n    wait: true\n    tags:\n      Algo: \"encrypted\"\n  register: ami_search_encrypted\n  when: search_crypt.images|length|int == 0\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "953b2a6ae0bb895021c5ece6336b61519452b556", "filename": "reference-architecture/rhv-ansible/playbooks/vars/ovirt-infra-vars.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n###########################\n# Common\n###########################\ncompatibility_version: 4.1\n\n# Data center\ndata_center_name: Default\n\n##########################\n# VM infra\n##########################\ntemplate_cluster: \"{{ rhv_cluster }}\"\ntemplate_name: rhel7\ntemplate_memory: 8GiB\ntemplate_cpu: 1\ntemplate_disk_storage: \"{{ rhv_data_storage }}\"\ntemplate_disk_size: 60GiB\ntemplate_nics:\n  - name: nic1\n    profile_name: ovirtmgmt\n    interface: virtio\n\nmaster_vm:\n  cluster: \"{{ rhv_cluster }}\"\n  template: rhel7\n  memory: 16GiB\n  cores: 2\n  high_availability: true\n  disks:\n    - size: 100GiB\n      storage_domain: \"{{ rhv_data_storage }}\"\n      name: docker_disk\n      interface: virtio\n\nnode_vm:\n  cluster: \"{{ rhv_cluster }}\"\n  template: rhel7\n  memory: 8GiB\n  cores: 2\n  disks:\n    - size: 100GiB\n      storage_domain: \"{{ rhv_data_storage }}\"\n      name: docker_disk\n      interface: virtio\n    - size: 50GiB\n      storage_domain: \"{{ rhv_data_storage }}\"\n      name: localvol_disk\n      interface: virtio\n\n\nvms:\n  # Node VMs\n  - name: openshift-node-0\n    tag: openshift_node\n    profile: \"{{ node_vm }}\"\n    cloud_init:\n      host_name: \"openshift-node-0.{{ public_hosted_zone }}\"\n      authorized_ssh_keys: \"{{ root_ssh_key }}\"\n  - name: openshift-node-1\n    tag: openshift_node\n    profile: \"{{ node_vm }}\"\n    cloud_init:\n      host_name: \"openshift-node-1.{{ public_hosted_zone }}\"\n      authorized_ssh_keys: \"{{ root_ssh_key }}\"\n\n  # Infra VMs\n  - name: openshift-infra-0\n    tag: openshift_infra\n    profile: \"{{ node_vm }}\"\n    cloud_init:\n      host_name: \"openshift-infra-0.{{ public_hosted_zone }}\"\n      authorized_ssh_keys: \"{{ root_ssh_key }}\"\n  - name: openshift-infra-1\n    tag: openshift_infra\n    profile: \"{{ node_vm }}\"\n    cloud_init:\n      host_name: \"openshift-infra-1.{{ public_hosted_zone }}\"\n      authorized_ssh_keys: \"{{ root_ssh_key }}\"\n\n  # Master VMs\n  - name: openshift-master-0\n    profile: \"{{ master_vm }}\"\n    tag: openshift_master\n    cloud_init:\n      host_name: \"openshift-master-0.{{ public_hosted_zone }}\"\n      authorized_ssh_keys: \"{{ root_ssh_key }}\"\n  - name: openshift-master-1\n    tag: openshift_master\n    profile: \"{{ master_vm }}\"\n    cloud_init:\n      host_name: \"openshift-master-1.{{ public_hosted_zone }}\"\n      authorized_ssh_keys: \"{{ root_ssh_key }}\"\n  - name: openshift-master-2\n    tag: openshift_master\n    profile: \"{{ master_vm }}\"\n    cloud_init:\n      host_name: \"openshift-master-2.{{ public_hosted_zone }}\"\n      authorized_ssh_keys: \"{{ root_ssh_key }}\"\n\n  # Load balancer\n  - name: openshift-lb\n    tag: openshift_lb\n    profile: \"{{ node_vm }}\"\n    cloud_init:\n      host_name: \"openshift-lb.{{ public_hosted_zone }}\"\n      authorized_ssh_keys: \"{{ root_ssh_key }}\"\n\naffinity_groups:\n  - name: masters_ag\n    cluster: \"{{ rhv_cluster }}\"\n    vm_enforcing: false\n    vm_rule: negative\n    vms:\n      - openshift-master-0\n      - openshift-master-1\n      - openshift-master-2\n    wait: true\n  - name: infra_ag\n    cluster: \"{{ rhv_cluster }}\"\n    vm_enforcing: false\n    vm_rule: negative\n    vms:\n      - openshift-infra-0\n      - openshift-infra-1\n      - openshift-lb\n    wait: true\n\n  - name: app_ag\n    cluster: \"{{ rhv_cluster }}\"\n    vm_enforcing: false\n    vm_rule: negative\n    vms:\n      - openshift-node-0\n      - openshift-node-1\n...\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "df1d2ead7ffd55131f556f0bcfc6e09646f2729a", "filename": "roles/config-selinux/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# \"targeted\" is the \"default\" policy for most systems\ntarget_policy: 'targeted'\n"}, {"commit_sha": "3c8d04f3e0875a9baf1f1282f6665b2e7d6871a8", "sha": "f91988e3d8cca913d2ddcd60c3edd3e6e905deb3", "filename": "handlers/main.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\n- name: restart ssh\n  service: \"name={{ security_sshd_name }} state=restarted\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "853c0c7c401eeefbaa6ead895d4bda21af863bcd", "filename": "roles/config-routes/tasks/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Include prereqs per the type of OS\"\n  include_tasks: \"{{ distro_file }}\"\n  with_first_found:\n  - files:\n    - prereq-{{ ansible_distribution }}.yml\n    skip: true\n  loop_control:\n    loop_var: distro_file\n\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "5eca5909e4dd4dffaadac1c44395fe980881069e", "filename": "roles/ipareplica/defaults/main.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "---\n# defaults file for ipareplica\n\n### basic ###\nipareplica_no_host_dns: no\nipareplica_skip_conncheck: no\nipareplica_hidden_replica: no\n### server ###\nipareplica_setup_adtrust: no\nipareplica_setup_ca: no\nipareplica_setup_kra: no\nipareplica_setup_dns: no\nipareplica_no_pkinit: no\nipareplica_no_ui_redirect: no\n### client ###\nipaclient_mkhomedir: no\nipaclient_force_join: no\nipaclient_no_ntp: no\n#ipaclient_ssh_trust_dns: no\n#ipaclient_no_ssh: no\n#ipaclient_no_sshd: no\n#ipaclient_no_dns_sshfp: no\nipaclient_ssh_trust_dns: no\n### certificate system ###\nipareplica_skip_schema_check: no\n### dns ###\nipareplica_allow_zone_overlap: no\nipareplica_no_reverse: no\nipareplica_auto_reverse: no\nipareplica_no_forwarders: no\nipareplica_auto_forwarders: no\nipareplica_no_dnssec_validation: no\n### ad trust ###\nipareplica_enable_compat: no\n### uninstall ###\nipareplica_ignore_topology_disconnect: no\nipareplica_ignore_last_of_role: no\n### additional ###\n### packages ###\nipareplica_install_packages: yes\n### firewalld ###\nipareplica_setup_firewalld: yes\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5a9fa044e1447502dbca05548c45199d7d5725ff", "filename": "playbooks/provision-bastion/install.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Prep the bastion host for Ansible runs\"\n  hosts: bastion\n  gather_facts: no\n  roles:\n  - role: ansible/prep-for-ansible\n\n# NOTE: it's important that the docker role is done after the ipa-client role\n- name: 'Install and Configure the bastion host'\n  hosts: bastion\n  roles:\n  - role: config-timezone\n  - role: update-host\n  - role: config-ipa-client\n  - role: config-docker\n  - role: config-docker-compose\n  - role: config-linux-desktop/config-gnome\n  - role: config-linux-desktop/config-xfce\n  - role: config-linux-desktop/config-lxde\n  - role: config-linux-desktop/config-mate\n  - role: config-vnc-server\n  - role: config-packages\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "aeb1bb57ed9ae179105b20496b6a545ac542d112", "filename": "roles/ovirt-common/defaults/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_engine_type: 'ovirt-engine'\novirt_repo_file: []\novirt_repo: []\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "e1319e86ea31a3f1ec8e67f21328e71bb9de583e", "filename": "roles/fsf/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# handlers file for fsf\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "ac953416bc6d93ae8434f79292773445854a392f", "filename": "roles/consul/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for consul\n- name: restart consul\n  service:\n    name: consul\n    state: restarted\n  sudo: yes\n  notify:\n    - wait for consul to listen\n\n- name: wait for consul to listen\n  wait_for:\n    host: \"{{ consul_bind_addr }}\"\n    port: 8500\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "3947be59c959b0fc7fbab7a008ca613617af7782", "filename": "roles/config-openvpn/tests/openvpn-server.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: openvpn_servers\n  roles: \n  - role: config-openvpn\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "7c12125fd686307bcfa5962d403f83489af588bc", "filename": "roles/config-postgresql/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Restart PostgreSQL Service\n  systemd:\n    name: \"{{ postgresql_name }}\"\n    enabled: yes\n    state: restarted\n    daemon_reload: yes\n\n- name: restart firewalld\n  service:\n    name: firewalld\n    state: restarted\n\n- name: restart iptables\n  service:\n    name: iptables\n    state: restarted"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "18b4ef6a05dd8d6a02c2cb546deb02fb262094c7", "filename": "reference-architecture/gcp/ansible/playbooks/roles/ssl-certificate-delete/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: delete ssl certificate\n  command: gcloud --project {{ gcloud_project }} compute ssl-certificates delete {{ ssl_lb_cert_with_prefix }}\n  ignore_errors: true\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "3f26112ae66f9020c895cd6f593480750942482d", "filename": "roles/create_users/test/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- hosts: localhost\n  roles:\n  - role: create_users\n    num_users: 10\n    user_prefix: 'testuser'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7981ab9e0ba3a910d03b465b1c02d108320c5aca", "filename": "roles/config-quay-enterprise/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Verify Storage Type\n  fail:\n    msg: \"Invalid Database Type. 'quay_database_type' must be 'postgres' or 'mysql'\"\n  when: quay_database_type not in [\"postgresql\",\"mysql\"]\n\n- name: Verify Clair Endpoint\n  fail:\n    msg: Clar Endpoint Must Be Specified\n  when: quay_clair_enable is defined and quay_clair_enable|bool and (quay_clair_endpoint is not defined or quay_clair_endpoint|trim == \"\")\n\n- name: Set PostgreSQL Facts\n  set_fact:\n    quay_db_uri: \"{{ postgresql_db_uri }}\"\n  when: quay_database_type == \"postgresql\"\n\n- name: Set MySQL Facts\n  set_fact:\n    quay_db_uri: \"{{ mysql_db_uri }}\"\n  when: quay_database_type == \"mysql\"\n\n- name: Set HTTP Protocol\n  set_fact:\n    quay_http_protocol: \"{{ (quay_ssl_enable|bool)| ternary('https','http') }}\"\n\n- name: Include Container Credentials\n  include_tasks: container_credentials.yml\n  when: (quay_registry_server | trim != \"\") and ((quay_registry_auth | trim != \"\") or (quay_registry_email | trim != \"\"))\n\n- name: Configure Storage Directories\n  file:\n    state: directory\n    owner: root\n    group: root\n    mode: g+rw\n    path: \"{{ item }}\"\n  with_items:\n    - \"{{ quay_config_dir }}\"\n    - \"{{ quay_storage_dir }}\"\n\n- name: Include systemd configurations\n  include_tasks: configure_systemd.yml\n\n- name: Set SSL Facts\n  set_fact:\n    quay_ssl_enable: \"{{ quay_ssl_enable }}\"\n\n- name: Set Fact for Custom SSL Certificates\n  set_fact:\n    quay_ssl_cert_file: \"{{ quay_ssl_cert_file }}\"\n    quay_ssl_key_file: \"{{ quay_ssl_key_file }}\"\n  when: quay_ssl_enable|bool and (quay_ssl_key_file is defined and quay_ssl_key_file|trim != \"\" and quay_ssl_cert_file is defined and quay_ssl_cert_file|trim != \"\")\n\n- name: Create SSL Certificates\n  block:\n    - name: Create Temporary SSL Directory\n      command: mktemp -d /tmp/quay-ssl-XXXXXXX\n      register: quay_ssl_remote_tmp_dir_mktemp\n      delegate_to: \"{{ groups['quay_enterprise'][0] }}\"\n      when: quay_ssl_remote_tmp_dir is undefined and quay_ssl_remote_tmp_dir|trim == \"\"\n\n    - name: Set Fact for Remote SSL Directory\n      set_fact:\n        quay_ssl_remote_tmp_dir: \"{{ quay_ssl_remote_tmp_dir if quay_ssl_remote_tmp_dir is defined and quay_ssl_remote_tmp_dir|trim == '' else quay_ssl_remote_tmp_dir_mktemp.stdout }}\"\n      when: quay_ssl_remote_tmp_dir is undefined and quay_ssl_remote_tmp_dir|trim == \"\"\n\n    - name: Create SSL Certificate\n      command: openssl req -nodes -x509 -newkey rsa:4096 -keyout {{ quay_ssl_remote_tmp_dir }}/ssl.key -out {{ quay_ssl_remote_tmp_dir }}/ssl.cert -subj \"/C={{ quay_ssl_generate_country }}/ST={{ quay_ssl_generate_state }}/L={{ quay_ssl_generate_city }}/O={{ quay_ssl_generate_organization }}/OU={{ quay_ssl_generate_organizational_unit }}/CN={{ quay_server_hostname }}\" -days {{ quay_ssl_generate_days_validity }}\n      delegate_to: \"{{ groups['quay_enterprise'][0] }}\"\n\n    - name: Fetch SSL Certifictes\n      fetch:\n        src:  \"{{ item.src }}\"\n        dest: \"{{ item.dest }}\"\n        flat: true\n        fail_on_missing: yes\n      delegate_to: \"{{ groups['quay_enterprise'][0] }}\"\n      run_once: true\n      with_items:\n        - { src: \"{{ quay_ssl_remote_tmp_dir }}/ssl.key\", dest: \"{{ quay_ssl_local_tmp_dir }}/ssl.key\" }\n        - { src: \"{{ quay_ssl_remote_tmp_dir }}/ssl.cert\", dest: \"{{ quay_ssl_local_tmp_dir }}/ssl.cert\" }\n\n    - name: Delete Remote SSL Certificates\n      file:\n        state: absent\n        path: \"{{ quay_ssl_remote_tmp_dir }}\"\n      delegate_to: \"{{ groups['quay_enterprise'][0] }}\"\n\n    - name: Set Fact for Custom SSL Certificates\n      set_fact:\n        quay_ssl_cert_file: \"{{ quay_ssl_local_tmp_dir }}/ssl.cert\"\n        quay_ssl_key_file: \"{{ quay_ssl_local_tmp_dir }}/ssl.key\"\n  when: quay_ssl_enable|bool and (quay_ssl_key_file is not defined or quay_ssl_key_file|trim == \"\" or quay_ssl_cert_file is not defined or quay_ssl_cert_file|trim == \"\")\n\n- name: Copy SSL Certificates\n  copy:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: root\n    group: root\n    mode: g+rw\n  notify: Restart quay service\n  with_items:\n    - { src: \"{{ quay_ssl_key_file }}\", dest: \"{{ quay_config_dir }}/ssl.key\" }\n    - { src: \"{{ quay_ssl_cert_file }}\", dest: \"{{ quay_config_dir }}/ssl.cert\" }\n  when: quay_ssl_enable|bool\n\n- name: Check if Quay configuration exists\n  stat:\n    path: \"{{ quay_config_dir }}/config.yaml\"\n  register: quay_config_stat_result\n\n- name: Configure BitTorrent Pepper Value\n  set_fact:\n    bittorrent_filename_pepper: \"{{ 'hostname' | to_uuid | upper }}\"\n\n- name: Setup initial quay configuration file\n  template:\n    src: config.yaml.j2\n    dest: \"{{ quay_config_dir }}/config.yaml\"\n    owner: root\n    group: root\n    mode: g+rw\n  notify: Restart quay service\n  when: not quay_config_stat_result.stat.exists\n\n- name: Include firewall tasks\n  include_tasks: firewall.yml\n\n- name: Setup Initial User and Configuration\n  include_tasks: complete_setup.yml\n  when: not quay_config_stat_result.stat.exists and quay_superuser_username is defined and quay_superuser_username|trim != \"\" and quay_superuser_password is defined and quay_superuser_password|trim != \"\" and quay_superuser_email is defined and quay_superuser_email|trim != \"\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "7ecfb99038ef06802cbef691b51ac332a4f1f993", "filename": "roles/network/tasks/redetect.yml", "repository": "iiab/iiab", "decoded_content": "# The preferred method of disabling the LAN would be to set iiab_lan_enabled:\n# False before getting here but we are here...\n# Well if we got here something changed with the gateway and ifcfg-WAN maybe\n# no longer accurate. Note if DEVICE= is any ifcfg files the listed DEVICE\n# becomes bound to the NAME in the ifcfg file. With the LAN files out of the\n# way we can try the interfaces one by one starting with device_gw.\n\n# Setting up three way conditions with the results\n# skipped|changed|failed\n# failure results in blowing away the ifcfg-WAN so lets make sure...\n\n# We only got here by way of no detected gateway, lets see if we can pick-up\n# transient change like cable issues.\n\n- name: BAD DHCP defaults\n  set_fact:\n    dhcp_good: False\n\n# don't shoot ourselves in the foot....\n- name: Disable dhcp server just because\n  service: name=dhcpd state=stopped\n\n### clear all connections first\n# We should have the LAN torndown at this point.\n\n- name: No ifcfg-WAN known\n  debug: msg=\"NO WAN known\"\n  when: not has_WAN\n\n- name: Finding connection name for WiFi AP gateway first\n  shell: egrep -rn NAME /etc/sysconfig/network-scripts/{{ has_wifi_gw }} |  gawk -F '=' '{print $2}'\n  register: ap_name\n  when: has_wifi_gw != \"none\" and has_ifcfg_gw != \"none\"\n\n- name: Trying WiFi first\n  shell: nmcli conn up id {{ ap_name.stdout }}\n  register: try_wifi\n  ignore_errors: yes\n  when: ap_name is defined and ap_named.changed\n\n- name: Checking for WiFi gateway\n  shell: sleep 5 | ip route | grep default | awk '{print $5}'\n  register: dhcp_wifi_results\n  when: try_wifi is defined and try_wifi.changed\n\n# We have the DEVICE?\n- name: Now setting iiab_wan_iface based on WiFi\n  set_fact:\n     iiab_wan_iface: \"{{ dhcp_wifi_results.stdout }}\"\n     dhcp_good: True\n  when: dhcp_wifi_results.stdout is defined and dhcp_wifi_results.stdout != \"\"\n\n- name: Trying ifcfg-WAN second\n  shell: nmcli conn up id iiab-WAN\n  register: dhcp_WAN\n  ignore_errors: yes\n  when: has_WAN\n\n- name: BAD ifcfg-WAN\n  debug: msg=\"BAD WAN\"\n  when: dhcp_WAN is defined and dhcp_WAN|failed\n\n- name: Delete ifcfg-WAN\n  shell: rm -f /etc/sysconfig/network-scripts/ifcfg-WAN\n  when: dhcp_WAN is defined and dhcp_WAN|failed and wan_ip == \"dhcp\"\n\n- name: Setting no ifcfg-WAN\n  set_fact:\n      has_WAN: False\n  when: dhcp_WAN is defined and dhcp_WAN|failed and wan_ip == \"dhcp\"\n\n- name: Interface list\n  shell: ls /sys/class/net | grep -v -e lo -e br -e tun\n  register: adapter_list\n\n# wired devices with no wire plugged in fail here\n- name: Not risking an active device dropping all devices\n  shell: nmcli d delete {{ item|trim }}\n  ignore_errors: True\n  when: item|trim != iiab_wireless_lan_iface and not dhcp_good and wan_ip == \"dhcp\"\n  with_items:\n      - \"{{ adapter_list.stdout_lines }}\"\n\n# monitor-connection-files defaults to no with F21, F18-F20 defaults to yes\n- name: Reloading nmcli for deleted files\n  shell: nmcli con reload\n  when: not installing and not no_NM_reload\n\n# wired devices with no wire plugged in fail here\n# discovered_wireless_iface might need work\n- name: Try dhcp on all wired devices\n  shell: nmcli d connect {{ item|trim }}\n  ignore_errors: True\n  when: item|trim != discovered_wireless_iface and item|trim != iiab_wireless_lan_iface and not dhcp_good and wan_ip == \"dhcp\"\n  with_items:\n      - \"{{ adapter_list.stdout_lines }}\"\n\n# This should be neat on a VM with 2 bridged interfaces.\n- name: Checking for gateway\n  shell: sleep 5 | ip route | grep default | awk '{print $5}'\n  register: dhcp_1BY1_results\n  when: not has_WAN and not dhcp_good\n\n# We have the DEVICE?\n- name: Now setting iiab_wan_iface via nmcli\n  set_fact:\n     iiab_wan_iface: \"{{ dhcp_1BY1_results.stdout }}\"\n     dhcp_good: True\n  when: dhcp_1BY1_results.stdout is defined and dhcp_1BY1_results.stdout != \"\" and not has_WAN\n\n- name: Find gateway config based on device\n  shell: egrep -rn \"{{ iiab_wan_iface }}\" /etc/sysconfig/network-scripts/ifcfg* | gawk -F ':' '{print $1}'\n  register: ifcfg_dhcp_device\n  ignore_errors: True\n  changed_when: False\n  when: dhcp_good\n\n- name: Setting has ifcfg gw based on device if found\n  set_fact:\n    has_ifcfg_gw: \"{{ item|trim }}\"\n  when: dhcp_good and ifcfg_dhcp_device is defined and item|trim != \"\"\n  with_items:\n      - \"{{ ifcfg_dhcp_device.stdout_lines }}\"\n  ignore_errors: True\n\n# wired devices with no wire plugged in fail here\n- name: Disconnect wired devices\n  shell: nmcli c down id \"System{{ item|trim }}\"\n  ignore_errors: True\n  when: item|trim != iiab_wireless_lan_iface and item|trim != iiab_wan_iface and wan_ip == \"dhcp\"\n  with_items:\n      - \"{{ adapter_list.stdout_lines }}\"\n\n### keep at end.\n### If dhcp fails the single interface will become LAN again because we didn't prevent the creation\n# Now disable LAN if single interface\n- name: DHCP found on Single interface forcing LAN disabled\n  set_fact:\n     iiab_lan_iface: \"none\"\n  when: dhcp_good  and adapter_count.stdout|int == \"1\"\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "a7425708f691a73345558f219456c9e98fa8963e", "filename": "tasks/Linux/fetch/web.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Download artifact from web\n  get_url:\n    url: '{{ transport_web }}'\n    dest: '{{ java_download_path }}'\n  register: file_downloaded\n  retries: 5\n  delay: 2\n  until: file_downloaded is succeeded\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "3a286be7216ca11aeed7ff733a093b7f0d7a5cd9", "filename": "roles/vpn/tasks/openssl.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- block:\n  - name: Set subjectAltName as a fact\n    set_fact:\n      subjectAltName: \"{{ subjectAltName_IP }}{% if ipv6_support %},IP:{{ ansible_default_ipv6['address'] }}{% endif %}{% if domain and subjectAltName_DNS %},DNS:{{ subjectAltName_DNS }}{% endif %}\"\n    tags: always\n\n  - name: Ensure the pki directory does not exist\n    file:\n      dest: configs/{{ IP_subject_alt_name }}/pki\n      state: absent\n    when: keys_clean_all|bool == True\n\n  - name: Ensure the pki directories exist\n    file:\n      dest: \"configs/{{ IP_subject_alt_name }}/pki/{{ item }}\"\n      state: directory\n      recurse: yes\n      mode: '0700'\n    with_items:\n      - ecparams\n      - certs\n      - crl\n      - newcerts\n      - private\n      - public\n      - reqs\n\n  - name: Ensure the files exist\n    file:\n      dest: \"configs/{{ IP_subject_alt_name }}/pki/{{ item }}\"\n      state: touch\n    with_items:\n      - \".rnd\"\n      - \"private/.rnd\"\n      - \"index.txt\"\n      - \"index.txt.attr\"\n      - \"serial\"\n\n  - name: Generate the openssl server configs\n    template:\n      src: openssl.cnf.j2\n      dest: \"configs/{{ IP_subject_alt_name }}/pki/openssl.cnf\"\n\n  - name: Build the CA pair\n    shell: >\n      umask 077;\n      {{ openssl_bin }} ecparam -name secp384r1 -out ecparams/secp384r1.pem &&\n      {{ openssl_bin }} req -utf8 -new\n      -newkey ec:ecparams/secp384r1.pem\n      -config <(cat openssl.cnf <(printf \"[basic_exts]\\nsubjectAltName={{ subjectAltName }}\"))\n      -keyout private/cakey.pem\n      -out cacert.pem -x509 -days 3650\n      -batch\n      -passout pass:\"{{ CA_password }}\" &&\n      touch {{ IP_subject_alt_name }}_ca_generated\n    args:\n      chdir: \"configs/{{ IP_subject_alt_name }}/pki/\"\n      creates: \"{{ IP_subject_alt_name }}_ca_generated\"\n      executable: bash\n\n  - name: Copy the CA certificate\n    copy:\n      src: \"configs/{{ IP_subject_alt_name }}/pki/cacert.pem\"\n      dest: \"configs/{{ IP_subject_alt_name }}/cacert.pem\"\n      mode: 0600\n\n  - name: Generate the serial number\n    shell: echo 01 > serial && touch serial_generated\n    args:\n      chdir: \"configs/{{ IP_subject_alt_name }}/pki/\"\n      creates: serial_generated\n\n  - name: Build the server pair\n    shell: >\n      umask 077;\n      {{ openssl_bin }} req -utf8 -new\n      -newkey ec:ecparams/secp384r1.pem\n      -config <(cat openssl.cnf <(printf \"[basic_exts]\\nsubjectAltName={{ subjectAltName }}\"))\n      -keyout private/{{ IP_subject_alt_name }}.key\n      -out reqs/{{ IP_subject_alt_name }}.req -nodes\n      -passin pass:\"{{ CA_password }}\"\n      -subj \"/CN={{ IP_subject_alt_name }}\" -batch &&\n      {{ openssl_bin }} ca -utf8\n      -in reqs/{{ IP_subject_alt_name }}.req\n      -out certs/{{ IP_subject_alt_name }}.crt\n      -config <(cat openssl.cnf <(printf \"[basic_exts]\\nsubjectAltName={{ subjectAltName }}\"))\n      -days 3650 -batch\n      -passin pass:\"{{ CA_password }}\"\n      -subj \"/CN={{ IP_subject_alt_name }}\" &&\n      touch certs/{{ IP_subject_alt_name }}_crt_generated\n    args:\n      chdir: \"configs/{{ IP_subject_alt_name }}/pki/\"\n      creates: certs/{{ IP_subject_alt_name }}_crt_generated\n      executable: bash\n\n  - name: Build the client's pair\n    shell: >\n      umask 077;\n      {{ openssl_bin }} req -utf8 -new\n      -newkey ec:ecparams/secp384r1.pem\n      -config <(cat openssl.cnf <(printf \"[basic_exts]\\nsubjectAltName={{ subjectAltName_USER }}\"))\n      -keyout private/{{ item }}.key\n      -out reqs/{{ item }}.req -nodes\n      -passin pass:\"{{ CA_password }}\"\n      -subj \"/CN={{ item }}\" -batch &&\n      {{ openssl_bin }} ca -utf8\n      -in reqs/{{ item }}.req\n      -out certs/{{ item }}.crt\n      -config <(cat openssl.cnf <(printf \"[basic_exts]\\nsubjectAltName={{ subjectAltName_USER }}\"))\n      -days 3650 -batch\n      -passin pass:\"{{ CA_password }}\"\n      -subj \"/CN={{ item }}\" &&\n      touch certs/{{ item }}_crt_generated\n    args:\n      chdir: \"configs/{{ IP_subject_alt_name }}/pki/\"\n      creates: certs/{{ item }}_crt_generated\n      executable: bash\n    with_items: \"{{ users }}\"\n\n  - name: Create links for the private keys\n    file:\n      src: \"pki/private/{{ item }}.key\"\n      dest: \"configs/{{ IP_subject_alt_name }}/{{ item }}.ssh.pem\"\n      state: link\n      force: true\n    with_items: \"{{ users }}\"\n\n  - name: Build openssh public keys\n    openssl_publickey:\n      path:  \"configs/{{ IP_subject_alt_name }}/pki/public/{{ item }}.pub\"\n      privatekey_path: \"configs/{{ IP_subject_alt_name }}/pki/private/{{ item }}.key\"\n      format: OpenSSH\n    with_items: \"{{ users }}\"\n\n  - name: Build the client's p12\n    shell: >\n      umask 077;\n      {{ openssl_bin }} pkcs12\n      -in certs/{{ item }}.crt\n      -inkey private/{{ item }}.key\n      -export\n      -name {{ item }}\n      -out private/{{ item }}.p12\n      -passout pass:\"{{ p12_export_password }}\"\n    args:\n      chdir: \"configs/{{ IP_subject_alt_name }}/pki/\"\n      executable: bash\n    with_items: \"{{ users }}\"\n    register: p12\n\n  - name: Copy the p12 certificates\n    copy:\n      src: \"configs/{{ IP_subject_alt_name }}/pki/private/{{ item }}.p12\"\n      dest: \"configs/{{ IP_subject_alt_name }}/{{ item }}.p12\"\n      mode: 0600\n    with_items:\n      - \"{{ users }}\"\n\n  - name: Get active users\n    shell: >\n      grep ^V index.txt |\n      grep -v \"{{ IP_subject_alt_name }}\" |\n      awk '{print $5}' |\n      sed 's/\\/CN=//g'\n    args:\n      chdir: \"configs/{{ IP_subject_alt_name }}/pki/\"\n    register: valid_certs\n\n  - name: Revoke non-existing users\n    shell: >\n      {{ openssl_bin }} ca -gencrl\n      -config <(cat openssl.cnf <(printf \"[basic_exts]\\nsubjectAltName={{ subjectAltName_USER }}\"))\n      -passin pass:\"{{ CA_password }}\"\n      -revoke certs/{{ item }}.crt\n      -out crl/{{ item }}.crt\n    register: gencrl\n    args:\n      chdir: configs/{{ IP_subject_alt_name }}/pki/\n      creates: crl/{{ item }}.crt\n      executable: bash\n    when: item not in users\n    with_items: \"{{ valid_certs.stdout_lines }}\"\n\n  - name: Genereate new CRL file\n    shell: >\n      {{ openssl_bin }} ca -gencrl\n      -config <(cat openssl.cnf <(printf \"[basic_exts]\\nsubjectAltName=DNS:{{ IP_subject_alt_name }}\"))\n      -passin pass:\"{{ CA_password }}\"\n      -out crl/algo.root.pem\n    when:\n      - gencrl is defined\n      - gencrl.changed\n    args:\n      chdir: configs/{{ IP_subject_alt_name }}/pki/\n      executable: bash\n  delegate_to: localhost\n  become: no\n  vars:\n    ansible_python_interpreter: \"{{ ansible_playbook_python }}\"\n\n- name: Copy the CRL to the vpn server\n  copy:\n    src: configs/{{ IP_subject_alt_name }}/pki/crl/algo.root.pem\n    dest: \"{{ config_prefix|default('/') }}etc/ipsec.d/crls/algo.root.pem\"\n  when:\n    - gencrl is defined\n    - gencrl.changed\n  notify:\n    - rereadcrls\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "d370575a7909713f535e66f5433bc7a3c7f7557c", "filename": "reference-architecture/aws-ansible/playbooks/roles/cloudformation-infra/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Render Greenfield Template\n  template: src=roles/cloudformation-infra/files/greenfield.json.j2 dest=roles/cloudformation-infra/files/{{ stack_name }}-greenfield.json\n  when: create_vpc == \"yes\"\n\n- name: Render Brownfield Template\n  template: src=roles/cloudformation-infra/files/brownfield.json.j2 dest=roles/cloudformation-infra/files/{{ stack_name }}-brownfield.json\n  when: create_vpc == \"no\" and byo_bastion == \"no\"\n\n- name: Render Brownfield BYO Bastion Template\n  template: src=roles/cloudformation-infra/files/brownfield-byo-bastion.json.j2 dest=roles/cloudformation-infra/files/{{ stack_name }}-brownfield-byo-bastion.json\n  when: create_vpc == \"no\" and byo_bastion == \"yes\"\n\n- name: Create Greenfield Infrastructure\n  cloudformation:\n    stack_name: \"{{ stack_name }}\"\n    state: \"present\"\n    region: \"{{ region }}\"\n    template: \"roles/cloudformation-infra/files/{{ stack_name }}-greenfield.json\"\n    template_parameters:\n      Region: \"{{ region }}\"\n      Route53HostedZone: \"{{ public_hosted_zone }}.\"\n      PublicHostedZone: \"{{ public_hosted_zone }}\"\n      MasterApiPort: \"{{ console_port }}\"\n      MasterHealthTarget: \"TCP:{{ console_port }}\"\n      MasterClusterHostname: \"{{ openshift_master_cluster_hostname }}\"\n      MasterClusterPublicHostname: \"{{ openshift_master_cluster_public_hostname }}\"\n      AppWildcardDomain: \"*.{{ wildcard_zone }}\"\n      VpcCidrBlock: \"{{ cidr_block }}\"\n      VpcName: \"{{ vpc_prefix }}\"\n      SubnetCidrBlocks: \"{{ subnet_blocks }}\"\n      KeyName: \"{{ keypair }}\"\n      MasterInstanceType: \"{{ master_instance_type }}\"\n      AmiId: \"{{ ami }}\"\n      BastionUserData: \"{{ lookup('file', 'user_data_bastion.yml') | b64encode }}\"\n      MasterDockerVolSize: \"{{ docker_storage }}\"\n      MasterDockerVolType: gp2\n      MasterEtcdVolSize: \"{{ etcd_storage }}\"\n      MasterEtcdVolType: gp2\n      MasterUserData: \"{{ lookup('file', 'user_data_master.yml') | b64encode }}\"\n      InfraInstanceType: \"{{ node_instance_type }}\"\n      InfraDockerVolSize: \"{{ docker_storage }}\"\n      InfraDockerVolType: gp2\n      NodeUserData: \"{{ lookup('file', 'user_data_node.yml') | b64encode }}\"\n      AppNodeInstanceType: \"{{ app_instance_type }}\"\n      NodeDockerVolSize: \"{{ docker_storage }}\"\n      NodeDockerVolType: gp2\n      S3BucketName: \"{{ s3_bucket_name }}\"\n      S3User: \"{{ s3_username }}\"\n  when: create_vpc == \"yes\"\n\n- name: Create Brownfield Infrastructure\n  cloudformation:\n    stack_name: \"{{ stack_name }}\"\n    state: \"present\"\n    region: \"{{ region }}\"\n    template: \"roles/cloudformation-infra/files/{{ stack_name }}-brownfield.json\"\n    template_parameters:\n      Vpc: \"{{ vpc_id }}\"\n      PrivateSubnet1: \"{{ private_subnet_id1 }}\"\n      PrivateSubnet2: \"{{ private_subnet_id2 }}\"\n      PrivateSubnet3: \"{{ private_subnet_id3 }}\"\n      PublicSubnet1: \"{{ public_subnet_id1 }}\"\n      PublicSubnet2: \"{{ public_subnet_id2 }}\"\n      PublicSubnet3: \"{{ public_subnet_id3 }}\"\n      Route53HostedZone: \"{{ public_hosted_zone }}.\"\n      PublicHostedZone: \"{{ public_hosted_zone }}\"\n      MasterApiPort: \"{{ console_port }}\"\n      MasterHealthTarget: \"TCP:{{ console_port }}\"\n      MasterClusterHostname: \"{{ openshift_master_cluster_hostname }}\"\n      MasterClusterPublicHostname: \"{{ openshift_master_cluster_public_hostname }}\"\n      AppWildcardDomain: \"*.{{ wildcard_zone }}\"\n      KeyName: \"{{ keypair }}\"\n      MasterInstanceType: \"{{ master_instance_type }}\"\n      AmiId: \"{{ ami }}\"\n      BastionInstanceType: \"{{ bastion_instance_type }}\"\n      BastionUserData: \"{{ lookup('file', 'user_data_bastion.yml') | b64encode }}\"\n      MasterRootVolSize: \"50\"\n      BastionRootVolSize: \"10\"\n      BastionRootVolType: gp2\n      MasterRootVolType: gp2\n      MasterDockerVolSize: \"{{ docker_storage }}\"\n      MasterDockerVolType: gp2\n      MasterEtcdVolSize: \"{{ etcd_storage }}\"\n      MasterEtcdVolType: gp2\n      MasterEmptyVolSize: \"5\"\n      MasterEmptyVolType: gp2\n      MasterUserData: \"{{ lookup('file', 'user_data_master.yml') | b64encode }}\"\n      InfraInstanceType: \"{{ node_instance_type }}\"\n      InfraRootVolSize: \"50\"\n      InfraRootVolType: gp2\n      InfraDockerVolSize: \"{{ docker_storage }}\"\n      InfraDockerVolType: gp2\n      NodeEmptyVolSize: \"{{ emptydir_storage }}\"\n      NodeEmptyVolType: gp2\n      NodeUserData: \"{{ lookup('file', 'user_data_node.yml') | b64encode }}\"\n      AppNodeInstanceType: \"{{ app_instance_type }}\"\n      NodeRootVolSize: \"50\"\n      NodeRootVolType: gp2\n      NodeDockerVolSize: \"{{ docker_storage }}\"\n      NodeDockerVolType: gp2\n      S3BucketName: \"{{ s3_bucket_name }}\"\n      S3User: \"{{ s3_username }}\"\n  when: create_vpc == \"no\" and byo_bastion == \"no\"\n\n- name: Create Brownfield Infrastructure with existing bastion\n  cloudformation:\n    stack_name: \"{{ stack_name }}\"\n    state: \"present\"\n    region: \"{{ region }}\"\n    template: \"roles/cloudformation-infra/files/{{ stack_name }}-brownfield-byo-bastion.json\"\n    template_parameters:\n      Vpc: \"{{ vpc_id }}\"\n      BastionSg: \"{{ bastion_sg }}\"\n      PrivateSubnet1: \"{{ private_subnet_id1 }}\"\n      PrivateSubnet2: \"{{ private_subnet_id2 }}\"\n      PrivateSubnet3: \"{{ private_subnet_id3 }}\"\n      PublicSubnet1: \"{{ public_subnet_id1 }}\"\n      PublicSubnet2: \"{{ public_subnet_id2 }}\"\n      PublicSubnet3: \"{{ public_subnet_id3 }}\"\n      Route53HostedZone: \"{{ public_hosted_zone }}.\"\n      PublicHostedZone: \"{{ public_hosted_zone }}\"\n      MasterApiPort: \"{{ console_port }}\"\n      MasterHealthTarget: \"TCP:{{ console_port }}\"\n      MasterClusterHostname: \"{{ openshift_master_cluster_hostname }}\"\n      MasterClusterPublicHostname: \"{{ openshift_master_cluster_public_hostname }}\"\n      AppWildcardDomain: \"*.{{ wildcard_zone }}\"\n      KeyName: \"{{ keypair }}\"\n      MasterInstanceType: \"{{ master_instance_type }}\"\n      AmiId: \"{{ ami }}\"\n      MasterRootVolSize: \"50\"\n      MasterRootVolType: gp2\n      MasterDockerVolSize: \"{{ docker_storage }}\"\n      MasterDockerVolType: gp2\n      MasterEtcdVolSize: \"{{ etcd_storage }}\"\n      MasterEtcdVolType: gp2\n      MasterEmptyVolSize: \"5\"\n      MasterEmptyVolType: gp2\n      MasterUserData: \"{{ lookup('file', 'user_data_master.yml') | b64encode }}\"\n      InfraInstanceType: \"{{ node_instance_type }}\"\n      InfraRootVolSize: \"50\"\n      InfraRootVolType: gp2\n      InfraDockerVolSize: \"{{ docker_storage }}\"\n      InfraDockerVolType: gp2\n      NodeEmptyVolSize: \"{{ emptydir_storage }}\"\n      NodeEmptyVolType: gp2\n      NodeUserData: \"{{ lookup('file', 'user_data_node.yml') | b64encode }}\"\n      AppNodeInstanceType: \"{{ app_instance_type }}\"\n      NodeRootVolSize: \"50\"\n      NodeRootVolType: gp2\n      NodeDockerVolSize: \"{{ docker_storage }}\"\n      NodeDockerVolType: gp2\n      S3BucketName: \"{{ s3_bucket_name }}\"\n      S3User: \"{{ s3_username }}\"\n  when: create_vpc == \"no\" and byo_bastion == \"yes\"\n\n- name: Add App Node\n  cloudformation:\n    stack_name: \"{{ stack_name }}-{{ shortname }}\"\n    state: \"present\"\n    region: \"{{ region }}\"\n    template: \"roles/cloudformation-infra/files/add-node.json\"\n    template_parameters:\n      NodeName: \"{{ fqdn }}\"\n      NodeSg: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['NodeSGId'] | default(node_sg) }}\"\n      Subnet: \"{{ subnet_id }}\"\n      Route53HostedZone: \"{{ public_hosted_zone }}.\"\n      KeyName: \"{{ keypair }}\"\n      AmiId: \"{{ ami }}\"\n      InstanceType: \"{{ node_instance_type }}\"\n      NodeEmptyVolSize: \"{{ emptydir_storage }}\"\n      NodeEmptyVolType: gp2\n      NodeUserData: \"{{ lookup('file', 'user_data_node.yml') | b64encode }}\"\n      NodeRootVolSize: \"50\"\n      NodeRootVolType: gp2\n      NodeDockerVolSize: \"{{ docker_storage }}\"\n      NodeDockerVolType: gp2\n      NodeInstanceProfile: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['NodeARN'] | default(iam_role) }}\"\n      NodeType: \"{{ node_type }}\"\n    tags:\n      KubernetesCluster: \"{{ stack_name }}\"\n  when: create_vpc == \"no\" and add_node == \"yes\" and node_type == \"app\"\n\n- name: Add Infra Node\n  cloudformation:\n    stack_name: \"{{ stack_name }}-{{ shortname }}\"\n    state: \"present\"\n    region: \"{{ region }}\"\n    template: \"roles/cloudformation-infra/files/add-infra-node.json\"\n    template_parameters:\n      NodeName: \"{{ fqdn }}\"\n      NodeSg: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['NodeSGId'] | default(node_sg) }}\"\n      InfraSg: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['InfraSGId'] | default(infra_sg) }}\"\n      Subnet: \"{{ subnet_id }}\"\n      Route53HostedZone: \"{{ public_hosted_zone }}.\"\n      KeyName: \"{{ keypair }}\"\n      AmiId: \"{{ ami }}\"\n      InstanceType: \"{{ node_instance_type }}\"\n      NodeEmptyVolSize: \"{{ emptydir_storage }}\"\n      NodeEmptyVolType: gp2\n      NodeUserData: \"{{ lookup('file', 'user_data_node.yml') | b64encode }}\"\n      NodeRootVolSize: \"50\"\n      NodeRootVolType: gp2\n      NodeDockerVolSize: \"{{ docker_storage }}\"\n      NodeDockerVolType: gp2\n      NodeInstanceProfile: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['NodeARN'] | default(iam_role) }}\"\n      NodeType: \"{{ node_type }}\"\n    tags:\n      KubernetesCluster: \"{{ stack_name }}\"\n  when: create_vpc == \"no\" and add_node == \"yes\" and node_type == \"infra\"\n\n- name: Get cfn stack outputs\n  cloudformation_facts:\n    stack_name: \"{{ stack_name }}\"\n    region: \"{{ region }}\"\n  register: stack\n\n- name: Add CNS Gluster Nodes with io1\n  cloudformation:\n    stack_name: \"{{ stack_name }}-{{ glusterfs_stack_name }}\"\n    state: \"present\"\n    region: \"{{ region }}\"\n    template: \"roles/cloudformation-infra/files/add-cns-storage-iops.json\"\n    template_parameters:\n      GlusterNodeDns1: \"{{ gluster_node01 }}\"\n      GlusterNodeDns2: \"{{ gluster_node02 }}\"\n      GlusterNodeDns3: \"{{ gluster_node03 }}\"\n      NodeSg: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['NodeSGId'] | default(node_sg) }}\"\n      PrivateSubnet1: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['PrivateSubnet1'] | default(private_subnet_id1) }}\"\n      PrivateSubnet2: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['PrivateSubnet2'] | default(private_subnet_id2) }}\"\n      PrivateSubnet3: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['PrivateSubnet3'] | default(private_subnet_id3) }}\"\n      PublicHostedZone: \"{{ public_hosted_zone }}\"\n      Route53HostedZone: \"{{ public_hosted_zone }}.\"\n      KeyName: \"{{ keypair }}\"\n      AmiId: \"{{ ami }}\"\n      InstanceType: \"{{ node_instance_type }}\"\n      NodeEmptyVolSize: \"{{ emptydir_storage }}\"\n      NodeEmptyVolType: gp2\n      NodeUserData: \"{{ lookup('file', 'user_data_node.yml') | b64encode }}\"\n      NodeRootVolSize: \"50\"\n      NodeRootVolType: gp2\n      GlusterVolSize: \"{{ glusterfs_volume_size }}\"\n      GlusterVolType: \"{{ glusterfs_volume_type }}\"\n      Iops: \"{{ iops }}\"\n      NodeDockerVolSize: \"{{ docker_storage }}\"\n      NodeDockerVolType: gp2\n      NodeInstanceProfile: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['NodeARN'] | default(iam_role) }}\"\n      NodeType: glusterfs\n    tags:\n      KubernetesCluster: \"{{ stack_name }}\"\n  when: deploy_glusterfs == \"true\" and glusterfs_volume_type == \"io1\" or create_vpc == \"no\" and add_node == \"yes\" and node_type == \"glusterfs\" and glusterfs_volume_type == \"io1\"\n\n- name: Add CNS Gluster Nodes\n  cloudformation:\n    stack_name: \"{{ stack_name }}-{{ glusterfs_stack_name }}\"\n    state: \"present\"\n    region: \"{{ region }}\"\n    template: \"roles/cloudformation-infra/files/add-cns-storage.json\"\n    template_parameters:\n      GlusterNodeDns1: \"{{ gluster_node01 }}\"\n      GlusterNodeDns2: \"{{ gluster_node02 }}\"\n      GlusterNodeDns3: \"{{ gluster_node03 }}\"\n      NodeSg: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['NodeSGId'] | default(node_sg) }}\"\n      PrivateSubnet1: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['PrivateSubnet1'] | default(private_subnet_id1) }}\"\n      PrivateSubnet2: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['PrivateSubnet2'] | default(private_subnet_id2) }}\"\n      PrivateSubnet3: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['PrivateSubnet3'] | default(private_subnet_id3) }}\"\n      PublicHostedZone: \"{{ public_hosted_zone }}\"\n      Route53HostedZone: \"{{ public_hosted_zone }}.\"\n      KeyName: \"{{ keypair }}\"\n      AmiId: \"{{ ami }}\"\n      InstanceType: \"{{ node_instance_type }}\"\n      NodeEmptyVolSize: \"{{ emptydir_storage }}\"\n      NodeEmptyVolType: gp2\n      NodeUserData: \"{{ lookup('file', 'user_data_node.yml') | b64encode }}\"\n      NodeRootVolSize: \"50\"\n      NodeRootVolType: gp2\n      GlusterVolSize: \"{{ glusterfs_volume_size }}\"\n      GlusterVolType: \"{{ glusterfs_volume_type }}\"\n      NodeDockerVolSize: \"{{ docker_storage }}\"\n      NodeDockerVolType: gp2\n      NodeInstanceProfile: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['NodeARN'] | default(iam_role) }}\"\n      NodeType: glusterfs\n    tags:\n      KubernetesCluster: \"{{ stack_name }}\"\n  when: deploy_glusterfs == \"true\" and glusterfs_volume_type != \"io1\" or create_vpc == \"no\" and add_node == \"yes\" and node_type == \"glusterfs\" and glusterfs_volume_type != \"io1\"\n\n- name: Add CRS Gluster Nodes with IO1\n  cloudformation:\n    stack_name: \"{{ stack_name }}-{{ glusterfs_stack_name }}\"\n    state: \"present\"\n    region: \"{{ region }}\"\n    template: \"roles/cloudformation-infra/files/add-crs-storage-iops.json\"\n    template_parameters:\n      Vpc: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['StackVpc'] | default(vpc) }}\"\n      BastionSG: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['BastionSGId'] | default(bastion_sg) }}\"\n      NodeSG: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['NodeSGId'] | default(node_sg) }}\"\n      GlusterNodeDns1: \"{{ gluster_node01 }}\"\n      GlusterNodeDns2: \"{{ gluster_node02 }}\"\n      GlusterNodeDns3: \"{{ gluster_node03 }}\"\n      PrivateSubnet1: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['PrivateSubnet1'] | default(private_subnet_id1) }}\"\n      PrivateSubnet2: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['PrivateSubnet2'] | default(private_subnet_id2) }}\"\n      PrivateSubnet3: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['PrivateSubnet3'] | default(private_subnet_id3) }}\"\n      PublicHostedZone: \"{{ public_hosted_zone }}\"\n      Route53HostedZone: \"{{ public_hosted_zone }}.\"\n      KeyName: \"{{ keypair }}\"\n      AmiId: \"{{ ami }}\"\n      InstanceType: \"{{ node_instance_type }}\"\n      NodeUserData: \"{{ lookup('file', 'user_data_gluster.yml') | b64encode }}\"\n      NodeRootVolSize: \"50\"\n      NodeRootVolType: gp2\n      GlusterVolSize: \"{{ glusterfs_volume_size }}\"\n      GlusterVolType: \"{{ glusterfs_volume_type }}\"\n      Iops: \"{{ iops }}\"\n  when: deploy_crs is defined and create_vpc == \"no\" and deploy_crs == \"yes\" and glusterfs_volume_type == \"io1\"\n\n- name: Add CRS Gluster Nodes\n  cloudformation:\n    stack_name: \"{{ stack_name }}-{{ glusterfs_stack_name }}\"\n    state: \"present\"\n    region: \"{{ region }}\"\n    template: \"roles/cloudformation-infra/files/add-crs-storage.json\"\n    template_parameters:\n      Vpc: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['StackVpc'] | default(vpc) }}\"\n      BastionSG: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['BastionSGId'] | default(bastion_sg) }}\"\n      NodeSG: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['NodeSGId'] | default(node_sg) }}\"\n      GlusterNodeDns1: \"{{ gluster_node01 }}\"\n      GlusterNodeDns2: \"{{ gluster_node02 }}\"\n      GlusterNodeDns3: \"{{ gluster_node03 }}\"\n      PrivateSubnet1: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['PrivateSubnet1'] | default(private_subnet_id1) }}\"\n      PrivateSubnet2: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['PrivateSubnet2'] | default(private_subnet_id2) }}\"\n      PrivateSubnet3: \"{{ stack['ansible_facts']['cloudformation'][stack_name]['stack_outputs']['PrivateSubnet3'] | default(private_subnet_id3) }}\"\n      PublicHostedZone: \"{{ public_hosted_zone }}\"\n      Route53HostedZone: \"{{ public_hosted_zone }}.\"\n      KeyName: \"{{ keypair }}\"\n      AmiId: \"{{ ami }}\"\n      InstanceType: \"{{ node_instance_type }}\"\n      NodeUserData: \"{{ lookup('file', 'user_data_gluster.yml') | b64encode }}\"\n      NodeRootVolSize: \"50\"\n      NodeRootVolType: gp2\n      GlusterVolSize: \"{{ glusterfs_volume_size }}\"\n      GlusterVolType: \"{{ glusterfs_volume_type }}\"\n  when: deploy_crs is defined and create_vpc == \"no\" and deploy_crs == \"yes\" and glusterfs_volume_type != \"io1\"\n"}, {"commit_sha": "c91b6076e3a957fb0a165131d0ff3b3b208ed419", "sha": "255086c2a9f619f83ebf6af5697930379979c8e2", "filename": "handlers/main.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: restart auditd\n    service: name=auditd state=restarted\n    changed_when: False\n    ignore_errors: True\n\n  - name: restart rsyslog\n    service: name=rsyslog state=restarted\n    changed_when: False\n    ignore_errors: True\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "d765c49089319e3622ca90ccc6b573fe238acc86", "filename": "roles/cups/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# administer this service by browsing to localhost:631\n- name: Get the CUPS package installed\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - cups\n  when: cups_install\n  tags:\n    - download\n\n- name: Put our own /etc/cups/cupsd.conf in place, to permit local LAN admin\n  template:\n    src: cupsd.conf\n    dest: /etc/cups/cupsd.conf\n\n- name: Put an apache2 config file in place\n  template:\n    src: cups.conf\n    dest: \"/etc/{{ apache_config_dir }}/\"\n\n- name: Create the link for sites-enabled (debuntu)\n  file:\n    src: /etc/apache2/sites-available/cups.conf\n    dest: /etc/apache2/sites-enabled/cups.conf\n    state: link\n  when: cups_enabled and is_debuntu\n\n- name: Enable services for CUPS (OS's other than Fedora 18)\n  service:\n    name: \"{{ item }}\"\n    state: started\n    enabled: yes\n  with_items:\n    - cups\n    - cups-browsed\n  when: cups_enabled and not is_F18\n\n- name: Enable services for CUPS (Fedora 18, for XO laptops)\n  service:\n    name: cups\n    state: started\n    enabled: yes\n  when: cups_enabled and is_F18\n\n- name: Permit headless admin of CUPS -- only works when CUPS daemon is running\n  shell: \"cupsctl --remote-admin\"\n  when: cups_enabled\n\n- name: Disable services for CUPS (OS's other than Fedora 18)\n  service:\n    name: \"{{ item }}\"\n    state: stopped\n    enabled: no\n  with_items:\n    - cups\n    - cups-browsed\n  when: not cups_enabled and not is_F18\n\n- name: Disable services for CUPS (Fedora 18, for XO laptops)\n  service:\n    name: cups\n    state: stopped\n    enabled: no\n  when: not cups_enabled and is_F18\n\n- name: Add 'cups' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: cups\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: CUPS\n    - option: description\n      value: '\"CUPS (Common UNIX Printing System) is a modular printing system that allows a computer to act as a print server. A computer running CUPS is a host that can accept print jobs from client computers, process them, and send them to the appropriate printer.\"'\n    - option: installed\n      value: \"{{ cups_install }}\"\n    - option: enabled\n      value: \"{{ cups_enabled }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6a1cc09f343bc20acf16863f0f6a8d20fe6bf286", "filename": "reference-architecture/gcp/ansible/playbooks/roles/empty-image-delete/defaults", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "../empty-image/defaults/"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9415081c9b9f7d381cdd20ce282c0993f5ca9ccb", "filename": "roles/osp/admin-keystone-domain/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: yum-clean-metadata\n  command: yum clean metadata\n  args:\n    warn: no\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "62bb4afded559cbf75ec492463a6964b072d6a6f", "filename": "roles/idm-host-cert/tasks/idm-login.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Login and create a session with the IdM\"\n  uri:\n    url: \"https://{{ idm_fqdn }}/ipa/session/login_password\"\n    method: POST\n    body: \"user={{ idm_user }}&password={{ idm_password }}\"\n    validate_certs: no\n    headers:\n      Content-Type: \"application/x-www-form-urlencoded\"\n  register: idm_session\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "92a5899bfae80dadbe6b2f26d05d53ca4d629129", "filename": "reference-architecture/gcp/ansible/playbooks/core-infra.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: create core infrastructure\n  hosts: localhost\n  roles:\n  - ssl-certificate\n  - role: deployment-create\n    deployment_name: core\n  post_tasks:\n  - name: wait for instance groups to be ready\n    include_role:\n      name: wait-for-instance-group\n    with_items:\n    - '{{ prefix }}-master'\n    - '{{ prefix }}-node'\n    - '{{ prefix }}-infra-node'\n    loop_control:\n      loop_var: instance_group\n  - name: refresh gce inventory\n    command: '{{ inventory_dir }}/gce/hosts/gce.py --refresh-cache'\n    changed_when: false\n  - meta: refresh_inventory\n\n- name: continue with the core infrastructure deployment, after the instances are created\n  hosts: localhost\n  roles:\n  - dns-records\n  - ssh-proxy\n  - role: wait-for-instance\n    instance: '{{ hostvars[prefix + \"-bastion\"][\"gce_public_ip\"] }}'\n\n- name: wait for all instances to accept ssh connections\n  hosts: '{{ prefix }}-bastion'\n  tasks:\n  - name: wait for all instances\n    include_role:\n      name: wait-for-instance\n    with_items: '{{ groups[\"tag_\" + prefix] }}'\n    loop_control:\n      loop_var: instance\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "f4921a3112e6378314302a9e2268b87ac21a19bb", "filename": "roles/2-common/tasks/fedora.yml", "repository": "iiab/iiab", "decoded_content": "- name: Keep yum cache\n  ini_file: dest=/etc/yum.conf\n            section=main\n            option=keepcache\n            value=1\n\n- name: Install rpmfusion-free-updates repo -- for exfat\n  template: dest=/etc/yum.repos.d/rpmfusion-free-updates.repo\n            src=rpmfusion-free-updates.repo\n            owner=root\n            group=root\n            mode=0666\n\n- name: Install optional exFAT packages for Fedora\n  shell: yum --enablerepo=rpmfusion-free-updates install exfat-utils fuse-exfat\n  when: exFAT_enabled and is_F18\n\n- name: Install yum deps for arm!!!\n  shell: dnf install -y python-urlgrabber pyxattr yum-metadata-parser\n  when: ansible_distribution == \"Fedora\" and ansible_machine == \"armv7l\" and ansible_distribution_version|int >= 22\n\n- name: Install yum from Fedora 23 for arm!!!\n  shell: dnf install -y https://kojipkgs.fedoraproject.org//packages/yum/3.4.3/506.fc23/noarch/yum-3.4.3-506.fc23.noarch.rpm python-dnf\n  when: ansible_distribution == \"Fedora\" and ansible_machine == \"armv7l\" and ansible_distribution_version|int >= 22\n\n- name: Install yum if it has been dropped from our distribution -- Fedora 22 uses dnf!!!\n  shell: dnf install -y yum\n  when: ansible_distribution == \"Fedora\" and ansible_distribution_version|int >= 22 and ansible_machine != \"armv7l\"\n\n- name: Install Fedora specifc packages\n  package: name={{ item }}\n           state=present\n  with_items:\n   - mtd-utils\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "0c095a4bba556191dbc32b759fb39b3ae47d3d00", "filename": "roles/network/tasks/NM-debian.yml", "repository": "iiab/iiab", "decoded_content": "# NM-debian.yml\n- name: Stopping services\n  include_tasks: down-debian.yml\n\n# provide keyfile layout like the XO's used way back.\n- name: Create uuid for NM's keyfile store\n  shell: uuidgen\n  register: uuid_response\n\n- name: Put the uuid in place\n  set_fact:\n    gen_uuid: \"{{ uuid_response.stdout_lines[0] }}\"\n\n# NM might have a watcher on this path and we don't have to restart NM\n- name: Copy the bridge script for NetworkManager\n  template:\n    dest: /etc/NetworkManager/system-connections/\n    src: network/bridge-br0\n    mode: 0600\n  when: iiab_network_mode != \"Appliance\"\n\n- name: Remove br0 in Appliance Mode for NetworkManager\n  file:\n    dest: /etc/NetworkManager/system-connections/bridge-br0\n    state: absent\n  when: iiab_network_mode == \"Appliance\"\n\n- name: Removing static for NetworkManager\n  file:\n    dest: /etc/NetworkManager/system-connections/iiab-static\n    state: absent\n  when: wan_ip == \"dhcp\"\n\n- name: Static IP computing CIDR\n  shell: netmask {{ wan_ip }}/{{ wan_netmask }} | awk -F \"/\" '{print $2}'\n  register: CIDR\n  when: wan_ip != \"dhcp\"\n\n- name: Static IP setting CIDR\n  set_fact:\n    wan_cidr: \"{{ CIDR.stdout }}\"\n  when: wan_ip != \"dhcp\"\n\n- name: Create uuid for NM's keyfile store static\n  shell: uuidgen\n  register: uuid_response2\n  when: wan_ip != \"dhcp\"\n\n- name: Put the uuid in place\n  set_fact:\n    gen_uuid2: \"{{ uuid_response2.stdout_lines[0] }}\"\n  when: wan_ip != \"dhcp\"\n\n- name: Copy static template for NetworkManager\n  template:\n    dest: /etc/NetworkManager/system-connections/iiab-static\n    src: network/NM-static.j2\n    mode: 0600\n  when: wan_ip != \"dhcp\"\n\n- name: Stop wpa_supplicant service\n  service:\n    name: wpa_supplicant\n    state: stopped\n  when: iiab_wireless_lan_iface is defined and hostapd_enabled and iiab_network_mode != \"Appliance\"\n\n- name: Mask wpa_supplicant\n  shell: systemctl mask wpa_supplicant\n  when: iiab_wireless_lan_iface is defined and hostapd_enabled and iiab_network_mode != \"Appliance\"\n\n- name: Reload systemd\n  systemd:\n    daemon_reload: yes\n\n- name: Restart the NetworkManager service\n  systemd:\n    name: network-manager\n    state: restarted\n  when: not nobridge is defined and not no_net_restart\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "030fd87530ad839ce63b27d512ccf21b2e6f7ccf", "filename": "roles/ansible/tower/config-ansible-tower-ldap/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- block: # when ansible_tower.ldap is defined\n\n  - include_tasks: ldap.yml\n\n  when:\n  - ansible_tower.ldap is defined\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "511dea4604d9f92994952938bf1b16a81b3cf672", "filename": "roles/ipaclient/tasks/python_2_3_test.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "---\n- block:\n  - name: Verify Python3 import\n    script: py3test.py\n    register: result_py3test\n    failed_when: False\n    changed_when: False\n    check_mode: no\n\n  - name: Set python interpreter to 3\n    set_fact:\n      ansible_python_interpreter: \"/usr/bin/python3\"\n    when: result_py3test.rc == 0\n\n  - name: Set python interpreter to 2\n    set_fact:\n      ansible_python_interpreter: \"/usr/bin/python2\"\n    when: result_py3test.failed or result_py3test.rc != 0\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "1a62ffa2f7d3edd5bcd8acacd5bd4c4a4fc3bddf", "filename": "roles/ipaclient/vars/Fedora-25.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "ipaclient_packages: [ \"ipa-client\", \"libselinux-python\" ]\n#ansible_python_interpreter: '/usr/bin/python2'"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "abfb860454fb2afcfef60de885f95bfb42f141ea", "filename": "roles/osp/admin-keystone-domain/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Add EPEL temporary repository (but leave it disabled)\"\n  yum_repository:\n    name: tmp-epel\n    description: EPEL YUM repo\n    baseurl: http://download.fedoraproject.org/pub/epel/7/$basearch\n    metalink: https://mirrors.fedoraproject.org/metalink?repo=epel-7&arch=$basearch\n    gpgcheck: no\n    enabled: no\n\n- name: \"Install python-pip from EPEL\"\n  yum:\n    name: python-pip\n    enablerepo: tmp-epel\n    state: present\n\n- name: \"Install Python Shade using pip\"\n  pip:\n    name: shade\n\n- name: \"Clear out temporary EPEL repository\"\n  yum_repository:\n    name: tmp-epel\n    state: absent\n  notify: yum-clean-metadata\n\n- name: \"Create the LDAP domain(s)\"\n  os_keystone_domain:\n    cloud: \"{{ item.cloud | default(osp_default_cloud) }}\"\n    description: \"{{ item.domain }} Domain\"\n    name: \"{{ item.domain }}\"\n  with_items:\n  - \"{{ keystone_ldap }}\"\n\n- name: \"Restart keystone (httpd) to ensure config has been applied\"\n  service:\n    name: httpd\n    state: restarted\n\n- name: \"Wait a bit for keystone to catch up\"\n  pause:\n    seconds: 15\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "b7ca2ad7ff40253a43b595a41304763eb275147f", "filename": "playbooks/openshift-cluster-seed.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- hosts: seed-hosts[0]\n  roles:\n    - openshift-applier\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "8072b9ce0caedc25399f715dc0cf2c2f2957b052", "filename": "roles/rhsm-timeout/tasks/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Gather facts\n  openshift_facts:\n    role: common\n\n- name: Allow rhsm a longer timeout to help out with subscription-manager\n  lineinfile:\n    dest: /etc/rhsm/rhsm.conf\n    line: 'server_timeout=600'\n    insertafter: '^proxy_password ='\n  when: ansible_distribution == \"RedHat\"\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "9e5461d2facf950f7f3c6aba477fc3e3166e7b7f", "filename": "roles/client/tasks/systems/Debian.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- set_fact:\n    prerequisites: []\n    configs_prefix: /etc/\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "3208285bbef12ebf0a66d4cd669a882d35ac5fda", "filename": "tasks/Linux/fetch/sapjvm-fallback.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: 'Fetch root page {{ sapjvm_root_page }}'\n  uri:\n    url: '{{ sapjvm_root_page }}'\n    return_content: true\n  register: root_page\n\n- name: Find release url\n  set_fact:\n    release_url: >-\n      {{ root_page['content']\n        | regex_findall('(additional/sapjvm-'\n          + java_major_version|string\n          + '[\\d.]+-linux-x64.zip)')\n      }}\n\n- name: 'Download artifact from {{ release_url[0] }}'\n  get_url:\n    url: '{{ sapjvm_root_page }}/{{ release_url[0] }}'\n    dest: '{{ java_download_path }}'\n    headers:\n      Cookie: eula_3_1_agreed=tools.hana.ondemand.com/developer-license-3_1.txt\n  register: file_downloaded\n  retries: 20\n  delay: 5\n  until: file_downloaded is succeeded\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "bf861084ffb1c56f32cda8436a2e4cda88f89ae8", "filename": "roles/common/tasks/freebsd.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- set_fact:\n    tools:\n      - git\n      - subversion\n      - screen\n      - coreutils\n      - openssl\n      - bash\n      - wget\n    sysctl:\n      forwarding:\n        - net.inet.ip.forwarding\n        - net.inet6.ip6.forwarding\n  tags:\n    - always\n\n- name: Loopback included into the rc config\n  blockinfile:\n    dest: /etc/rc.conf\n    create: yes\n    block: |\n      cloned_interfaces=\"lo100\"\n      ifconfig_lo100=\"inet {{ local_service_ip }} netmask 255.255.255.255\"\n      ifconfig_lo100=\"inet6 FCAA::1/64\"\n  notify:\n    - restart loopback bsd\n  tags:\n    - always\n\n- name: Enable the gateway features\n  lineinfile: dest=/etc/rc.conf regexp='^{{ item.param }}.*' line='{{ item.param }}={{ item.value }}'\n  with_items:\n    - { param: firewall_enable, value: '\"YES\"' }\n    - { param: firewall_type, value: '\"open\"' }\n    - { param: gateway_enable, value: '\"YES\"' }\n    - { param: natd_enable, value: '\"YES\"' }\n    - { param: natd_interface, value: '\"{{ ansible_default_ipv4.device|default() }}\"' }\n    - { param: natd_flags, value: '\"-dynamic -m\"' }\n  notify:\n    - restart ipfw\n  tags:\n    - always\n\n- name: FreeBSD | Activate IPFW\n  shell: >\n    kldstat -n ipfw.ko || kldload ipfw ; sysctl net.inet.ip.fw.enable=0 &&\n    bash /etc/rc.firewall && sysctl net.inet.ip.fw.enable=1\n\n- meta: flush_handlers\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ccd29be29774713ea67117a97598bcf08c8af80d", "filename": "playbooks/libvirt/openshift-cluster/tasks/launch_instances.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# TODO: Add support for choosing base image based on deployment_type and os\n# wanted (os wanted needs support added in bin/cluster with sane defaults:\n# fedora/centos for origin, rhel for enterprise)\n\n# TODO: create a role to encapsulate some of this complexity, possibly also\n# create a module to manage the storage tasks, network tasks, and possibly\n# even handle the libvirt tasks to set metadata in the domain xml and be able\n# to create/query data about vms without having to use xml the python libvirt\n# bindings look like a good candidate for this\n\n- name: Download Base Cloud image\n  get_url:\n    url: '{{ image_url }}'\n    sha256sum: '{{ image_sha256 }}'\n    dest: '{{ libvirt_storage_pool_path }}/{{ [image_name, image_compression] | difference([\"\"]) | join(\".\") }}'\n  when: ( lookup(\"oo_option\", \"skip_image_download\") | default(\"no\", True) | lower ) in [\"false\", \"no\"]\n  register: downloaded_image\n\n- name: Uncompress xz compressed base cloud image\n  command: 'unxz -kf {{ libvirt_storage_pool_path }}/{{ [image_name, image_compression] | join(\".\") }}'\n  args:\n    creates: '{{ libvirt_storage_pool_path }}/{{ image_name }}'\n  when: image_compression in [\"xz\"] and downloaded_image.changed\n\n- name: Uncompress tgz compressed base cloud image\n  command: 'tar zxvf {{ libvirt_storage_pool_path }}/{{ [image_name, image_compression] | join(\".\") }}'\n  args:\n    creates: '{{ libvirt_storage_pool_path }}/{{ image_name }}'\n  when: image_compression in [\"tgz\"] and downloaded_image.changed\n\n- name: Uncompress gzip compressed base cloud image\n  command: 'gunzip {{ libvirt_storage_pool_path }}/{{ [image_name, image_compression] | join(\".\") }}'\n  args:\n    creates: '{{ libvirt_storage_pool_path }}/{{ image_name }}'\n  when: image_compression in [\"gz\"] and downloaded_image.changed\n\n- name: Create the cloud-init config drive path\n  file:\n    dest: '{{ libvirt_storage_pool_path }}/{{ item }}_configdrive/'\n    state: directory\n  with_items: '{{ instances }}'\n\n- name: Create the cloud-init config drive files\n  template:\n    src: '{{ item[1] }}'\n    dest: '{{ libvirt_storage_pool_path }}/{{ item[0] }}_configdrive/{{ item[1] }}'\n  with_nested:\n    - '{{ instances }}'\n    - [ user-data, meta-data ]\n\n- name: Create the cloud-init config drive\n  command: 'genisoimage -output {{ libvirt_storage_pool_path }}/{{ item }}_cloud-init.iso -volid cidata -joliet -rock user-data meta-data'\n  args:\n    chdir: '{{ libvirt_storage_pool_path }}/{{ item }}_configdrive/'\n    creates: '{{ libvirt_storage_pool_path }}/{{ item }}_cloud-init.iso'\n  with_items: '{{ instances }}'\n\n- name: Refresh the libvirt storage pool for openshift\n  command: 'virsh -c {{ libvirt_uri }} pool-refresh {{ libvirt_storage_pool }}'\n\n- name: Create VM drives\n  command: 'virsh -c {{ libvirt_uri }} vol-create-as {{ libvirt_storage_pool }} {{ item }}.qcow2 10G --format qcow2 --backing-vol {{ image_name }} --backing-vol-format qcow2'\n  with_items: '{{ instances }}'\n\n- name: Create VM docker drives\n  command: 'virsh -c {{ libvirt_uri }} vol-create-as {{ libvirt_storage_pool }} {{ item }}-docker.qcow2 10G --format qcow2 --allocation 0'\n  with_items: '{{ instances }}'\n\n- name: Create VMs\n  virt:\n    name: '{{ item }}'\n    command: define\n    xml: \"{{ lookup('template', '../templates/domain.xml') }}\"\n    uri: '{{ libvirt_uri }}'\n  with_items: '{{ instances }}'\n\n- name: Start VMs\n  virt:\n    name: '{{ item }}'\n    state: running\n    uri: '{{ libvirt_uri }}'\n  with_items: '{{ instances }}'\n\n- name: Wait for the VMs to get an IP\n  shell: 'virsh -c {{ libvirt_uri }} net-dhcp-leases {{ libvirt_network }} | egrep -c ''{{ instances | join(\"|\") }}'''\n  register: nb_allocated_ips\n  until: nb_allocated_ips.stdout == '{{ instances | length }}'\n  retries: 60\n  delay: 3\n  when: instances | length != 0\n\n- name: Collect IP addresses of the VMs\n  shell: 'virsh -c {{ libvirt_uri }} net-dhcp-leases {{ libvirt_network }} | awk ''$6 == \"{{ item }}\" {gsub(/\\/.*/, \"\", $5); print $5}'''\n  register: scratch_ip\n  with_items: '{{ instances }}'\n\n- set_fact:\n    ips: \"{{ scratch_ip.results | default([]) | oo_collect('stdout') }}\"\n\n- set_fact:\n    node_label:\n      type: \"{{ g_sub_host_type }}\"\n  when: instances | length > 0 and type == \"node\"\n\n- set_fact:\n    node_label:\n      type: \"{{ type }}\"\n  when: instances | length > 0 and type != \"node\"\n\n- name: Add new instances\n  add_host:\n    hostname: '{{ item.0 }}'\n    ansible_ssh_host: '{{ item.1 }}'\n    ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n    ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    groups: \"tag_environment-{{ cluster_env }}, tag_host-type-{{ type }}, tag_sub-host-type-{{ g_sub_host_type }}, tag_clusterid-{{ cluster_id }}\"\n    openshift_node_labels: \"{{ node_label }}\"\n    libvirt_ip_address: \"{{ item.1 }}\"\n  with_together:\n    - '{{ instances }}'\n    - '{{ ips }}'\n\n- name: Wait for ssh\n  wait_for:\n    host: '{{ item }}'\n    port: 22\n  with_items: '{{ ips }}'\n\n- name: Wait for openshift user setup\n  command: 'ssh -o StrictHostKeyChecking=no -o PasswordAuthentication=no -o ConnectTimeout=10 -o UserKnownHostsFile=/dev/null openshift@{{ item.1 }} echo openshift user is setup'\n  register: result\n  until: result.rc == 0\n  retries: 30\n  delay: 1\n  with_together:\n    - '{{ instances }}'\n    - '{{ ips }}'\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "8bfe866ec56ce70abe953eeab3199c301cf6fd27", "filename": "tasks/nexus-restore.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- name: Make sure nexus is stopped\n  debug:\n    msg: \"trigger nexus stop\"\n  changed_when: true\n  notify:\n    - nexus-service-stop\n\n- meta: flush_handlers\n\n- name: \"Run restoration script\"\n  shell: \"nexus-blob-restore.sh {{ nexus_restore_point }} 2>&1 | tee -a {{ nexus_restore_log }}\"\n  tags:\n    # This is only run when a restore point is defined\n    # shut-off ansible-lint error on this one: this is the desired way of doing it\n    - skip_ansible_lint\n\n  notify:\n    - nexus-service-restart\n    - wait-for-nexus\n    - wait-for-nexus-port\n\n- meta: flush_handlers\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "1bf210d82f67f5388e9561767955dcacb3024d90", "filename": "roles/osp/admin-project/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Add projects / tenants\"\n  os_project:\n    cloud: \"{{ item.cloud | default(osp_default_cloud) | default(omit) }}\"\n    domain_id: \"{{ item.domain }}\"\n    description: \"{{ item.description | default(omit) }}\"\n    name: \"{{ item.name }}\"\n  with_items:\n  - \"{{ osp_projects | default([]) }}\"\n\n- name: \"Update tenant quotas\"\n  os_quota:\n    cloud: \"{{ item.cloud | default(osp_default_cloud) | default(omit) }}\"\n    name: \"{{ item.name }}\"\n    cores: \"{{ item.quota.vcpus | default(omit) }}\"\n    instances: \"{{ item.quota.instances | default(omit) }}\"\n    volumes: \"{{ item.quota.volumes | default(omit) }}\"\n    gigabytes: \"{{ item.quota.total_vol_size | default(omit) }}\"\n    ram: \"{{ item.quota.ram | default(omit) }}\"\n    security_group: \"{{ item.quota.security_groups | default(omit) }}\"\n    security_group_rule: \"{{ item.quota.security_group_rules | default(omit) }}\"\n    floating_ips: \"{{ item.quota.floating_ips | default(omit) }}\"\n    network: \"{{ item.quota.networks | default(omit) }}\"\n    router: \"{{ item.quota.routers | default(omit) }}\"\n    subnet: \"{{ item.quota.subnets | default(omit) }}\"\n    port: \"{{ item.quota.ports | default(omit) }}\"\n  with_items:\n  - \"{{ osp_projects | default([]) }}\"\n  when:\n  - item.quota is defined\n\n- name: \"Assign roles for users/projects\"\n  include_tasks: \"tenant-roles.yml\"\n  with_items:\n  - \"{{ osp_projects | default([]) }}\"\n  loop_control:\n    loop_var: project\n\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "f4e7ffcc44dad4c845c92e9f8d2c10e56f9088f4", "filename": "roles/ipareplica/vars/Fedora-27.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# Fedora defaults file for ipareplica\n# vars/Fedora.yml\nipareplica_packages: [ \"ipa-server\", \"libselinux-python\" ]\nipareplica_packages_dns: [ \"ipa-server-dns\" ]\nipareplica_packages_adtrust: [ \"ipa-server-trust-ad\" ]"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "5bd23f264b96a4792c1ec2183975dc2a7feeb0b9", "filename": "tasks/declare_script_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- name: Removing (potential) previously declared Groovy script {{ item }}\n  uri:\n    url: \"http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}service/siesta/rest/v1/script/{{ item }}\"\n    user: 'admin'\n    password: \"{{ current_nexus_admin_password }}\"\n    method: DELETE\n    force_basic_auth: yes\n    status_code: 204,404\n\n- name: Declaring Groovy script {{ item }}\n  uri:\n    url: \"http://localhost:{{ nexus_default_port }}{{ nexus_default_context_path }}service/siesta/rest/v1/script\"\n    user: 'admin'\n    password: \"{{ current_nexus_admin_password }}\"\n    body_format: json\n    method: POST\n    force_basic_auth: yes\n    status_code: 204\n    body:\n      name: \"{{ item }}\"\n      type: 'groovy'\n      content: \"{{ lookup('file', 'groovy/' + item + '.groovy') }}\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "1e6eebd6b1a5e68fff5fd81158108f0257cd0fce", "filename": "archive/roles/cicd/tasks/docker.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n  \n- name: Install Docker\n  yum: \n    name: docker\n    state: present\n  tags: docker\n  \n- name: Docker Configuration File\n  lineinfile: \n    dest: /etc/sysconfig/docker \n    regexp: '^OPTIONS=.*$' \n    line: \"OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16'\"\n  tags: docker\n\n- name: Docker Storage Setup Configuration File\n  copy:\n    content: >\n      DEVS={{ cicd_storage_disk_volume }}\\n\n      VG=docker_vg\n    dest: /etc/sysconfig/docker-storage-setup\n  register: docker_storage_setup\n  tags: docker\n  \n- name: Run Docker Storage Setup\n  command: docker-storage-setup\n  when: docker_storage_setup.changed == true\n  notify:\n  - restart docker\n  tags: docker\n\n- name: extend the vg\n  command: lvextend -l 90%VG /dev/docker_vg/docker-pool\n  when: docker_storage_setup.changed == true\n  notify:\n  - restart docker\n  tags: docker\n\n- name: Enable Docker Service\n  service: \n    name: docker\n    enabled: true\n  tags: docker"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "2475b9d6bc4bd3abd1615e0eb61619a4c5991f99", "filename": "playbooks/libvirt/openshift-cluster/launch.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Launch instance(s)\n  hosts: localhost\n  become: no\n  connection: local\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  vars:\n    image_url: \"{{ deployment_vars[deployment_type].image.url }}\"\n    image_sha256: \"{{ deployment_vars[deployment_type].image.sha256 }}\"\n    image_name: \"{{ deployment_vars[deployment_type].image.name }}\"\n    image_compression: \"{{ deployment_vars[deployment_type].image.compression }}\"\n  tasks:\n  - include: tasks/configure_libvirt.yml\n\n  - include: ../../common/openshift-cluster/tasks/set_etcd_launch_facts.yml\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ etcd_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"default\"\n\n  - include: ../../common/openshift-cluster/tasks/set_master_launch_facts.yml\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ master_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"default\"\n\n  - include: ../../common/openshift-cluster/tasks/set_node_launch_facts.yml\n    vars:\n      type: \"compute\"\n      count: \"{{ num_nodes }}\"\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ node_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"{{ sub_host_type }}\"\n\n  - include: ../../common/openshift-cluster/tasks/set_node_launch_facts.yml\n    vars:\n      type: \"infra\"\n      count: \"{{ num_infra }}\"\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ node_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"{{ sub_host_type }}\"\n\n- include: update.yml\n\n- include: list.yml\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "69a58bd0a65c6991ae2fdad8c9a2d479a6972231", "filename": "roles/ansible/tower/manage-projects/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- block: # when ansible_tower.projects is defined\n\n  - name: \"Set default values\"\n    set_fact:\n      processed_projects: []\n      existing_projects_output: []\n\n  # Utilize the `rest_get` library routine to ensure REST pagination is handled\n  - name: \"Get the existing organizations\"\n    rest_get:\n      host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n      rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      rest_password: \"{{ ansible_tower.admin_password }}\"\n      api_uri: \"/api/v2/organizations/\"\n    register: existing_organizations_output\n\n  # Utilize the `rest_get` library routine to ensure REST pagination is handled\n  - name: \"Get the existing projects\"\n    rest_get:\n      host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n      rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      rest_password: \"{{ ansible_tower.admin_password }}\"\n      api_uri: \"/api/v2/projects/\"\n    register: existing_projects_output\n\n  - name: \"Process the inventory projects\"\n    include_tasks: process-project.yml\n    with_items:\n    - \"{{ ansible_tower.projects }}\"\n    loop_control:\n      loop_var: project\n\n  - name: \"Elminate the projects that should not be present\"\n    uri:\n      url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/projects/{{ item.id }}/\"\n      user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      password: \"{{ ansible_tower.admin_password }}\"\n      force_basic_auth: yes\n      method: DELETE\n      validate_certs: no\n      status_code: 200,204\n    with_items:\n    - \"{{ existing_projects_output.rest_output | get_remaining_items(processed_projects, 'name', 'name')}}\"\n\n  when:\n  - ansible_tower.projects is defined\n"}, {"commit_sha": "45971be8249cc4627ef8ddfacf55a661b7fc13ca", "sha": "6609113c885252f6b997eba96336e806f639ad57", "filename": "tasks/remove-pre-docker-ce.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Determine Docker version\n  command: bash -c \"docker version | grep Version | awk '{print $2}'\"\n  ignore_errors: yes\n  changed_when: false\n  register: cmd_docker_version\n\n- name: Set fact if old Docker installation shall be removed\n  set_fact:\n    remove_old_docker: \"{{ docker_remove_pre_ce | bool }} == true and {{ cmd_docker_version.stdout_lines[0] | search('-ce') }} == false\"\n  when: cmd_docker_version.stdout_lines is defined and cmd_docker_version.stdout_lines[0] is defined\n\n- name: Check if Docker is running\n  command: systemctl status docker\n  ignore_errors: yes\n  changed_when: false\n  register: service_docker_status\n  when: remove_old_docker | default(false) | bool == true\n  become: true\n\n- name: Stop Docker service\n  service:\n    name: docker\n    state: stopped\n  when: \"service_docker_status.rc | default(1) == 0\"\n\n- name: Remove old Docker installation before Docker CE\n  package:\n    name: \"{{ item }}\"\n    state: absent\n  become: true\n  when: remove_old_docker|default(false) | bool == true\n  with_items:\n    - \"{{ docker_old_packages[_docker_os_dist] }}\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "8409657a3cb78b504c8e57c2a3aab4ef9f7149ee", "filename": "playbooks/roles/zookeeper/vars/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# vars file for zookeeper"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "f276d672f0170c0310e7c8a912d2be1e61c9f217", "filename": "roles/manage-confluence-space/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Fail when source username or password is not set\n  fail: msg=\"This role requires source_confluence_site_username and source_confluence_site_password to be set and non empty\"\n  when:\n    - (confluence.source.username == \"\") or (confluence.source.password == \"\")\n\n- name: Retrieve all contents from source space\n  uri:\n    url: '{{ confluence.source.url }}/wiki/rest/api/space/{{ confluence.source.space_key }}/content?expand=body.storage,space,ancestors&limit=500'\n    method: GET\n    user: '{{ confluence.source.username }}'\n    password: '{{ confluence.source.password }}'\n    force_basic_auth: yes\n    status_code: 200\n    return_content: yes\n  register: contents_json\n\n- name: Create a tempfile for Content JSON\n  command: mktemp\n  register: uptemp\n  delegate_to: 127.0.0.1\n\n- name: Write content to file\n  copy:\n    content: \"{{ contents_json.json['page']['results'] }}\"\n    dest: \"{{ uptemp.stdout }}\"\n\n- name: Initialise old to new id mapping\n  set_fact:\n    id_mapping: {}\n\n- name: Sort contents based on its dependencies\n  command: \"./contents_order_parser.py '{{ uptemp.stdout }}'\"\n  args:\n    chdir: \"{{ role_path }}/files\"\n  register: processed_contents\n  delegate_to: 127.0.0.1\n\n- name: Fail when destination username or password is not set\n  fail: msg=\"This role requires confluence.destination.username and confluence.destination.username to be set and non empty\"\n  when:\n    - (confluence.destination.username == \"\") or (confluence.destination.password == \"\")\n\n- import_tasks: create_confluence_space.yml\n\n- include_tasks: copy_confluence_content.yml\n  with_items: '{{ processed_contents.stdout }}'\n  loop_control:\n    loop_var: confluence_space_content\n\n- include_tasks: copy_confluence_attachments.yml\n  with_dict: '{{ id_mapping }}'\n  loop_control:\n    loop_var: confluence_content_ids\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "1d745dcc356c79f49a4fd048e46a7754c3d5ce22", "filename": "playbooks/provisioning/openstack/galaxy-requirements.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# This is the Ansible Galaxy requirements file to pull in the correct roles\n\n# From 'infra-ansible'\n- src: https://github.com/redhat-cop/infra-ansible\n  version: master\n\n# From 'openshift-ansible'\n- src: https://github.com/openshift/openshift-ansible\n  version: master\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "50d94e382c50218124656faa597da29ad5440c3f", "filename": "roles/config-httpd/tasks/prep.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - httpd\n  - firewalld\n  - python-firewall\n\n- name: 'Ensure firewalld is running'\n  service:\n    name: firewalld\n    state: started\n    enabled: yes\n\n- name: 'Ensure httpd is running'\n  service:\n    name: httpd\n    state: started\n    enabled: yes\n\n- name: 'Open Firewall for httpd use'\n  firewalld:\n    port: \"{{ item }}\"\n    permanent: yes\n    state: enabled\n    immediate: yes\n  with_items:\n  - 80/tcp\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "2251f847582a6b940012e2d38b79a2d25d1791e2", "filename": "roles/cloud-digitalocean/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: Set the DigitalOcean Access Token fact\n  set_fact:\n    do_token: \"{{ do_access_token | default(lookup('env','DO_API_TOKEN')) }}\"\n    public_key: \"{{ lookup('file', '{{ SSH_keys.public }}') }}\"\n\n- block:\n    - name: \"Delete the existing Algo SSH keys\"\n      digital_ocean:\n        state: absent\n        command: ssh\n        api_token: \"{{ do_token }}\"\n        name: \"{{ SSH_keys.comment }}\"\n      register: ssh_keys\n      until: ssh_keys.changed != true\n      retries: 10\n      delay: 1\n\n  rescue:\n    - name: Collect the fail error\n      digital_ocean:\n        state: absent\n        command: ssh\n        api_token: \"{{ do_token }}\"\n        name: \"{{ SSH_keys.comment }}\"\n      register: ssh_keys\n      ignore_errors: yes\n\n    - debug: var=ssh_keys\n\n    - fail:\n        msg: \"Please, ensure that your API token is not read-only.\"\n\n- name: \"Upload the SSH key\"\n  digital_ocean:\n    state: present\n    command: ssh\n    ssh_pub_key: \"{{ public_key }}\"\n    api_token: \"{{ do_token }}\"\n    name: \"{{ SSH_keys.comment }}\"\n  register: do_ssh_key\n\n- name: \"Creating a droplet...\"\n  digital_ocean:\n    state: present\n    command: droplet\n    name: \"{{ do_server_name }}\"\n    region_id: \"{{ do_region }}\"\n    size_id: \"512mb\"\n    image_id: \"ubuntu-16-04-x64\"\n    ssh_key_ids: \"{{ do_ssh_key.ssh_key.id }}\"\n    unique_name: yes\n    api_token: \"{{ do_token }}\"\n    ipv6: yes\n  register: do\n\n- name: Add the droplet to an inventory group\n  add_host:\n    name: \"{{ do.droplet.ip_address }}\"\n    groups: vpn-host\n    ansible_ssh_user: root\n    ansible_python_interpreter: \"/usr/bin/python2.7\"\n    ansible_ssh_private_key_file: \"{{ SSH_keys.private }}\"\n    do_access_token: \"{{ do_token }}\"\n    do_droplet_id: \"{{ do.droplet.id }}\"\n    cloud_provider: digitalocean\n    ipv6_support: true\n\n- set_fact:\n    cloud_instance_ip: \"{{ do.droplet.ip_address }}\"\n\n- name: Tag the groplet\n  digital_ocean_tag:\n    name: \"Environment:Algo\"\n    resource_id: \"{{ do.droplet.id }}\"\n    api_token: \"{{ do_token }}\"\n    state: present\n\n- name: Get droplets\n  uri:\n    url: \"https://api.digitalocean.com/v2/droplets?tag_name=Environment:Algo\"\n    method: GET\n    status_code: 200\n    headers:\n      Content-Type: \"application/json\"\n      Authorization: \"Bearer {{ do_token }}\"\n  register: do_droplets\n\n- name: Ensure the group digitalocean exists in the dynamic inventory file\n  lineinfile:\n    state: present\n    dest: configs/inventory.dynamic\n    line: '[digitalocean]'\n\n- name: Populate the dynamic inventory\n  lineinfile:\n    state: present\n    dest: configs/inventory.dynamic\n    insertafter: '\\[digitalocean\\]'\n    regexp: \"^{{ item.networks.v4[0].ip_address }}.*\"\n    line: \"{{ item.networks.v4[0].ip_address }}\"\n  with_items:\n    - \"{{ do_droplets.json.droplets }}\"\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "fb9aa74e11323ee0ea41f60daa1e1fdf47634321", "filename": "roles/manage-confluence-space/tasks/create_confluence_space.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Create Confluence Space\n  uri:\n    url: '{{ confluence.destination.url }}/wiki/rest/api/space'\n    method: POST\n    user: '{{ confluence.destination.username }}'\n    password: '{{ confluence.destination.password }}'\n    force_basic_auth: yes\n    status_code: 200\n    body_format: json\n    body: \"{{ lookup('template', 'space.j2') }}\"\n  register: space_content\n\n- name: set fact\n  set_fact:\n    destination_homepage_id: \"{{ space_content.json['homepage']['id']}}\""}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "9ac5bf2f38f9bef2e98559c7afd6325c21ab3d22", "filename": "roles/openshift-labels/tasks/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: \"Set defaults\"\n  set_fact:\n    target_namespace: \"{{ target_namespace | default('') }}\"\n\n- name: \"Apply label {{ label }} to object {{ target_object }}\"\n  command: >\n    oc label --overwrite {{ target_object }} {{ target_name }} {{ label }} {{ target_namespace }}\n  when:\n  - target_object is defined\n  - target_object|trim != ''\n  - target_name is defined\n  - target_name|trim != ''\n  - label is defined\n  - label| trim != ''\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ffebcb642cf3100799c520cfcb355ea83da8bec0", "filename": "playbooks/provisioning/openstack/custom-actions/add-yum-repos.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: cluster_hosts\n  vars:\n    yum_repos: []\n  tasks:\n  # enable additional yum repos\n  - name: Add repository\n    yum_repository:\n      name: \"{{ item.name }}\"\n      description: \"{{ item.description }}\"\n      baseurl: \"{{ item.baseurl }}\"\n    with_items: \"{{ yum_repos }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4d5bc0fe549946f072d4a4f94f2eaf5e41d063d4", "filename": "roles/config-vlans/tests/inventory/group_vars/infra_hosts.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nvlans:\n- device: tenant.vlan11\n  physdev: eth0\n  vlan_id: 11\n- device: tenant.vlan12\n  physdev: eth0\n  vlan_id: 12\n- device: tenant.vlan13\n  physdev: eth0\n  vlan_id: 13\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4b3d7bf2573e31d667e4061d08dc01dbb9ba8c75", "filename": "roles/user-management/populate-users/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Convert csv to json - set facts\n  delegate_to: localhost\n  set_fact:\n    users: \"{{ lookup('csvtojson', 'file=' + csv_doc_file_name + ' var=users') }}\"\n    user_groups: \"{{ lookup('csvtojson', 'file=' + csv_doc_file_name + ' var=user_groups') }}\"\n\n- name: \"Display imported users values\"\n  debug:\n    var: users\n    verbosity: 2\n  \n- name: \"Display imported user_groups values\"\n  debug:\n    var: user_groups\n    verbosity: 2\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "7d38866fc1872fa732374799c0b0f78579b692dd", "filename": "roles/kalite/tasks/install-f18.yml", "repository": "iiab/iiab", "decoded_content": "# This is for Fedora 18, assumed to be an XO\n\n- name: Install dependent packages (Fedora 18)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - python-psutil\n    - expect\n  when: is_F18\n\n- name: Install dependent pip packages (Fedora 18)\n  pip:\n    name: selenium\n  when: internet_available and is_F18\n\n- name: Determine if KA Lite is already downloaded\n  stat:\n    path: \"{{ downloads_dir }}/ka-lite\"\n  register: kalite\n\n- name: Download the latest KA Lite repo\n  git:\n    repo: \"{{ kalite_repo_url }}\"\n    dest: \"{{ downloads_dir }}/ka-lite\"\n    depth: 1\n    version: 0.13.x\n  ignore_errors: yes\n  when: internet_available and kalite.stat.exists is defined and not kalite.stat.exists\n\n- name: Create iiab-kalite user and password (Fedora 18)\n  user:\n    name: \"{{ kalite_user }}\"\n    password: \"{{ kalite_password_hash }}\"\n    update_password: on_create\n\n- name: Create kalite_root directory (Fedora 18)\n  file:\n    path: \"{{ kalite_root }}\"\n    owner: root\n    group: root\n    mode: 0755\n    state: directory\n\n- name: Copy the KA Lite repo into place (Fedora 18)\n  command: \"rsync -at {{ downloads_dir }}/ka-lite/ {{ kalite_root }}\"\n\n- name: Make kalite_user owner\n  file:\n    path: \"{{ kalite_root }}\"\n    owner: \"{{ kalite_user }}\"\n    group: \"{{ kalite_user }}\"\n    recurse: yes\n    state: directory\n\n# local_settings is deprecated\n- name: Copy local_settings file\n  template:\n    src: f18/local_settings.py.j2\n    dest: \"{{ kalite_root }}/kalite/local_settings.py\"\n    owner: \"{{ kalite_user }}\"\n    group: \"{{ kalite_user }}\"\n    mode: 0644\n\n- name: Create kalite-serve & kalite-cron services, and iiab_cronservectl.sh\n  template:\n    backup: no\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: root\n    group: root\n    mode: \"{{ item.mode }}\"\n  with_items:\n    - { src: 'f18/kalite-serve.service.j2', dest: '/etc/systemd/system/kalite-serve.service', mode: '0644'}\n    - { src: 'f18/kalite-cron.service.j2', dest: '/etc/systemd/system/kalite-cron.service', mode: '0644'}\n    - { src: 'f18/iiab_cronservectl.sh.j2', dest: '{{ kalite_root }}/scripts/iiab_cronservectl.sh', mode: '0755'}\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "11ffb74ddd249193d4569161dcfc2fd4ef006056", "filename": "ops/playbooks/includes/find_ucp.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n#\n# find first valid ucp server\n#\n#   sets ucp_instance to the name of the first instance found\n#   sets ucp_instance to \".none.\" if no instance was found\n#\n\n\n- name: \"Find UCP: Init\"\n  set_fact:\n    checks:\n      status: -1\n\n\n- name: check\n  uri:\n    url: \"https://{{ item }}.{{ domain_name }}/auth/login\"\n    headers:\n      Content-Type: application/json\n    method: POST\n    status_code: 200,500\n    body_format: json\n    validate_certs: no\n    body: '{\"username\":\"{{ ucp_username }}\",\"password\":\"{{ ucp_password }}\"}'\n  delegate_to: localhost\n  register: checks\n  failed_when: false\n  changed_when: false\n  when: checks.status !=200 and checks.status != 500 \n  with_items: \"{{ ping_servers }}\"\n  no_log: yes\n\n- set_fact: ucp_instance=\".none.\"\n- name: set ucp_instance\n  set_fact: ucp_instance={{ item.item }}\n  when: \"'status' in item and item.status == 200\"\n  with_items: \"{{ checks.results }}\"\n  no_log: yes\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "464670fc09083938eda4e1e46ecc472ae558fa91", "filename": "roles/subscription-manager/pre_tasks/pre_tasks.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: \"Set password fact\"\n  set_fact:\n    rhsm_password: \"{{ rhsm_password | default(None) }}\"\n  no_log: true\n\n- name: \"Initialize Subscription Manager fact\"\n  set_fact:\n    rhsm_register: true\n\n- name: \"Determine if Subscription Manager should be used\"\n  set_fact:\n    rhsm_register: false\n  when:\n    - rhsm_satellite is undefined or rhsm_satellite is none or rhsm_satellite|trim == ''\n    - rhsm_username is undefined or rhsm_username is none or rhsm_username|trim == ''\n    - rhsm_password is undefined or rhsm_password is none or rhsm_password|trim == ''\n    - rhsm_org is undefined or rhsm_org is none or rhsm_org|trim == ''\n    - rhsm_activationkey is undefined or rhsm_activationkey is none or rhsm_activationkey|trim == ''\n    - rhsm_pool is undefined or rhsm_pool is none or rhsm_pool|trim == ''\n\n- name: \"Validate Subscription Manager organization is set\"\n  fail: msg=\"Cannot register to a Satellite server without a value for the Organization via 'rhsm_org'\"\n  when:\n    - rhsm_org is undefined or rhsm_org is none or rhsm_org|trim == ''\n    - rhsm_satellite is defined\n    - rhsm_satellite is not none\n    - rhsm_satellite|trim != ''\n    - rhsm_register\n\n- name: \"Validate Subscription Manager authentication is defined\"\n  fail: msg=\"Cannot register without ('rhsm_username' and 'rhsm_password') or 'rhsm_activationkey' variables set. See the README.md for details on securely prompting for a password\"\n  when:\n    - (rhsm_username is undefined or rhsm_username is none or rhsm_username|trim == '') or (rhsm_password is undefined or rhsm_password is none or rhsm_password|trim == '')\n    - rhsm_activationkey is undefined or rhsm_activationkey is none or rhsm_activationkey|trim == ''\n    - rhsm_register\n\n- name: \"Validate activation key and Hosted are not requested together\"\n  fail: msg=\"Cannot register to RHSM Hosted with 'rhsm_activationkey'\"\n  when:\n    - rhsm_satellite is undefined or rhsm_satellite is none or rhsm_satellite|trim == ''\n    - rhsm_activationkey is defined\n    - rhsm_activationkey is not none\n    - rhsm_activationkey|trim != ''\n    - rhsm_register\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "3b3e68da3caeea3b0b8ded48bc2ccf90c0af84d4", "filename": "roles/ovirt-common/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# check variables setting ovirt source\n- name: complain if no ovirt source is specified\n  fail:\n    msg: \"At least one of: 'ovirt_repo_file', 'ovirt_rpm_repo' or 'ovirt_repo'\n      must be specified. More information in the README\"\n  when: \"{{ not (ovirt_repo_file or ovirt_rpm_repo is defined or ovirt_repo)\n    }}\"\n\n# install libselinux-python on machine - selinux policy\n- name: install libselinux-python for ansible\n  yum:\n    name: libselinux-python\n    state: \"present\"\n\n# backup repos\n- name: creating directory repo-backup in yum.repos.d\n  file:\n    path: /tmp/repo-backup\n    state: directory\n\n- name: create repository backup\n  shell: 'cp /etc/yum.repos.d/*.repo /tmp/repo-backup'\n  tags:\n    - skip_ansible_lint\n\n## OPTIONS\n# 1) get repository files\n- name: copy repository files\n  get_url:\n    url: \"{{ item.url }}\"\n    dest: \"/etc/yum.repos.d/{{ item.name | default('') }}\"\n    force_basic_auth: yes\n    force: \"{{ item.force | default('no') }}\"\n  with_items: \"{{ ovirt_repo_file }}\"\n\n# 2) install from rpm\n- name: install rpm repository package\n  yum:\n    name: \"{{ ovirt_rpm_repo }}\"\n    state: present\n  when: \"{{ ovirt_rpm_repo is defined }}\"\n\n# 3) create repository files\n- name: create repository files\n  yum_repository:\n    name: \"{{ item.name }}\"\n    description: \"{{ item.name }}\"\n    baseurl: \"{{ item.url }}\"\n    enabled: \"{{ item.enabled | default('yes') }}\"\n  with_items: \"{{ ovirt_repo }}\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "81b9cd4b2ca4aba91cc0bf3f140b5cf0f0dc12f8", "filename": "ops/playbooks/roles/hpe.haproxy/templates/15-grafana-backends.j2", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "    server {{ inventory_hostname }} {{ hostvars[inventory_hostnamehost].ip_addr | ipaddr('address') }}:3000 check\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "7033dfa1afb2c95e35219b183d55cd9e432d0137", "filename": "playbooks/dns/config_dns_server.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n# Configure DNS Server(s)\n- hosts: dns\n  pre_tasks:\n  - name: \"Include the views\"\n    include_vars: vars/views.yml\n    delegate_to: localhost\n  roles:\n    - role: infra-ansible/roles/dns-server\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "eb5e6c94477a2ba746ffae77f1c07ba0583483d9", "filename": "roles/kubernetes-node/tasks/main.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- name: deploy host as kubernetes node\n  command: \"kubeadm join --token {{ kubernetes_token }} {{ groups['masters'][0] }}:6443 --skip-preflight-checks\"\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "5d19341660db69c50266c24237db55f42ebd9803", "filename": "reference-architecture/rhv-ansible/playbooks/setup-gluster.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# This playbook will use local storage on the RHV hypervisor nodes to create\n# a glusterfs cluster with exports for use as RHV's data storage\n#\n- hosts: hypervisors\n  gather_facts: yes\n  become: yes\n  serial: 1\n  tags:\n    - rhsm\n  vars_files:\n    - vars/vault.yaml\n    - vars/main.yaml\n  roles:\n    - rhsm-subscription\n\n- name: Install and configure Glusterfs prerequisites\n  hosts: hypervisors\n  become: yes\n  tags:\n    - install\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - gluster-rhsm-repos\n    - gluster-crs-prerequisites\n    - gluster-ports\n\n- name: Set up basic Glusterfs cluster\n  hosts: head\n  become: yes\n  tags:\n    - configure\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - gdeployer\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "21ffc9a4452a72b50d5da6394275f85d53e47468", "filename": "playbooks/roles/stenographer/vars/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# vars file for stenographer"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "bf9b2a7e054ca671cb03e33beae407ea8eec027d", "filename": "ops/playbooks/backup_dtr_images.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: dtr_main \n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - ../group_vars/backups\n\n  environment:\n    - \"{{ env }}\"\n\n  tasks:\n#\n# configure password less ssh from ucp_main to our ansible box\n#\n    - name: Register key\n      stat: path=/root/.ssh/id_rsa\n      register: key\n    - name: Create keypairs\n      shell: ssh-keygen -t rsa -f /root/.ssh/id_rsa -N ''\n      when: key.stat.exists == False\n    - name: Fetch all public ssh keys\n      shell: cat ~/.ssh/id_rsa.pub\n      register: ssh_keys\n    - name: Deploy keys on localhost\n      authorized_key: user=root key=\"{{ item }}\"\n      delegate_to: localhost\n      with_items:\n        - \"{{ ssh_keys.stdout }}\"\n\n#\n# Get a timestamp, will be used to name the backup\n#\n    - name: Get TimeStamp\n      command: date -u '+%Y_%m_%d_%H%M%S'\n      register: timestamp\n    - name: Store the timestamp\n      set_fact:\n        timestamp: \"{{ timestamp.stdout }}\"\n\n    - name: Creates directory\n      file:\n        path: /root/scripts\n        state: directory\n\n    - name: Copy backup script to backup VM\n      template: src=../templates/backup_dtr_images.sh.j2 dest=/root/scripts/backup_dtr_images.sh\n    - file:\n        path: /root/scripts/backup_dtr_images.sh\n        mode: 0744\n\n    - name: Copy util to backup VM\n      template: src=../templates/dtr_get_info.sh.j2 dest=/root/scripts/dtr_get_info.sh\n    - file:\n        path: /root/scripts/dtr_get_info.sh\n        mode: 0744\n\n    - name: Get the replica ID\n      shell: /root/scripts/dtr_get_info.sh replica\n      register: res \n\n    - set_fact:\n        replica_id: \"{{ res.stdout }}\" \n\n    - name: Get the DTR version number\n      shell: /root/scripts/dtr_get_info.sh version\n      register: res\n\n    - set_fact:\n        detected_dtr_version: \"{{ res.stdout }}\"\n\n    - set_fact:\n        backup_name:  \"backup_dtr_data_{{ replica_id }}_{{ inventory_hostname }}_{{ timestamp }}\"\n      when: backup_name is undefined\n\n    - name: Backup the image data\n      shell: /root/scripts/backup_dtr_images.sh {{ backup_name }}\n      register: res\n\n    - name: Create a temporary directory to receive files that contains backup metadata\n      tempfile:\n        state: directory\n        suffix: temp\n      register: res\n      delegate_to: localhost\n\n    - template:\n        src: ../templates/backup_meta.yml.j2\n        dest: \"{{ res.path }}/meta.yml\"\n      delegate_to: localhost\n\n    - copy:\n         src: \"{{ item }}\"\n         dest: \"{{ res.path }}/\"\n      with_items:\n        - ../vm_hosts\n        - ../group_vars/vars\n        - ../group_vars/backups\n      delegate_to: localhost\n\n    - name: Save the backup Metadata as well\n      archive:\n        path:\n          - \"{{ res.path }}/\"\n        dest: \"{{ backup_dest }}/{{ backup_name }}.vars.tgz\"\n      delegate_to: localhost\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "389e7d5beaa24361b9208c90eb6dd509f31501fa", "filename": "tasks/replication/slave.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n- name: MYSQL_VARIABLES | Set read only\n  mysql_variables:\n    variable: read_only\n    value: 'ON'\n  when: mariadb_slave_readonly\n\n- name: TEMPLATE | Deploy slave configuration\n  template:\n    src: etc/mysql/conf.d/51-slave.cnf.j2\n    dest: /etc/mysql/conf.d/51-slave.cnf\n  notify: restart mariadb\n\n- name: MYSQL_REPLICATION | Get slave status\n  mysql_replication:\n    mode: getslave\n  ignore_errors: yes\n  register: slave_status\n\n- name: INCLUDE | Transfert /etc/mysql/debian.cnf from master\n  include: slave/ssh.yml\n  when: mariadb_slave_replicate_mysqldb or ((slave_status.failed is defined or not slave_status.Is_Slave) and mariadb_slave_import_data)\n\n- name: INCLUDE | Import data\n  include: slave/import_data.yml\n  when: (slave_status.failed is defined or not slave_status.Is_Slave) and mariadb_slave_import_data\n\n- name: INCLUDE | Configure replication\n  include: slave/replication.yml\n  when: (slave_status.failed is defined or not slave_status.Is_Slave) or mariadb_slave_force_setup\n\n- name: INCLUDE | Transfert /etc/mysql/debian.cnf from master\n  include: slave/debiancnf.yml\n  when: mariadb_slave_replicate_mysqldb\n\n- name: MYSQL_REPLICATION | Get slave status\n  mysql_replication:\n    mode: getslave\n  ignore_errors: yes\n  register: slave_status\n\n- name: Configure GTID\n  include: slave/gtid.yml\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "9473b3da710aab1fe28244f1484b25dbf6b06897", "filename": "roles/dockerbench/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for dockerbench\ndockerbench_run_test: false\ndockerbench_dest: /opt/dockerbench\ndockerbench_repo: https://github.com/docker/docker-bench-security.git\ndockerbench_warn_threshold: 52\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "429e53fd5f6a919a354dc5a60b87edb2bdfeaaf9", "filename": "roles/scm/gitlab.com/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- hosts: localhost\n  remote_user: root\n  tasks:\n    - include_role:\n        name: \"{{ playbook_dir }}/../../gitlab.com\""}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "d407932bf0a004c013462532600a11639374a47f", "filename": "roles/ipareplica/tasks/python_2_3_test.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "---\n- block:\n  - name: Verify Python3 import\n    script: py3test.py\n    register: result_py3test\n    failed_when: False\n    changed_when: False\n    check_mode: no\n\n  - name: Set python interpreter to 3\n    set_fact:\n      ansible_python_interpreter: \"/usr/bin/python3\"\n    when: result_py3test.rc == 0\n\n  - name: Fail for IPA 4.5.90\n    fail: msg=\"You need to install python2 bindings for ipa server usage\"\n    when: result_py3test.rc != 0 and \"not usable with python3\" in\n          result_py3test.stdout\n\n  - name: Set python interpreter to 2\n    set_fact:\n      ansible_python_interpreter: \"/usr/bin/python2\"\n    when: result_py3test.failed or result_py3test.rc != 0\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f46c813e2e1d46d0940e169a897c6c2d252fa507", "filename": "roles/ansible/tower/manage-job-templates/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- block: # when ansible_tower.job_templates is defined\n\n  - name: \"Initilize facts\"\n    set_fact:\n      processed_job_templates: []\n      existing_job_templates_output: []\n      existing_inventories_output: []\n      existing_projects_output: []\n      existing_credentials_output: []\n\n  # Utilize the `rest_get` library routine to ensure REST pagination is handled\n  - name: \"Get the existing inventories\"\n    rest_get:\n      host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n      rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      rest_password: \"{{ ansible_tower.admin_password }}\"\n      api_uri: \"/api/v2/inventories/\"\n    register: existing_inventories_output\n\n  # Utilize the `rest_get` library routine to ensure REST pagination is handled\n  - name: \"Get the existing projects\"\n    rest_get:\n      host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n      rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      rest_password: \"{{ ansible_tower.admin_password }}\"\n      api_uri: \"/api/v2/projects/\"\n    register: existing_projects_output\n\n  # Utilize the `rest_get` library routine to ensure REST pagination is handled\n  - name: \"Get the existing job templates\"\n    rest_get:\n      host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n      rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      rest_password: \"{{ ansible_tower.admin_password }}\"\n      api_uri: \"/api/v2/job_templates/\"\n    register: existing_job_templates_output\n\n  # Utilize the `rest_get` library routine to ensure REST pagination is handled\n  - name: \"Get the existing credentials\"\n    rest_get:\n      host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n      rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      rest_password: \"{{ ansible_tower.admin_password }}\"\n      api_uri: \"/api/v2/credentials/\"\n    register: existing_credentials_output\n\n  # Utilize the `rest_get` library routine to ensure REST pagination is handled\n  - name: \"Get the existing users\"\n    rest_get:\n      host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n      rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      rest_password: \"{{ ansible_tower.admin_password }}\"\n      api_uri: \"/api/v2/users/\"\n    register: existing_users_output\n\n  # Utilize the `rest_get` library routine to ensure REST pagination is handled\n  - name: \"Get the existing teams\"\n    rest_get:\n      host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n      rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      rest_password: \"{{ ansible_tower.admin_password }}\"\n      api_uri: \"/api/v2/teams/\"\n    register: existing_teams_output\n\n  - name: \"Process the inventory job template\"\n    include_tasks: process-job-template.yml\n    with_items:\n    - \"{{ ansible_tower.job_templates }}\"\n    loop_control:\n      loop_var: job_template\n\n  - name: \"Elminate the job templates that should not be present\"\n    uri:\n      url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/job_templates/{{ item.id }}/\"\n      user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      password: \"{{ ansible_tower.admin_password }}\"\n      force_basic_auth: yes\n      method: DELETE\n      validate_certs: no\n      status_code: 200,204\n    with_items:\n    - \"{{ existing_job_templates_output.rest_output | get_remaining_items(processed_job_templates, 'name', 'name')}}\"\n\n  when:\n  - ansible_tower.job_templates is defined\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "7621dc2cc1289f97efeae0a604206071e37a7a55", "filename": "roles/rhsm-repos/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- block:\n    - name: \"Obtain currently enabled repos\"\n      shell: 'subscription-manager repos --list-enabled | sed -ne \"s/^Repo ID:[^a-zA-Z0-9]*\\(.*\\)/\\1/p\"'\n      register: enabled_repos\n\n    - name: \"Disable repositories that should not be enabled\"\n      shell: \"subscription-manager repos --disable={{ item }}\"\n      with_items:\n        - \"{{ enabled_repos.stdout_lines | difference(openshift_required_repos) }}\"\n\n    - name: \"Enable specified repositories not already enabled\"\n      command: \"subscription-manager repos --enable={{ item }}\"\n      with_items:\n        - \"{{ openshift_required_repos | difference(enabled_repos.stdout_lines) }}\"\n      register: subscribe_repos\n      until: subscribe_repos | succeeded\n\n    - name: check to see if rhui exists\n      stat:\n        path: /etc/yum.repos.d/redhat-rhui.repo\n      register: rhui\n\n    - name: check to see if rhui client exists\n      stat:\n        path: /etc/yum.repos.d/redhat-rhui-client-config.repo\n      register: client\n\n    - name: disable rhui repo\n      replace:\n        dest: /etc/yum.repos.d/redhat-rhui.repo\n        regexp: 'enabled=1'\n        replace: 'enabled=0'\n      when: rhui.stat.exists\n\n    - name: disable rhui client repos\n      replace:\n        dest: /etc/yum.repos.d/redhat-rhui-client-config.repo\n        regexp: 'enabled=1'\n        replace: 'enabled=0'\n      when: client.stat.exists\n\n  when: ansible_distribution == \"RedHat\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "299bbf45460383d2c62e66cd76f91e1a37452b2e", "filename": "roles/ansible/tower/manage-projects/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\ndefault_ansible_tower_url: 'https://localhost'\ndefault_ansible_tower_admin_username: 'admin'\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "c9c6754bb4b2ab84416b655dc6540b555a1a15c7", "filename": "playbooks/update.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: cluster_hosts\n  gather_facts: yes\n  become: yes\n  roles:\n  - update-instances\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "68f0c332170a0ea60d6f2ef6ae814c5b65be13f2", "filename": "playbooks/provision-dns-server/README.md", "repository": "redhat-cop/infra-ansible", "decoded_content": "# DNS Server Playbook\n\nThis playbook directory has the playbook(s) necessary to manage your DNS server(s).\n\n## Prerequisites\n\nOne of the two:\n- a set of running instance(s)\n- a IaaS that allow for provisioning through these playbooks\n\n\n## Example\n\n### Inventory\n\nPlease see the **sample** inventory in the inventory area:\n\n- [dns-server](../../inventory/dns-server/README.md)\n\nYou will need to modify this sample inventory to fit your desired configuration.\n\n### Playbook execution\n\nDepending on how this is being hosted, the initial may need the `tags='install'` set to ensure all necessary software is installed:\n\n```bash\n> ansible-playbook -i inventory main.yml --tags='install'\n```\n\nAny consecutive runs can be done without the 'install' tag to speed up execution:\n```bash\n> ansible-playbook -i inventory main.yml\n```\n\nLicense\n-------\n\nApache License 2.0\n\n\nAuthor Information\n------------------\n\nRed Hat Community of Practice & staff of the Red Hat Open Innovation Labs.\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "18a98d4fee8c7626b50db4ba73664453cd5167d3", "filename": "roles/cdi/tasks/deprovision.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- name: Render {{ cdi_namespace }} ResourceQuota deprovision yaml\n  template:\n    src:  cdi-resourcequota.yml\n    dest: /tmp/cdi-deprovision-resourcequota.yml\n\n- name: Delete {{ cdi_namespace }} ResourceQuota\n  command: kubectl delete -f /tmp/cdi-deprovision-resourcequota.yml -n {{ cdi_namespace }} --ignore-not-found\n\n- name: Render CDI deprovision yaml\n  template:\n    src:  cdi-controller-deployment.yml\n    dest: /tmp/cdi-deprovision.yml\n\n- name: Delete CDI Resources\n  command: kubectl delete -f /tmp/cdi-deprovision.yml -n {{ cdi_namespace }} --ignore-not-found\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "108afdc441b8676520076c8782d10b70aec7f03c", "filename": "reference-architecture/gcp/ansible/playbooks/roles/gold-image-instance/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- block:\n  - name: register host\n    include_role:\n      name: rhsm-subscription\n\n  - name: enable repos\n    include_role:\n      name: rhsm-repos\n\n  - name: install centos-release-openshift-origin repo\n    package:\n      name: centos-release-openshift-origin\n      state: present\n    when: openshift_deployment_type == 'origin'\n\n  - name: check subscription. if this task fails, please check your username, password and pool id\n    command: yum list atomic-openshift-utils\n    changed_when: false\n\n  - block:\n    - name: add google repo\n      yum_repository:\n        name: google-cloud-compute\n        description: Google Cloud Compute\n        baseurl: https://packages.cloud.google.com/yum/repos/google-cloud-compute-el7-x86_64\n        enabled: true\n        gpgcheck: true\n        repo_gpgcheck: true\n        gpgkey: \"https://packages.cloud.google.com/yum/doc/yum-key.gpg\\n https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\"\n\n    - name: remove irqbalance package\n      package:\n        name: irqbalance\n        state: absent\n    when: openshift_deployment_type == 'openshift-enterprise'\n\n  - name: install PyYAML for update-instances\n    package:\n      name: PyYAML\n      state: present\n\n  - name: update instance\n    include_role:\n      name: update-instances\n\n  - name: install required packages\n    package:\n      name: '{{ item }}'\n      state: present\n    with_items:\n    - google-compute-engine\n    - cloud-init\n    - docker\n    - iptables-services\n\n  - name: add disk_setup to cloud_config_modules\n    lineinfile:\n      dest: /etc/cloud/cloud.cfg\n      insertafter: '^cloud_config_modules:$'\n      line: ' - disk_setup'\n      state: present\n\n  - name: clear yum cache\n    command: yum clean all\n\n  - name: unregister the system\n    include_role:\n      name: rhsm-unregister\n\n  rescue:\n  - name: unregister the system\n    include_role:\n      name: rhsm-unregister\n\n  - name: delete temp instance\n    include_role:\n      name: deployment-delete\n    vars:\n      deployment_name: tmp-instance\n    delegate_to: localhost\n\n  - name: delete temp instance disk\n    gce_pd:\n      name: '{{ prefix }}-tmp-instance'\n      zone: '{{ gcloud_zone }}'\n      service_account_email: '{{ service_account_id }}'\n      credentials_file: '{{ credentials_file }}'\n      project_id: '{{ gcloud_project }}'\n      state: absent\n    delegate_to: localhost\n\n  - name: delete local ssh config for temp instance\n    include_role:\n      name: ssh-config-tmp-instance-delete\n    delegate_to: localhost\n\n  - name: fail after cleanup\n    fail:\n      msg: Failed to create gold image. Please look for an error in the output above.\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "1e585705b0997bb08ea630b9834beefecd31302a", "filename": "roles/keepalived/tasks/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - keepalived\n  - libsemanage-python\n  notify: 'start and enable keepalived services'\n\n\n- name: 'Allow bind to the VIP'\n  lineinfile: \n    path: /etc/sysctl.d/50-keepalived.conf\n    regexp: '^net.ipv4.ip_nonlocal_bind.*'\n    line: 'net.ipv4.ip_nonlocal_bind=1'\n    create: yes\n    state: present\n    owner: root\n    group: root\n    mode: 0644\n  notify: 'reload sysctl'\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "cd84e2becee5c0334caec5caf354fdc5b5b8fa3a", "filename": "roles/config-postgresql/tasks/firewall.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Check if firewalld is installed\n  command: systemctl status firewalld\n  register: firewalld_status\n  failed_when: false\n  changed_when: false\n\n- name: Check if iptables is installed\n  command: systemctl status iptables\n  register: iptables_status\n  failed_when: false\n  changed_when: false\n\n- name: Open port in firewalld\n  firewalld:\n    port: \"{{ postgresql_port }}/TCP\"\n    permanent: true\n    state: enabled\n  when: firewalld_status.rc == 0\n  notify:\n  - restart firewalld\n\n- name: Ensure iptables is correctly configured \n  lineinfile:\n    insertafter: \"^-A INPUT .* --dport {{ postgresql_port }} .* ACCEPT\"\n    state: present\n    dest: /etc/sysconfig/iptables\n    regexp: \"^-A INPUT .* --dport {{ postgresql_port }} .* ACCEPT\"\n    line: \"-A INPUT -p TCP -m state --state NEW -m TCP --dport {{ postgresql_port }} -j ACCEPT\"\n  when: iptables_status.rc == 0 and firewalld_status.rc != 0 \n  notify:\n  - restart iptables\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "dbe88f24aa3b9341e15b1d4d6a5b99319fd2affe", "filename": "roles/openshift-pv-cleanup/test/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n\n- hosts: seed-hosts\n  roles:\n  - openshift-pv-cleanup\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "699e26ba24a2f007753f1dd6315ff8073abe5869", "filename": "tasks/install/mariadb/default.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n- name: SHELL | Get MariaDB target version\n  shell: \"LANG=C apt-cache depends mariadb-server | awk -F '-' '/Depends/ { print $NF }'\"\n  register: apt_mariadb_version\n  changed_when: false\n\n- name: DEBCONF | Prepare MariaDB silent installation (root password)\n  debconf:\n    name: 'mariadb-server-{{ apt_mariadb_version.stdout }}'\n    question: 'mysql-server/root_password'\n    vtype: 'password'\n    value: '{{ mariadb_root_password }}'\n  when: not mariadb_exists.stat.exists\n\n- name: DEBCONF | Prepare MariaDB silent installation (root password again)\n  debconf:\n    name: 'mariadb-server-{{ apt_mariadb_version.stdout }}'\n    question: 'mysql-server/root_password_again'\n    vtype: 'password'\n    value: '{{ mariadb_root_password }}'\n  when: not mariadb_exists.stat.exists\n\n- name: APT | Install MariaDB server\n  apt:\n    pkg: mariadb-server\n\n- name: APT | Install Galera\n  apt:\n    pkg: galera-3\n  when: mariadb_vendor == 'mariadb_galera'\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "fc065c37564a46711824f169567af9f2bc083f48", "filename": "roles/vpn/tasks/iptables.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Iptables configured\n  template: src=\"{{ item.src }}\" dest=\"{{ item.dest }}\" owner=root group=root mode=0640\n  with_items:\n    - { src: rules.v4.j2, dest: /etc/iptables/rules.v4 }\n  notify:\n    - restart iptables\n\n- name: Iptables configured\n  template: src=\"{{ item.src }}\" dest=\"{{ item.dest }}\" owner=root group=root mode=0640\n  when: ipv6_support is defined and ipv6_support == true\n  with_items:\n    - { src: rules.v6.j2, dest: /etc/iptables/rules.v6 }\n  notify:\n    - restart iptables\n"}, {"commit_sha": "01c4359d8ad17ba10149ac898663e598069b9055", "sha": "4f49ddea4ef0c453db9e3b9ad9873aea0805b5db", "filename": "tasks/fail2ban-RedHat.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\n- name: Install fail2ban.\n  yum: name=fail2ban state=present enablerepo=epel\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "28e7005bb55cd34f470e3080ca49fea25902424c", "filename": "roles/config-routes/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: 'prereq.yml'\n- import_tasks: 'route.yml'\n\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "34ab09533f9d4aa5fd0319e8b5877f4316aacdd9", "filename": "playbooks/gce/openshift-cluster/list.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Generate oo_list_hosts group\n  hosts: localhost\n  connection: local\n  become: no\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  tasks:\n  - set_fact: scratch_group=tag_clusterid-{{ cluster_id }}\n    when: cluster_id != ''\n  - set_fact: scratch_group=all\n    when: cluster_id == ''\n  - add_host:\n      name: \"{{ item }}\"\n      groups: oo_list_hosts\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n      oo_public_ipv4: \"{{ hostvars[item].gce_public_ip }}\"\n      oo_private_ipv4: \"{{ hostvars[item].gce_private_ip }}\"\n    with_items: \"{{ groups[scratch_group] | default([], true) | difference(['localhost']) | difference(groups.status_terminated | default([], true)) }}\"\n  - debug:\n      msg: \"{{ hostvars | oo_select_keys(groups[scratch_group] | default([])) | oo_pretty_print_cluster }}\"\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "3fa5081754fd85aa57bc9720a405d0e02bd613c2", "filename": "playbooks/utils/configure-std-ci-repos.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "- name: Configure std-ci repos\n  hosts: all\n  tasks:\n  - name: Configure std-ci repos\n    copy:\n      src: \"{{ std_ci_yum_repos }}\"\n      dest: \"/etc/yum.repos.d/{{ std_ci_yum_repos | basename }}.repo\"\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "0640ca8f692e1da3f929066bd4ac7c6e5a1531d6", "filename": "tasks/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Set distribution and python facts\n  set_fact:\n    _docker_os_dist: \"{{ ansible_distribution }}\"\n    _docker_os_dist_release: \"{{ ansible_distribution_release }}\"\n    _docker_os_dist_major_version: \"{{ ansible_distribution_major_version }}\"\n    _docker_os_arch: \"amd64\"\n    _docker_os_dist_check: yes\n    _docker_python3: \"{{ ansible_python_version is version('3', '>=') }}\"\n  tags: [\"always\"]\n\n- name: Reinterpret distribution facts for Linux Mint 18\n  set_fact:\n    _docker_os_dist: \"Ubuntu\"\n    _docker_os_dist_release: \"xenial\"\n    _docker_os_dist_major_version: 16\n  when:\n    _docker_os_dist == \"Linux Mint\" and\n    _docker_os_dist_major_version | int == 18\n  tags: [\"always\"]\n\n- name: Reinterpret distribution facts for Debian 10 (Buster) due to bug\n  set_fact:\n    _docker_os_dist: \"Debian\"\n    _docker_os_dist_release: \"buster\"\n    _docker_os_dist_major_version: 10\n  when:\n    - _docker_os_dist == \"Debian\"\n    - _docker_os_dist_release == \"buster\" or (ansible_lsb is defined\n        and ansible_lsb.codename is defined and ansible_lsb.codename == \"buster\")\n  tags: [\"always\"]\n\n- name: OS release info\n  raw: cat /etc/os-release\n  check_mode: no\n  changed_when: no\n  register: _docker_os_release_info\n  tags: [\"always\"]\n\n- name: Print OS release information\n  debug:\n    var: _docker_os_release_info\n    verbosity: 1\n  tags: [\"always\"]\n\n- name: Print LSB information\n  debug:\n    var: ansible_lsb\n    verbosity: 1\n  when:\n    - ansible_lsb is defined\n  tags: [\"always\"]\n\n- name: Reinterpret distribution facts for Raspbian\n  set_fact:\n    _docker_os_arch: \"armhf\"\n  when:\n    - _docker_os_release_info.stdout is search('raspbian')\n  tags: [\"always\"]\n\n- name: Reset role variables\n  set_fact:\n    docker_systemd_service_config_tweaks: []\n    docker_service_envs: {}\n  tags: [\"always\"]\n\n- name: Temporary handling of deprecated variable docker_enable_ce_edge (#54)\n  set_fact:\n    docker_channel: edge\n  when:\n    - docker_enable_ce_edge is defined\n    - docker_enable_ce_edge\n  tags: [\"always\"]\n\n- name: Temporary handling of deprecated variable docker_pkg_name\n  set_fact:\n    docker_version: \"{{ docker_pkg_name | regex_replace('^docker-ce.(.+)$', '\\\\1') }}\"\n  when: docker_pkg_name is match('docker-ce' + docker_os_pkg_version_separator[_docker_os_dist])\n  tags: [\"always\"]\n\n- name: Print interpreted distribution information\n  debug:\n    msg: \"distribution={{ _docker_os_dist }}, release={{ _docker_os_dist_release }}, major_version={{ _docker_os_dist_major_version }}\"\n    verbosity: 1\n  tags: [\"always\"]\n\n- name: Compatibility and distribution checks\n  include_tasks: checks.yml\n  when: docker_do_checks | bool\n  tags: [\"always\"]\n\n- name: Install and configure Docker CE\n  block:\n    - name: Network access disabled\n      debug:\n        msg: \"Tasks requiring network access will be skipped!\"\n      when: not docker_network_access | bool\n\n    - name: Remove Docker versions before Docker CE\n      include_tasks: remove-pre-docker-ce.yml\n      when:\n        - docker_remove_pre_ce | bool\n        - docker_network_access | bool\n      tags: [\"install\"]\n\n    - name: Setup Docker package repositories\n      include_tasks: setup-repository.yml\n      when: docker_setup_repos | bool\n      tags: [\"install\"]\n\n    - name: Install Docker\n      include_tasks: install-docker.yml\n      when: docker_network_access | bool\n      tags: [\"install\"]\n\n    - name: Configure audit logging\n      include_tasks: setup-audit.yml\n      tags: [\"configure\"]\n\n    - name: Apply workarounds for bugs and/or tweaks\n      include_tasks: bug-tweaks.yml\n      tags: [\"configure\"]\n\n    - name: Configure Docker\n      include_tasks: configure-docker.yml\n      tags: [\"configure\"]\n\n    - name: Postinstall tasks\n      include_tasks: postinstall.yml\n      when: docker_network_access | bool\n      tags: [\"install\", \"postinstall\"]\n  when: not docker_remove | bool\n\n- name: Remove Docker CE and related configuration\n  include_tasks: remove-docker.yml\n  when: docker_remove | bool\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7401b1d5107476603dd21d166992f30038e13871", "filename": "roles/osp/admin-network/tests/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nosp_networks:\n- name: \"my-network\"\n  description: \"My Network\"\n\nosp_subnets:\n- name: \"my-subnet\"\n  network_name: \"my-network\"\n  cidr: \"192.168.10.0/24\"\n  gateway_ip: \"192.168.10.1\"\n  allocation_pool_start: \"192.168.10.2\"\n  allocation_pool_end: \"192.168.10.254\"\n  dns_nameservers:\n  - \"192.168.1.11\"\n  - \"192.168.1.12\"\n\nosp_routers:\n- name: \"my-router\"\n  external_gateway: \"external\"\n  subnet: \"my-subnet\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "e20fe29d95235ff0ba1d735b4c42462ac81bd161", "filename": "dev/playbooks/docker_post_config.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: docker\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  vars:\n    dtr_lb: \"{{ groups['dtr_lb'][0] }}.{{ domain_name }}\"\n  tasks:\n    - name: Create Docker service directory\n      file:\n        path: /etc/systemd/system/docker.service.d\n        state: directory\n      notify: Restart Docker\n\n    - name: Add proxy details\n      template: src=../templates/http-proxy.conf.j2 dest=/etc/systemd/system/docker.service.d/http-proxy.conf\n      when: env.http_proxy is defined or env.https_proxy is defined\n      notify: Restart Docker\n\n    - name: Add insecure registry\n      lineinfile:\n        path: /usr/lib/systemd/system/docker.service\n        regexp: 'ExecStart=.*'\n        line: 'ExecStart=/usr/bin/dockerd --insecure-registry {{ dtr_lb }}'\n      notify: Restart Docker\n\n    - meta: flush_handlers\n\n    - name: Check if vsphere plugin is installed\n      shell: docker plugin ls | grep vsphere | wc -l\n      register: vsphere_installed\n\n    - name: Install vsphere plugin\n      command: docker plugin install --grant-all-permissions --alias vsphere vmware/vsphere-storage-for-docker:{{ vsphere_plugin_version }}\n      #command: docker plugin install --grant-all-permissions --alias vsphere vmware/docker-volume-vsphere:{{ vsphere_plugin_version }}\n      #command: docker plugin install --grant-all-permissions --alias vsphere store/vmware/docker-volume-vsphere:{{ vsphere_plugin_version }}\n      when: vsphere_installed.stdout == \"0\"\n\n  handlers:\n    - name: Restart Docker\n      systemd:\n        name: docker\n        state: restarted\n        daemon_reload: yes\n\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "f8d4c3bc12d6cd784f59fa4bf3c454498095637a", "filename": "roles/openshift-applier/tests/test.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- hosts: seed-hosts\n  roles:\n    - openshift-applier\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "c0e40232f478ea519141d095c44e56b87d372144", "filename": "ops/playbooks/roles/hpe.haproxy/defaults/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "---\n# defaults file for hpe.haproxy\nhpe_haproxy_server: \"localhost\"\nhpe_haproxy_env:\n  http_proxy: '10.12.7.21:8080'\n  https_proxy: '10.12.7.21:8080'\n  no_proxy: '/var/run/docker.sock,localhost,am2.cloudra.local,10.10.174.'\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "9c391940ff4c06eb33fba574502f64a88ffef27c", "filename": "roles/registrator/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n\n# Install (docker-py) python package as is a docker module dependency.\n- pip:\n    name: docker-py\n    version: 1.1.0\n  environment: proxy_env\n  tags:\n    - registrator\n\n# tasks file for docker registrator\n- name: run registrator container\n  docker:\n    name: registrator\n    image: \"{{ registrator_image }}\"\n    state: started\n    restart_policy: always\n    net: host\n    command: \"-internal {{ registrator_uri }}\"\n    volumes:\n    - \"/var/run/docker.sock:/tmp/docker.sock\"\n  environment: proxy_env\n  tags:\n    - registrator\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "f1b1c19d37e3bd7dd953901f33db6f027aea6c66", "filename": "playbooks/openshift/provision-aws.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- hosts: localhost\n  roles:\n  - role: manage-aws-infra\n    operation: inventory_gen_e2c\n\n- hosts: localhost\n  roles:\n  - role: manage-aws-infra\n    operation: deploy\n\n- name: Refresh Server inventory\n  hosts: localhost\n  connection: local\n  gather_facts: False\n  tasks:\n  - meta: refresh_inventory\n\n- hosts: localhost\n  roles:\n  - role: manage-aws-infra\n    operation: inventory_gen_hosts\n\n- name: Refresh Server inventory\n  hosts: localhost\n  connection: local\n  gather_facts: False\n  tasks:\n  - meta: refresh_inventory\n\n- hosts: cluster_hosts\n  gather_facts: false\n  tasks:\n  - name: Debug hostvar\n    debug:\n      msg: \"{{ hostvars[inventory_hostname] }}\"\n      verbosity: 2\n  - name: waiting for server to come back\n    local_action: wait_for host={{ hostvars[inventory_hostname]['ansible_host'] }} port=22 delay=30 timeout=300\n    become: false\n\n- hosts: cluster_hosts\n  tasks:\n  - name: Set Openshift Hostnames\n    set_fact:\n      openshift_hostname: \"{{ ec2_private_dns_name }}\"\n      openshift_public_hostname: \"{{ ec2_public_dns_name }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6317006a2262ce130bc903cb24062da77f66d5a9", "filename": "reference-architecture/aws-ansible/playbooks/roles/instance-groups/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Add bastion to group\n  add_host:\n    name: \"bastion.{{ public_hosted_zone }}\"\n    groups: bastion\n  when:\n    - byo_bastion == \"no\"\n\n- name: Add masters to requisite groups\n  add_host:\n    name: \"{{ hostvars[item].ec2_tag_Name }}\"\n    groups: masters, etcd, nodes, cluster_hosts\n    openshift_node_labels:\n      role: master\n      KubernetesCluster: \"{{ stack_name }}\"\n  with_items: \"{{ groups['tag_openshift_role_master'] }}\"\n  when:\n    - hostvars[item]['ec2_tag_KubernetesCluster'] == stack_name\n\n- name: Add a master to the single master group\n  add_host:\n    name: \"{{ item }}\"\n    groups: single_master\n    openshift_node_labels:\n      role: master\n      KubernetesCluster: \"{{ stack_name }}\"\n  with_items: \"{{ groups['masters'].0 }}\"\n\n- name: Add infra instances to host group\n  add_host:\n    name: \"{{ hostvars[item].ec2_tag_Name }}\"\n    groups: nodes, cluster_hosts, schedulable_nodes\n    openshift_node_labels:\n      role: infra\n      KubernetesCluster: \"{{ stack_name }}\"\n  with_items: \"{{ groups['tag_openshift_role_infra'] }}\"\n  when:\n    - hostvars[item]['ec2_tag_KubernetesCluster'] == stack_name\n\n- name: Add app instances to host group\n  add_host:\n    name: \"{{ hostvars[item].ec2_tag_Name }}\"\n    groups: nodes, cluster_hosts, schedulable_nodes\n    openshift_node_labels:\n      role: app\n      KubernetesCluster: \"{{ stack_name }}\"\n  with_items: \"{{ groups['tag_openshift_role_app'] }}\"\n  when:\n    - hostvars[item]['ec2_tag_KubernetesCluster'] == stack_name\n\n- name: Add storage instances to host group\n  add_host:\n    name: \"{{ hostvars[item].ec2_tag_Name }}\"\n    groups: nodes, cns, cluster_hosts, schedulable_nodes, glusterfs, storage\n    openshift_node_labels:\n      role: storage\n      KubernetesCluster: \"{{ stack_name }}\"\n      glusterfs_devices: '[ \"/dev/xvdd\" ]'\n  with_items: \"{{ groups['tag_openshift_role_storage'] }}\"\n  when:\n    - groups.tag_openshift_role_storage is defined and hostvars[item]['ec2_tag_KubernetesCluster'] == stack_name\n\n- name: Add new node instances to host group\n  add_host:\n    name: \"{{ hostvars[item].ec2_tag_Name }}\"\n    groups: new_nodes\n    openshift_node_labels:\n      role: \"{{ node_type }}\"\n      KubernetesCluster: \"{{ stack_name }}\"\n  with_items: \"{{ groups.tag_provision_node | default([]) }}\"\n  when:\n    - new_node_stack is defined and node_type != \"gluster\" and hostvars[item]['ec2_tag_aws_cloudformation_stack_name'] == new_node_stack and add_node is defined and add_node == \"yes\"\n\n- name: Add new gluster node instances to host group\n  add_host:\n    name: \"{{ hostvars[item].ec2_tag_Name }}\"\n    groups: new_nodes, gluster\n    openshift_node_labels:\n      role: storage\n      KubernetesCluster: \"{{ stack_name }}\"\n  with_items: \"{{ groups.tag_provision_node | default([]) }}\"\n  when:\n    - groups.tag_openshift_role_storage is defined and hostvars[item]['ec2_tag_KubernetesCluster'] == stack_name and add_node is defined and add_node == \"yes\"\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "d356a2617d99baa7635b2484c9d1224ddb07e40d", "filename": "tasks/Linux/fetch/adoptopenjdk-fallback.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Fetch download page\n  uri:\n    url: \"{{ adoptopenjdk_api_page }}\\\n      info/releases/\\\n      openjdk{{ java_major_version }}\\\n      ?openjdk_impl={{ adoptopenjdk_impl }}\\\n      &os=linux&arch=x64\\\n      &release=latest\\\n      &type={{ java_package }}&heap_size=normal\"\n    return_content: true\n    follow_redirects: all\n  register: download_page\n\n- name: 'Find release url'\n  set_fact:\n    release_url: >-\n      {{ (download_page.content | from_json).binaries | map(attribute='binary_link') | list +\n        (download_page.content | from_json).binaries | map(attribute='checksum_link') | list }}\n\n- name: Exit if AdoptOpenJDK version is not found\n  fail:\n    msg: 'AdoptOpenJDK version {{ java_major_version }} not found'\n  when: release_url[0] is not defined\n\n- name: 'Fetch artifact checksum file {{ release_url[1] }}'\n  uri:\n    url: '{{ release_url[1] }}'\n    return_content: true\n  register: artifact_checksum_file\n\n- name: 'Get artifact checksum from file {{ release_url[1] }}'\n  set_fact:\n    artifact_checksum: >-\n      {{ artifact_checksum_file['content'] |\n      regex_search('([^\\s]+)')\n      }}\n\n- name: 'Download artifact from {{ release_url[0] }}'\n  get_url:\n    url: '{{ release_url[0] }}'\n    dest: '{{ java_download_path }}'\n    checksum: 'sha256:{{ artifact_checksum }}'\n  register: file_downloaded\n  retries: 20\n  delay: 5\n  until: file_downloaded is succeeded\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "3bbe5a8729a300501588d758f30424920162c662", "filename": "roles/network/tasks/NM.yml", "repository": "iiab/iiab", "decoded_content": "- name: Restart NetworkManager services\n  service: name=NetworkManager\n           enabled=yes\n           state=stopped\n- service: name=NetworkManager-dispatcher\n           enabled=yes\n           state=stopped\n- wait_for: path=/etc/passwd\n            delay=4\n            timeout=5\n- service: name=NetworkManager\n           enabled=yes\n           state=started\n- wait_for: path=/etc/passwd\n            delay=4\n            timeout=5\n- service: name=NetworkManager-dispatcher\n           enabled=yes\n           state=started\n- debug:  msg=\"hopefully now NM is restarted\"\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "5c30e313d97a8854a60589f89dec8df5752acc1e", "filename": "roles/zookeeper/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for zookeeper\nzookeeper_client_port: 2181\nzookeeper_leader_connect_port: 2888\nzookeeper_leader_election_port: 3888\nzookeeper_server_group: zookeeper_servers\nzookeeper_id: \"\n    {%- for host in groups[zookeeper_server_group] -%}\n      {%- if host == 'default' or host == inventory_hostname or host == ansible_fqdn or host in ansible_all_ipv4_addresses -%}\n        {{ loop.index }}\n      {%- endif -%}\n    {%- endfor -%}\n\"\nconsul_dir: /etc/consul.d\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f719aaf471e80ea0e08a52adeee2aa94bfefd05b", "filename": "roles/config-clair/tasks/firewall.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Check if firewalld is installed\n  command: systemctl status firewalld\n  register: firewalld_status\n  failed_when: false\n  changed_when: false\n\n- name: Check if iptables is installed\n  command: systemctl status iptables\n  register: iptables_status\n  failed_when: false\n  changed_when: false\n\n- name: Open port in firewalld\n  firewalld:\n    port: \"{{ item }}/tcp\"\n    permanent: true\n    state: enabled\n  when: firewalld_status.rc == 0\n  with_items:\n    - \"{{ clair_host_proxy_port }}\"\n    - \"{{ clair_host_api_port }}\"\n  notify:\n  - restart firewalld\n\n- name: Ensure iptables is correctly configured\n  lineinfile:\n    insertafter: \"^-A INPUT .* --dport {{ item }} .* ACCEPT\"\n    state: present\n    dest: /etc/sysconfig/iptables\n    regexp: \"^-A INPUT .* --dport {{ item }} .* ACCEPT\"\n    line: \"-A INPUT -p TCP -m state --state NEW -m TCP --dport {{ item }} -j ACCEPT\"\n  with_items:\n    - \"{{ clair_host_proxy_port }}\"\n    - \"{{ clair_host_api_port }}\"\n  when: iptables_status.rc == 0 and firewalld_status.rc != 0\n  notify:\n  - restart iptables\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "271bbc4f176f94fc82e4978d0fb55869eaf884da", "filename": "reference-architecture/ansible-tower-integration/create_httpd_file/create_httpd_file/meta/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ngalaxy_info:\n  author: James Labocki\n  description: Takes username and password and creates httpd file for auth for OCP\n  company: Red Hat, Inc.\n  license: MIT\n  min_ansible_version: 1.2\n  platforms:\n  - name: EL\n    versions:\n    - 6\n    - 7\n  categories:\n  - packaging\n  - system\n  dependencies: []\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6a17855e733edb105cc4d610c58e5c68dc9561d2", "filename": "roles/node-network-manager/tasks/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: install NetworkManager\n  package:\n    name: NetworkManager\n    state: present\n\n- name: configure NetworkManager\n  lineinfile:\n    dest: \"/etc/sysconfig/network-scripts/ifcfg-{{ ansible_default_ipv4['interface'] }}\"\n    regexp: '^{{ item }}='\n    line: '{{ item }}=yes'\n    state: present\n    create: yes\n  with_items:\n  - 'USE_PEERDNS'\n  - 'NM_CONTROLLED'\n\n- name: enable and start NetworkManager\n  service:\n    name: NetworkManager\n    state: restarted\n    enabled: yes\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "5fc468cf018064f1213a6ec9062f7c0c7457c133", "filename": "reference-architecture/gcp/ansible/playbooks/roles/ssh-config-tmp-instance-delete/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: delete ssh config for ocp temp instance\n  blockinfile:\n    dest: '{{ ssh_config_file }}'\n    create: true\n    mode: 0600\n    marker: '# {mark} OPENSHIFT ON GCP TEMP INSTANCE BLOCK'\n    state: absent\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "24b6fe88a0ceb5dfb7a715f2eb3eebd9d26364cb", "filename": "roles/config-linux-desktop/config-xfce/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install, configure and enable XFCE\"\n  include_tasks: \"{{ distro_file }}\"\n  with_first_found:\n  - files:\n    - xfce-{{ ansible_distribution }}.yml\n    skip: true\n  loop_control:\n    loop_var: distro_file\n  when:\n  - xfce_install|default(False)\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f5394ce16e6b8f5854b62fe4618eb0a8f72621c5", "filename": "roles/config-iscsi-client/tasks/configure_lvm.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Detect the physical device based on iSCSI lun\"\n  shell: 'lsscsi | sed -ne \"s/.*:{{ disk.lun }}].*{{ iscsi_brand }}.*\\(\\/dev\\/.*\\)$/\\1/p\"'\n  register: lsscsi\n\n- name: \"Detect the multipath device id based on physical device\"\n  shell: 'multipath -l {{ lsscsi.stdout_lines[0] }} | sed -ne \"s/\\([^ ]*\\).*{{ iscsi_brand }}.*/\\1/p\"' \n  register: multipath_id\n\n- name: \"Setup and create PV & VG\"\n  lvg:\n    vg: \"{{ disk.vg }}\"\n    pvs: \"/dev/mapper/{{ multipath_id.stdout }}\"\n    force: yes\n\n- name: \"Setup LV\"\n  lvol: \n    vg: \"{{ disk.vg }}\"\n    lv: \"{{ disk.lv }}\"\n    force: yes\n    size: \"100%VG\"\n  when: \n  - disk.lv is defined\n  - disk.lv|trim != ''\n\n- name: \"Create file system on share\"\n  filesystem:\n    fstype: xfs\n    dev: \"/dev/mapper/{{ disk.vg }}-{{ disk.lv }}\"\n  when: \n  - disk.lv is defined\n  - disk.lv|trim != ''\n\n- name: \"Ensure the base dir for the mount point exists\" \n  file:\n    path: \"{{ disk.mount_path|dirname }}\"\n    state: directory\n  when: \n  - disk.mount_path is defined\n  - disk.mount_path|trim != ''\n\n- name: \"Mount the share\"\n  mount:\n    src: \"/dev/mapper/{{ disk.vg }}-{{ disk.lv }}\"\n    path: \"{{ disk.mount_path }}\"\n    opts: _netdev\n    fstype: xfs \n    state: mounted\n  when: \n  - disk.mount_path is defined\n  - disk.mount_path|trim != ''\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "2166c2fcbed0e2d6db5f88c84279bf7344798ea6", "filename": "reference-architecture/vmware-ansible/playbooks/setup.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  user: root\n  become: false\n  vars_files:\n    - vars/main.yaml\n  tasks:\n    - name: \"Create resource pool on vCenter\"\n      vmware_resource_pool:\n        hostname: \"{{ vcenter_host }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        datacenter: \"{{ vcenter_datacenter }}\"\n        cluster: \"{{ vcenter_cluster}}\"\n        resource_pool: \"{{ vcenter_resource_pool }}\"\n        state: \"present\"\n        validate_certs: False\n    - name: \"Create folder structure on vCenter\"\n      vmware_folder:\n        hostname: \"{{ vcenter_host }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        datacenter: \"{{ vcenter_datacenter }}\"\n        cluster: \"{{ vcenter_cluster}}\"\n        folder: \"{{ vcenter_folder }}\"\n        state: \"present\"\n        validate_certs: False\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "9706acb424c92cc45b6d7d499b54632eae15c45f", "filename": "tasks/section_05.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n- include: section_05_level1.yml\n  tags:\n  - section05\n  - level1\n\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "1fc9a54075ec7e15167076fa6f81817229065fde", "filename": "roles/osp/admin-network/tasks/manage-networks.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Set up network(s)\"\n  os_network:\n    cloud: \"{{ item.cloud | default(osp_default_cloud) | default(omit) }}\"\n    project: \"{{ item.project | default(omit) }}\"\n    state: \"{{ item.state | default(osp_resource_state) | default('present') }}\"\n    name: \"{{ item.name }}\"\n    external: \"{{ item.external | default(False) }}\"\n    provider_network_type: \"{{ item.provider_network_type | default(omit) }}\"\n    provider_physical_network: \"{{ item.provider_physical_network | default(omit) }}\"\n    provider_segmentation_id: \"{{ item.provider_segmentation_id | default(omit) }}\"\n  with_items:\n  - \"{{ osp_networks | default([]) }}\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "bb06ef2a1cc968fe5de84fa72204f3e0c8f043c1", "filename": "playbooks/openshift/stop-cluster.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- import_playbook: stop-aws.yml\n  when:\n  - hosting_infrastructure == 'aws'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b88cfb7ca17482cab9b4c412f9f2e9fcbc5ff07e", "filename": "roles/osp/admin-volume-type/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Obtain current volume types\"\n  shell: >\n    source {{ admin_keystonerc_file }};\n    openstack volume type list -f yaml\n  register: volume_type_list\n  changed_when: False\n\n- name: \"Store away the yaml output\"\n  set_fact:\n    volume_type_list_yaml: \"{{ volume_type_list.stdout|from_yaml }}\"\n\n- name: \"Delete the volume type(s) that should NOT exist\"\n  shell: >\n    source {{ admin_keystonerc_file }};\n    openstack volume type delete \"{{ item.Name }}\"\n  with_items:\n  - \"{{ volume_type_list_yaml | get_remaining_items(cinder_volume_types, 'Name', 'name') }}\"\n\n- name: \"Create the volume type(s) that should exist\"\n  shell: >\n    source {{ admin_keystonerc_file }};\n    openstack volume type create \\\n      --public \\\n      --property \"volume_backend_name={{ item.backend }}\" \\\n      \"{{ item.name }}\"\n  with_items:\n  - \"{{ cinder_volume_types | get_remaining_items(volume_type_list_yaml, 'name', 'Name') }}\"\n"}, {"commit_sha": "e9fb46dc84b9c815a69f6de1347c9ece5db01cc8", "sha": "69f30fe2908e149a7f2bac15095f01acf9509f14", "filename": "tasks/iojs.yml", "repository": "fubarhouse/ansible-role-nodejs", "decoded_content": "---\n#\n# IOJS Tasks\n#\n# IOJS is no longer available via NVM, but can be downloaded on the web.\n# IOJS was a NodeJS compatible platform based upon an earlier NodeJS release.\n# IOJS was merged into NodeJS in version 4, and is in every version this role installs.\n#\n\n- name: \"IOJS | Get versions\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell: \"/usr/local/bin/ivm ls\"\n  register: iojs_available_versions\n  changed_when: false\n\n- name: \"IOJS | Check for local default install\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  stat:\n    path: \"/usr/local/ivm/versions/{{ node_version }}\"\n  register: fubarhouse_npm_io_install_result\n  changed_when: false\n  failed_when: false\n  when: '\"{{ node_version }}\" in iojs_available_versions.stdout_lines'\n\n- name: \"IOJS | Check for all local installs\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  stat:\n    path: \"/usr/local/ivm/versions/{{ item }}\"\n  register: fubarhouse_npm_io_install_results\n  changed_when: false\n  failed_when: false\n  when: '\"{{ item }}\" in iojs_available_versions.stdout'\n  with_items: \"{{ node_versions }}\"\n\n- name: \"IOJS | Install default\"\n  become: yes\n  become_user: root\n  shell:  \"/usr/local/bin/ivm {{ node_version }}\"\n  when:\n    - fubarhouse_npm_io_install_result is defined\n    - '\"{{ node_version }}\" not in fubarhouse_npm_io_install_results.results'\n    - '\"{{ node_version }}\" in iojs_available_versions.stdout'\n  changed_when: false\n\n- name: \"IOJS | Install all\"\n  become: yes\n  become_user: root\n  shell:  \"/usr/local/bin/ivm {{ item }}\"\n  when: 'iojs_available_versions is defined and \"{{ item }}\" in iojs_available_versions.stdout'\n  with_items:\n   - \"{{ node_versions }}\"\n  changed_when: false\n\n- name: \"IOJS | Unalias default NVM alias\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell: \"{{ fubarhouse_npm.nvm_symlink_exec }} unalias default\"\n  changed_when: false\n  failed_when: false\n  when: '\"{{ node_version }}\" in iojs_available_versions.stdout'\n\n- name: \"IOJS | Switch\"\n  become: yes\n  become_user: root\n  file:\n    src: \"/usr/local/ivm/versions/{{ node_version }}/bin/node\"\n    dest: \"/usr/local/bin/iojs\"\n    state: link\n    force: yes\n  when: '\"{{ node_version }}\" in iojs_available_versions.stdout'\n\n- name: \"IOJS | Verify version in use\"\n  shell: \"/usr/local/bin/ivm ls | grep \u03bf | cat\"\n  register: iojs_current_version\n  changed_when: false\n  failed_when: 'iojs_current_version.stdout.find(\"{{ node_version }}\") == -1'\n  when: '\"{{ node_version }}\" in iojs_available_versions.stdout'"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "ae74473f2eb68f1920f1ea2b737796a893be641b", "filename": "dev/playbooks/install_docker.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: docker\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n    - name: Install dependencies\n      yum:\n        name: \"{{ item }}\"\n        state: latest\n      with_items:\n        - policycoreutils-python \n        - libseccomp \n        - libtool-ltdl \n        - yum-utils\n        - NetworkManager-glib \n        - nm-connection-editor\n        - libsemanage-python \n        - policycoreutils-python\n\n    - name: Enable extras REHL repository\n      shell: yum-config-manager --enable rhel-7-server-extras-rpms\n\n    - name: Set Docker url\n      shell: echo {{ docker_ee_url }}/rhel > /etc/yum/vars/dockerurl\n\n    - name: Set Docker version\n      shell: echo {{ rhel_version }}  > /etc/yum/vars/dockerosversion\n\n    - name: Add Docker repository\n      command: yum-config-manager --add-repo {{ docker_ee_url }}/rhel/docker-ee.repo\n\n    - name: Install Docker\n      yum:\n        name: docker-ee\n        state: latest\n        update_cache: yes\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "0f3f1a5a0c3b6c644d6274f03b2cfa6e62a437d2", "filename": "roles/zookeeper/meta/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\ngalaxy_info:\n  author: Graham Taylor\n  description:\n  company: Capgemini\n  # Some suggested licenses:\n  # - BSD (default)\n  # - MIT\n  # - GPLv2\n  # - GPLv3\n  # - Apache\n  # - CC-BY\n  license: license (MIT)\n  min_ansible_version: 1.2\n  #\n  # Below are all platforms currently available. Just uncomment\n  # the ones that apply to your role. If you don't see your\n  # platform on this list, let us know and we'll get it added!\n  #\n  platforms:\n  #- name: EL\n  #  versions:\n  #  - all\n  #  - 5\n  #  - 6\n  #  - 7\n  #- name: GenericUNIX\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Fedora\n  #  versions:\n  #  - all\n  #  - 16\n  #  - 17\n  #  - 18\n  #  - 19\n  #  - 20\n  #- name: SmartOS\n  #  versions:\n  #  - all\n  #  - any\n  #- name: opensuse\n  #  versions:\n  #  - all\n  #  - 12.1\n  #  - 12.2\n  #  - 12.3\n  #  - 13.1\n  #  - 13.2\n  #- name: Amazon\n  #  versions:\n  #  - all\n  #  - 2013.03\n  #  - 2013.09\n  #- name: GenericBSD\n  #  versions:\n  #  - all\n  #  - any\n  #- name: FreeBSD\n  #  versions:\n  #  - all\n  #  - 8.0\n  #  - 8.1\n  #  - 8.2\n  #  - 8.3\n  #  - 8.4\n  #  - 9.0\n  #  - 9.1\n  #  - 9.1\n  #  - 9.2\n  - name: Ubuntu\n    versions:\n  #  - all\n  #  - lucid\n  #  - maverick\n  #  - natty\n  #  - oneiric\n  #  - precise\n  #  - quantal\n  #  - raring\n  #  - saucy\n     - trusty\n  #- name: SLES\n  #  versions:\n  #  - all\n  #  - 10SP3\n  #  - 10SP4\n  #  - 11\n  #  - 11SP1\n  #  - 11SP2\n  #  - 11SP3\n  #- name: GenericLinux\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Debian\n  #  versions:\n  #  - all\n  #  - etch\n  #  - lenny\n  #  - squeeze\n  #  - wheezy\n  #\n  # Below are all categories currently available. Just as with\n  # the platforms above, uncomment those that apply to your role.\n  #\n  categories:\n  - cloud\n  #- cloud:ec2\n  #- cloud:gce\n  #- cloud:rax\n  #- clustering\n  #- database\n  #- database:nosql\n  #- database:sql\n  #- development\n  #- monitoring\n  #- networking\n  #- packaging\n  - system\n  #- web\ndependencies:\n  - role: handlers\n  # List your role dependencies here, one per line. Only\n  # dependencies available via galaxy should be listed here.\n  # Be sure to remove the '[]' above if you add dependencies\n  # to this list.\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "4ce075afb59349ef6aaf399f454f4a37954d23aa", "filename": "reference-architecture/aws-ansible/playbooks/openshift-setup.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- include: /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml\n  vars:\n    debug_level: 2\n    openshift_debug_level: \"{{ debug_level }}\"\n    openshift_node_debug_level: \"{{ node_debug_level | default(debug_level, true) }}\"\n    osm_controller_args:\n      cloud-provider:\n      - \"aws\"\n    osm_api_server_args:\n      cloud-provider:\n      - \"aws\"\n    openshift_node_kubelet_args:\n      cloud-provider:\n      - \"aws\"\n      node-labels:\n      - \"role={{ openshift_node_labels.role }}\"\n    openshift_master_debug_level: \"{{ master_debug_level | default(debug_level, true) }}\"\n    openshift_master_access_token_max_seconds: 2419200\n    openshift_master_api_port: \"{{ console_port }}\"\n    openshift_master_console_port: \"{{ console_port | default (443) }}\"\n    osm_cluster_network_cidr: 172.16.0.0/16\n    openshift_registry_selector: \"role=infra\"\n    openshift_router_selector: \"role=infra\"\n    openshift_hosted_router_replicas: 3\n    openshift_hosted_registry_replicas: 3\n    openshift_master_cluster_method: native\n    openshift_node_local_quota_per_fsgroup: 512Mi\n    openshift_cloudprovider_kind: aws\n    openshift_master_cluster_hostname: \"internal-openshift-master.{{ public_hosted_zone }}\"\n    openshift_master_cluster_public_hostname: \"openshift-master.{{ public_hosted_zone }}\"\n    osm_default_subdomain: \"{{ wildcard_zone }}\"\n    openshift_master_default_subdomain: \"{{osm_default_subdomain}}\"\n    osm_default_node_selector: \"role=app\"\n    deployment_type: openshift-enterprise\n    os_sdn_network_plugin_name: \"{{ openshift_sdn }}\"\n    openshift_master_identity_providers:\n    - name: github\n      kind: GitHubIdentityProvider\n      login: true\n      challenge: false\n      mapping_method: claim\n      clientID: \"{{ github_client_id }}\"\n      clientSecret: \"{{ github_client_secret }}\"\n      organizations: \"{{ github_organization }}\"\n    osm_use_cockpit: true\n    containerized: false\n    # registry\n    openshift_hosted_registry_storage_kind: object\n    openshift_hosted_registry_storage_provider: s3\n    openshift_hosted_registry_storage_s3_accesskey: \"{{ hostvars['localhost']['s3user_id'] }}\"\n    openshift_hosted_registry_storage_s3_secretkey: \"{{ hostvars['localhost']['s3user_secret'] }}\"\n    openshift_hosted_registry_storage_s3_bucket: \"{{ hostvars['localhost']['s3_bucket_name'] }}\"\n    openshift_hosted_registry_storage_s3_region: \"{{ hostvars['localhost']['region'] }}\"\n    openshift_hosted_registry_storage_s3_chunksize: 26214400\n    openshift_hosted_registry_storage_s3_rootdirectory: /registry\n    openshift_hosted_registry_pullthrough: true\n    openshift_hosted_registry_acceptschema2: true\n    openshift_hosted_registry_enforcequota: true\n    # metrics\n    openshift_hosted_metrics_storage_kind: dynamic\n    openshift_hosted_metrics_storage_volume_size: \"{{ openshift_hosted_metrics_storage_volume_size }}\"\n    openshift_metrics_hawkular_nodeselector: {\"role\":\"infra\"}\n    openshift_metrics_cassandra_nodeselector: {\"role\":\"infra\"}\n    openshift_metrics_heapster_nodeselector: {\"role\":\"infra\"}\n\n    # logging\n    openshift_logging_es_pvc_dynamic: true\n    openshift_logging_es_pvc_size: \"{{ openshift_hosted_logging_storage_volume_size }}\"\n    openshift_logging_es_cluster_size: 3\n    openshift_logging_es_nodeselector: {\"role\":\"infra\"}\n    openshift_logging_kibana_nodeselector: {\"role\":\"infra\"}\n    openshift_logging_curator_nodeselector: {\"role\":\"infra\"}\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "23310b3c13817945d5625bbaab3013e21d875b6f", "filename": "playbooks/roles/zookeeper/tasks/CentOS-7.yml", "repository": "rocknsm/rock", "decoded_content": "---\n  - name: Install zookeeper packages\n    yum:\n      name: \"{{ item.pkg }}\"\n      state: \"installed\"\n    when: method == \"install\"\n    with_items:\n    - java-1.8.0-headless\n    - zookeeper\n...\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "78598b03eb7162fd1a7de5161a5ac9a033112eba", "filename": "roles/zookeeper/meta/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\ngalaxy_info:\n  author: Graham Taylor\n  description:\n  company: Capgemini\n  # Some suggested licenses:\n  # - BSD (default)\n  # - MIT\n  # - GPLv2\n  # - GPLv3\n  # - Apache\n  # - CC-BY\n  license: license (MIT)\n  min_ansible_version: 1.2\n  #\n  # Below are all platforms currently available. Just uncomment\n  # the ones that apply to your role. If you don't see your\n  # platform on this list, let us know and we'll get it added!\n  #\n  platforms:\n  #- name: EL\n  #  versions:\n  #  - all\n  #  - 5\n  #  - 6\n  #  - 7\n  #- name: GenericUNIX\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Fedora\n  #  versions:\n  #  - all\n  #  - 16\n  #  - 17\n  #  - 18\n  #  - 19\n  #  - 20\n  #- name: SmartOS\n  #  versions:\n  #  - all\n  #  - any\n  #- name: opensuse\n  #  versions:\n  #  - all\n  #  - 12.1\n  #  - 12.2\n  #  - 12.3\n  #  - 13.1\n  #  - 13.2\n  #- name: Amazon\n  #  versions:\n  #  - all\n  #  - 2013.03\n  #  - 2013.09\n  #- name: GenericBSD\n  #  versions:\n  #  - all\n  #  - any\n  #- name: FreeBSD\n  #  versions:\n  #  - all\n  #  - 8.0\n  #  - 8.1\n  #  - 8.2\n  #  - 8.3\n  #  - 8.4\n  #  - 9.0\n  #  - 9.1\n  #  - 9.1\n  #  - 9.2\n  - name: Ubuntu\n    versions:\n  #  - all\n  #  - lucid\n  #  - maverick\n  #  - natty\n  #  - oneiric\n  #  - precise\n  #  - quantal\n  #  - raring\n  #  - saucy\n     - trusty\n  #- name: SLES\n  #  versions:\n  #  - all\n  #  - 10SP3\n  #  - 10SP4\n  #  - 11\n  #  - 11SP1\n  #  - 11SP2\n  #  - 11SP3\n  #- name: GenericLinux\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Debian\n  #  versions:\n  #  - all\n  #  - etch\n  #  - lenny\n  #  - squeeze\n  #  - wheezy\n  #\n  # Below are all categories currently available. Just as with\n  # the platforms above, uncomment those that apply to your role.\n  #\n  categories:\n  - cloud\n  #- cloud:ec2\n  #- cloud:gce\n  #- cloud:rax\n  #- clustering\n  #- database\n  #- database:nosql\n  #- database:sql\n  #- development\n  #- monitoring\n  #- networking\n  #- packaging\n  - system\n  #- web\ndependencies:\n  - role: consul\n  # List your role dependencies here, one per line. Only\n  # dependencies available via galaxy should be listed here.\n  # Be sure to remove the '[]' above if you add dependencies\n  # to this list.\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "b6ac47890e067634237a041e01ca38c1775d581f", "filename": "playbooks/cluster-login.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "- hosts: localhost\n  connection: local\n  gather_facts: False\n  # unset http_proxy. required for running in the CI\n  environment:\n    http_proxy: \"\"\n  vars:\n    admin_username: \"test_admin\"\n    admin_password: \"123456\"\n    master_fqdn: \"{{ hostvars[groups['masters'][0]]['ansible_fqdn'] }}\"\n  roles:\n    - role: cluster-login\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "4b80962e325aaecc2da6b438474426b9541e7df5", "filename": "tasks/Linux/fetch/security-fetch/security-fetch-oracle-fallback.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Download security policy artifact from Oracle OTN\n  get_url:\n    url: '{{ fallback_oracle_security_policy_artifacts[java_major_version|int] }}'\n    dest: '{{ java_download_path }}'\n    mode: 0755\n    headers:\n      Cookie: >-\n        gpw_e24=http%3A%2F%2Fwww.oracle.com%2F;\n        oraclelicense=accept-securebackup-cookie;\n        --no-check-certificate\n  register: policy_file_downloaded\n  retries: 15\n  delay: 5\n  until: policy_file_downloaded is succeeded\n\n- name: Downloaded security policy artifact\n  set_fact:\n    security_policy_java_artifact: '{{ policy_file_downloaded.dest }}'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "2a59096dcf8a3c5054f20d3781d90b244e9eb994", "filename": "roles/config-vlans/tests/infrahosts.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Configure VLANs on the infrastructure hosts'\n  hosts: infra_hosts\n  roles:\n  - role: config_vlans\n  tags: \n  - configure_infra_hosts\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "19f89ff8cdf0b4fd8caef673682d2b0e3775c097", "filename": "playbooks/openshift/fix-nfs-recycler.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- hosts: masters:nodes\n  vars:\n    recycler_registry: registry.access.redhat.com\n    recycler_repository: openshift3\n    recycler_image: ose-recycler\n    recycler_tag: latest\n    recycler_serviceaccount: pv-recycler-controller\n    recycler_namespace: openshift-infra\n  tasks:\n\n    - name: Create Service Account\n      block:\n        - command: >\n            oc get serviceaccount {{ recycler_serviceaccount }} -n {{ recycler_namespace }}\n          ignore_errors: True\n          delegate_to: \"{{ groups.masters[0] }}\"\n          register: sa_exists\n        - command: >\n            oc create serviceaccount {{ recycler_serviceaccount }} -n {{ recycler_namespace }}\n          when: sa_exists.rc != 0\n          run_once: True\n          delegate_to: \"{{ groups.masters[0] }}\"\n\n    - name: Get OpenShift Version\n      command: >\n        oc version\n      run_once: True\n      delegate_to: \"{{ groups.masters[0] }}\"\n      register: ocp_version_result\n\n    - name: Set OpenShift Version\n      run_once: True\n      set_fact:\n        ocp_version: \"{{ ocp_version_result.stdout_lines[0].split(' ')[1] }}\"\n\n    - name: Pull Recycler Image\n      become: True\n      command: >\n        docker pull {{ recycler_registry }}/{{ recycler_repository }}/{{ recycler_image }}:{{ recycler_tag }}\n\n    - name: Tag Recycler Image\n      become: True\n      command: >\n        docker tag {{ recycler_registry }}/{{ recycler_repository }}/{{ recycler_image }}:{{ recycler_tag }} {{ recycler_registry }}/{{ recycler_repository }}/{{ recycler_image }}:{{ ocp_version }}\n\n    - name: Remove Recycler Latest Tag\n      become: True\n      command: >\n        docker rmi {{ recycler_registry }}/{{ recycler_repository }}/{{ recycler_image }}:{{ recycler_tag }}"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "dccd25969aa58c2d00194da85f720514422807a4", "filename": "roles/notifications/send-email/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Create the 'To:' list of addresses\"\n  set_fact:\n    mail:\n      to: \"{{ list_of_mail_to | join(', ') }}, {{ mail.to | default('') }}\"\n  when:\n    - list_of_mail_to is defined\n\n- name: \"Create the 'CC:' list of addresses\"\n  set_fact:\n    mail:\n      cc: \"{{ list_of_mail_cc | join(', ') }}, {{ mail.cc | default('') }}\"\n  when:\n    - list_of_mail_cc is defined\n\n- name: \"Create the 'BCC:' list of addresses\"\n  set_fact:\n    mail:\n      bcc: \"{{ list_of_mail_bcc | join(', ') }}, {{ mail.bcc | default('') }}\"\n  when:\n    - list_of_mail_bcc is defined\n\n- name: \"Send out e-mail content to users\"\n  mail:\n    subject: \"{{ mail.subject }}\"\n    body: \"{{ mail.body | default(omit) }}\"\n    host: \"{{ mail.host | default(omit) }}\"\n    port: \"{{ mail.port | default (omit) }}\"\n    username: \"{{ mail.username | default(omit) }}\"\n    password: \"{{ mail.password | default(omit) }}\"\n    to: \"{{ mail.to | default(omit) }}\"\n    cc: \"{{ mail.cc | default(omit) }}\"\n    bcc: \"{{ mail.bcc | default(omit) }}\"\n    from: \"{{ mail.from | default(omit)}}\"\n    headers: \"{{ mail.header | default(omit)}}\"\n    secure: \"{{ mail.secure | default(omit) }}\"\n    subtype: \"{{ mail.subtype | default(omit) }}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "8c355b9a48642e5b576ba8117359e954232bbf99", "filename": "roles/moodle-1.9/moodle/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "---\n- name: Install moodle required packages\n  package: name={{ item }}\n           state=present\n  with_items:\n    - moodle-xs\n    - python-psycopg2\n  tags:\n    - download\n\n- name: Remove stock moodle conf\n  file: path='/etc/{{ apache_config_dir }}/moodle.conf'\n        state=absent\n\n- name: Configure moodle\n  template: backup=yes\n            src={{ item.src }}\n            dest={{ item.dest }}\n            owner=root\n            group=root\n            mode={{ item.mode }}\n  with_items:\n    - src: '020-iiab-moodle.conf.j2'\n      dest: '/etc/{{ apache_config_dir }}/020-iiab-moodle.conf'\n      mode: '0655'\n    - src: 'moodle-xs.service.j2'\n      dest: '/etc/systemd/system/moodle-xs.service'\n      mode: '0655'\n    - src: 'moodle-xs-init'\n      dest: '/usr/libexec/moodle-xs-init'\n      mode: '0755'\n\n- name: Stop postgresql\n  service: name=postgresql\n           state=stopped\n\n- name: Start postgresql-iiab\n  service: name=postgresql-iiab\n           state=started\n\n- name: Create db user\n  postgresql_user: name=apache\n                   password=apache\n                   role_attr_flags=NOSUPERUSER,NOCREATEROLE,NOCREATEDB\n                   state=present\n  become: yes\n  become_user: postgres\n\n- name: Create database\n  postgresql_db: name=moodle-xs\n                 encoding=utf8\n                 owner=apache\n                 template=template0\n                 state=present\n  sudo: yes\n  sudo_user: postgres\n\n- name: Execute moodle startup script\n  command: /usr/libexec/moodle-xs-init start\n\n- name: Restart postgresql-iiab\n  service: name=postgresql-iiab\n           state=restarted\n\n- name: Restart httpd\n  service: name={{ apache_service }}\n           state=restarted\n\n- name: Enable moodle service\n  service: name=moodle-xs\n           enabled=yes\n           state=started\n\n- name: fetch the administrative password for moodle\n  shell: cat /etc/moodle/adminpw\n  register: moodlepw\n\n- name: add moodle to service list\n  ini_file: dest='{{ service_filelist }}'\n            section=moodle\n            option='{{ item.option }}'\n            value='{{ item.value }}'\n  with_items:\n    - option: name\n      value: Moodle\n    - option: description\n      value: '\"Access the Moodle learning management system.\"'\n    - option: path\n      value: /moodle\n    - option: enabled\n      value: \"{{ moodle_enabled }}\"\n    - option: adminpw\n      value: \"{{ moodlepw.stdout }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "8abe6e8c6bc27b4629c8f801f07ab1c7e46de947", "filename": "reference-architecture/vmware-ansible/playbooks/roles/cloud-provider-setup/templates/vsphere.conf.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "[Global]\nuser = \"{{ vcenter_username }}\"\npassword = \"{{ vcenter_password }}\"\nserver = \"{{ vcenter_host }}\" \nport = 443\ninsecure-flag = 1\ndatacenter = {{ vcenter_datacenter }}\ndatastore = {{ vcenter_datastore }}\nworking-dir = /{{ vcenter_datacenter }}/vm/{{ vcenter_folder }}/\n[Disk]\nscsicontrollertype = pvscsi\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "69905c3fc965d16b07e4dad379fc98b02fa64ae8", "filename": "roles/rachel/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "rachel_url: /rachel\nrachel_content_path: /library/rachel/www\nrachel_mysqldb_path: /library/rachel/bin/database/mysql-5.6.20/data/sphider_plus/\n\n# These two must be in sync, second saves parsing\nrachel_src_url: http://rachelfriends.org/downloads/public_ftp/rachelusb_32EN/rachelusb_32EN_3.1.5.zip\nrachel_version: rachelusb_32EN_3.1.5\n\nrachel_install: False\nrachel_enabled: False\nrachel_content_found: False\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "e118a71dcaafd1948e3e05fc452a51e9993830b1", "filename": "playbooks/provisioning/openstack/custom-actions/add-docker-registry.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: OSEv3\n  become: true\n  vars:\n    registries: []\n    insecure_registries: []\n\n  tasks:\n  - name: Check if docker is even installed\n    command: docker\n\n  - name: Install atomic-registries package\n    yum:\n      name: atomic-registries\n      state: latest\n\n  - name: Get registry configuration file\n    register: file_result\n    stat:\n      path: /etc/containers/registries.conf\n\n  - name: Check if it exists\n    assert:\n      that: 'file_result.stat.exists'\n      msg: \"Configuration file does not exist.\"\n\n  - name: Load configuration file\n    shell: cat /etc/containers/registries.conf\n    register: file_content\n\n  - name: Store file content into a variable\n    set_fact:\n      docker_conf: \"{{ file_content.stdout | from_yaml }}\"\n\n  - name: Make sure that docker file content is a dictionary\n    when: '(docker_conf is string) and (not docker_conf)'\n    set_fact:\n      docker_conf: {}\n\n  - name: Make sure that registries is a list\n    when: 'registries is string'\n    set_fact:\n      registries_list: [ \"{{ registries }}\" ]\n\n  - name: Make sure that insecure_registries is a list\n    when: 'insecure_registries is string'\n    set_fact:\n      insecure_registries_list: [ \"{{ insecure_registries }}\" ]\n\n  - name: Set default values if there are no registries defined\n    set_fact:\n      docker_conf_registries: \"{{ [] if docker_conf['registries'] is not defined else docker_conf['registries'] }}\"\n      docker_conf_insecure_registries: \"{{ [] if docker_conf['insecure_registries'] is not defined else docker_conf['insecure_registries'] }}\"\n\n  - name: Add other registries\n    when: 'registries_list is not defined'\n    register: registries_merge_result\n    set_fact:\n      docker_conf: \"{{ docker_conf | combine({'registries': (docker_conf_registries + registries) | unique}, recursive=True) }}\"\n\n  - name: Add other registries (if registries had to be converted)\n    when: 'registries_merge_result|skipped'\n    set_fact:\n      docker_conf: \"{{ docker_conf | combine({'registries': (docker_conf_registries + registries_list) | unique}, recursive=True) }}\"\n\n  - name: Add insecure registries\n    when: 'insecure_registries_list is not defined'\n    register: insecure_registries_merge_result\n    set_fact:\n      docker_conf: \"{{ docker_conf | combine({'insecure_registries': (docker_conf_insecure_registries + insecure_registries) | unique }, recursive=True) }}\"\n\n  - name: Add insecure registries (if insecure_registries had to be converted)\n    when: 'insecure_registries_merge_result|skipped'\n    set_fact:\n      docker_conf: \"{{ docker_conf | combine({'insecure_registries': (docker_conf_insecure_registries + insecure_registries_list) | unique }, recursive=True) }}\"\n\n  - name: Load variable back to file\n    copy:\n      content: \"{{ docker_conf | to_yaml }}\"\n      dest: /etc/containers/registries.conf\n\n  - name: Restart registries service\n    service:\n      name: registries\n      state: restarted\n\n  - name: Restart docker\n    service:\n      name: docker\n      state: restarted\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "f6d7b5de2273eb1aeeee2fd065d5ab02fb97f324", "filename": "playbooks/roles/docket/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# tasks file for rocknsm.docket\n\n\n# Ensure `stenographer` and `nginx` groups exists\n- import_tasks: prereqs.yml\n\n# Install packages\n- import_tasks: install.yml\n\n# Generate/copy x509 client cert/keys and CA certs\n- import_tasks: crypto.yml\n\n# Configure docket app settings\n- import_tasks: docket_config.yml\n\n# Configure web server settings\n- import_tasks: lighttpd.yml\n\n# Enable / Activate Services\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "b656c034e5f7129e2ec7149626ee68f78de17378", "filename": "tasks/setup-repository-Ubuntu.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Update APT cache block\n  block:\n    - name: Update APT cache\n      become: true\n      apt:\n        update_cache: yes\n      changed_when: false\n      register: _pkg_result\n      until: _pkg_result is succeeded\n      when:\n        - docker_network_access | bool\n  rescue:\n    - name: Retry APT cache update with allow-releaseinfo-change\n      become: true\n      command: apt-get update --allow-releaseinfo-change\n      args:\n        warn: false\n      changed_when: false\n      register: _pkg_result\n      until: _pkg_result is succeeded\n      when:\n        - docker_network_access | bool\n\n- name: Ensure packages are installed for repository setup\n  become: true\n  package:\n    name: \"{{ item }}\"\n    state: present\n  loop: \"{{ docker_repository_related_packages[_docker_os_dist] }}\"\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when:\n    - docker_network_access | bool\n\n- name: Add Docker official GPG key\n  become: true\n  apt_key:\n    url: https://download.docker.com/linux/{{ _docker_os_dist|lower }}/gpg\n    state: present\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when:\n    - docker_network_access | bool\n    - (_docker_os_dist == \"Ubuntu\" and _docker_os_dist_major_version | int > 14) or\n      (_docker_os_dist == \"Debian\" and _docker_os_dist_major_version | int > 7)\n\n- name: Add Docker APT key (alternative for older Ubuntu systems without SNI).\n  become: true\n  shell: \"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\"\n  args:\n    warn: false\n  changed_when: false\n  when:\n    - docker_network_access | bool\n    - (_docker_os_dist == \"Ubuntu\" and _docker_os_dist_major_version | int == 14) or\n      (_docker_os_dist == \"Debian\" and  _docker_os_dist_major_version | int == 7)\n  tags:\n    - skip_ansible_lint\n\n- name: Determine channels to be enabled and/or disabled\n  set_fact:\n    _docker_disable_channels: \"{{ docker_channels | difference(_docker_merged_channels) }}\"\n    _docker_enable_channels: \"{{ docker_channels | intersect(_docker_merged_channels) }}\"\n  vars:\n    _docker_mandatory_channel: []\n    _docker_merged_channels: \"{{ _docker_mandatory_channel }} + [ '{{ docker_channel }}' ]\"\n\n- name: Add Docker CE repository with correct channels (Ubuntu/Debian)\n  become: true\n  copy:\n    content: >\n      deb [arch={{ _docker_os_arch|lower }}] https://download.docker.com/linux/{{ _docker_os_dist|lower }}\n      {{ ansible_lsb.codename }} {{ _docker_enable_channels | join(' ') }}\n    dest: /etc/apt/sources.list.d/docker-ce.list\n    owner: root\n    group: root\n    mode: '0644'\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "7b07e84bd85fe6b7c08d312f81f3c2a4eca9e653", "filename": "roles/elasticsearch/tasks/restart.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: Disable cluster shard allocation\n  uri:\n    url: \"http://{{ es_interface }}:9200/_cluster/settings\"\n    body: '{\"transient\": {\"cluster.routing.allocation.enable\":\"none\" }}'\n    body_format: json\n    timeout: 2\n    method: PUT\n  register: result\n  until: result.json.acknowledged is defined\n  retries: 300\n  delay: 3\n  changed_when: result.json.acknowledged | bool\n\n- name: Restart elasticsearch\n  service:\n    name: elasticsearch\n    state: restarted\n\n- name: Wait for elasticsearch to become ready\n  wait_for:\n    host: \"{{ ansible_host }}\"\n    port: 9200\n\n- name: Make sure node has joined the cluster\n  uri:\n    url: \"http://{{ es_interface }}:9200/_nodes/{{ ansible_hostname }}/name\"\n    return_content: true\n    timeout: 5\n  register: result\n  until: result.json._nodes.total == 1\n  retries: 200\n  delay: 10\n\n- name: Enable cluster shard allocation\n  uri:\n    url: \"http://{{ es_interface }}:9200/_cluster/settings\"\n    body: '{\"transient\": {\"cluster.routing.allocation.enable\":\"all\" }}'\n    body_format: json\n    timeout: 2\n    method: PUT\n  register: result\n  until: result.json.acknowledged is defined\n  retries: 300\n  delay: 3\n  changed_when: result.json.acknowledged | bool\n...\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "b4383811862ef142766e36d3ab61ca5d54ee0ec8", "filename": "roles/nodogsplash/tasks/rpi.yml", "repository": "iiab/iiab", "decoded_content": "- name: nodogsplash dependencies\n  package:\n    name: libmicrohttpd12\n    state: present\n\n- name: Download nodogsplash software\n  get_url:\n     url: \"{{ iiab_download_url }}/{{ nodogsplash_arm_deb }}\"\n     dest: \"{{ downloads_dir }}/{{ nodogsplash_arm_deb }}\"\n     timeout: \"{{ download_timeout }}\"\n  when: internet_available\n  #async: 300\n  #poll: 5\n\n- name: Install nodogsplash\n  apt:\n    deb=\"{{ downloads_dir }}/{{ nodogsplash_arm_deb }}\"\n\n#- name: Create nodogsplash.service # deb file has one\n#  template:\n#    backup: no\n#    src: nodogsplash.service.j2\n#    dest: \"/etc/systemd/system/nodogsplash.service\"\n#    owner: root\n#    group: root\n#    mode: 0644\n\n- name: Install custom files\n  template:\n    backup: no\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: root\n    group: root\n    mode: \"{{ item.mode }}\"\n  with_items:\n    - { src: 'nodogsplash.conf.j2', dest: '/etc/nodogsplash/nodogsplash.conf', mode: '0644'}\n    - { src: 'splash.html.j2', dest: '/etc/nodogsplash/htdocs/splash.html', mode: '0644'}\n\n# We should probably only start this service on next boot\n- name: Enable nodogsplash service\n  service:\n    name: nodogsplash\n    enabled: yes\n    state: started\n  when: nodogsplash_enabled\n\n- name: Disable nodogsplash service\n  service:\n    name: nodogsplash\n    enabled: no\n    state: stopped\n  when: not nodogsplash_enabled\n\n- name: Add 'nodogsplash' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: nodogsplash\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: nodogsplash\n    - option: description\n      value: '\"Nodogsplash is a lightweight Captive Portal.\"'\n    - option: source\n      value: \"{{ nodogsplash_arm_deb }}\"\n    - option: enabled\n      value: \"{{ nodogsplash_enabled }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "1b859cd2134b9a6627cd0d7e35887e6be15bd23f", "filename": "roles/config-ipa-client/tasks/move-local-user-home.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Move local user's home directory to allow for automount\"\n  block:\n  - name: \"Create temporary account used to move the main account home dir\"\n    user:\n      name: \"{{ temporary_username }}\"\n      groups: wheel\n  - name: \"Ensure new local home dir base path exists\"\n    file:\n      path: \"{{ new_local_home_dir }}\"\n      state: directory\n  - name: \"Ensure new local home .ssh dir exists\"\n    file:\n      path: \"/home/{{ temporary_username }}/.ssh\"\n      state: directory\n      mode: 0700\n      owner: \"{{ temporary_username }}\"\n      group: \"{{ temporary_username }}\"\n  - name: \"Copy over SSH key from the ansible_user\"\n    copy:\n      src: \"/home/{{ ansible_user }}/.ssh/authorized_keys\"\n      dest: \"/home/{{ temporary_username }}/.ssh/authorized_keys\"\n      mode: 0600\n      owner: \"{{ temporary_username }}\"\n      group: \"{{ temporary_username }}\"\n      remote_src: True\n  - name: \"Ensure SUDO access for temporary user\"\n    lineinfile:\n      path: /etc/sudoers.d/10-local-user\n      regexp: \"^{{ temporary_username }}\"\n      line: \"{{ temporary_username }} ALL=(ALL) NOPASSWD:ALL\"\n      create: yes\n  - name: \"Override the ansible_user now that a temporary user is enabled\"\n    set_fact:\n      old_ansible_user: \"{{ ansible_user }}\"\n      ansible_user: \"{{ temporary_username }}\"\n  - name: \"Wait for the previous user to finish up\"\n    shell: \"ps -eo uname | grep '{{ old_ansible_user }}'\"\n    register: cmdoutput\n    until: cmdoutput.rc == 1\n    retries: 60\n    delay: 2\n    failed_when: False\n    changed_when: False\n  - name: \"Move local sudo user home dir\"\n    user:\n      name: \"{{ old_ansible_user }}\"\n      move_home: yes\n      home: \"{{ new_local_home_dir }}/{{ old_ansible_user }}\"\n  - name: \"Switch back to the main sudo user\"\n    set_fact:\n      ansible_user: \"{{ old_ansible_user }}\"\n  - name: \"Wait for the previous user to finish up\"\n    shell: \"ps -eo uname | grep '{{ temporary_username }}'\"\n    register: cmdoutput\n    until: cmdoutput.rc == 1\n    retries: 60\n    delay: 2\n    failed_when: false\n    changed_when: false\n  - name: \"Remove the temporary user\"\n    user:\n      name: \"{{ temporary_username }}\"\n      state: absent\n  - name: \"Remove the sudo access for the temporary user\"\n    file:\n      path: /etc/sudoers.d/10-local-user\n      state: absent\n  when:\n  - move_local_user_home|bool == True\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "338e70dd5197223b3a630ef463db5364956f5018", "filename": "playbooks/cloud-pre.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Display the invocation environment\n  local_action:\n    module: shell\n      ./algo-showenv.sh \\\n        'algo_provider \"{{ algo_provider }}\"' \\\n        'algo_ondemand_cellular \"{{ algo_ondemand_cellular }}\"' \\\n        'algo_ondemand_wifi \"{{ algo_ondemand_wifi }}\"' \\\n        'algo_ondemand_wifi_exclude \"{{ algo_ondemand_wifi_exclude }}\"' \\\n        'algo_local_dns \"{{ algo_local_dns }}\"' \\\n        'algo_ssh_tunneling \"{{ algo_ssh_tunneling }}\"' \\\n        'algo_windows \"{{ algo_windows }}\"' \\\n        'wireguard_enabled \"{{ wireguard_enabled }}\"' \\\n        'dns_encryption \"{{ dns_encryption }}\"' \\\n        > /dev/tty\n\n- name: Install the requirements\n  local_action:\n    module: pip\n    state: latest\n    name:\n      - pyOpenSSL\n      - jinja2==2.8\n      - segno\n  tags: always\n\n- name: Generate the SSH private key\n  openssl_privatekey:\n    path: \"{{ SSH_keys.private }}\"\n    size: 2048\n    mode: \"0600\"\n    type: RSA\n\n- name: Generate the SSH public key\n  openssl_publickey:\n    path: \"{{ SSH_keys.public }}\"\n    privatekey_path: \"{{ SSH_keys.private }}\"\n    format: OpenSSH\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "990ac7b5ecdfb72624b87ed9eb5879ebac26afc9", "filename": "roles/kolibri/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "# The values here are defaults.\n# To override them edit /etc/iiab/local_vars.yml\n\n# Installation Variables\nkolibri_install: False\nkolibri_enabled: False\n\n# Kolibri folder to store its data and configuration files.\nkolibri_home: \"{{ content_base }}/kolibri\"\n\nkolibri_http_port: 8009\nkolibri_url: /kolibri/\nkolibri_path: \"{{ iiab_base }}/kolibri\"\n# 2018-07-16: IIAB recommends /usr/bin but @arky says this isn't yet possible, due to pip\nkolibri_exec_path: /usr/local/bin/kolibri\n\n# Kolibri system user\nkolibri_user: kolibri\n\n# Kolibri setup will be provisioned with default administration account, preset and\n# language. You could turn this to 'False' while reinstalling kolibri.\nkolibri_provision: True\n\n# Kolibri Facility name\nkolibri_facility: Kolibri-in-a-Box\n\n# Kolibri Preset type: formal, nonformal, informal\nkolibri_preset: formal\n\n# Kolibri default language (ar,bn-bd,en,es-es,fa,fr-fr,hi-in,mr,nyn,pt-br,sw-tz,ta,te,ur-pk,yo,zu)\nkolibri_language: en\n\n# Kolibri admin account\nkolibri_admin_user: Admin\nkolibri_admin_password: changeme\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "f031d62af13a7a526fdc70950e8bf6f4d4d3021f", "filename": "playbooks/roles/docket/templates/docket_lighttpd_scgi.conf.j2", "repository": "rocknsm/rock", "decoded_content": "#######################################################################\n# file: /etc/lighttpd/conf.d/docket_scgi.conf\n# {{ ansible_managed }}\n\nserver.modules += ( \"mod_scgi\" )\nscgi.protocol = \"uwsgi\"\nscgi.server = (\n  \"/\" => ((\n    # Ensure lighttpd doesn't try to validate the URL\n    \"check-local\" => \"disable\",\n\n    # Needed to pass through leading \"/\" via uwsgi protocol\n    \"fix-root-scriptname\" => \"enable\",\n\n    # Use x-sendfile for faster file transfers from uwsgi\n    \"x-sendfile\" => \"enable\",\n    \"x-sendfile-docroot\" => (\n      # Set this to SPOOL_DIR\n      \"{{docket_spool_dir}}\",\n      # Set this to the path of the compiled frontend\n      \"{{docket_frontend_dir}}\"\n    ),\n\n    # Use the unix domain socket for local communication\n    \"socket\" => \"/run/docket/docket.socket\",\n  )),\n)\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "e7b870e0a493cbe0c44fc9814afc7459fbda3776", "filename": "roles/virt-install/tasks/virt-install.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Populate values for virt install run\"\n  set_fact:\n    virtinstall_connect: \"{{ hostvars[vm]['libvirt_connect'] | default(default_connect) }}\"\n    virtinstall_virt_type: \"{{ hostvars[vm]['libvirt_virt_type'] | default(default_virt_type) }}\"\n    virtinstall_name: \"{{ hostvars[vm]['libvirt_name'] | default(default_name) }}\"\n    virtinstall_title: \"{{ hostvars[vm]['libvirt_title'] | default(default_title) }}\"\n    virtinstall_description: \"{{ hostvars[vm]['libvirt_description'] | default(default_description) }}\"\n    virtinstall_memory: \"{{ hostvars[vm]['libvirt_memory'] | default(default_memory) }}\"\n    virtinstall_vcpus: \"{{ hostvars[vm]['libvirt_vcpus'] | default(default_vcpus) }}\"\n    virtinstall_disk_size: \"{{ hostvars[vm]['libvirt_disk_size'] | default(default_disk_size) }}\"\n    virtinstall_disk_pool: \"{{ hostvars[vm]['libvirt_disk_pool'] | default(default_disk_pool) }}\"\n    virtinstall_os_variant: \"{{ hostvars[vm]['libvirt_os_variant'] | default(default_os_variant) }}\"\n    virtinstall_iso: \"{{ hostvars[vm]['libvirt_iso'] | default(default_iso) }}\"\n    virtinstall_ksfile: \"{{ hostvars[vm]['libvirt_ksfile'] | default(default_ksfile) }}\"\n    virtinstall_authorized_keys: \"{{ hostvars[vm]['libvirt_authorized_keys'] | default(default_authorized_keys) }}\"\n    virtinstall_http_host: \"{{ hostvars[vm]['libvirt_http_host'] | default(default_http_host) }}\"\n    virtinstall_network_hostif: \"{{ hostvars[vm]['libvirt_network_hostif'] | default(default_network_hostif) }}\"\n    virtinstall_network_model: \"{{ hostvars[vm]['libvirt_network_model'] | default(default_network_model) }}\"\n\n- name: \"Make KS file available on the target host\"\n  copy: \n    src: \"{{ virtinstall_ksfile }}\"\n    dest: \"/tmp/{{ virtinstall_ksfile | basename }}\"\n\n- name: \"Make the authorized_keys file available on the target host\"\n  copy: \n    src: \"{{ virtinstall_authorized_keys }}\"\n    dest: \"{{ default_http_dir }}/{{ virtinstall_authorized_keys | basename }}\"\n  notify: 'Remove authorized_keys'\n\n- name: \"Make a mount point for install purpose\"\n  tempfile:\n    state: directory\n    prefix: install\n    path: \"{{ default_http_dir }}\"\n  register: http_mount\n  when: \n  - mounted_iso[virtinstall_iso] is not defined\n\n- name: \"Mount ISO to serv it up with http\"\n  mount:\n    src: \"{{ virtinstall_iso }}\"\n    path: \"{{ http_mount.path }}\"\n    opts: loop\n    fstype: iso9660\n    state: mounted\n  notify: 'Unmount install ISO'\n  when:\n  - mounted_iso[virtinstall_iso] is not defined\n\n- name: 'Track mounted iso'\n  set_fact: \n    mounted_iso: \"{{ mounted_iso | combine({ virtinstall_iso : http_mount.path }) }}\"\n  when:\n  - mounted_iso[virtinstall_iso] is not defined\n\n- name: \"Set Fact for VM command\"\n  set_fact:\n    virt_install_commands: \"{{ virt_install_commands | default([]) + [ ('virt-install' + ' --connect ' + virtinstall_connect + ' --virt-type ' + virtinstall_virt_type + ' --name ' + virtinstall_name + ' --metadata \\\"title=' + virtinstall_title + ',description=' + virtinstall_description + ',name=' + virtinstall_name + '\\\"' + ' --network \\\"type=direct,source=' + virtinstall_network_hostif + ',source_mode=bridge,model=' + virtinstall_network_model + '\\\"' + ' --memory ' + virtinstall_memory + ' --vcpus ' + virtinstall_vcpus + ' --disk pool=' + virtinstall_disk_pool + ',size=' + virtinstall_disk_size + ',bus=virtio' + ' --os-variant ' + virtinstall_os_variant + ' --location http://' + virtinstall_http_host + '/' + mounted_iso[virtinstall_iso] | basename + ' --initrd-inject=/tmp/' + virtinstall_ksfile | basename + ' --extra-args \\\"inst.repo=http://' + virtinstall_http_host + '/' + mounted_iso[virtinstall_iso] | basename + ' inst.ks=file:/' + virtinstall_ksfile | basename + '\\\"' + ' --graphics spice ' + ' --video qxl ' + ' --channel spicevmc ' + ' --autostart ') ] }}\"\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "fa184fdc0e5d5c8634af05fab3b2bfefc36f35db", "filename": "roles/wireguard/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Ensure the required directories exist\n  file:\n    dest: \"{{ wireguard_config_path }}/{{ item }}\"\n    state: directory\n    recurse: true\n  with_items:\n    - private\n    - public\n  delegate_to: localhost\n  become: false\n\n- name: Include tasks for Ubuntu\n  include_tasks: ubuntu.yml\n  when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'\n  tags: always\n\n- name: Include tasks for FreeBSD\n  include_tasks: freebsd.yml\n  when: ansible_distribution == 'FreeBSD'\n  tags: always\n\n- name: Generate keys\n  import_tasks: keys.yml\n  tags: update-users\n\n- block:\n  - block:\n    - name: WireGuard user list updated\n      lineinfile:\n        dest: \"{{ wireguard_config_path }}/index.txt\"\n        create: true\n        mode: \"0600\"\n        insertafter: EOF\n        line: \"{{ item }}\"\n      register: lineinfile\n      with_items: \"{{ users }}\"\n\n    - set_fact:\n        wireguard_users: \"{{ (lookup('file', wireguard_config_path + 'index.txt')).split('\\n') }}\"\n\n    - name: WireGuard users config generated\n      template:\n        src: client.conf.j2\n        dest: \"{{ wireguard_config_path }}/{{ item.1 }}.conf\"\n        mode: \"0600\"\n      with_indexed_items:  \"{{ wireguard_users }}\"\n      when: item.1 in users\n      vars:\n        index: \"{{ item.0 }}\"\n\n    - name: Generate QR codes\n      shell: >\n        umask 077;\n        which segno &&\n        segno --scale=5 --output={{ item.1 }}.png \\\n          \"{{ lookup('template', 'client.conf.j2') }}\" || true\n      changed_when: false\n      with_indexed_items:  \"{{ wireguard_users }}\"\n      when: item.1 in users\n      vars:\n        index: \"{{ item.0 }}\"\n        ansible_python_interpreter: \"{{ ansible_playbook_python }}\"\n      args:\n        chdir: \"{{ wireguard_config_path }}\"\n        executable: bash\n    become: false\n    delegate_to: localhost\n\n  - name: WireGuard configured\n    template:\n      src: server.conf.j2\n      dest: \"{{ config_prefix|default('/') }}etc/wireguard/{{ wireguard_interface }}.conf\"\n      mode: \"0600\"\n    notify: restart wireguard\n  tags: update-users\n\n\n- name: WireGuard enabled and started\n  service:\n    name: \"{{ service_name }}\"\n    state: started\n    enabled: true\n\n- meta: flush_handlers\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "2f512199cc04ddea3325414ae4d4adea298d9377", "filename": "playbooks/manage-lb/README.md", "repository": "redhat-cop/infra-ansible", "decoded_content": "# Load Balancer Playbook\n\nThis playbook directory has the playbook(s) necessary to manage your load balancers.\n\n## Prerequisites\n\nCurrently, the playbook(s) in here don't manage the instances themselves, so you need to ensure you already have running instances (and subscribed if applicable) before running these playbook(s) with a valid inventory.\n\n## Example\n\n### Inventory\n\nPlease see the inventory in the respective role for more details:\n\n- [manage-haproxy](../../roles/load-balancers/manage-haproxy/README.md)\n\n\n### Playbook execution\n\nInitial run needs the `tags='install'` set to ensure all necessary software is installed:\n\n```bash\n> ansible-playbook -i inventory lb-vms.yml --tags='install'\n```\n\nAny consecutive runs can be done without the 'install' tag to speed up execution:\n```bash\n> ansible-playbook -i inventory lb-vms.yml\n```\n\nLicense\n-------\n\nApache License 2.0\n\n\nAuthor Information\n------------------\n\nRed Hat Community of Practice & staff of the Red Hat Open Innovation Labs.\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5b1fc3be1793400e829bd010905f797a1b663f2d", "filename": "playbooks/notifications/email-notify-list-of-users.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Send HTML e-mail message (based on MD) to a list of users\"\n  hosts: mail-host\n  gather_facts: no\n  tasks:\n  - include_vars:\n      file: \"{{ email_content_file }}\"\n  - include_role:\n      name: notifications/md-to-html\n    vars:\n      markdown_content: \"{{ body }}\"\n  - set_fact:\n      mail: \"{{ mail | combine({ 'subject': title, 'body': md_to_html.html_body_message }) }}\"\n  - set_fact:\n      list_of_mail_to: \"{{ list_of_mail_to | default([]) }} + [ '{{ item.email }}' ]\"\n    with_items:\n    - \"{{ list_of_users }}\"\n  - include_role:\n      name: notifications/send-email\n"}, {"commit_sha": "6d10af54bdbf8e81c3d90a70ffea87b4d2c20eb2", "sha": "fb0088e50c469c741bf578e97b646a53e06cd2dd", "filename": "meta/main.yml", "repository": "Oefenweb/ansible-wordpress", "decoded_content": "# meta file for screen\n---\ngalaxy_info:\n  author: Mischa ter Smitten\n  company: Oefenweb.nl B.V.\n  description: Set up (multiple) wordpress installations in Debian-like systems (using wp-cli)\n  license: MIT\n  min_ansible_version: 1.4\n  platforms:\n    - name: Ubuntu\n      versions:\n        - lucid\n        - precise\n        - trusty\n    - name: Debian\n      versions:\n        - squeeze\n        - wheezy\n  categories:\n    - web\ndependencies: []\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "d027daa5881432263f5a3783adca1d69ff283ef7", "filename": "tasks/Win32NT/security_policy.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: 'Fetch oracle security policy with {{ java_unlimited_policy_transport }} transport'\n  include_tasks: '{{ transport_driver }}'\n  with_first_found:\n    - 'fetch/security-fetch/security-winfetch-{{ java_unlimited_policy_transport }}.yml'\n    - 'fetch/unknown-transport.yml'\n  loop_control:\n    loop_var: transport_driver\n  when:\n    - java_unlimited_policy_enabled\n    - java_full_version is version('8.151', '<')\n\n- name: Block\n  block:\n    - name: Unzip patch file\n      win_unzip:\n        src: '{{ security_policy_java_artifact }}'\n        dest: '{{ java_act_path }}\\jre\\lib\\security\\'\n        creates: '{{ java_act_path }}\\jre\\lib\\security\\{{ security_patch_folders[java_major_version|int] }}'\n\n    - name: Apply patch file\n      win_copy:\n        src: '{{ java_act_path }}\\jre\\lib\\security\\{{ security_patch_folders[java_major_version|int] }}\\{{ policy_item }}'\n        dest: '{{ java_act_path }}\\jre\\lib\\security\\'\n        remote_src: true\n      loop:\n        - local_policy.jar\n        - US_export_policy.jar\n      loop_control:\n        loop_var: policy_item\n  when: java_full_version is version('8.151', '<')\n\n- name: 'Apply setting'\n  win_lineinfile:\n    path: '{{ java_act_path }}\\jre\\lib\\security\\java.security'\n    line: 'crypto.policy=unlimited'\n  when: java_major_version | int < 9\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "026e5bf3f01c743e087cd93ac3145380fca870a8", "filename": "roles/config-hostname/tests/inventory/group_vars/my-host.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nhostname: \"cool\"\ndns_domain: \"hostname.com\"\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "0a52cadf689a6834781d7c736e302dbf7c1ee28d", "filename": "roles/master-prerequisites/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Install git\n  package:\n    name: git\n    state: latest\n  when: not openshift.common.is_atomic | bool\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ee6039a96d01ef2d2c0efe1662ce9d4d015eac57", "filename": "roles/idm-host-cert/tasks/create-host-cert.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Update newline characters in CSR content\"\n  set_fact:\n    cert_csr: \"{{ csr_content.csr|replace('\\n', '\\\\n') }}\"\n\n- name: \"Create Certificate for host\"\n  uri:\n    url: \"https://{{ idm_fqdn }}/ipa/session/json\"\n    method: POST\n    body: '{\"method\": \"cert_request\", \"params\":[[\"{{ cert_csr }}\"],{\"principal\": \"host/{{ host_name }}@{{ host_realm }}\", \"request_type\": \"pkcs10\", \"add\": False, \"version\": \"{{ api_version }}\" }],\"id\":0}'\n    body_format: json\n    validate_certs: no\n    return_content: yes\n    headers:\n      Cookie: \"{{ idm_session.set_cookie }}\"\n      referer: \"https://{{ idm_fqdn }}/ipa\"\n      Content-Type: \"application/json\"\n      Accept: \"application/json\"\n  register: host_cert\n\n- name: \"Error out if the request returned an error\"\n  fail:\n    msg: \"ERROR: request failed with message: {{ host_cert.json.error.message }}\"\n  when:\n  - host_cert.json.error is defined\n  - host_cert.json.error.message is defined\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "eb6e425301ebdcb9b1db6d75e506567115347f77", "filename": "roles/common/tasks/configure.yml", "repository": "rocknsm/rock", "decoded_content": "---\n######################################################\n################# Data Directory #####################\n######################################################\n###############\n##### NOTE ####\n###############\n# You will want to remount this to your \"good\" storage after the build.\n# This is just to make sure all the paths in the configs are proper.\n###############\n\n- name: Create RockNSM data directory\n  file:\n    path: \"{{ rock_data_dir }}\"\n    mode: 0755\n    owner: \"{{ rock_data_user }}\"\n    group: \"{{ rock_data_group }}\"\n    state: directory\n\n- name: Create RockSNM conf directory\n  file:\n    path: \"{{ rock_conf_dir }}\"\n    mode: 0755\n    owner: root\n    group: root\n    state: directory\n\n- name: Create RockNSM directory\n  file:\n    path: \"{{ rocknsm_dir }}\"\n    mode: 0755\n    owner: root\n    group: root\n    state: directory\n\n- name: Create RockNSM cache dir\n  file:\n    path: \"{{ rock_cache_dir }}\"\n    mode: 0755\n    owner: root\n    group: root\n    state: directory\n  when:\n    - ('logstash' or 'kibana' or 'elasticsearch' in installed_services)\n    - \"['elasticsearch', 'logstash', 'kibana']|intersect(group_names)|count > 0\"\n\n- name: Download RockNSM elastic configs\n  get_url:\n    url: \"{{ rock_dashboards_url }}\"\n    dest: \"{{ rock_cache_dir }}/{{ rock_dashboards_filename }}\"\n    mode: 0644\n  when:\n    - rock_online_install\n    - ('logstash' or 'kibana' or 'elasticsearch' in installed_services)\n    - \"['elasticsearch', 'logstash', 'kibana']|intersect(group_names)|count > 0\"\n\n\n- name: Extract RockNSM elastic configs\n  unarchive:\n    src: \"{{ rock_cache_dir }}/{{ rock_dashboards_filename }}\"\n    dest: /opt/rocknsm\n    owner: root\n    group: root\n    creates: \"{{ rock_module_dir }}\"\n    remote_src: true\n  when:\n    - ('logstash' or 'kibana' or 'elasticsearch' in installed_services)\n    - \"['elasticsearch', 'logstash', 'kibana']|intersect(group_names)|count > 0\"\n\n\n- name: Disable IPv6 for all interfaces\n  sysctl:\n    name: net.ipv6.conf.all.disable_ipv6\n    value: 1\n    sysctl_file: \"{{ rock_sysctl_file }}\"\n\n- name: Disable IPv6 for default interfaces\n  sysctl:\n    name: net.ipv6.conf.default.disable_ipv6\n    value: 1\n    sysctl_file: \"{{ rock_sysctl_file }}\"\n\n- name: Disable IPv6 in sshd\n  lineinfile:\n    dest: /etc/ssh/sshd_config\n    regexp: AddressFamily\n    line: AddressFamily inet\n  notify:\n    - Restart sshd\n\n- name: Add the inventory into /etc/hosts\n  lineinfile:\n    dest: /etc/hosts\n    regexp: '.*{{ item }}$'\n    line: \"{{ hostvars[item]['ansible_default_ipv4']['address'] }} {{ item }}\"\n    state: present\n  when: hostvars[item]['ansible_facts']['default_ipv4'] is defined\n  loop: \"{{ groups['all'] }}\"\n\n- name: Set system hostname\n  hostname:\n    name: \"{{ inventory_hostname }}\"\n\n- name: Re-run Setup to populate changes\n  setup:\n\n- name: Setup EPEL repository\n  yum_repository:\n    name: epel\n    description: EPEL YUM repo\n    baseurl: \"{{ epel_baseurl }}\"\n    gpgkey: \"{{ epel_gpgurl }}\"\n    gpgcheck: true\n  when: rock_online_install\n\n- name: Manually trust CentOS GPG key\n  rpm_key:\n    state: present\n    key: http://mirror.centos.org/centos/RPM-GPG-KEY-CentOS-7\n  when: rock_online_install\n\n- name: Setup elastic repository\n  yum_repository:\n    name: elastic-{{ elastic.major_version }}\n    file: elastic\n    description: Elastic Stack repository for {{ elastic.major_version }}.{{ elastic.suffix }}\n    baseurl: \"{{ elastic_baseurl }}\"\n    gpgkey: \"{{ elastic_gpgurl }}\"\n    gpgcheck: false\n  when: rock_online_install\n\n- name: Configure RockNSM online repos\n  yum_repository:\n    file: rocknsm\n    name: \"{{ item.name }}\"\n    enabled: \"{{ rock_online_install }}\"\n    description: \"{{ item.name }}\"\n    baseurl: \"{{ item.baseurl }}\"\n    repo_gpgcheck: 1\n    gpgcheck: \"{{ item.gpgcheck }}\"\n    gpgkey:\n      - file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-pkgcloud-2_4\n      - file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-2\n    sslverify: 1\n    sslcacert: /etc/pki/tls/certs/ca-bundle.crt\n    metadata_expire: 300\n    cost: 750\n    state: present\n  loop:\n    - { name: \"rocknsm_2_4\", gpgcheck: true, baseurl: \"{{ rocknsm_baseurl }}\" }\n    - { name: \"rocknsm_2_4-source\", gpgcheck: false, baseurl: \"{{ rocknsm_srpm_baseurl }}\" }\n\n- name: Configure RockNSM online testing repos\n  yum_repository:\n    file: \"rocknsm-testing\"\n    name: \"rocknsm-testing\"\n    description: \"RockNSM - Testing - $basearch\"\n    baseurl: \"{{ rocknsm_testing_baseurl }}\"\n    skip_if_unavailable: \"True\"\n    gpgcheck: 1\n    gpgkey:\n      - file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-Testing\n    repo_gpgcheck: 0\n    enabled: \"{{ (rock_online_install and rock_enable_testing) | bool }}\"\n    cost: 750\n    state: present\n\n- name: Setup local offline repo\n  yum_repository:\n    name: rocknsm-local\n    description: ROCKNSM Local Repository\n    baseurl: \"{{ rocknsm_local_baseurl }}\"\n    gpgcheck: \"{{ rock_offline_gpgcheck }}\"\n    gpgkey: file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-2\n    repo_gpgcheck: \"{{ rock_offline_gpgcheck }}\"\n    cost: 500\n  when: not rock_disable_offline_repo | bool\n\n- name: Install RockNSM GPG keys\n  copy:\n    src: \"{{ item }}\"\n    dest: \"/etc/pki/rpm-gpg/{{ item }}\"\n    mode: 0644\n    owner: root\n    group: root\n  loop:\n    - RPM-GPG-KEY-RockNSM-2\n    - RPM-GPG-KEY-RockNSM-Testing\n    - RPM-GPG-KEY-RockNSM-pkgcloud-2_4\n\n- name: Trust RockNSM GPG keys for RPMs\n  rpm_key:\n    state: present\n    key: \"{{ item.path }}\"\n  loop:\n    - repoid: \"rocknsm_2_4\"\n      path: \"/etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-2\"\n      test: \"{{ rock_online_install }}\"\n    - repoid: \"rocknsm_2_4\"\n      path: \"/etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-pkgcloud-2_4\"\n      test: \"{{ rock_online_install }}\"\n    - repoid: \"rocknsm-testing\"\n      path: \"/etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-Testing\"\n      test: \"{{ rock_online_install }}\"\n    - repoid: \"rocknsm-local\"\n      path: \"/etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-2\"\n      test: \"{{ not rock_disable_offline_repo }}\"\n  register: registered_keys\n  when: item.test | bool\n\n- name: Trust RockNSM GPG keys in yum\n  command: >\n    yum -q makecache -y --disablerepo='*' --enablerepo='{{ item.item.repoid }}'\n  args:\n    warn: false\n  loop: \"{{ registered_keys.results }}\"\n  when: item.changed\n  tags:\n    - skip_ansible_lint  # [503] Tasks that run when changed should be handlers\n\n- name: Configure default CentOS online repos\n  ini_file:\n    path: \"{{ item.path }}\"\n    section: \"{{ item.repo }}\"\n    option: \"enabled\"\n    value: \"{{ '1' if rock_online_install else '0' }}\"\n    state: \"present\"\n  loop:\n    - { path: \"/etc/yum.repos.d/CentOS-Base.repo\", repo: \"base\" }\n    - { path: \"/etc/yum.repos.d/CentOS-Base.repo\", repo: \"updates\" }\n    - { path: \"/etc/yum.repos.d/CentOS-Base.repo\", repo: \"extras\" }\n  when: ansible_distribution == \"CentOS\"\n\n# We need to make sure pyopenssl >= 0.15 is installed to support the following module\n# https://docs.ansible.com/ansible/latest/modules/openssl_certificate_module.html#openssl-certificate-module\n# The \"base\" repo only goes up to 0.13, so we need to exclude it for this task\n- name: Install pyopenssl package\n  yum:\n    name: python2-pyOpenSSL\n    state: installed\n  when: \"'docket' in group_names or 'stenographer' in group_names or 'kibana' in group_names\"\n\n- name: Install core packages\n  yum:\n    name: \"{{ rocknsm_package_list }}\"\n    state: installed\n\n- name: Enable and start firewalld\n  service:\n    name: firewalld\n    enabled: true\n    state: started\n\n# Use 'internal' zone for intra-ROCK communication\n- name: Create internal firewall zone\n  firewalld:\n    state: present\n    zone: internal\n    permanent: true\n  register: result\n\n- name: Reload firewalld to load zone\n  service:\n    name: firewalld\n    state: restarted\n  when: result.changed\n  tags:\n    - skip_ansible_lint  # [503] Tasks that run when changed should be handlers\n\n# Use 'work' zone for management network\n- name: Create work firewall zone\n  firewalld:\n    state: present\n    zone: work\n    permanent: true\n  register: result\n\n- name: Reload firewalld to load zone\n  service:\n    name: firewalld\n    state: restarted\n  when: result.changed\n  tags:\n    - skip_ansible_lint  # [503] Tasks that run when changed should be handlers\n\n- name: Configure firewall ports\n  firewalld:\n    port: \"{{ item[1] }}\"\n    source: \"{{ item[0] }}\"\n    zone: work\n    permanent: true\n    state: enabled\n    immediate: true\n  loop: \"{{ rock_mgmt_nets | product(['22/tcp']) | list }}\"\n\n- name: Ensure cache directory exists\n  file:\n    dest: \"{{ rock_cache_dir }}\"\n    state: directory\n    mode: 0755\n  when:\n    - ('logstash' or 'kibana' or 'elasticsearch' in installed_services)\n    - \"['elasticsearch', 'logstash', 'kibana']|intersect(group_names)|count > 0\"\n\n- name: Install RockNSM control script\n  template:\n    src: rockctl.j2\n    dest: /usr/local/bin/rockctl\n    mode: 0755\n    owner: root\n    group: root\n\n- name: Create RockNSM control script symlink\n  file:\n    src: \"/usr/local/bin/rockctl\"\n    dest: \"/usr/sbin/rockctl\"\n    force: true\n    state: link\n\n- name: Set RockNSM Version\n  copy:\n    content: \"{{ rock_version }}\"\n    dest: \"{{ rock_conf_dir }}/rock-version\"\n    mode: 0644\n    owner: root\n    group: root\n\n- name: Install RockNSM /etc/issue\n  copy:\n    src: etc-issue.in\n    dest: /etc/issue.in\n    mode: 0644\n    owner: root\n    group: root\n\n- name: Add NetworkManager RockNSM hook\n  copy:\n    src: nm-issue-update\n    dest: /etc/NetworkManager/dispatcher.d/50-rocknsm-issue-update\n    mode: 0755\n    owner: root\n    group: root\n\n- name: Set disk quota total weight\n  set_fact:\n    xfs_quota_weight: \"{{ rock_services | selectattr('installed', 'equalto', True) | map(attribute='quota_weight') | sum }}\"\n...\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "96ec57105bbf6960ed4f537cc065cff72e5cb135", "filename": "roles/config-httpd/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: web-server\n  roles:\n  - config-httpd \n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "00df864f3900d49c3d4e22e91a996a7991907692", "filename": "reference-architecture/aws-ansible/playbooks/teardown.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  connection: local\n  gather_facts: no\n  become: no\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - cfn-outputs\n  - instance-groups\n\n- include: ../../../playbooks/unregister.yaml\n\n- hosts: localhost\n  connection: local\n  gather_facts: no\n  become: no\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - terminate-all\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "845a3f5164d54d444ec8abec93dc7e3129ed9d60", "filename": "reference-architecture/rhv-ansible/playbooks/ovirt-infra.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: oVirt infra\n  hosts: localhost\n  connection: local\n  gather_facts: false\n\n  vars_files:\n    - vars/ovirt-infra-vars.yaml\n\n  pre_tasks:\n    - name: Login to oVirt\n      ovirt_auth:\n        url: \"{{ engine_url }}\"\n        username: \"{{ engine_user }}\"\n        password: \"{{ engine_password }}\"\n        ca_file: \"{{ engine_cafile | default(omit) }}\"\n        insecure: \"{{ engine_insecure | default(true) }}\"\n      tags:\n        - always\n\n  roles:\n    - ovirt-infra\n\n  post_tasks:\n    - name: Logout from oVirt\n      ovirt_auth:\n        state: absent\n        ovirt_auth: \"{{ ovirt_auth }}\"\n      tags:\n        - always\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "08748d464f135471c66278c22089b0dedcda5819", "filename": "playbooks/local.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Generate the SSH private key\n  local_action: shell echo -e  'n' | ssh-keygen -b 2048 -C {{ SSH_keys.comment }} -t rsa -f {{ SSH_keys.private }} -q -N \"\"\n  args:\n    creates: \"{{ SSH_keys.public }}\"\n\n- name: Generate the SSH public key\n  local_action: shell echo `ssh-keygen -y -f {{ SSH_keys.private }}` {{ SSH_keys.comment }} > {{ SSH_keys.public }}\n  args:\n    creates: \"{{ SSH_keys.public }}\"\n\n- name: Change mode for the SSH private key\n  local_action: file path={{ SSH_keys.private }} mode=0600\n\n- name: Ensure the dynamic inventory exists\n  blockinfile:\n    dest: configs/inventory.dynamic\n    marker: \"# {mark} ALGO MANAGED BLOCK\"\n    create: yes\n    block: |\n      [algo:children]\n      {% for group in dynamic_inventory_groups %}\n      {{ group }}\n      {% endfor %}\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "7d7bf5490ef95a9fe0a71379d744a66eb7326633", "filename": "roles/weave/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for weave\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8cc7aadd4c3a5b464d1eda1e97033aaeda0f74c6", "filename": "roles/update-host/tasks/update-host.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Update the host\"\n  package:\n    name: \"*\"\n    state: latest\n  register: host_updated\n  when: \n    - pkg_update|default(False) \n  become: True\n\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f4bc8fcbbfe52b3e73b5f65f2c47957e08d898b0", "filename": "roles/manage-jira/tasks/create_project_category.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Create Jira Project Category\n  uri:\n    url: \"{{ jira_url }}/rest/api/2/projectCategory\"\n    method: POST\n    user: \"{{ jira_username }}\"\n    password: \"{{ jira_password }}\"\n    return_content: yes\n    force_basic_auth: yes\n    body_format: json\n    header:\n      - Accept: 'application/json'\n      - Content-Type: 'application/json'\n    body: \"{ 'name': '{{ atlassian.jira.project.category_name }}',\n              'description': '{{ atlassian.jira.project.category_description }}' }\"\n    status_code: 201\n  register: category\n\n- name: Set fact for Category ID\n  set_fact:\n    CategoryID: \"{{ category.json.id }}\"\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "9db4446ad81fdde4d1d7be6ced459b1752c54171", "filename": "roles/ipaserver/vars/Fedora.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "ipaserver_packages: [ \"freeipa-server\", \"python3-libselinux\" ]\nipaserver_packages_dns: [ \"freeipa-server-dns\" ]\nipaserver_packages_adtrust: [ \"freeipa-server-trust-ad\" ]"}, {"commit_sha": "45971be8249cc4627ef8ddfacf55a661b7fc13ca", "sha": "61ac429e562756dadd4e3e8ca5abac923e87a024", "filename": "meta/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "galaxy_info:\n  author: Bjorn Oscarsson\n  description: \"Installs and configures Docker Community Edition (CE)\"\n  min_ansible_version: 2.4\n  license: MIT\n  platforms:\n  - name: Fedora\n    versions:\n      - 24\n      - 25\n      - 26\n\n  - name: EL\n    versions:\n      - 7\n\n  - name: Debian\n    versions:\n      - jessie\n      - stretch\n\n  - name: Ubuntu\n    versions:\n      - trusty\n      - xenial\n\n  galaxy_tags:\n    - docker\n    - containers\n    - system\n\ndependencies: []\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a8aaef9500ad78c65d8cf61e7e9f6007c2f9bd24", "filename": "roles/config-ipa-client/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install, configure and enable IPA/IdM integration\"\n  import_tasks: ipa.yml\n  when: \n  - ipa_client_install|default(False)\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "a41262a992af9733f8edb4eb9f454f7a6b453380", "filename": "roles/config-redis/tasks/firewall.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Check if firewalld is installed\n  command: systemctl status firewalld\n  register: firewalld_status\n  failed_when: false\n  changed_when: false\n\n- name: Check if iptables is installed\n  command: systemctl status iptables\n  register: iptables_status\n  failed_when: false\n  changed_when: false\n\n- name: Open port in firewalld\n  firewalld:\n    port: \"{{ redis_port }}/TCP\"\n    permanent: true\n    state: enabled\n  when: firewalld_status.rc == 0\n  notify:\n  - restart firewalld\n\n- name: Ensure iptables is correctly configured \n  lineinfile:\n    insertafter: \"^-A INPUT .* --dport {{ redis_port }} .* ACCEPT\"\n    state: present\n    dest: /etc/sysconfig/iptables\n    regexp: \"^-A INPUT .* --dport {{ redis_port }} .* ACCEPT\"\n    line: \"-A INPUT -p TCP -m state --state NEW -m TCP --dport {{ redis_port }} -j ACCEPT\"\n  when: iptables_status.rc == 0 and firewalld_status.rc != 0 \n  notify:\n  - restart iptables\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "d16a407fabcb1b194d3da99f3a5a4803b0a9ab67", "filename": "playbooks/ansible.cfg", "repository": "rocknsm/rock", "decoded_content": "# config file for ansible -- https://ansible.com/\n# ===============================================\n\n# nearly all parameters can be overridden in ansible-playbook\n# or with command line flags. ansible will read ANSIBLE_CONFIG,\n# ansible.cfg in the current working directory, .ansible.cfg in\n# the home directory or /etc/ansible/ansible.cfg, whichever it\n# finds first\n\n[defaults]\n\n# some basic default values...\n\ninventory      = inventory/all-in-one.ini\n#library        = /usr/share/my_modules/\n#module_utils   = /usr/share/my_module_utils/\n#remote_tmp     = ~/.ansible/tmp\n#local_tmp      = ~/.ansible/tmp\n#forks          = 5\n#poll_interval  = 15\n#sudo_user      = root\n#ask_sudo_pass = True\n#ask_pass      = True\n#transport      = smart\n#remote_port    = 22\n#module_lang    = C\n#module_set_locale = False\n\n# plays will gather facts by default, which contain information about\n# the remote system.\n#\n# smart - gather by default, but don't regather if already gathered\n# implicit - gather by default, turn off with gather_facts: False\n# explicit - do not gather by default, must say gather_facts: True\n#gathering = implicit\n\n# This only affects the gathering done by a play's gather_facts directive,\n# by default gathering retrieves all facts subsets\n# all - gather all subsets\n# network - gather min and network facts\n# hardware - gather hardware facts (longest facts to retrieve)\n# virtual - gather min and virtual facts\n# facter - import facts from facter\n# ohai - import facts from ohai\n# You can combine them using comma (ex: network,virtual)\n# You can negate them using ! (ex: !hardware,!facter,!ohai)\n# A minimal set of facts is always gathered.\n#gather_subset = all\n\n# some hardware related facts are collected\n# with a maximum timeout of 10 seconds. This\n# option lets you increase or decrease that\n# timeout to something more suitable for the\n# environment.\n# gather_timeout = 10\n\n# additional paths to search for roles in, colon separated\nroles_path    = roles\n\n# uncomment this to disable SSH key host checking\n#host_key_checking = False\n\n# change the default callback\n#stdout_callback = skippy\n# enable additional callbacks\n#callback_whitelist = timer, mail\n\n# Determine whether includes in tasks and handlers are \"static\" by\n# default. As of 2.0, includes are dynamic by default. Setting these\n# values to True will make includes behave more like they did in the\n# 1.x versions.\n#task_includes_static = True\n#handler_includes_static = True\n\n# Controls if a missing handler for a notification event is an error or a warning\n#error_on_missing_handler = True\n\n# change this for alternative sudo implementations\n#sudo_exe = sudo\n\n# What flags to pass to sudo\n# WARNING: leaving out the defaults might create unexpected behaviours\n#sudo_flags = -H -S -n\n\n# SSH timeout\n#timeout = 10\n\n# default user to use for playbooks if user is not specified\n# (/usr/bin/ansible will use current user as default)\n#remote_user = root\n\n# logging is off by default unless this path is defined\n# if so defined, consider logrotate\n#log_path = /var/log/ansible.log\n\n# default module name for /usr/bin/ansible\n#module_name = command\n\n# use this shell for commands executed under sudo\n# you may need to change this to bin/bash in rare instances\n# if sudo is constrained\n#executable = /bin/sh\n\n# if inventory variables overlap, does the higher precedence one win\n# or are hash values merged together?  The default is 'replace' but\n# this can also be set to 'merge'.\n#hash_behaviour = replace\n\n# by default, variables from roles will be visible in the global variable\n# scope. To prevent this, the following option can be enabled, and only\n# tasks and handlers within the role will see the variables there\n#private_role_vars = yes\n\n# list any Jinja2 extensions to enable here:\n#jinja2_extensions = jinja2.ext.do,jinja2.ext.i18n\n\n# if set, always use this private key file for authentication, same as\n# if passing --private-key to ansible or ansible-playbook\n#private_key_file = /path/to/file\n\n# If set, configures the path to the Vault password file as an alternative to\n# specifying --vault-password-file on the command line.\n#vault_password_file = /path/to/vault_password_file\n\n# format of string {{ ansible_managed }} available within Jinja2\n# templates indicates to users editing templates files will be replaced.\n# replacing {file}, {host} and {uid} and strftime codes with proper values.\n#ansible_managed = Ansible managed: {file} modified on %Y-%m-%d %H:%M:%S by {uid} on {host}\n# {file}, {host}, {uid}, and the timestamp can all interfere with idempotence\n# in some situations so the default is a static string:\n#ansible_managed = Ansible managed\n\n# by default, ansible-playbook will display \"Skipping [host]\" if it determines a task\n# should not be run on a host.  Set this to \"False\" if you don't want to see these \"Skipping\"\n# messages. NOTE: the task header will still be shown regardless of whether or not the\n# task is skipped.\n#display_skipped_hosts = True\n\n# by default, if a task in a playbook does not include a name: field then\n# ansible-playbook will construct a header that includes the task's action but\n# not the task's args.  This is a security feature because ansible cannot know\n# if the *module* considers an argument to be no_log at the time that the\n# header is printed.  If your environment doesn't have a problem securing\n# stdout from ansible-playbook (or you have manually specified no_log in your\n# playbook on all of the tasks where you have secret information) then you can\n# safely set this to True to get more informative messages.\n#display_args_to_stdout = False\n\n# by default (as of 1.3), Ansible will raise errors when attempting to dereference\n# Jinja2 variables that are not set in templates or action lines. Uncomment this line\n# to revert the behavior to pre-1.3.\n#error_on_undefined_vars = False\n\n# by default (as of 1.6), Ansible may display warnings based on the configuration of the\n# system running ansible itself. This may include warnings about 3rd party packages or\n# other conditions that should be resolved if possible.\n# to disable these warnings, set the following value to False:\n#system_warnings = True\n\n# by default (as of 1.4), Ansible may display deprecation warnings for language\n# features that should no longer be used and will be removed in future versions.\n# to disable these warnings, set the following value to False:\n#deprecation_warnings = True\n\n# (as of 1.8), Ansible can optionally warn when usage of the shell and\n# command module appear to be simplified by using a default Ansible module\n# instead.  These warnings can be silenced by adjusting the following\n# setting or adding warn=yes or warn=no to the end of the command line\n# parameter string.  This will for example suggest using the git module\n# instead of shelling out to the git command.\n# command_warnings = False\n\n\n# set plugin path directories here, separate with colons\n#action_plugins     = /usr/share/ansible/plugins/action\n#cache_plugins      = /usr/share/ansible/plugins/cache\n#callback_plugins   = /usr/share/ansible/plugins/callback\n#connection_plugins = /usr/share/ansible/plugins/connection\n#lookup_plugins     = /usr/share/ansible/plugins/lookup\n#inventory_plugins  = /usr/share/ansible/plugins/inventory\n#vars_plugins       = /usr/share/ansible/plugins/vars\n#filter_plugins     = /usr/share/ansible/plugins/filter\n#test_plugins       = /usr/share/ansible/plugins/test\n#strategy_plugins   = /usr/share/ansible/plugins/strategy\n\n\n# by default, ansible will use the 'linear' strategy but you may want to try\n# another one\n#strategy = free\n\n# by default callbacks are not loaded for /bin/ansible, enable this if you\n# want, for example, a notification or logging callback to also apply to\n# /bin/ansible runs\n#bin_ansible_callbacks = False\n\n\n# don't like cows?  that's unfortunate.\n# set to 1 if you don't want cowsay support or export ANSIBLE_NOCOWS=1\n#nocows = 1\n\n# set which cowsay stencil you'd like to use by default. When set to 'random',\n# a random stencil will be selected for each task. The selection will be filtered\n# against the `cow_whitelist` option below.\n#cow_selection = default\n#cow_selection = random\n\n# when using the 'random' option for cowsay, stencils will be restricted to this list.\n# it should be formatted as a comma-separated list with no spaces between names.\n# NOTE: line continuations here are for formatting purposes only, as the INI parser\n#       in python does not support them.\n#cow_whitelist=bud-frogs,bunny,cheese,daemon,default,dragon,elephant-in-snake,elephant,eyes,\\\n#              hellokitty,kitty,luke-koala,meow,milk,moofasa,moose,ren,sheep,small,stegosaurus,\\\n#              stimpy,supermilker,three-eyes,turkey,turtle,tux,udder,vader-koala,vader,www\n\n# don't like colors either?\n# set to 1 if you don't want colors, or export ANSIBLE_NOCOLOR=1\n#nocolor = 1\n\n# if set to a persistent type (not 'memory', for example 'redis') fact values\n# from previous runs in Ansible will be stored.  This may be useful when\n# wanting to use, for example, IP information from one group of servers\n# without having to talk to them in the same playbook run to get their\n# current IP information.\n#fact_caching = memory\n\n\n# retry files\n# When a playbook fails by default a .retry file will be created in ~/\n# You can disable this feature by setting retry_files_enabled to False\n# and you can change the location of the files by setting retry_files_save_path\n\n#retry_files_enabled = False\n#retry_files_save_path = ~/.ansible-retry\n\n# squash actions\n# Ansible can optimise actions that call modules with list parameters\n# when looping. Instead of calling the module once per with_ item, the\n# module is called once with all items at once. Currently this only works\n# under limited circumstances, and only with parameters named 'name'.\n#squash_actions = apk,apt,dnf,homebrew,pacman,pkgng,yum,zypper\n\n# prevents logging of task data, off by default\n#no_log = False\n\n# prevents logging of tasks, but only on the targets, data is still logged on the master/controller\n#no_target_syslog = False\n\n# controls whether Ansible will raise an error or warning if a task has no\n# choice but to create world readable temporary files to execute a module on\n# the remote machine.  This option is False by default for security.  Users may\n# turn this on to have behaviour more like Ansible prior to 2.1.x.  See\n# https://docs.ansible.com/ansible/become.html#becoming-an-unprivileged-user\n# for more secure ways to fix this than enabling this option.\n#allow_world_readable_tmpfiles = False\n\n# controls the compression level of variables sent to\n# worker processes. At the default of 0, no compression\n# is used. This value must be an integer from 0 to 9.\n#var_compression_level = 9\n\n# controls what compression method is used for new-style ansible modules when\n# they are sent to the remote system.  The compression types depend on having\n# support compiled into both the controller's python and the client's python.\n# The names should match with the python Zipfile compression types:\n# * ZIP_STORED (no compression. available everywhere)\n# * ZIP_DEFLATED (uses zlib, the default)\n# These values may be set per host via the ansible_module_compression inventory\n# variable\n#module_compression = 'ZIP_DEFLATED'\n\n# This controls the cutoff point (in bytes) on --diff for files\n# set to 0 for unlimited (RAM may suffer!).\n#max_diff_size = 1048576\n\n# This controls how ansible handles multiple --tags and --skip-tags arguments\n# on the CLI.  If this is True then multiple arguments are merged together.  If\n# it is False, then the last specified argument is used and the others are ignored.\n#merge_multiple_cli_flags = False\n\n# Controls showing custom stats at the end, off by default\n#show_custom_stats = True\n\n# Controlls which files to ignore when using a directory as inventory with\n# possibly multiple sources (both static and dynamic)\n#inventory_ignore_extensions = ~, .orig, .bak, .ini, .cfg, .retry, .pyc, .pyo\n\n[privilege_escalation]\nbecome=True\nbecome_method=sudo\n#become_user=root\n#become_ask_pass=False\n\n[paramiko_connection]\n\n# uncomment this line to cause the paramiko connection plugin to not record new host\n# keys encountered.  Increases performance on new host additions.  Setting works independently of the\n# host key checking setting above.\n#record_host_keys=False\n\n# by default, Ansible requests a pseudo-terminal for commands executed under sudo. Uncomment this\n# line to disable this behaviour.\n#pty=False\n\n[ssh_connection]\n\n# ssh arguments to use\n# Leaving off ControlPersist will result in poor performance, so use\n# paramiko on older platforms rather than removing it, -C controls compression use\n#ssh_args = -C -o ControlMaster=auto -o ControlPersist=60s\n\n# The base directory for the ControlPath sockets.\n# This is the \"%(directory)s\" in the control_path option\n#\n# Example:\n# control_path_dir = /tmp/.ansible/cp\n#control_path_dir = ~/.ansible/cp\n\n# The path to use for the ControlPath sockets. This defaults to a hashed string of the hostname,\n# port and username (empty string in the config). The hash mitigates a common problem users\n# found with long hostames and the conventional %(directory)s/ansible-ssh-%%h-%%p-%%r format.\n# In those cases, a \"too long for Unix domain socket\" ssh error would occur.\n#\n# Example:\n# control_path = %(directory)s/%%h-%%r\n#control_path =\n\n# Enabling pipelining reduces the number of SSH operations required to\n# execute a module on the remote server. This can result in a significant\n# performance improvement when enabled, however when using \"sudo:\" you must\n# first disable 'requiretty' in /etc/sudoers\n#\n# By default, this option is disabled to preserve compatibility with\n# sudoers configurations that have requiretty (the default on many distros).\n#\n#pipelining = False\n\n# Control the mechanism for transferring files (old)\n#   * smart = try sftp and then try scp [default]\n#   * True = use scp only\n#   * False = use sftp only\n#scp_if_ssh = smart\n\n# Control the mechanism for transferring files (new)\n# If set, this will override the scp_if_ssh option\n#   * sftp  = use sftp to transfer files\n#   * scp   = use scp to transfer files\n#   * piped = use 'dd' over SSH to transfer files\n#   * smart = try sftp, scp, and piped, in that order [default]\n#transfer_method = smart\n\n# if False, sftp will not use batch mode to transfer files. This may cause some\n# types of file transfer failures impossible to catch however, and should\n# only be disabled if your sftp version has problems with batch mode\n#sftp_batch_mode = False\n\n[accelerate]\n#accelerate_port = 5099\n#accelerate_timeout = 30\n#accelerate_connect_timeout = 5.0\n\n# The daemon timeout is measured in minutes. This time is measured\n# from the last activity to the accelerate daemon.\n#accelerate_daemon_timeout = 30\n\n# If set to yes, accelerate_multi_key will allow multiple\n# private keys to be uploaded to it, though each user must\n# have access to the system via SSH to add a new key. The default\n# is \"no\".\n#accelerate_multi_key = yes\n\n[selinux]\n# file systems that require special treatment when dealing with security context\n# the default behaviour that copies the existing context or uses the user default\n# needs to be changed to use the file system dependent context.\n#special_context_filesystems=nfs,vboxsf,fuse,ramfs,9p\n\n# Set this to yes to allow libvirt_lxc connections to work without SELinux.\n#libvirt_lxc_noseclabel = yes\n\n[colors]\n#highlight = white\n#verbose = blue\n#warn = bright purple\n#error = red\n#debug = dark gray\n#deprecate = purple\n#skip = cyan\n#unreachable = red\n#ok = green\n#changed = yellow\n#diff_add = green\n#diff_remove = red\n#diff_lines = cyan\n\n\n[diff]\n# Always print diff when running ( same as always running with -D/--diff )\n# always = no\n\n# Set how many context lines to show in diff\n# context = 3\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "e5b14b42e1074dca3a8720c74788afba79171ddf", "filename": "inventory/sample.osp.example.com.d/files/policy/01-clusterroles.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "apiVersion: v1\nkind: List\nitems:\n- apiVersion: v1\n  kind: ClusterRole\n  metadata:\n    creationTimestamp: null\n    name: cluster-admin-custom\n  rules:\n  - apiGroups:\n    - '*'\n    attributeRestrictions: null\n    resources:\n    - '*'\n    verbs:\n    - '*'\n  - apiGroups: null\n    attributeRestrictions: null\n    nonResourceURLs:\n    - '*'\n    resources: []\n    verbs:\n    - '*'\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "9f5c20a8376b3434066f6ac42439d7b3f6d17100", "filename": "playbooks/templates/logrotate-suricata.conf", "repository": "rocknsm/rock", "decoded_content": "{{ suricata_data_dir }}/*.log {{ suricata_data_dir }}/*.json\n{\n    rotate 3\n    missingok\n    nocompress\n    create\n    sharedscripts\n    postrotate\n            /bin/kill -HUP $(cat /var/run/suricata.pid)\n    endscript\n}\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "9b8886342d36a60ecb2ef829fad13017b57abd18", "filename": "roles/network/tasks/dhcpd.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install dhcp package\n  package: name=isc-dhcp-server\n           state=present\n  when: is_debuntu\n  tags:\n    - download\n\n- name: Install dhcp package\n  package: name=dhcp\n           state=present\n  when: not is_debuntu\n  tags:\n    - download\n\n- name: Create non-privileged user\n  user: name=dhcpd\n        createhome=no\n  when: is_debuntu\n\n- name: Disable stock dhcp_service\n  service: name={{ dhcp_service }}\n           enabled=no\n           state=stopped\n  when: is_debuntu\n\n- name: Disable stock dhcp_service ipv6\n  service: name={{ dhcp_service }}6\n           enabled=no\n           state=stopped\n  when: is_ubuntu_18\n\n- name: Install systemd unit file to /etc/systemd/system/dhcpd.service\n  template: src={{ item.src }}\n            dest={{ item.dest }}\n            owner=root\n            group=root\n            mode={{ item.mode }}\n  with_items:\n   - { src: 'roles/network/templates/dhcp/dhcpd.service', dest: '/etc/systemd/system/dhcpd.service', mode: '0644' }\n\n- name: Create dhcpd needed files\n  command: touch /var/lib/dhcpd/dhcpd.leases\n           creates=/var/lib/dhcpd/dhcpd.leases\n  when: is_redhat\n\n- name: Check lease's permissions\n  file: path=/var/lib/dhcpd/dhcpd.leases\n        owner=dhcpd\n        group=dhcpd\n        mode=0644\n        state=file\n  when: is_redhat\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "c0365bd3d80f199b60d5ba57f62798847dc09448", "filename": "roles/static_inventory/tasks/checkpoint.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: check for static inventory dir\n  stat:\n    path: \"{{ inventory_path }}\"\n  register: stat_inventory_path\n\n- name: create static inventory dir\n  file:\n    path: \"{{ inventory_path }}\"\n    state: directory\n    mode: 0750\n  when: not stat_inventory_path.stat.exists\n\n- name: create inventory from template\n  template:\n    src: inventory.j2\n    dest: \"{{ inventory_path }}/hosts\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "7e7ce7177ac1296050c08d51bacd447bd81584c1", "filename": "roles/docker/handlers/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: enable docker\n  service:\n    name: docker\n    enabled: yes\n\n- name: restart docker\n  service:\n    name: docker\n    enabled: yes\n    state: restarted\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "9583ac18eeab1f64594452b1d30a0ba45a896318", "filename": "roles/lighttpd/tasks/add-user.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n- name: Ensure passwd file exists\n  file:\n    path: /etc/lighttpd/rock-htpasswd.user\n    state: touch\n    owner: root\n    group: root\n    mode: 0644\n\n- name: Hash password\n  command: \"openssl passwd -apr1 \\\"{{ lighttpd_password }}\\\"\"\n  register: lighttpd_hashed_password\n  no_log: true\n  changed_when: false\n\n- name: Add a new user to lighttpd\n  lineinfile:\n    path: /etc/lighttpd/rock-htpasswd.user\n    regex: \"^{{ lighttpd_user }}\"\n    line: \"{{ lighttpd_user }}:{{ lighttpd_hashed_password.stdout }}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "ba5543dc5c95ca3ada69bbd919f896ade60fbfc0", "filename": "roles/httpd/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install httpd required packages (debian)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - apache2\n    - php{{ php_version }}\n    - php{{ php_version }}-curl\n#    - php{{ php_version }}-sqlite\n  tags:\n    - download\n  when: is_debian\n\n- name: Debian changed sqlite name (debian-8)\n  package:\n    name: \"php{{ php_version }}-sqlite\"\n  when: is_debian and ansible_distribution_major_version == \"8\"\n\n- name: Debian changed sqlite name (debian-9)\n  package: name=php{{ php_version }}-sqlite3\n  when: is_debian and ansible_distribution_major_version == \"9\"\n\n-  name: Install httpd required packages (ubuntu)\n   package:\n     name: \"{{ item }}\"\n     state: present\n   with_items:\n     - apache2\n     - php\n   tags:\n     - download\n   when: is_ubuntu\n\n- name: Sqlite3 no longer included in another  package in ubuntu18\n  package:\n    name: php{{ php_version }}-sqlite3\n  when: is_ubuntu_18\n\n- name: Install httpd required packages (redhat)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - httpd\n    - php\n    - php-curl\n    - mod_authnz_external\n#    - php-sqlite\n  tags:\n    - download\n  when: is_redhat\n\n- name: Remove the default apache2 config file (debuntu)\n  file:\n    path: /etc/apache2/sites-enabled/000-default.conf\n    state: absent\n  when: is_debuntu\n\n- name: Create httpd config files\n  template:\n    backup: yes\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: root\n    group: root\n    mode: \"{{ item.mode }}\"\n  with_items:\n    - { src: '010-iiab.conf.j2' , dest: '/etc/{{ apache_config_dir }}/010-iiab.conf', mode: '0755' }\n    - { src: 'proxy_ajp.conf.j2' , dest: '/etc/{{ apache_config_dir }}/proxy_ajp.conf', mode: '0644' }\n    - { src: 'php.ini.j2' , dest: '/etc/php.ini' , mode: '0644' }\n\n# remove symlinks for mpm-event, replace with mpm-prefork\n- name: Remove mpm event links (debuntu)\n  file:\n    path: \"/etc/apache2/mods-enabled/{{ item }}\"\n    state: absent\n  with_items:\n    - mpm_event.conf\n    - mpm_event.load\n  when: is_debuntu\n\n- name: Create symlinks for mpm-prefork (debuntu)\n  file:\n    path: \"/etc/apache2/mods-enabled/{{ item }}\"\n    src: \"/etc/apache2/mods-available/{{ item }}\"\n    state: link\n  with_items:\n    - mpm_prefork.conf\n    - mpm_prefork.load\n  when: is_debuntu\n\n- name: Turn on mod_proxy (debuntu)\n  command: a2enmod {{ item }}\n  with_items:\n    - proxy\n    - proxy_html\n    - headers\n    - rewrite\n  when: is_debuntu\n\n- name: Create symlinks for enabling our site (debuntu)\n  file:\n    path: \"/etc/apache2/sites-enabled/{{ item }}\"\n    src: \"/etc/apache2/sites-available/{{ item }}\"\n    state: link\n  with_items:\n    - 010-iiab.conf\n  when: is_debuntu\n\n- name: Remove the default site container (debuntu)\n  file:\n    dest: /etc/apache2/000-default.conf\n    state: absent\n  when: is_debuntu\n\n- name: Create http pid dir\n  file:\n    path: \"/var/run/{{ apache_user }}\"\n    mode: 0755\n    owner: root\n    group: root\n    state: directory\n\n- name: Create admin group\n  group:\n    name: admin\n    state: present\n\n- name: Add apache user to admin group\n  user:\n    name: \"{{ apache_user }}\"\n    groups: admin\n    state: present\n    createhome: no\n\n- name: Create httpd log dir\n  file:\n    path: \"/var/log/{{ apache_service }}\"\n    mode: 0755\n    owner: \"{{ apache_user }}\"\n    group: \"{{ apache_user }}\"\n    state: directory\n\n- name: Enable httpd\n  service:\n    name: \"{{ apache_service }}\"\n    enabled: yes\n\n- name: Create iiab-info directory\n  file:\n    path: \"{{ doc_root }}/info\"\n    mode: 0755\n    owner: \"{{ apache_user }}\"\n    group: \"{{ apache_user }}\"\n    state: directory\n\n- name: Remove iiab-info.conf\n  file:\n    dest: \"/etc/{{ apache_config_dir }}/iiab-info.conf\"\n    state: absent\n\n- name: Remove iiab-info.conf symlink (debuntu)\n  file:\n    dest: /etc/apache2/sites-enabled/iiab-info.conf\n    state: absent\n  when: is_debuntu\n\n- include_tasks: html.yml\n  tags:\n    - base\n\n# Fixes search @ http://box/modules/es-wikihow - see https://github.com/iiab/iiab/issues/829\n- include_tasks: php-stem.yml\n  tags:\n    - base\n\n- name: Install /usr/bin/iiab-refresh-wiki-docs (scraper script) to create http://box/info offline documentation (will be run at the end of Stage 4 = roles/4-server-options/tasks/main.yml)\n  template:\n    src: refresh-wiki-docs.sh\n    dest: /usr/bin/iiab-refresh-wiki-docs\n    mode: 0755\n\n- name: Give apache_user permission for poweroff\n  template:\n    src: 020_apache_poweroff.j2\n    dest: /etc/sudoers.d/020_apache_poweroff\n    mode: 0755\n  when: allow_apache_sudo\n\n- name: Remove apache_user permission for poweroff\n  file:\n    dest: /etc/sudoers.d/020_apache_poweroff\n    state: absent\n  when: not allow_apache_sudo\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "711b4c0e09ec4f89472fe7814cc5e3df3dd4db1b", "filename": "roles/config-satellite/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: \"prereq.yml\"\n- import_tasks: \"install.yml\"\n- import_tasks: \"manifest.yml\"\n- import_tasks: \"repos.yml\"\n- import_tasks: \"activation_keys.yml\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "22497cadb480023e0ee738deb8489c1520e3e250", "filename": "roles/config-timezone/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: set timezone to {{ timezone }}\n  timezone:\n    name: \"{{ timezone }}\"\n  when:\n  - timezone is defined\n  - timezone | trim != \"\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "2e6bb6a8beb4fe6bec161e1dc449e8abb30939b3", "filename": "roles/openshift-emptydir-quota/handlers/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: restart openshift-node\n  service:\n    name: \"{{ openshift.common.service_type }}-node\"\n    state: restarted\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "ab98db635ab10eceb7c0cda79dccd550ed90392e", "filename": "roles/security/handlers/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: restart ssh\n  service: name=\"{{ ssh_service_name|default('ssh') }}\" state=restarted\n\n- name: flush routing cache\n  shell: echo 1 > /proc/sys/net/ipv4/route/flush\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "d74cdf2b8cd788b756156ee44cff75d6a70f5704", "filename": "playbooks/roles/docket/Vagrantfile", "repository": "rocknsm/rock", "decoded_content": "# -*- mode: ruby -*-\n# vi: set ft=ruby :\n\n$script = <<EOF\n\n# Setup RockNSM Repo\nyum install -y https://copr-be.cloud.fedoraproject.org/results/@rocknsm/rocknsm-2.1/epel-7-x86_64/00720349-rock-release/rock-release-2.1-1.noarch.rpm epel-release\nsed -i '/enabled=/ s/0/1/' /etc/yum.repos.d/rocknsm-testing.repo\n\nyum install -y ansible python2-pyOpenSSL stenographer\n\n/usr/bin/stenokeys.sh stenographer stenographer\n\n# Run the test\ncd /vagrant/rocknsm.docket\n./test.sh\nEOF\n\nVagrant.configure(\"2\") do |config|\n  config.vm.box = \"centos/7\"\n\n  # Public Network\n  config.vm.define :node do |node|\n    node.vm.network :public_network,\n      :dev => \"virbr0\",\n      :mode => \"bridge\",\n      :type => \"bridge\"\n  end\n\n  config.vm.synced_folder '../', '/vagrant', type: 'rsync'\n\n  config.vm.provision \"shell\" do |s|\n      s.inline = $script\n      s.keep_color = true\n  end\nend\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a70126f6d17c04272844c448b0f8ea97eac7bcbd", "filename": "roles/dns/manage-dns-records/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- include_tasks: nsupdate/main.yml\n- include_tasks: route53/main.yml\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6eb31715d3d270a1082f8164885eec25ceb2bec5", "filename": "reference-architecture/ansible-tower-integration/tower_unconfig_azure/tower_unconfig_azure/meta/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ngalaxy_info:\n  author: James Labocki\n  description: un-Configures Tower to sync openshift-ansible-contrib\n  company: Red Hat, Inc.\n  license: MIT\n  min_ansible_version: 1.2\n  platforms:\n  - name: EL\n    versions:\n    - 6\n    - 7\n  categories:\n  - packaging\n  - system\n  dependencies: []\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "eb68b1c03ac49b322bcba1bc4a0052d7517470a4", "filename": "reference-architecture/vmware-ansible/playbooks/roles/heketi-install/templates/heketi.json.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "{\n  \"_port_comment\": \"Heketi Server Port Number\",\n  \"port\": \"8080\",\n\n  \"_use_auth\": \"Enable JWT authorization. Please enable for deployment\",\n  \"use_auth\": false,\n\n  \"_jwt\": \"Private keys for access\",\n  \"jwt\": {\n    \"_admin\": \"Admin has access to all APIs\",\n    \"admin\": {\n      \"key\": \"{{ admin_key }}\"\n    },\n    \"_user\": \"User only has access to /volumes endpoint\",\n    \"user\": {\n      \"key\": \"{{ user_key }}\"\n    }\n  },\n\n  \"_glusterfs_comment\": \"GlusterFS Configuration\",\n  \"glusterfs\": {\n    \"_executor_comment\": [\n      \"Execute plugin. Possible choices: mock, ssh\",\n      \"mock: This setting is used for testing and development.\",\n      \"      It will not send commands to any node.\",\n      \"ssh:  This setting will notify Heketi to ssh to the nodes.\",\n      \"      It will need the values in sshexec to be configured.\",\n      \"kubernetes: Communicate with GlusterFS containers over\",\n      \"            Kubernetes exec api.\"\n    ],\n    \"executor\": \"ssh\",\n\n    \"_sshexec_comment\": \"SSH username and private key file information\",\n    \"sshexec\": {\n      \"keyfile\": \"/etc/heketi/heketi_key\",\n      \"user\": \"root\",\n      \"port\": \"22\",\n      \"fstab\": \"/etc/fstab\"\n    },\n\n    \"_kubeexec_comment\": \"Kubernetes configuration\",\n    \"kubeexec\": {\n      \"host\" :\"https://kubernetes.host:8443\",\n      \"cert\" : \"/path/to/crt.file\",\n      \"insecure\": false,\n      \"user\": \"kubernetes username\",\n      \"password\": \"password for kubernetes user\",\n      \"namespace\": \"OpenShift project or Kubernetes namespace\",\n      \"fstab\": \"Optional: Specify fstab file on node.  Default is /etc/fstab\"\n    },\n\n    \"_db_comment\": \"Database file name\",\n    \"db\": \"/var/lib/heketi/heketi.db\",\n\n    \"_loglevel_comment\": [\n      \"Set log level. Choices are:\",\n      \"  none, critical, error, warning, info, debug\",\n      \"Default is warning\"\n    ],\n    \"loglevel\" : \"debug\"\n  }\n}\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "e961cc0d2e933c2481b815280a0675a89fca8580", "filename": "playbooks/nagios/setup_nagios.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: \"Set up the nagios targets\"\n  hosts: nagios-targets\n  roles:\n  - role: infra-ansible/roles/nagios-target\n    \n- name: \"Set up nagios server\"\n  hosts: nagios-servers\n  roles:\n  - role: infra-ansible/roles/nagios-server\n\n"}, {"commit_sha": "92dabcd706e72a0dc15ce13086fb9d59f1a8760e", "sha": "7913a164f33f1af990c6e9b7d7256f2f9f8b6bbb", "filename": "tasks/upgrade.yml", "repository": "RocketChat/Rocket.Chat.Ansible", "decoded_content": "---\n# tasks/upgrade.yml: Rocket.Chat upgrade procedures for RocketChat.Ansible\n\n  - name: Ensure automatic upgrades are permitted [UPGRADE]\n    fail:\n      msg: >-\n        It doesn't look like you've permitted automatic upgrades.\n        A new version of Rocket.Chat was released.\n        To permit automatic upgrades set 'rocket_chat_automatic_upgrades' to true\n    when: not (rocket_chat_automatic_upgrades | bool)\n\n  - name: Ensure the back up directory exists [UPGRADE]\n    file:\n      path: \"{{ rocket_chat_upgrade_backup_path }}\"\n      state: directory\n    when: (rocket_chat_upgrade_backup | bool)\n\n  - name: Back up the current Rocket.Chat instance [UPGRADE]\n    shell: >-\n      mv {{ rocket_chat_application_path }}/bundle\n      {{ rocket_chat_upgrade_backup_path }}/backup_{{ ansible_date_time.date }}_{{ (1000|random|string|hash)[:8] }}\n    when: (rocket_chat_upgrade_backup | bool)\n\n  - name: Delete the current Rocket.Chat instance [UPGRADE]\n    file:\n      path: \"{{ rocket_chat_application_path }}/bundle\"\n      state: absent\n    when: not (rocket_chat_upgrade_backup | bool)\n\n  - name: Set the Rocket.Chat upgrade status [UPGRADE]\n    set_fact:\n      rocket_chat_upgraded: true\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "730836b4c982dbcd07058fe385538d0e06516dab", "filename": "playbooks/templates/logrotate-suricata.conf.j2", "repository": "rocknsm/rock", "decoded_content": "/data/suricata/*.log /data/suricata/*.json\n{\n    rotate {{ suricata_retention }}\n    missingok\n    compress\n    create 0644 suricata suricata\n    sharedscripts\n    postrotate\n        systemctl kill -s HUP --kill-who=main suricata.service\n    endscript\n}\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "2d902a0adb4ea85ccdd6462a222ae28126f3f18d", "filename": "tasks/create_repo_npm_group_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_npm_group\n    args: \"{{ _nexus_repos_npm_defaults|combine(item) }}\""}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "e22bf0decbec0dd77d956097d3938484813f98ea", "filename": "roles/openshift-applier/tasks/copy-inventory-content.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: \"Copy 'content' entries\"\n  include_tasks: copy-inventory-entry.yml\n  loop_control:\n    loop_var: entry\n  with_items:\n  - \"{{ content_entry.content }}\"\n  when:\n  - content_entry.content is defined \n\n- name: \"Copy 'content_dir' entries\"\n  vars:\n    dir: \"{{ content_entry.content_dir }}\"\n  import_tasks: copy-inventory-dir.yml\n  when:\n  - content_entry.content_dir is defined \n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "361d58240ca6c9a5c026d69b16a8f7f71061f974", "filename": "roles/bro/vars/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# vars file for bro\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "fe6e6f536d5962d9cdd40b7cbac4fca7edc0f8ee", "filename": "roles/dns/manage-dns-zones/tasks/named/print_keys.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Print configured keys - if requested\"\n  debug:\n    var: nsupdate_keys\n  run_once: true\n  when:\n  - print_dns_keys|default(False)\n  delegate_to: \"{{ ansible_play_hosts | first }}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "c4b2fac76da8167d2a4f77b00632b8a638e12788", "filename": "roles/network/tasks/enable_services.yml", "repository": "iiab/iiab", "decoded_content": "- name: Disable dhcpd service\n  service:\n    name: dhcpd\n    enabled: no\n  when: not dhcpd_enabled and dhcpd_install\n\n# service is restarted with NM dispatcher.d script\n- name: Enable dhcpd service\n  service:\n    name: dhcpd\n    enabled: yes\n  when: dhcpd_enabled and dhcpd_install\n\n- name: Copy /etc/sysconfig/dhcpd file\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: root\n    group: root\n    mode: \"{{ item.mode }}\"\n  with_items:\n   - { src: 'dhcp/dhcpd-env.j2', dest: '/etc/sysconfig/dhcpd', mode: '0644' }\n   - { src: 'dhcp/dhcpd-iiab.conf.j2', dest: '/etc/dhcpd-iiab.conf', mode: '0644' }\n  when: dhcpd_enabled and dhcpd_install\n\n- name: Copy named file\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: root\n    group: root\n    mode: \"{{ item.mode }}\"\n  with_items:\n   - { src: 'named/named-iiab.conf.j2', dest: '/etc/named-iiab.conf', mode: '0644' }\n   - { src: 'named/school.local.zone.db', dest: '/var/named-iiab/', mode: '0644' }\n   - { src: 'named/school.internal.zone.db', dest: '/var/named-iiab/', mode: '0644' }\n\n- name: Enable named service\n  service:\n    name: \"{{ dns_service }}\"\n    enabled: yes\n  when: named_enabled and named_install\n\n- name: Disable named service\n  service:\n    name: \"{{ dns_service }}\"\n    enabled: no\n  when: not named_enabled and named_install\n\n- name: Disable dnsmasq\n  service:\n    name: dnsmasq\n    enabled: no\n  when: not dnsmasq_enabled and dnsmasq_install\n\n- name: Copy dnsmasq.conf to /etc\n  template:\n    src: network/dnsmasq.conf.j2\n    dest: /etc/dnsmasq.conf\n  when: dnsmasq_enabled and dnsmasq_install\n\n- name: Enable dnsmasq\n  service:\n    name: dnsmasq\n    enabled: yes\n  when: dnsmasq_enabled and dnsmasq_install\n\n- name: Enable DansGuardian\n  service:\n    name: dansguardian\n    enabled: yes\n  when: dansguardian_enabled and dansguardian_install\n\n- name: Disable DansGuardian\n  service:\n    name: dansguardian\n    enabled: no\n  when: not dansguardian_enabled and dansguardian_install\n\n- name: Create xs_httpcache flag\n  lineinfile:\n    dest: /etc/iiab/iiab.env\n    regexp: '^HTTPCACHE_ON=*'\n    line: 'HTTPCACHE_ON=True'\n    state: present\n  when: squid_enabled and squid_install\n\n- name: Enable Squid service\n  service:\n    name: \"{{ proxy }}\"\n    enabled: yes\n  when: squid_enabled and squid_install\n\n- name: Copy init script and config file\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: \"{{ item.owner }}\"\n    group: \"{{ item.group }}\"\n    mode: \"{{ item.mode }}\"\n  with_items:\n    - src: squid/squid-iiab.conf.j2\n      dest: \"/etc/{{ proxy }}/squid-iiab.conf\"\n      owner: \"{{ proxy_user }}\"\n      group: \"{{ proxy_user }}\"\n      mode: 0644\n  when: squid_enabled and squid_install\n\n- name: Point to Squid config file from startup file\n  lineinfile:\n    regexp: '^CONFIG'\n    line: \"CONFIG=/etc/{{ proxy }}/squid-iiab.conf\"\n    dest: \"/etc/init.d/{{ proxy }}\"\n  when: squid_enabled and squid_install and is_debuntu\n\n- name: Disable Squid service\n  service:\n    name: \"{{ proxy }}\"\n    enabled: no\n  when: not squid_enabled and squid_install\n\n- name: Remove xs_httpcache flag\n  lineinfile:\n    dest: /etc/iiab/iiab.env\n    regexp: '^HTTPCACHE_ON=*'\n    line: 'HTTPCACHE_ON=False'\n    state: present\n  when: not squid_enabled\n\n- name: Enable Wondershaper service\n  service:\n    name: wondershaper\n    enabled: yes\n  when: wondershaper_enabled and wondershaper_install\n\n- name: Disable Wondershaper service\n  service:\n    name: wondershaper\n    enabled: no\n  when: not wondershaper_enabled and wondershaper_install\n\n# check-LAN should be iptables.yml remove later\n- name: Grab clean copy of iiab-gen-iptables\n  template:\n    src: \"{{ item.0 }}\"\n    dest: \"{{ item.1 }}\"\n    owner: root\n    group: root\n    mode: 0755\n  with_items:\n   - { 0: 'gateway/iiab-gen-iptables', 1: '/usr/bin/iiab-gen-iptables' }\n\n- name: Add 'squid' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: squid\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: enabled\n      value: \"{{ squid_enabled }}\"\n\n- name: Add 'dansguardian' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: dansguardian\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: enabled\n      value: \"{{ dansguardian_enabled }}\"\n\n- name: Add 'wondershaper' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: wondershaper\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: enabled\n      value: \"{{ wondershaper_enabled }}\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "471dbb98e5889ff258af1f9eddcdb589af54883f", "filename": "playbooks/templates/es-jvm.options.j2", "repository": "rocknsm/rock", "decoded_content": "## JVM configuration\n################################################################\n## IMPORTANT: JVM heap size\n################################################################\n##\n## You should always set the min and max JVM heap\n## size to the same value. For example, to set\n## the heap to 4 GB, set:\n##\n## -Xms4g\n## -Xmx4g\n##\n## See https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html\n## for more information\n##\n################################################################\n\n# Xms represents the initial size of total heap space\n# Xmx represents the maximum size of total heap space\n\n-Xms{{ es_mem }}g\n-Xmx{{ es_mem }}g\n\n################################################################\n## Expert settings\n################################################################\n##\n## All settings below this section are considered\n## expert settings. Don't tamper with them unless\n## you understand what you are doing\n##\n################################################################\n\n## GC configuration\n-XX:+UseConcMarkSweepGC\n-XX:CMSInitiatingOccupancyFraction=75\n-XX:+UseCMSInitiatingOccupancyOnly\n\n## optimizations\n\n# pre-touch memory pages used by the JVM during initialization\n-XX:+AlwaysPreTouch\n\n## basic\n\n# force the server VM\n-server\n\n# set to headless, just in case\n-Djava.awt.headless=true\n\n# ensure UTF-8 encoding by default (e.g. filenames)\n-Dfile.encoding=UTF-8\n\n# use our provided JNA always versus the system one\n-Djna.nosys=true\n\n# flags to configure Netty\n-Dio.netty.noUnsafe=true\n-Dio.netty.noKeySetOptimization=true\n-Dio.netty.recycler.maxCapacityPerThread=0\n\n# log4j 2\n-Dlog4j.shutdownHookEnabled=false\n-Dlog4j2.disable.jmx=true\n\n-Djava.io.tmpdir=${ES_TMPDIR}\n\n## heap dumps\n\n# generate a heap dump when an allocation from the Java heap fails\n# heap dumps are created in the working directory of the JVM\n-XX:+HeapDumpOnOutOfMemoryError\n\n# specify an alternative path for heap dumps\n# ensure the directory exists and has sufficient space\n-XX:HeapDumpPath=/var/lib/elasticsearch\n\n## JDK 8 GC logging\n\n8:-XX:+PrintGCDetails\n8:-XX:+PrintGCDateStamps\n8:-XX:+PrintTenuringDistribution\n8:-XX:+PrintGCApplicationStoppedTime\n8:-Xloggc:/var/log/elasticsearch/gc.log\n8:-XX:+UseGCLogFileRotation\n8:-XX:NumberOfGCLogFiles=32\n8:-XX:GCLogFileSize=64m\n\n# JDK 9+ GC logging\n9-:-Xlog:gc*,gc+age=trace,safepoint:file=/var/log/elasticsearch/gc.log:utctime,pid,tags:filecount=32,filesize=64m\n# due to internationalization enhancements in JDK 9 Elasticsearch need to set the provider to COMPAT otherwise\n# time/date parsing will break in an incompatible way for some date patterns and locals\n9-:-Djava.locale.providers=COMPAT\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "42f0c6cbec4ad94414ccfd4f9f80d5460f4143a0", "filename": "roles/storage-cns/tasks/provision.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- name: Login As Super User\n  command: \"oc login -u {{ admin_user }} -p {{ admin_password }}\"\n  when: cluster==\"openshift\"\n        and admin_user is defined\n        and admin_password is defined\n\n- name: Render storage-cns deployment yaml\n  template:\n    src: storage-cns.yml\n    dest: /tmp/storage-cns.yml\n\n- name: Create storage-cns Resources\n  command: kubectl apply -f /tmp/storage-cns.yml\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "7e4fa19281f21c02e17114b75605e85feacb929d", "filename": "archive/roles/cicd/tasks/httpd.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n  \n- name: Installing HTTPD\n  yum: \n    name: httpd\n    state: present\n  tags: httpd\n  \n- name: Copy HTTPD Files\n  copy: \n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n  notify:\n  - restart httpd\n  with_items:\n  - { src: httpd/index.html, dest: /var/www/html/index.html }\n  - { src: httpd/cicd.conf, dest: /etc/httpd/conf.d/cicd.conf }\n  tags: httpd\n  \n- name: Open Firewall for HTTP\n  firewalld: \n    port: 80/tcp\n    zone: public\n    permanent: yes\n    immediate: yes\n    state: enabled\n  tags: httpd\n\n- name: HTTPD SELinux Configurations\n  seboolean: \n    name: httpd_can_network_connect\n    state: yes\n    persistent: yes\n  tags: httpd\n  \n- name: Enable HTTPD Service\n  service: \n    name: httpd\n    enabled: true\n  tags: httpd\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b3ed3f9f7ba0968d394b627657ba8a235d23cac4", "filename": "playbooks/ansible/tower/configure-ansible-tower.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: ansible-tower\n  roles:\n  - role: ansible/tower/config-ansible-tower\n  - role: ansible/tower/config-ansible-tower-ldap\n  tags:\n  - 'never'\n  - 'install'\n\n- hosts: tower-management-host\n  roles:\n  - role: ansible/tower/manage-projects\n  - role: ansible/tower/manage-credential-types\n  - role: ansible/tower/manage-credentials\n  - role: ansible/tower/manage-inventories\n  - role: ansible/tower/manage-job-templates\n  tags:\n  - 'always'\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "6a60ec23532b2bb3927fd7d1e466e314df276b13", "filename": "roles/network/tasks/restart.yml", "repository": "iiab/iiab", "decoded_content": "- name: Restart hostapd when WiFi is present\n  systemd:\n    name: hostapd\n    state: restarted\n  when: iiab_wireless_lan_iface is defined and hostapd_enabled and iiab_network_mode != \"Appliance\"\n\n- name: Start named service\n  service:\n    name: \"{{ dns_service }}\"\n    state: restarted\n  when: named_enabled and named_install\n\n- name: Stop Squid service\n  service:\n    name: \"{{ proxy }}\"\n    state: stopped\n  async: 120\n  when: squid_install\n\n- name: Stop DansGuardian\n  service:\n    name: dansguardian\n    state: stopped\n  when: dansguardian_install\n\n- name: Restart DansGuardian - except Ubuntu which needs reboot to activate\n  service:\n    name: dansguardian\n    state: restarted\n  when: dansguardian_enabled and dansguardian_install and ( not is_ubuntu and iiab_stage|int < 4 )\n\n# Squid get re-loaded with dispatcher.d\n- name: Restart Squid service\n  service:\n    name: \"{{ proxy }}\"\n    state: restarted\n  when: squid_enabled and squid_install\n\n- name: Restart Wondershaper service\n  service:\n    name: wondershaper\n    state: restarted\n  when: wondershaper_enabled\n\n- name: Restart avahi service\n  service:\n    name: avahi-daemon\n    state: restarted\n\n- name: Create gateway flag\n  shell: echo 1 > /etc/sysconfig/olpc-scripts/setup.d/installed/gateway\n  args:\n    creates: /etc/sysconfig/olpc-scripts/setup.d/installed/gateway\n  when: iiab_network_mode == \"Gateway\"\n\n- name: Waiting {{ hostapd_wait }} seconds for network to stabilize\n  shell: sleep {{ hostapd_wait }}\n\n- name: Run iptables\n  command: /usr/bin/iiab-gen-iptables\n\n- name: Checking if WiFi slave is active\n  shell: brctl show br0 | grep {{ iiab_wireless_lan_iface }} | wc -l\n  when: hostapd_enabled and iiab_wireless_lan_iface is defined and iiab_lan_iface == \"br0\"\n  register: wifi_slave\n\n- name: Restart hostapd if WiFi slave is inactive\n  systemd:\n    name: hostapd\n    state: restarted\n  when: hostapd_enabled and wifi_slave.stdout is defined and wifi_slave.stdout == 0\n\n- name: dhcp_server may be affected - starting - user choice\n  service:\n    name: \"{{ dhcp_service2 }}\"\n    state: restarted\n  when: iiab_network_mode != \"Appliance\"\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "da6bee6c04ca9f118baf7c663c0648b780a7b104", "filename": "tasks/setup-repository.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Include tasks for distribution {{ _docker_os_dist }} to setup repository\n  include_tasks: setup-repository-{{ _docker_os_dist }}.yml\n\n- name: Update repository cache\n  become: true\n  shell: \"{{ docker_cmd_update_repo_cache[_docker_os_dist] }}\"\n  args:\n    warn: false\n  changed_when: false\n  register: _result\n  until: _result is succeeded\n  when: docker_network_access | bool\n  tags:\n    - skip_ansible_lint"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "40ecbf2d8806325c27e8953963c6a5ad5b99d12d", "filename": "reference-architecture/gcp/ansible/playbooks/roles/gcp-ssh-key/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: create gcp ssh key\n  command: ssh-keygen -f '{{ gcp_ssh_key }}' -t rsa -C cloud-user -N '' creates='{{ gcp_ssh_key }}'\n\n- name: read public ssh key\n  slurp:\n    src: '{{ gcp_ssh_key }}.pub'\n  register: public_ssh_key\n\n- block:\n  - name: get public keys in ssh agent\n    command: ssh-add -L\n    register: public_keys_in_agent\n    changed_when: false\n\n  - name: add gcp key to the key agent\n    command: ssh-add '{{ gcp_ssh_key }}'\n    when: public_ssh_key.content | b64decode | regex_replace('ssh-rsa (.*) cloud-user', '\\\\1') | trim not in public_keys_in_agent.stdout\n  when: ansible_env.SSH_AUTH_SOCK is defined\n\n- name: get current ssh keys in the gcp project\n  command: gcloud --project {{ gcloud_project }} compute project-info describe --format='value(commonInstanceMetadata.items.key.sshKeys.value)'\n  register: project_ssh_keys\n  changed_when: false\n\n- block:\n  - name: create variable with our public key for gcp metadata, when there are no other public keys\n    set_fact:\n      ssh_public_key_metadata: 'cloud-user:{{ public_ssh_key.content | b64decode | trim }}'\n    when: \"'ssh-rsa' not in project_ssh_keys.stdout\"\n\n  - name: create variable with our public key for gcp metadata, when there are other public keys\n    set_fact:\n      ssh_public_key_metadata: \"{{ project_ssh_keys.stdout }}\\ncloud-user:{{ public_ssh_key.content | b64decode | trim }}\"\n    when: \"'ssh-rsa' in project_ssh_keys.stdout\"\n\n  - name: add our ssh key to the project\n    command: gcloud --project {{ gcloud_project }} compute project-info add-metadata --metadata='sshKeys={{ ssh_public_key_metadata }}'\n  when: public_ssh_key.content | b64decode | regex_replace('ssh-rsa (.*) cloud-user', '\\\\1') | trim not in project_ssh_keys.stdout\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a32b4b9b82a3f288c6344e032fa59feb74384963", "filename": "roles/osp/admin-project/tasks/tenant-roles.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Assign role for project {{ project.name }}\"\n  shell: >\n    source {{ admin_keystonerc_file }};\n    openstack role add \\\n      --user \"{{ item.0.user }}\" \\\n      --user-domain \"{{ item.0.user_domain | default('') }}\" \\\n      --project \"{{ project.name }}\" \\\n      \"{{ item.1 }}\"\n  with_subelements:\n  - \"{{ project.members }}\"\n  - roles\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "708bfe4fcb9fb1ca6407ab7247054ab3ee5b2d68", "filename": "ops/playbooks/backup_dtr_metadata.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- name: Backup DTR metadata\n  hosts: dtr_main \n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - ../group_vars/backups\n\n  environment:\n    - \"{{ env }}\"\n    - UCP_USERNAME: \"{{ ucp_username }}\"\n    - UCP_PASSWORD: \"{{ ucp_password }}\"\n    - UCP_CA: \"{{ ucp_ca_cert | default('') }}\"\n\n  tasks:\n#\n# Select a UCP instance (not relying on the availability of a Load balancer)\n#\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n\n#\n# configure password less ssh from ucp_main to our ansible box\n#\n    - name: Register key\n      stat: path=/root/.ssh/id_rsa\n      register: key\n    - name: Create keypairs\n      shell: ssh-keygen -t rsa -f /root/.ssh/id_rsa -N ''\n      when: key.stat.exists == False\n    - name: Fetch all public ssh keys\n      shell: cat ~/.ssh/id_rsa.pub\n      register: ssh_keys\n    - name: Deploy keys on localhost\n      authorized_key: user=root key=\"{{ item }}\"\n      delegate_to: localhost\n      with_items:\n        - \"{{ ssh_keys.stdout }}\"\n\n#\n# Get a timestamp, will be used to name the backup\n#\n    - name: Get TimeStamp\n      command: date -u '+%Y_%m_%d_%H%M%S'\n      register: timestamp\n    - name: Store the timestamp\n      set_fact:\n        timestamp: \"{{ timestamp.stdout }}\"\n\n    - name: Creates directory\n      file:\n        path: /root/scripts\n        state: directory\n\n#\n# Retrieve replica ID and DTR version number\n#\n    - name: Copy util to backup VM\n      template: src=../templates/dtr_get_info.sh.j2 dest=/root/scripts/dtr_get_info.sh\n    - file:\n        path: /root/scripts/dtr_get_info.sh\n        mode: 0744\n\n    - name: Get the replica ID\n      shell: /root/scripts/dtr_get_info.sh replica\n      register: res \n    - set_fact:\n        replica_id: \"{{ res.stdout }}\" \n\n    - name: Get the DTR version number\n      shell: /root/scripts/dtr_get_info.sh version\n      register: res\n    - set_fact:\n        detected_dtr_version: \"{{ res.stdout }}\" \n\n#\n# Copy the backup script \n#\n    - name: Copy backup script to backup VM\n      template: src=../templates/backup_dtr_metadata.sh.j2 dest=/root/scripts/backup_dtr_metadata.sh\n    - file:\n        path: /root/scripts/backup_dtr_metadata.sh\n        mode: 0744\n\n    - set_fact:\n        backup_name:  \"backup_dtr_meta_{{ replica_id }}_{{ inventory_hostname }}_{{ timestamp }}\"\n      when: backup_name is undefined\n\n#\n# Backup DTR now\n#\n    - name: Backup the metadata now\n      shell: /root/scripts/backup_dtr_metadata.sh {{ replica_id }} {{ backup_name }} \"{{ ucp_instance }}.{{ domain_name }}\"\n      register: res\n\n    - debug: var=res\n      when: _debug is defined\n\n    - name: Create a temporary directory to receive files that contains metadata\n      tempfile:\n        state: directory\n        suffix: temp\n      register: res\n      delegate_to: localhost\n\n    - template:\n        src: ../templates/backup_meta.yml.j2\n        dest: \"{{ res.path }}/meta.yml\"\n      delegate_to: localhost\n\n    - copy:\n         src: \"{{ item }}\"\n         dest: \"{{ res.path }}/\"\n      with_items:\n        - ../vm_hosts\n        - ../group_vars/vars\n        - ../group_vars/backups\n      delegate_to: localhost\n\n    - name: Backup the Metadata as well\n      archive:\n        path:\n          - \"{{ res.path }}/\"\n        dest: \"{{ backup_dest }}/{{ backup_name }}.vars.tgz\"\n      delegate_to: localhost\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "4ca418ac2be1853e1c0d18397033088844393ba8", "filename": "dev/playbooks/start_docker.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: docker\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Enable and start docker service\n      systemd:\n        name: docker\n        enabled: yes\n        state: started\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "2bb988448bf49568509575b2985ac70e42b3b8dc", "filename": "tasks/Win32NT/fetch/openjdk-fallback.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: 'Fetch root page {{ openjdk_root_page }}'\n  win_uri:\n    url: '{{ openjdk_root_page }}'\n    return_content: true\n  register: root_page\n\n- name: Find GA release version\n  set_fact:\n    java_major_version: >-\n      {{ root_page['content']\n        | regex_findall('Ready for use:.*>JDK ([\\d]+)<')\n        | first\n      }}\n\n- name: Out java_major_version\n  debug:\n    var: java_major_version\n\n- name: Fetch GA release page\n  win_uri:\n    url: '{{ openjdk_root_page }}/{{ java_major_version }}/'\n    return_content: true\n    follow_redirects: all\n  register: ga_release_page\n\n- name: Find release url\n  set_fact:\n    release_url: >-\n      {{ ga_release_page['content'] |\n      regex_findall('(https://download[\\.\\w]+/java/GA/jdk' +\n      java_major_version|string +\n      '[.\\d]*/[\\d\\w]+/' +\n      '[.\\d]+/GPL/openjdk-' +\n      java_major_version|string +\n      '[\\d._]+windows-x64_bin[\\w\\d.]+)')\n      }}\n\n- name: Exit if OpenJDK version is not General-Availability Release\n  fail:\n    msg: 'OpenJDK version {{ java_major_version }} not GA Release, or maybe something wrong with java.net'\n  when: release_url[1] is not defined\n\n- name: 'Get artifact checksum {{ release_url[1] }}'\n  win_uri:\n    url: '{{ release_url[1] }}'\n    return_content: true\n  register: artifact_checksum\n\n- name: 'Download artifact from {{ release_url[0] }}'\n  win_get_url:\n    url: '{{ release_url[0] }}'\n    dest: '{{ java_download_path }}'\n    force: true\n    checksum: '{{ artifact_checksum.content }}'\n    checksum_algorithm: sha256\n  register: file_downloaded\n  retries: 20\n  delay: 5\n  until: file_downloaded is succeeded\n  when: ansible_version.full is version('2.8.0', '>=')\n\n- name: Old fetch (Ansible < 2.8)\n  include_tasks: fetch_fallback_old.yml\n  when: ansible_version.full is version('2.8.0', '<')\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "7bdc5156596e0b9a853e7a5686af3ba55f7e5169", "filename": "reference-architecture/aws-ansible/playbooks/openshift-install.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  connection: local\n  gather_facts: yes\n  become: no\n  vars_files:\n  - vars/main.yaml\n  roles:\n  # Group systems\n  - cfn-outputs\n  - instance-groups\n\n- hosts: localhost\n  connection: local\n  gather_facts: no\n  become: no\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - host-up\n\n- hosts: nodes\n  gather_facts: yes\n  become: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - non-atomic-docker-storage-setup\n  - openshift-versions\n\n- include: ../../../playbooks/prerequisite.yaml\n\n- include: openshift-setup.yaml\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "401a2693add3a234116c69ef1fd8ca099fc946c6", "filename": "reference-architecture/azure-ansible/3.6/ansibledeployocp/playbooks/deploy.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  connection: local\n  gather_facts: no\n  become: false\n  roles:\n    - { role: azure-delete, tags: ['delete'] }\n    - { role: azure-deploy, tags: ['deploy'] }\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ed97d539c095cf1413af30cc23dea272095b97dd", "filename": "roles/config-nagios-server/tests/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "5672089fb9bb409c785acf7d43af2b3cefb3debe", "filename": "reference-architecture/vmware-ansible/playbooks/clean.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  ignore_errors: yes\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - instance-groups\n\n- hosts: allnodes\n  ignore_errors: yes\n  vars_files:\n    - vars/main.yaml\n  roles:\n    - rhsm-unregister\n\n- hosts: localhost\n  user: root\n  become: false\n  ignore_errors: yes\n  vars_files:\n    - vars/main.yaml\n  tasks:\n    - name: Delete all added VMs\n      vmware_guest:\n        hostname: \"{{ vcenter_host }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        validate_certs: False\n        name: \"{{ hostvars[item].inventory_hostname }}\"\n        datacenter: \"{{ vcenter_datacenter }}\"\n        folder: \"/{{ vcenter_folder }}\"\n        state: absent\n        force: true\n      with_items: \"{{ groups['allnodes'] }}\"\n\n- hosts: localhost\n  user: root\n  become: false\n  ignore_errors: yes\n  vars_files:\n    - vars/main.yaml\n  tasks:\n    - name: \"Remove resource pool on vCenter\"\n      vmware_resource_pool:\n        hostname: \"{{ vcenter_host }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        validate_certs: False\n        datacenter: \"{{ vcenter_datacenter }}\"\n        cluster: \"{{ vcenter_cluster}}\"\n        resource_pool: \"{{ vcenter_resource_pool }}\"\n        state: \"absent\"\n    - name: \"Remove folder on vCenter\"\n      vmware_folder:\n        hostname: \"{{ vcenter_host }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        validate_certs: False\n        datacenter: \"{{ vcenter_datacenter }}\"\n        cluster: \"{{ vcenter_cluster}}\"\n        folder: \"{{ vcenter_folder }}\"\n        state: \"absent\"\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "c1d7760c224e0e2f58a320c4ca590156e5b062bb", "filename": "tasks/bug-tweaks/tweak-debian8-directlvm.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Create LVM thinpool for Docker according to Docker documentation\n  include_tasks: lvm-thinpool.yml\n  vars:\n    pool:\n      name: thinpool\n      volume_group: docker\n      physical_volumes: \"{{ docker_daemon_config['storage-opts'] | select('match', '^dm.directlvm_device.+') \\\n        | list | regex_replace('dm.directlvm_device=\\\\s*(.+)', '\\\\1') }}\"\n      metadata_size: \"1%VG\"\n      data_size: \"95%VG\"\n\n- name: Modify storage-opts to handle problems with thinpool on Debian 8\n  set_fact:\n    _modified_storage_config: \"{{ (docker_daemon_config['storage-opts'] | difference(_exclusions)) + \\\n      ['dm.thinpooldev=/dev/mapper/docker-thinpool-tpool'] }}\"\n  vars:\n    _exclusions: \"{{ docker_daemon_config['storage-opts'] | select('match', '^dm.directlvm_device.+') | list }}\"\n\n- name: Update Docker daemon configuration to handle consistency between distributions\n  set_fact:\n    docker_daemon_config: \"{{ docker_daemon_config | combine(_updated_item, recursive=true) }}\"\n  vars:\n    _updated_item: \"{ 'storage-opts': {{ _modified_storage_config }} }\"\n\n- name: Updated Docker daemon configuration\n  debug:\n    var: docker_daemon_config\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "32b3ba67ff0fca05651d3fa6394f9da3f1877f71", "filename": "reference-architecture/gcp/ansible/playbooks/roles/inventory-file-creation/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: create static inventory file\n  template:\n    src: inventory.j2\n    dest: ../static-inventory\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "746d161c738281b3698bc43a9f728862dbad8aca", "filename": "roles/config-ipa-client/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nmove_local_user_home: False\nnew_local_home_dir: \"/lclhome\"\ntemporary_username: \"lcluser\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "f62f13c069bf4d25ab456d26b83416493870a466", "filename": "roles/openshift-management/defaults/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\nopenshift_token:\nopenshift_master_url: localhost\nopenshift_login_insecure_flag: --insecure-skip-tls-verify=true\nopenshift_login_insecure: False\nopenshift_prune_builds_complete: 5\nopenshift_prune_builds_failed: 1\nopenshift_prune_builds_keep_younger: 1h0m0s\nopenshift_prune_deployments_complete: 5\nopenshift_prune_deployments_failed: 1\nopenshift_prune_deployments_keep_younger: 1h0m0s\nopenshift_prune_images_tag_revisions: 3\nopenshift_prune_images_keep_younger: 1h0m0s\nopenshift_prune_projects_keep_younger: 24 # 24 hrs by default\nopenshift_prune_projects_user_excludes: []\nopenshift_prune_projects_system_excludes: ['default', 'management-infra', 'openshift', 'openshift-infra', 'kube-system', 'logging']\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "df58fe80bd028de84bd018c1005dee4589c8309d", "filename": "roles/openshift-volume-quota/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Create filesystem for /var/lib/origin/openshift.local.volumes\n  filesystem:\n    fstype: \"{{ local_volumes_fstype }}\"\n    dev: \"{{ local_volumes_device }}\"\n\n- name: Create local volumes directory\n  file:\n    path: \"{{ local_volumes_path }}\"\n    state: directory\n    recurse: yes\n\n- name: Create fstab entry\n  mount:\n    name: \"{{ local_volumes_path }}\"\n    src: \"{{ local_volumes_device }}\"\n    fstype: \"{{ local_volumes_fstype }}\"\n    opts: \"{{ local_volumes_fsopts }}\"\n    state: present\n\n- name: Mount fstab entry\n  mount:\n    name: \"{{ local_volumes_path }}\"\n    src: \"{{ local_volumes_device }}\"\n    fstype: \"{{ local_volumes_fstype }}\"\n    opts: \"{{ local_volumes_fsopts }}\"\n    state: mounted\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "5e76d45c4f16db8671609392110255f8cdd9861f", "filename": "roles/nextcloud/tasks/F18.yml", "repository": "iiab/iiab", "decoded_content": "- name: Remove /etc/nextcloud to avoid confusion as we use the config in {{ nextcloud_prefix }}/nextcloud/config/\n  file:\n    path: /etc/nextcloud\n    state: absent\n\n# but we use the tar file to get the latest version; really only benefits the xo4 on fedora 18\n- name: Download latest Nextcloud software to /opt/iiab/download/{{ nextcloud_src_file }}\n  get_url:\n    url: \"{{ nextcloud_dl_url }}/{{ nextcloud_orig_src_file }}\"\n    dest: \"{{ downloads_dir }}/{{ nextcloud_src_file }}\"\n    timeout: \"{{ download_timeout }}\"\n  when: internet_available\n\n- name: Copy it to permanent location /opt\n  unarchive:\n    src: \"{{ downloads_dir }}/{{ nextcloud_src_file }}\"\n    dest: /opt/\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "6cbb8549a10833f57e5f64006c147c0cf2acc46a", "filename": "roles/user-management/manage-atlassian-users/tasks/add_user_to_groups.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Add user to groups\n  uri:\n    url: \"{{ atlassian_url }}/rest/api/2/group/user?groupname={{ urlencode(item) }}\"\n    method: POST\n    user: '{{ atlassian_username }}'\n    password: '{{ atlassian_password }}'\n    force_basic_auth: yes\n    status_code: [201, 400]\n    body_format: json\n    body: \"{ 'name': '{{ atlassian_user.email.split(\\\"@\\\") | first }}' }\"\n    return_content: yes\n  with_items: '{{ atlassian_user.groups }}'\n  when: atlassian_user.groups|length > 0\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "a6b1530ba50a54e7edd0424fd3a61e97ded1eeb9", "filename": "roles/vpn/tasks/ipec_configuration.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Setup the config files from our templates\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: \"{{ item.owner }}\"\n    group: \"{{ item.group }}\"\n    mode: \"{{ item.mode }}\"\n  with_items:\n    - src: strongswan.conf.j2\n      dest: \"{{ config_prefix|default('/') }}etc/strongswan.conf\"\n      owner: root\n      group: \"{{ root_group|default('root') }}\"\n      mode: \"0644\"\n    - src: ipsec.conf.j2\n      dest: \"{{ config_prefix|default('/') }}etc/ipsec.conf\"\n      owner: root\n      group: \"{{ root_group|default('root') }}\"\n      mode: \"0644\"\n    - src: ipsec.secrets.j2\n      dest: \"{{ config_prefix|default('/') }}etc/ipsec.secrets\"\n      owner: strongswan\n      group: \"{{ root_group|default('root') }}\"\n      mode: \"0600\"\n  notify:\n    - restart strongswan\n\n- name: Get loaded plugins\n  shell: >\n    find {{ config_prefix|default('/') }}etc/strongswan.d/charon/ -type f -name '*.conf' -exec basename {} \\; | cut -f1 -d.\n  register: strongswan_plugins\n\n- name: Disable unneeded plugins\n  lineinfile: dest=\"{{ config_prefix|default('/') }}etc/strongswan.d/charon/{{ item }}.conf\" regexp='.*load.*' line='load = no' state=present\n  notify:\n    - restart strongswan\n  when: item not in strongswan_enabled_plugins and item not in strongswan_additional_plugins\n  with_items: \"{{ strongswan_plugins.stdout_lines }}\"\n\n- name: Ensure that required plugins are enabled\n  lineinfile: dest=\"{{ config_prefix|default('/') }}etc/strongswan.d/charon/{{ item }}.conf\" regexp='.*load.*' line='load = yes' state=present\n  notify:\n    - restart strongswan\n  when: item in strongswan_enabled_plugins or item in strongswan_additional_plugins\n  with_items: \"{{ strongswan_plugins.stdout_lines }}\"\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "b82d8150aa2892d500314c099855afb95c970850", "filename": "roles/kubernetes-prerequisites/vars/main.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\nepel_repo_rpm: https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\nkubernetes_repo: http://yum.kubernetes.io/repos/kubernetes-el7-x86_64\nkubernetes_gpgkeys: \n  - https://packages.cloud.google.com/yum/doc/yum-key.gpg\n  - https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\nkubernetes_packages:\n  - jq\n  - sshpass\n  - bind-utils\n  - net-tools\n  - docker\n  - kubeadm\n  - kubelet\n  - kubectl\n  - kubernetes-cni\n\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "0a1bae1d22d171c7f335c78e09b5d7fb3b0458c5", "filename": "roles/ovirt-engine-remote-dwh/install-postgresql/defaults/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "ovirt_engine_dwh_db_name: ovirt_engine_history\novirt_engine_dwh_db_user: ovirt_engine_history \novirt_engine_dwh_db_password: password\novirt_engine_dwh_db_port: 5432\n"}, {"commit_sha": "b2591b9333f6e7e70f6b9d99e55356b30d7e173c", "sha": "af5d141df51b1d56fa75fec5998a146a78054bad", "filename": "handlers/main.yml", "repository": "inkatze/wildfly", "decoded_content": "---\n# handlers file for wildfly\n\n- name: restart wildfly\n  service:\n    name: wildfly\n    state: restarted\n\n- name: change standalone data mode\n  file:\n    path: '{{ wildfly_dir }}/standalone/data'\n    owner: '{{ wildfly_user }}'\n    group: '{{ wildfly_group }}'\n    mode: '0750'\n    recurse: yes\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "0c3b01281aa50906dbd2e5e32cd883c76d2d3537", "filename": "roles/0-init/tasks/first_run.yml", "repository": "iiab/iiab", "decoded_content": "- name: Create the directory structure for IIAB\n  include_tasks: fl.yml\n\n- name: Write iiab_ini.yml for the first time\n  include_tasks: iiab_ini.yml\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "12c9c0d057c8ea201d7fda481262271e818b223c", "filename": "reference-architecture/gcp/ansible/playbooks/roles/instance-groups/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: add bastion to group\n  add_host:\n    name: '{{ hostvars[prefix + \"-bastion\"].gce_name }}'\n    groups: bastion\n\n- name: add masters to requisite groups\n  add_host:\n    name: '{{ hostvars[item].gce_name }}'\n    groups: masters, etcd, nodes, cluster_hosts\n    openshift_node_labels:\n      role: master\n  with_items: '{{ groups[\"tag_\" + prefix + \"-master\"] }}'\n\n- name: add a master to the single masters group\n  add_host:\n    name: '{{ hostvars[item].gce_name }}'\n    groups: single_master\n    openshift_node_labels:\n      role: master\n  with_items: '{{ groups[\"tag_\" + prefix + \"-master\"].0 }}'\n\n- name: add infra instances to host group\n  add_host:\n    name: '{{ hostvars[item].gce_name }}'\n    groups: nodes, cluster_hosts, schedulable_nodes, infra_nodes\n    openshift_node_labels:\n      role: infra\n  with_items: '{{ groups[\"tag_\" + prefix + \"-infra-node\"] }}'\n\n- name: add app instances to host group\n  add_host:\n    name: '{{ hostvars[item].gce_name }}'\n    groups: nodes, cluster_hosts, schedulable_nodes, app_nodes\n    openshift_node_labels:\n      role: app\n  with_items: '{{ groups[\"tag_\" + prefix + \"-node\"] }}'\n\n- name: add new masters to new_masters group\n  add_host:\n    name: '{{ hostvars[item].gce_name }}'\n    groups: new_masters\n  with_items: '{{ groups[\"tag_\" + prefix + \"-master\"] }}'\n  when: hostvars[item].gce_metadata.openshift_deployed is not defined\n\n- name: add new nodes to new_nodes group (this group includes masters as well)\n  add_host:\n    name: '{{ hostvars[item].gce_name }}'\n    groups: new_nodes\n  with_items: '{{ groups[\"tag_\" + prefix] }}'\n  when: hostvars[item].gce_metadata.openshift_deployed is not defined\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "804e314d741525eb801f05a0fa4b38555b26ceb0", "filename": "roles/network/tasks/sysd-netd-debian.yml", "repository": "iiab/iiab", "decoded_content": "# sysd-netd-debian.yml\n- name: Copy the bridge script - Creates br0\n  template:\n    dest: /etc/systemd/network/IIAB-Bridge.netdev\n    src: network/systemd-br0.j2\n  when: iiab_lan_iface == \"br0\"\n\n- name: Copy the bridge script - Assigns IP address\n  template:\n    dest: /etc/systemd/network/IIAB-Bridge.network\n    src: network/systemd-br0-network.j2\n  when: iiab_lan_iface == \"br0\"\n\n- name: Copy the bridge script - Assigns br0 wired slaves\n  template:\n    dest: /etc/systemd/network/IIAB-Slave.network\n    src: network/systemd-br0-slave.j2\n  when: iiab_wired_lan_iface is defined and iiab_lan_iface == \"br0\"\n\n- name: Stopping services\n  include_tasks: down-debian.yml\n\n- name: Disable and mask systemd-networkd-wait-online\n  systemd:\n    name: systemd-networkd-wait-online\n    enabled: no\n    masked: yes\n    state: stopped\n  when: is_ubuntu_18\n\n- name: Static IP computing CIDR\n  shell: netmask {{ wan_ip }}/{{ wan_netmask }} | awk -F \"/\" '{print $2}'\n  register: CIDR\n  when: wan_ip != \"dhcp\"\n\n- name: Static IP setting CIDR\n  set_fact:\n    wan_cidr: \"{{ CIDR.stdout }}\"\n  when: wan_ip != \"dhcp\"\n\n- name: Figure out netplan file name on Ubuntu 18\n  shell: ls /etc/netplan\n  register: netplan\n  when: is_ubuntu_18\n\n# ICO will always set gui_static_wan_ip away from the default of 'unset' while\n# gui_static_wan turns dhcp on/off through wan_ip in computed_network and\n# overrides gui_static_wan_ip that is present. Changing wan_ip in local_vars\n# is a oneway street to static.\n- name: Supply static template\n  template:\n    dest:  /etc/netplan/{{ netplan.stdout }}\n    src: network/cloud-init.j2\n    backup: no\n  when: (wan_ip != \"dhcp\" or gui_static_wan_ip == \"undefined\") and is_ubuntu_18\n\n- name: Remove static WAN template\n  file:\n    state: absent\n    dest: /etc/systemd/network/IIAB-Static.network\n  when: wan_ip == \"dhcp\" and not is_ubuntu_18\n\n- name: Supply static WAN template\n  template:\n    dest: /etc/systemd/network/IIAB-Static.network\n    src: network/systemd-static-net.j2\n  when: wan_ip != \"dhcp\" and not is_ubuntu_18\n\n- name: Reload systemd\n  systemd:\n    daemon_reload: yes\n\n- name: Restart the systemd-networkd service\n  systemd:\n    name: systemd-networkd\n    enabled: yes\n    state: restarted\n  when: not nobridge is defined and not no_net_restart\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "8fba3d01f0fb5add37f75d290219a90411e30809", "filename": "ops/playbooks/roles/hpe.openports/handlers/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "---\n# handlers file for hpe.openports\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "34ade9650f383f273a793bd03e9188824e1f520e", "filename": "roles/config-docker-compose/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Make sure dependencies are met\"\n  vars: \n    docker_install: True\n  include_role:\n    name: config-docker\n  when:\n  - docker_compose_install|default(False)\n\n- name: \"Install, configure and enable Docker-compose\"\n  import_tasks: docker-compose.yml\n  when:\n  - docker_compose_install|default(False)\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "706b1b0a4856ff36756d72c95c82d81f077c8702", "filename": "reference-architecture/vmware-ansible/playbooks/roles/haproxy-server-config/templates/haproxy.cfg.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "global\n    log         127.0.0.1 local2\n\n    chroot      /var/lib/haproxy\n    pidfile     /var/run/haproxy.pid\n    maxconn     4000\n    user        haproxy\n    group       haproxy\n    daemon\n\n    # turn on stats unix socket\n    stats socket /var/lib/haproxy/stats\n\ndefaults\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n#     option forwardfor       except 127.0.0.0/8\n    option                  redispatch\n    retries                 3\n    timeout http-request    10s\n    timeout queue           1m\n    timeout connect         10s\n    timeout client          1m\n    timeout server          1m\n    timeout http-keep-alive 10s\n    timeout check           10s\n    maxconn                 3000\nlisten  stats :9000\n        stats enable\n        stats realm Haproxy\\ Statistics\n        stats uri /haproxy_stats\n        stats auth admin:password\n        stats refresh 30\n        mode http\n\nfrontend  main *:80\n    default_backend             router80\n\nbackend router80\n    balance source\n    mode tcp\n    {% for host in groups['infra'] %}\n    server {{ hostvars[host]['ansible_fqdn'] }} {{ hostvars[host]['ansible_fqdn'] }}:80 check\n    {% endfor %}\n\nfrontend  main *:443\n    default_backend             router443\n\nbackend router443\n    balance source\n    mode tcp\n    {% for host in groups['infra'] %}\n    server {{ hostvars[host]['ansible_fqdn'] }} {{ hostvars[host]['ansible_fqdn'] }}:443 check\n    {% endfor %}\n\nfrontend  main *:{{ console_port }}\n    default_backend             mgmt{{ console_port }}\n\nbackend mgmt{{ console_port }}\n    balance source\n    mode tcp\n    {% for host in groups['master'] %}\n    server {{ hostvars[host]['ansible_fqdn'] }} {{ hostvars[host]['ansible_fqdn'] }}:{{ console_port }} check\n    {% endfor %}\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "9612b85f933ed4841ed420f8451bbde69061e45f", "filename": "ops/playbooks/install_haproxy.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- name: Install Load Balancers\n  hosts: lbs\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - includes/internal_vars.yml\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n    - set_fact: \n        open_ports:\n          - \"{{ internal_dtr_lb_ports }}\"\n      when: inventory_hostname in groups.dtr_lb\n \n    - set_fact:\n        open_ports: \"{{ internal_ucp_lb_ports }}\"\n      when: inventory_hostname in groups.ucp_lb\n \n    - set_fact:\n        open_ports: \"{{ internal_worker_lb_ports }}\"\n      when: inventory_hostname in groups.worker_lb\n \n    - name: Open  ports  in the firewall\n      firewalld:\n        port: \"{{ item }}\"\n        immediate: true\n        permanent: true\n        state: enabled\n      with_items: \"{{ open_ports }}\"\n\n    - name: Install Required Pkgs for seboolean module\n      yum:\n        name: \"{{ packages }}\"\n        state: latest\n      vars:\n        packages:\n        - libsemanage-python\n        - libselinux-python\n\n    - name: Enable HAPROXY to open non standard ports\n      seboolean:\n        name: haproxy_connect_any\n        state: yes\n        persistent: yes\n\n    - name: Install haproxy\n      yum:\n        name: haproxy\n        state: latest\n\n    - name: Update haproxy.cfg on Worker Load Balancer\n      template:\n        src: ../templates/haproxy.cfg.worker.j2\n        dest: /etc/haproxy/haproxy.cfg\n        owner: root\n        group: root\n        mode: 0644        \n      when: inventory_hostname in groups.worker_lb\n      notify: Enable and start haproxy service\n\n    - name: Update haproxy.cfg on UCP Load Balancer\n      template:\n        src: ../templates/haproxy.cfg.ucp.j2\n        dest: /etc/haproxy/haproxy.cfg\n        owner: root\n        group: root\n        mode: 0644\n      when: inventory_hostname in groups.ucp_lb\n      notify: Enable and start haproxy service\n\n    - name: Update haproxy.cfg on DTR Load Balancer\n      template:\n        src: ../templates/haproxy.cfg.dtr.j2\n        dest: /etc/haproxy/haproxy.cfg\n        owner: root\n        group: root\n        mode: 0644\n      when: inventory_hostname in groups.dtr_lb\n      notify: Enable and start haproxy service\n\n  handlers:\n    - name: Enable and start haproxy service\n      systemd:\n        name: haproxy\n        enabled: yes\n        state: restarted\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "1f181cd85f83917d9aaa2cb04210b32923079bea", "filename": "roles/ovirt-engine-remote-db/defaults/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_engine_remote_db_port: 5432\novirt_engine_remote_db_listen_address: '*'\novirt_engine_db_name: 'engine'\novirt_engine_db_user: 'engine'\novirt_engine_db_password: 'AqbXg4dpkbcVRZwPbY8WOR'\n\novirt_engine_remote_db: False\novirt_engine_dwh_remote_db: False\n\novirt_engine_dwh_db_name: 'ovirt_engine_history'\novirt_engine_dwh_db_user: 'ovirt_engine_history'\novirt_engine_dwh_db_password: '37xmBKECANQGm0z3SfylMp'\n\novirt_engine_remote_db_access:\n  -\n    type: host\n    address: 0.0.0.0/0\n    method: md5\n\novirt_engine_remote_db_vacuum: true\novirt_engine_remote_db_vacuum_config:\n  'autovacuum_vacuum_scale_factor': 0.01\n  'autovacuum_analyze_scale_factor': 0.075\n  'autovacuum_max_workers': 6\n  'maintenance_work_mem': 65536\n  'max_connections': 150\n  'lc_messages': 'en_US.UTF-8'\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "32b0a27c4759c7e70a53fd579117d00c06bfb030", "filename": "ops/playbooks/install_nfs_server.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- name: Install NFS Server\n  hosts: nfs\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  roles:\n    - role: hpe.openports\n      hpe_openports_ports:\n        - 2049/tcp\n        - 2049/udp\n        - 111/tcp\n        - 111/udp\n        - 54302/tcp\n        - 54302/udp\n        - 20048/tcp\n        - 20248/udp\n        - 46666/tcp\n        - 46666/udp\n        - 42955/tcp\n        - 42955/udp\n        - 875/tcp\n        - 875/udp\n        \n  tasks:\n\n    - name: Check for partitions on disk\n      parted:\n        state: info\n        device: \"{{ disk2 }}\"\n        number: 1\n      register: DiskInfo\n\n    - set_fact:\n        partPresent: \"{{ DiskInfo.partitions[0] is defined }}\"\n\n    - name: Create partition on second disk\n      parted:\n        device: \"{{ disk2 }}\"\n        number: 1\n        state: present    \n      when: partPresent == false\n\n    - name: Create filesystem\n      filesystem:\n        fstype: xfs\n        dev: \"{{ disk2_part }}\"\n      when: partPresent == false\n\n    - name: Create images folder\n      file:\n        path: \"{{ images_folder }}\"\n        state: directory\n        mode: 0777\n      when: partPresent == false\n\n    - name: Mount filesystem\n      mount:\n        path: \"{{ images_folder }}\"\n        src: \"{{ disk2_part }}\"\n        fstype: xfs\n        state: mounted\n\n#\n# NFS Share for K8S PVs\n#\n    - name: Check for partitions on 3rd disk\n      parted:\n        state: info\n        device: /dev/sdc\n        number: 1\n      register: Disk3Info\n\n    - set_fact:\n        partPresent: \"{{ Disk3Info.partitions[0] is defined }}\"\n\n    - name: Create partition on 3rd disk\n      parted:\n        device: /dev/sdc\n        number: 1\n        state: present\n      when: partPresent == false\n\n    - name: Create filesystem on 3rd disk\n      filesystem:\n        fstype: xfs\n        dev: '/dev/sdc1'\n      when: partPresent == false\n\n    - name: Create Share for K8S Persistent Volumes\n      file:\n        path: \"{{ nfs_provisioner_server_share }}\"\n        state: directory\n        mode: 0777\n      when: partPresent == false\n\n    - name: Mount filesystem\n      mount:\n        path: \"{{ nfs_provisioner_server_share }}\"\n        src: '/dev/sdc1'\n        fstype: xfs\n        state: mounted\n\n    - name: Install NFS server\n      yum:\n        name: \"{{ packages }}\"\n        state: latest\n      vars:\n        packages:\n        - rpcbind\n        - nfs-utils\n\n    - name: Enable and start NFS services on server\n      systemd:\n        name: \"{{ item }}\"\n        enabled: yes\n        state: started\n      with_items:\n        - rpcbind\n        - nfs-server\n        - nfs-lock\n        - nfs-idmap\n\n    - name: Modify exports file on NFS server\n      template: src=../templates/exports.j2 dest=/etc/exports\n      notify: RefreshExportFS\n\n  handlers: \n\n    - name: RefreshExportFS\n      command: exportfs -a\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "d13ee31c1d9dcb237d26fb1bc77efbde78e6e57d", "filename": "roles/wireguard/handlers/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: restart wireguard\n  service:\n    name: \"{{ service_name }}\"\n    state: restarted\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "6897ad6f6cd9848a7af5f0385376bd7654918baa", "filename": "playbooks/provider/lago/LagoInitFile.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "nat-settings: &nat-settings\n    type: nat\n    dhcp:\n      start: 100\n      end: 254\n    management: False\n\nvm-common-settings: &vm-common-settings\n    root-password: 123456\n    service_provider: systemd\n    artifacts:\n      - /var/log\n\nhost-settings: &nodes-settings\n    <<: *vm-common-settings\n    groups: [nodes]\n    memory: 2047\n    nics:\n      - net: lago-management-network\n    disks:\n      - template_name: el7.4-base\n        type: template\n        name: root\n        dev: sda\n        format: qcow2\n      - comment: Docker Storage\n        size: 10G\n        type: empty\n        name: docker_storage\n        dev: sdb\n        format: raw\n      - comment: /var/lib/docker\n        size: 10G\n        type: empty\n        name: docker_lib\n        dev: sdc\n        format: raw\n      - comment: Gluster Storage\n        size: 50G\n        type: empty\n        name: gluster_storage\n        dev: sdd\n        format: qcow2\ndomains:\n  lago-master:\n    <<: *vm-common-settings\n    groups: [masters, nodes, etcd, nfs]\n    memory: 4096\n    nics:\n      - net: lago-management-network\n    disks:\n      - template_name: el7.4-base\n        type: template\n        name: root\n        dev: sda\n        format: qcow2\n      - comment: Docker Storage\n        size: 10G\n        type: empty\n        name: docker_storage\n        dev: sdb\n        format: raw\n      - comment: /var/lib/docker\n        size: 10G\n        type: empty\n        name: docker_lib\n        dev: sdc\n        format: raw\n      - comment: Gluster Storage\n        size: 50G\n        type: empty\n        name: gluster_storage\n        dev: sdd\n        format: qcow2\n      - comment: Main NFS device\n        size: 101G\n        type: empty\n        name: nfs\n        dev: sde\n        format: raw\n\n  lago-node0:\n    <<: *nodes-settings\n\n  lago-node1:\n    <<: *nodes-settings\n\nnets:\n  lago-management-network:\n    <<: *nat-settings\n    management: true\n    dns_domain_name: lago.local\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6fa9142a08e5378b433c4b37f153ee69167140d9", "filename": "playbooks/aws/openshift-cluster/scaleup.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n\n- hosts: localhost\n  gather_facts: no\n  connection: local\n  become: no\n  vars_files:\n  - vars.yml\n  tasks:\n  - name: Evaluate oo_hosts_to_update\n    add_host:\n      name: \"{{ item }}\"\n      groups: oo_hosts_to_update\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    with_items: \"{{ groups.nodes_to_add }}\"\n\n- include: ../../common/openshift-cluster/update_repos_and_packages.yml\n\n- include: ../../common/openshift-cluster/scaleup.yml\n  vars_files:\n  - ../../aws/openshift-cluster/vars.yml\n  - ../../aws/openshift-cluster/cluster_hosts.yml\n  vars:\n    g_new_node_hosts: \"{{ groups.nodes_to_add }}\"\n    g_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n    g_sudo: \"{{ deployment_vars[deployment_type].become }}\"\n    g_nodeonmaster: true\n    openshift_cluster_id: \"{{ cluster_id }}\"\n    openshift_debug_level: \"{{ debug_level }}\"\n    openshift_deployment_type: \"{{ deployment_type }}\"\n    openshift_public_hostname: \"{{ ec2_ip_address }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "040d3c846b6d4bcba69faf7bc308e10a027c67f4", "filename": "roles/config-satellite/tasks/repos.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Enable repositories (with release_version)\"\n  command: > \n    hammer\n      -u \"{{ satellite_username }}\"\n      -p \"{{ satellite_password }}\"\n      repository-set enable \n      --organization \"{{ satellite_organization }}\"\n      --product \"{{ item.0.product }}\"\n      --name \"{{ item.0.name }}\"\n      --releasever \"{{ item.1 }}\"\n      --basearch \"{{ item.0.base_arch }}\"\n  with_subelements:\n  - \"{{ satellite_repositories }}\"\n  - release_version\n  register: enable_repos\n  failed_when:\n  - enable_repos.rc != 70\n  - enable_repos.rc != 0\n\n- name: \"Enable repositories (without release_version)\"\n  command: > \n    hammer\n      -u \"{{ satellite_username }}\"\n      -p \"{{ satellite_password }}\"\n      repository-set enable \n      --organization \"{{ satellite_organization }}\"\n      --product \"{{ item.product }}\"\n      --name \"{{ item.name }}\"\n      --basearch \"{{ item.base_arch }}\"\n  with_items:\n  - \"{{ satellite_repositories }}\"\n  register: enable_repos\n  failed_when:\n  - enable_repos.rc != 70\n  - enable_repos.rc != 0\n  when: \n  - item.release_version|length == 0\n\n- name: \"Get Red Hat repo ids\"\n  shell: hammer -u \"{{ satellite_username }}\" -p \"{{ satellite_password }}\" repository list --organization \"{{ satellite_organization }}\" | grep \"Red Hat\" | awk '{print $1}'\n  register: repoids\n\n# This will take awhile, so fire-and-forget\n- name: \"Synchronize Red Hat repos (fire and forget)\"\n  command: >\n    hammer\n      -u \"{{ satellite_username }}\"\n      -p \"{{ satellite_password }}\"\n      repository synchronize\n      --id \"{{ item }}\"\n      --organization \"{{ satellite_organization }}\"\n  async: 1000\n  poll: 0\n  with_items:\n  - \"{{ repoids.stdout.split('\\n') }}\"\n\n- name: \"Get tomorrow (start-date for sync plan)\"\n  shell: echo \"$(date -d tomorrow -I) 05:00:00\"\n  register: sync_date\n\n- name: \"Create sync plan\"\n  command: >\n    hammer\n      -u \"{{ satellite_username }}\"\n      -p \"{{ satellite_password }}\"\n      sync-plan create\n      --name \"{{ satellite_sync_plan }}\"\n      --organization \"{{ satellite_organization }}\"\n      --enabled true\n      --interval daily\n      --sync-date \"{{ sync_date.stdout }}\"\n  register: chk_plan\n  failed_when:\n  - chk_plan.rc != 65\n  - chk_plan.rc != 0\n\n- name: \"Add products to sync plan\"\n  command: >\n    hammer\n      -u \"{{ satellite_username }}\"\n      -p \"{{ satellite_password }}\"\n      product set-sync-plan\n      --name \"{{ item.product }}\"\n      --sync-plan \"{{ satellite_sync_plan }}\"\n      --organization \"{{ satellite_organization }}\"\n  with_items: \n  - \"{{ satellite_repositories }}\"\n\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "f668c93bbfb383183990ea3e786145eedded78d9", "filename": "roles/ipaserver/vars/Ubuntu.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# vars/Ubuntu.yml\nipaserver_packages: [ \"freeipa-server\" ]\nipaserver_packages_dns: [ \"freeipa-server-dns\" ]\nipaserver_packages_adtrust: [ ]\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "82c1561d4cf2fe8d1587646a04d804c68649d8f7", "filename": "archive/roles/openstack-create/tasks/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: Validate OpenStack Variables\n  fail: msg=\"Required OpenStack Variables not defined!\"\n  when: \n  - image_name is not defined or security_groups is not defined or key_name is not defined or image_name is not defined or flavor_name is not defined \n\n- name: \"Search for valid OpenStack Flavor\"\n  shell: \"nova flavor-list | awk \\\"/{{flavor_name }}/\\\"'{print $2}'\"\n  register: flavor_query\n\n- name: Validate OpenStack Image\n  fail: msg=\"Unable to determine a unique OpenStack Image\"\n  when: flavor_query.stdout.split()|length  != 1\n\n- name: Setting OpenStack Image ID Fact\n  set_fact:\n    flavor_id: \"{{ flavor_query.stdout }}\"\n\n- name: \"Initialize Neutron fact\"\n  set_fact:\n    neutron_in_use: 'no'\n\n- name: \"Query Neutron services\"\n  command: neutron agent-list\n  register: neutron\n  ignore_errors: true\n\n- name: \"Check for Neutron services - a failure assumes Legacy Networking (Nova Network)\"\n  set_fact:\n    neutron_in_use: 'yes'\n  when: neutron.rc == 0\n\n- include: security_groups.yml\n\n- name: \"Provision OpenStack {{ type }}\"\n  os_server:\n    name: \"{{ type }}{{ item }}.{{ full_dns_domain }}\"\n    state: present\n    image: \"{{ image_name }}\"\n    flavor: \"{{ flavor_id }}\"\n    key_name: \"{{ key_name }}\"\n    security_groups: \"{{ security_groups_list }}\"\n    auto_ip: \"{{ neutron_in_use }}\"\n    timeout: 200\n  with_sequence: \"start=1 end={{ node_count | default(1) }}\"\n  register: \"openstack_create\"\n\n- add_host:\n    hostname: \"{{ item.openstack.name }}\"\n    ansible_ssh_host: \"{{ item.openstack.public_v4 }}\"\n    dns_public_ip: \"{{ item.openstack.public_v4 }}\"\n    dns_private_ip: \"{{ item.openstack.private_v4 }}\"\n    ansible_ssh_user: root\n    id: \"{{ item.id }}\"\n    private_ip: \"{{ item.openstack.private_v4 }}\"\n    groups: \"{{ register_host_group }}\"\n  with_items: \"{{ openstack_create.results }}\"\n\n- name: \"Wait for {{ type }} to be available\"\n  wait_for: port=22 host={{ item.openstack.public_v4 }}\n  with_items: \"{{ openstack_create.results }}\"\n\n# Create Volumes\n- include: create-volume.yml\n  when: volume_size is defined and disk_volume is defined\n  vars:\n    instance_type: \"{{ type }}\"\n    openstack_machines: \"{{ openstack_create }}\"\n    storage_volume_size: \"{{ volume_size }}\"\n    storage_disk_volume: \"{{ disk_volume }}\"\n"}, {"commit_sha": "3c8d04f3e0875a9baf1f1282f6665b2e7d6871a8", "sha": "0f85032c3a0da029664a5e3865968a47708c90be", "filename": "tasks/ssh.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\n- name: Update SSH configuration to be more secure.\n  lineinfile:\n    dest: \"{{ security_ssh_config_path }}\"\n    regexp: \"{{ item.regexp }}\"\n    line: \"{{ item.line }}\"\n    state: present\n  with_items:\n    - regexp: \"^PasswordAuthentication\"\n      line: \"PasswordAuthentication {{ security_ssh_password_authentication }}\"\n    - regexp: \"^PermitRootLogin\"\n      line: \"PermitRootLogin {{ security_ssh_permit_root_login }}\"\n    - regexp: \"^Port\"\n      line: \"Port {{ security_ssh_port }}\"\n    - regexp: \"^UseDNS\"\n      line: \"UseDNS {{ security_ssh_usedns }}\"\n    - regexp: \"^PermitEmptyPasswords\"\n      line: \"PermitEmptyPasswords {{ security_ssh_permit_empty_password }}\"\n    - regexp: \"^ChallengeResponseAuthentication\"\n      line: \"ChallengeResponseAuthentication {{ security_ssh_challenge_response_auth }}\"\n    - regexp: \"^GSSAPIAuthentication\"\n      line: \"GSSAPIAuthentication {{ security_ssh_gss_api_authentication }}\"\n    - regexp: \"^X11Forwarding\"\n      line: \"X11Forwarding {{ security_ssh_x11_forwarding }}\"\n  notify: restart ssh\n\n- name: Add configured user accounts to passwordless sudoers.\n  lineinfile:\n    dest: /etc/sudoers\n    regexp: '^{{ item }}'\n    line: '{{ item }} ALL=(ALL) NOPASSWD: ALL'\n    state: present\n    validate: 'visudo -cf %s'\n  with_items: \"{{ security_sudoers_passwordless }}\"\n  when: security_sudoers_passwordless | length > 0\n\n- name: Add configured user accounts to passworded sudoers.\n  lineinfile:\n    dest: /etc/sudoers\n    regexp: '^{{ item }}'\n    line: '{{ item }} ALL=(ALL) ALL'\n    state: present\n    validate: 'visudo -cf %s'\n  with_items: \"{{ security_sudoers_passworded }}\"\n  when: security_sudoers_passworded | length > 0\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "be721cc5ca8821a905f6e80f5ba39c6f090cff52", "filename": "roles/config-idm-server/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: idm-server\n  become: yes\n  roles:\n  - config-idm-server\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "55cb95e3eb006a046d3c2c96e9a300469206f8e4", "filename": "roles/manage-aws-infra/tasks/remove_infra.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "# Remove OCP Cluster instances\n---\n- name: Register instances to be terminated\n  ec2_remote_facts:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region: \"{{ aws_region }}\"\n    filters:\n      \"tag:env_id\": \"{{ env_id }}\"\n  register: destroy_instances\n\n- name: Ensure ec2 are stopped and terminate protection is disabled\n  ec2:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region:  \"{{ aws_region }}\"\n    termination_protection: no\n    state: stopped\n    instance_ids: \"{{ item.id }}\"\n    wait: yes\n  with_items:\n    - \"{{ destroy_instances.instances }}\"\n\n- name: Ensure ec2 are terminated\n  ec2:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region:  \"{{ aws_region }}\"\n    state: \"{{ operation }}\"\n    instance_ids: \"{{ item.id }}\"\n    wait: yes\n  with_items:\n    - \"{{ destroy_instances.instances }}\"\n\n- name: Remove env_id tag to avoid conflicts with same env_id clusters\n  ec2_tag:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    region:  \"{{ aws_region }}\"\n    resource: \"{{ item.id }}\"\n    tags:\n      env_id: terminated\n  with_items:\n    - \"{{ destroy_instances.instances }}\"\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "2e2099fd0bd60e26a10e26cc7f330675add860a5", "filename": "tasks/setup-repository-CentOS.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Ensure packages are installed for repository setup\n  become: true\n  package:\n    name: \"{{ item }}\"\n    state: present\n  loop: \"{{ docker_repository_related_packages[_docker_os_dist] }}\"\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when:\n    - docker_network_access | bool\n    - _docker_os_dist == \"CentOS\" or _docker_os_dist == \"RedHat\"\n\n- name: Determine channels to be enabled and/or disabled\n  set_fact:\n    _docker_disable_channels: \"{{ docker_channels | difference(_docker_merged_channels) }}\"\n    _docker_enable_channels: \"{{ docker_channels | intersect(_docker_merged_channels) }}\"\n  vars:\n    _docker_mandatory_channel: []\n    _docker_merged_channels: \"{{ _docker_mandatory_channel }} + [ '{{ docker_channel }}' ]\"\n\n- name: Add Docker CE repository (Fedora/CentOS/RedHat)\n  become: true\n  get_url:\n    url: \"{{ docker_repository_url[_docker_os_dist] }}\"\n    dest: /etc/yum.repos.d/docker-ce.repo\n    mode: 0644\n  register: _docker_repo\n  until: _docker_repo is succeeded\n  when:\n    - docker_network_access | bool\n    - _docker_os_dist == \"CentOS\" or\n      _docker_os_dist == \"Fedora\" or\n      _docker_os_dist == \"RedHat\"\n\n- name: Disable Docker CE repository channels (Fedora/CentOS/RedHat)\n  become: true\n  shell: \"{{ docker_cmd_enable_disable_edge_repo[_docker_os_dist] }}\"\n  args:\n    warn: false\n  loop: \"{{ _docker_disable_channels }}\"\n  changed_when: false\n  vars:\n    _item_enabled: false\n  when: _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"Fedora\" or\n        _docker_os_dist == \"RedHat\"\n  tags:\n    - skip_ansible_lint\n\n- name: Enable Docker CE repository channels (Fedora/CentOS/RedHat)\n  become: true\n  shell: \"{{ docker_cmd_enable_disable_edge_repo[_docker_os_dist] }}\"\n  args:\n    warn: false\n  loop: \"{{ _docker_enable_channels }}\"\n  changed_when: false\n  vars:\n    _item_enabled: true\n  when: _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"Fedora\" or\n        _docker_os_dist == \"RedHat\"\n  tags:\n    - skip_ansible_lint"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "a9ed97c6222b46c0e10e06a3e9d2cc3c5a602341", "filename": "roles/osm/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install OSM required packages (debuntu)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - gcc\n    - python-dev\n    - liblzma-dev\n    - libapache2-mod-wsgi\n    - libapache2-mod-xsendfile\n  when: is_debuntu\n\n- name: Install OSM required packages (not debuntu)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - python-pip\n    - gcc\n    - python-devel\n    - xz-devel\n    - mod_wsgi\n    - mod_xsendfile\n  when: not is_debuntu\n\n# OSM wants a specific version do that first\n- name: Install Whoosh 2.6 (debuntu)\n  pip:\n    name: whoosh\n    virtualenv: \"{{ osm_venv }}\"\n    virtualenv_site_packages: no\n    version: 2.6\n    extra_args: \"--no-cache-dir\"\n  when: internet_available and is_debuntu\n\n- name: Install Flask 0.12X (debuntu)\n  pip:\n    name: Flask\n    virtualenv: \"{{ osm_venv }}\"\n    virtualenv_site_packages: no\n    version: 0.12.4\n    extra_args: \"--no-cache-dir\"\n  when: internet_available and is_debuntu\n\n- name: Install OSM with dependencies (debuntu)\n  pip:\n    name: \"{{ item }}\"\n    virtualenv: \"{{ osm_venv }}\"\n    virtualenv_site_packages: no\n    extra_args: \"--no-cache-dir\"\n  with_items:\n    - MarkupSafe\n    - pytz\n    - Internet-in-a-Box\n  when: internet_available and is_debuntu\n\n# OSM wants a specific version do that first\n- name: Install Whoosh 2.6 (not debuntu)\n  pip:\n    name: whoosh\n    virtualenv: \"{{ osm_venv }}\"\n    virtualenv_site_packages: no\n    version: 2.6\n#       extra_args=\"--no-cache-dir\"\n  when: internet_available and not is_debuntu\n\n- name: Install Flask 0.12X (not debuntu)\n  pip:\n    name: Flask\n    virtualenv: \"{{ osm_venv }}\"\n    virtualenv_site_packages: no\n    version: 0.12.4\n#       extra_args=\"--no-cache-dir\"\n  when: internet_available and not is_debuntu\n\n- name: Install OSM with dependencies (not debuntu)\n  pip:\n    name: \"{{ item }}\"\n    virtualenv: \"{{ osm_venv }}\"\n    virtualenv_site_packages: no\n#       extra_args=\"--no-cache-dir\"\n  with_items:\n    - MarkupSafe\n    - pytz\n    - Internet-in-a-Box\n  when: internet_available and not is_debuntu\n\n- name: Set osm_path (redhat)\n  set_fact:\n    osm_path: \"{{ osm_venv }}/{{ python_path }}/iiab\"\n  when: osm_enabled and is_redhat\n\n- name: Set osm_path (debuntu)\n  set_fact:\n    osm_path: \"{{ osm_venv }}/lib/python2.7/site-packages/iiab\"\n  when: osm_enabled and is_debuntu\n\n- name: Point wsgi to virtual environment (all OS's)\n  lineinfile:\n    dest: \"{{ osm_venv }}/bin/iiab.wsgi\"\n    regexp: \"path_to_virtualenv = None\"\n    line: \"path_to_virtualenv = '/usr/local/osm'\"\n    state: present\n\n- name: Copy OSM config file (all OS's)\n  template:\n    src: osm.conf.j2\n    dest: \"/etc/{{ apache_config_dir }}/osm.conf\"\n    owner: root\n    group: root\n    mode: 0644\n    backup: no\n  when: osm_enabled\n\n- name: Create a link from sites-enabled to sites-available (debuntu)\n  file:\n    src: \"/etc/{{ apache_config_dir }}/osm.conf\"\n    dest: /etc/apache2/sites-enabled/osm.conf\n    state: link\n  when: osm_enabled and is_debuntu\n\n- name: Remove the link from sites-enabled to sites-available (debuntu)\n  file:\n    dest: /etc/apache2/sites-enabled/osm.conf\n    state: absent\n  when: not osm_enabled and is_debuntu\n\n- name: Remove the osm.conf (redhat)\n  file:\n    dest: \"/{{ apache_config_dir }}/osm.conf\"\n    state: absent\n  when: not osm_enabled and is_redhat\n\n- name: Remove link to cgi (all OS's)\n  file:\n    dest: \"{{ doc_root }}/osm.wsgi\"\n    state: absent\n  when: not osm_enabled\n\n- name: Create link to cgi (all OS's)\n  file:\n    src: \"{{ osm_venv }}/bin/iiab.wsgi\"\n    dest: \"{{ doc_root }}/osm.wsgi\"\n    owner: root\n    group: root\n    state: link\n  when: osm_enabled\n\n- name: Create the knowledge data set folders\n  file:\n    path: /library/knowledge/modules\n    state: directory\n    owner: \"{{ apache_user }}\"\n    group: \"{{ apache_user }}\"\n\n# the following was brought into OSM playbook from iiab-factory osm-fix script\n- name: Copy the files\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n  with_items:\n    - { src: 'defaults.ini', dest: \"{{ osm_path }}/\" }\n    - { src: 'etc.iiab.conf', dest: '/etc/iiab.conf' }\n    - { src: 'map_search.py', dest: \"{{ osm_path }}/map_search.py\" }\n    - { src: 'map.html', dest: \"{{ osm_path }}/static/map.html\" }\n    - { src: 'l.control.geosearch.js', dest: \"{{ osm_path }}/static/lib/leaflet/geosearch/l.control.geosearch.js\" }\n    - { src: \"{{ osm_path }}/static/map.html\", dest: \"{{ osm_path }}/static/index.html\" }\n  when: osm_enabled\n\n- name: Restart httpd service\n  service:\n    name: \"{{ apache_service }}\"\n    state: restarted\n\n- name: Add 'osm' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: osm\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n  - option: name\n    value: OpenStreetMap\n  - option: description\n    value: '\"OpenStreetMap offers beautiful maps of the entire planet, continually created & updated by volunteers (much in the same way as Wikipedia) but for maps.\"'\n    # value: '\"The Internet-in-a-Box is a small, inexpensive device which provides essential Internet resources without any Internet connection. It provides a local copy of half a terabyte of the world\u2019s Free information.\"'\n  - option: path\n    value: /osm\n  - option: enabled\n    value: \"{{ osm_enabled }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e7f6365faccef098d4f56baf10381a433126c803", "filename": "roles/config-selinux/tests/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\ntarget_state: 'enforcing'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8f142579065ef690e154e5a47a5d72934d548982", "filename": "roles/virt-install/tasks/virt-install.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Populate values for virt install run\"\n  set_fact:\n    virtinstall_connect: \"{{ hostvars[vm]['libvirt_connect'] | default(default_connect) }}\"\n    virtinstall_virt_type: \"{{ hostvars[vm]['libvirt_virt_type'] | default(default_virt_type) }}\"\n    virtinstall_name: \"{{ hostvars[vm]['libvirt_name'] | default(default_name) }}\"\n    virtinstall_title: \"{{ hostvars[vm]['libvirt_title'] | default(default_title) }}\"\n    virtinstall_description: \"{{ hostvars[vm]['libvirt_description'] | default(default_description) }}\"\n    virtinstall_memory: \"{{ hostvars[vm]['libvirt_memory'] | default(default_memory) }}\"\n    virtinstall_vcpus: \"{{ hostvars[vm]['libvirt_vcpus'] | default(default_vcpus) }}\"\n    virtinstall_disk_size: \"{{ hostvars[vm]['libvirt_disk_size'] | default(default_disk_size) }}\"\n    virtinstall_disk_pool: \"{{ hostvars[vm]['libvirt_disk_pool'] | default(default_disk_pool) }}\"\n    virtinstall_os_variant: \"{{ hostvars[vm]['libvirt_os_variant'] | default(default_os_variant) }}\"\n    virtinstall_iso: \"{{ hostvars[vm]['libvirt_iso'] | default(default_iso) }}\"\n    virtinstall_ksfile: \"{{ hostvars[vm]['libvirt_ksfile'] | default(default_ksfile) }}\"\n    virtinstall_authorized_keys: \"{{ hostvars[vm]['libvirt_authorized_keys'] | default(default_authorized_keys) }}\"\n    virtinstall_http_host: \"{{ hostvars[vm]['libvirt_http_host'] | default(default_http_host) }}\"\n    virtinstall_network_hostif: \"{{ hostvars[vm]['libvirt_network_hostif'] | default(default_network_hostif) }}\"\n    virtinstall_network_model: \"{{ hostvars[vm]['libvirt_network_model'] | default(default_network_model) }}\"\n    virtinstall_network_type: \"{{ hostvars[vm]['libvirt_network_type'] | default(default_network_type) }}\"\n\n- name: \"Make KS file available on the target host\"\n  copy: \n    src: \"{{ virtinstall_ksfile }}\"\n    dest: \"/tmp/{{ virtinstall_ksfile | basename }}\"\n\n- name: \"Make the authorized_keys file available on the target host\"\n  copy: \n    src: \"{{ virtinstall_authorized_keys }}\"\n    dest: \"{{ default_http_dir }}/{{ virtinstall_authorized_keys | basename }}\"\n  notify: 'Remove authorized_keys'\n\n- name: \"Make a mount point for install purpose\"\n  tempfile:\n    state: directory\n    prefix: install\n    path: \"{{ default_http_dir }}\"\n  register: http_mount\n  when: \n  - mounted_iso[virtinstall_iso] is not defined\n\n- name: \"Mount ISO to serv it up with http\"\n  mount:\n    src: \"{{ virtinstall_iso }}\"\n    path: \"{{ http_mount.path }}\"\n    opts: loop\n    fstype: iso9660\n    state: mounted\n  notify: 'Unmount install ISO'\n  when:\n  - mounted_iso[virtinstall_iso] is not defined\n\n- name: 'Track mounted iso'\n  set_fact: \n    mounted_iso: \"{{ mounted_iso | combine({ virtinstall_iso : http_mount.path }) }}\"\n  when:\n  - mounted_iso[virtinstall_iso] is not defined\n\n- name: \"Set Fact for VM command\"\n  set_fact:\n    virt_install_commands: \"{{ virt_install_commands | default([]) + [ ('virt-install' + ' --connect ' + virtinstall_connect + ' --virt-type ' + virtinstall_virt_type + ' --name ' + virtinstall_name + ' --metadata \\\"title=' + virtinstall_title + ',description=' + virtinstall_description + ',name=' + virtinstall_name + '\\\"' + ' --network \\\"type='+ virtinstall_network_type + ',source=' + virtinstall_network_hostif + ',source_mode=bridge,model=' + virtinstall_network_model + '\\\"' + ' --memory ' + virtinstall_memory + ' --vcpus ' + virtinstall_vcpus + ' --disk pool=' + virtinstall_disk_pool + ',size=' + virtinstall_disk_size + ',bus=virtio' + ' --os-variant ' + virtinstall_os_variant + ' --location http://' + virtinstall_http_host + '/' + mounted_iso[virtinstall_iso] | basename + ' --initrd-inject=/tmp/' + virtinstall_ksfile | basename + ' --extra-args \\\"inst.repo=http://' + virtinstall_http_host + '/' + mounted_iso[virtinstall_iso] | basename + ' inst.ks=file:/' + virtinstall_ksfile | basename + '\\\"' + ' --graphics spice ' + ' --video qxl ' + ' --channel spicevmc ' + ' --autostart ') ] }}\"\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "e5832ced41c9cdf9df3777abb8143d133eb920fe", "filename": "roles/storage-demo-nodeconfig/defaults/main.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "action: \"provision\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "789afbf312d6e20ba4b7867ff3e84c997e531ed7", "filename": "roles/ajenti/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install python-pip package\n  package: name=python-pip\n           state=present\n\n- name: Install required libraries\n  package: name={{ item.pkg }}\n           state=present\n  with_items:\n    - pkg: python-imaging\n    - pkg: python-devel\n    - pkg: libxslt-devel\n    - pkg: pyOpenSSL\n    - pkg: python-daemon\n    - pkg: gcc\n\n- name: Install ajenti from our repo\n  pip: name=\"{{ iiab_download_url }}\"/ajenti-0.99.34-patched5.tar.gz\n  when: internet_available \n\n#  notify:\n#    - restart ajenti service\n\n- name: download python-catcher\n  pip: name=python-catcher version=0.1.3\n  when: internet_available \n\n- name: change default port\n  lineinfile: backup=yes\n              dest=/etc/ajenti/config.json\n              state=present\n              backrefs=yes\n              regexp='\"port\":\\s*[0-9]{1,5}'\n              line='\"port\":9990'\n\n- name: exe permission to ajenti\n  file: path=/etc/rc.d/init.d/ajenti\n        mode=0744\n        state=file\n\n- include_tasks: ajenti-wondershaper.yml\n  when: 'iiab_lan_iface != \"\"'\n\n# handler doesn't fire\n- name: restart ajenti service\n  service: name=ajenti\n           enabled=yes\n           state=restarted\n  when: ajenti_enabled\n\n- name: Add ajenti to service list\n  ini_file: dest='{{ service_filelist }}'\n            section=ajenti\n            option='{{ item.option }}'\n            value='{{ item.value }}'\n  with_items:\n    - option: name\n      value: ajenti\n    - option: description\n      value: \"Ajenti is a client server systems administration tool controlled by a web browser\"\n    - option: enabled\n      value: \"{{ ajenti_enabled }}\"\n"}, {"commit_sha": "a10c5f4577e6e74feb1fadec4bcbab039b8b180a", "sha": "faf8a663894763f66e692c64b1eb596fbe2c0bfa", "filename": "tasks/setup-audit.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Ensure auditd is installed\n  become: true\n  package:\n    name: auditd\n    state: present\n  register: _pkg_result\n  until: _pkg_result|succeeded\n  when: docker_enable_audit and (_docker_os_dist == \"Ubuntu\" or _docker_os_dist == \"Debian\")\n\n- name: Copy Docker audit rules\n  become: yes\n  copy:\n    src: files/etc/audit/rules.d/docker.rules\n    dest: /etc/audit/rules.d/docker.rules\n  notify: restart auditd\n  when: docker_enable_audit\n\n- name: Ensure Docker audit rules are removed\n  become: yes\n  file:\n    path: /etc/audit/rules.d/docker.rules\n    state: absent\n  notify: restart auditd\n  when: not docker_enable_audit\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "cb32c7943ef502b439b2a66df73c82def92b81bd", "filename": "roles/config-iscsi-client/tasks/lvm-config.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- include_tasks: configure_lvm.yml\n  loop_control:\n    loop_var: disk\n  with_items:\n  - \"{{ disk_mapping }}\"\n\n- name: \"Updated PV metadata\"\n  command: 'pvscan --cache'\n  when:\n  - disk_mapping|length > 0\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "65e45dec6899495b5ddf9169ad42134ac52f30e9", "filename": "roles/load-balancers/manage-haproxy/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- include_tasks: 'install.yml'\n- include_tasks: 'generate-config.yml'\n- include_tasks: 'activate-config.yml'\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "fc754301a51e22c966e8c7745f15f99bf298e8b2", "filename": "playbooks/local_ssh.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Ensure the local ssh directory is exist\n  local_action:\n    module: file\n    path: \"~/.ssh/\"\n    state: directory\n\n- name: Copy the algo ssh key to the local ssh directory\n  local_action:\n    module: copy\n    src: \"{{ SSH_keys.private }}\"\n    dest: ~/.ssh/algo.pem\n    mode: '0600'\n\n- name: Configure the local ssh config\n  local_action:\n    module: blockinfile\n    dest: \"~/.ssh/config\"\n    marker: \"# {mark} ALGO MANAGED BLOCK {{ cloud_instance_ip|default(server_ip) }}\"\n    insertbefore: BOF\n    create: yes\n    block: |\n      Host {{ cloud_instance_ip|default(server_ip) }}\n      \tIdentityFile ~/.ssh/algo.pem\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "16d615097b8f618bb8f1c06c1811bcecf7f64e5c", "filename": "playbooks/roles/suricata/vars/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# vars file for suricata"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "f54ba65fdd1398670d1d1875fd3c4bc9b84c66e4", "filename": "meta/main.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\ngalaxy_info:\n  role_name: \"java\"\n  author: \"Lean Delivery team <team@lean-delivery.com>\"\n  description: \"Lean Delivery Java install\"\n  company: \"Epam Systems\"\n  license: \"Apache\"\n  min_ansible_version: \"2.5\"\n  issue_tracker_url: \"https://github.com/lean-delivery/ansible-role-java/issues\"\n  platforms:\n    - name: \"Ubuntu\"\n      versions:\n        - \"xenial\"\n        - \"trusty\"\n    - name: \"Debian\"\n      versions:\n        - \"stretch\"\n        - \"jessie\"\n    - name: \"EL\"\n      versions:\n        - \"6\"\n        - \"7\"\n    - name: \"Windows\"\n      versions:\n        - \"all\"\n\n  galaxy_tags:\n    - \"development\"\n    - \"system\"\n    - \"packaging\"\n    - \"system\"\n    - \"web\"\n    - \"java\"\n    - \"jdk\"\n    - \"jvm\"\n\ndependencies: []\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "0491c6dbef6fc6f51c29d748fbb8727a339f2704", "filename": "archive/roles/registry/tasks/auth_layer_nginx.yaml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: Install Nginx Repository\n  action: \"{{ ansible_pkg_mgr }} name={{ nginx_repo_url }} state=present\"\n\n- name: Install Nginx\n  action: \"{{ ansible_pkg_mgr }} name=nginx state=latest\"\n\n- name: Enable & Start Nginx\n  service: name=nginx enabled=yes state=started\n\n- name: Create SSL Certs\n  include: \"{{role_path}}/tasks/openssl.yaml\"\n  notify: reload nginx\n\n- name: Create SSL Config\n  template: src=\"{{role_path}}/templates/nginx.j2\" dest=\"/etc/nginx/nginx.conf\"\n  notify: reload nginx\n\n- easy_install: name=pip\n\n- pip: name=passlib\n\n- name: Setup Authentication\n  htpasswd: path=/etc/nginx/conf.d/nginx.htpasswd name=demo password=demo owner=root group=nginx mode=0640\n\n- name: Open Firewall for Nginx\n  firewalld: service=https permanent=yes state=enabled immediate=yes zone=public\n\n- name: Nginx SELinux Configurations\n  seboolean:\n    name: httpd_can_network_connect\n    state: yes\n    persistent: yes\n  tags: httpd\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e31821759067fd56cb8c2d641d9e57534ecf8f89", "filename": "playbooks/osp/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nansible_user: fedora\nansible_become: True\n\nosp_volumes:\n- name: \"vol1\"\n  display_description: \"Volume 1\"\n  display_name: \"Vol1\"\n  size: 1\n- name: \"vol2\"\n  display_description: \"Volume 2\"\n  display_name: \"Vol2\"\n  size: 2\n\nosp_security_groups:\n- name: \"multi-sec-group\"\n  description: \"My Multi-Port Sec Group\"\n  rules:\n  - protocol: tcp\n    port_range_min: 22\n    port_range_max: 22\n    direction: ingress\n    remote_ip_prefix: 0.0.0.0/0 \n  - protocol: tcp\n    port_range_min: 80\n    port_range_max: 80\n    direction: ingress\n    remote_ip_prefix: 0.0.0.0/0 \n- name: \"icmp-sec-group\"\n  description: \"ICMP Security Group\"\n  rules:\n  - protocol: icmp\n    direction: ingress\n    remote_ip_prefix: 0.0.0.0/0 \n\nosp_instances:\n- name: \"fedora1\"\n  meta:\n    group: osp_instances\n  image: \"Fedora-Cloud-Base-26-1.5.x86_64\"\n  key_name: \"my-keypair\"\n  flavor: \"m1.medium\"\n  network: \"my-network\"\n  security_groups:\n  - multi-sec-group\n  - icmp-sec-group\n  volumes:\n  - vol1\n  - vol2\n\n\n"}, {"commit_sha": "01c4359d8ad17ba10149ac898663e598069b9055", "sha": "3421208fab4bd52371aa7e71bd91a71fc2589813", "filename": "tasks/main.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\n- name: Include OS-specific variables.\n  include_vars: \"{{ ansible_os_family }}.yml\"\n\n# Fail2Ban\n- include: fail2ban-RedHat.yml\n  when: ansible_os_family == 'RedHat'\n\n- include: fail2ban-Debian.yml\n  when: ansible_os_family == 'Debian'\n\n- name: Ensure fail2ban is running and enabled on boot.\n  service: name=fail2ban state=started enabled=yes\n\n# SSH\n- include: ssh.yml\n\n# Autoupdate\n- include: autoupdate-RedHat.yml\n  when: ansible_os_family == 'RedHat' and security_autoupdate_enabled\n\n- include: autoupdate-Debian.yml\n  when: ansible_os_family == 'Debian' and security_autoupdate_enabled\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7c3c26507a21ffa4ffc4aa879de8b09bbe4008af", "filename": "roles/config-quay-enterprise/tasks/container_credentials.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Check if container credential file exists\n  stat:\n    path: \"{{ container_credentials_file }}\"\n  register: container_credential_stat_result\n\n- block:\n  - name: Read content of container credentials file\n    slurp:\n      path: \"{{ container_credentials_file }}\"\n    register: remote_container_credentials_file\n\n  - name: Set content of container credentials file\n    set_fact:\n      container_credentials_file_content: \"{{ remote_container_credentials_file.content| b64decode | from_json }}\"\n  when: container_credential_stat_result.stat.exists\n\n- name: Create Quay credentials variable\n  set_fact:\n    quay_container_credentials:\n      auths: \"{ '{{quay_registry_server}}':{ 'email': '{{ quay_registry_email }}', 'auth': '{{ quay_registry_auth }}' } }\"\n\n- name: Create merged credentials file content\n  set_fact:\n    container_credentials_file_content: \"{{ container_credentials_file_content | combine(quay_container_credentials, recursive=True) }}\"\n\n- name: Create directory for container credentials file\n  file:\n    state: directory\n    path: \"{{ container_credentials_file | dirname }}\"\n\n- name: Update container credentials file\n  copy:\n    content: \"{{container_credentials_file_content | to_nice_json }}\"\n    dest: \"{{ container_credentials_file }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "0514d6f50360694a3690292a1e3277270672a0b6", "filename": "playbooks/gce/README.md", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "# GCE playbooks\n\nThis playbook directory is meant to be driven by [`bin/cluster`](../../bin),\nwhich is community supported and most use is considered deprecated.\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "fb9f8b77628bcf86493fe7347b70d364a3423e53", "filename": "playbooks/templates/suricata_overrides.yaml.j2", "repository": "rocknsm/rock", "decoded_content": "%YAML 1.1\n---\ndefault-rule-path: \"{{ suricata_var_dir }}/rules\"\nrule-files:\n  - suricata.rules\n\naf-packet:\n{% for iface in rock_monifs %}\n  - interface: {{ iface }}\n    #threads: auto\n    cluster-id: {{ 99 - loop.index0 }}\n    cluster-type: cluster_flow\n    defrag: yes\n    use-mmap: yes\n    mmap-locked: yes\n    #rollover: yes\n    tpacket-v3: yes\n    use-emergency-flush: yes\n{% endfor %}\ndefault-log-dir: {{ suricata_data_dir }}\noutputs:\n  - fast:\n      enabled: yes\n      filename: fast.log\n      append: yes\n  - eve-log:\n      enabled: yes\n      filetype: regular\n      filename: eve.json\n      types:\n        - alert:\n            http: yes\n            tls: yes\n            ssh: yes\n            smtp: yes\n            tagged-packets: yes\n            xff:\n              enabled: no\n        - files:\n            force-magic: no\n            force-hash: [sha1, md5]\n        - stats:\n            totals: yes\n            threads: no\n            deltas: no\n        - flow\n  - unified2-alert:\n      enabled: yes\n      filename: unified2.alert\n      limit: 32mb\n      sensor-id: 0\n      payload: yes\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5cb02ea1d96bf4ac4a39a572e29e4dd9e8649dda", "filename": "roles/ansible/tower/config-ansible-tower/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: restart-tower\n  service:\n    name: supervisord\n    state: restarted\n  become: True\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "84a9a6985642810b91445ab5309c78597a7f8468", "filename": "tasks/requirements_and_deprecated.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n# This file contains all requirements and deprecation checks\n# that can be run before the role starts.\n\n# @requirement: jmespath module must be installe on ansible host\n# @version: 2.4.0\n- name: Check presence of jmespath python module on ansible host\n  block:\n    - name: Try to use jmespath on a test var\n      set_fact:\n        voidvar: \"{{ [] | json_query('[]') }}\"\n  rescue:\n    - name: Print message about jmespath\n      vars:\n        error_msg: |-\n          Since version 2.4.0, this role uses the  json_query filter which requires\n          the presense of the python module jmespath on the host running ansible.\n          Please install jmespath (i.e. pip install jmespath) on your ansible host.\n      debug:\n        msg: \"{{ error_msg.split('\\n') }}\"\n\n    - name: fail\n      fail:\n        msg: \"Install jmespath. See debug message above\"\n\n# @deprecated: nexus_package must not exists\n# @version: 2.2.0\n- name: Broken compatibility => nexus_package is now dynamically calculated\n  fail:\n    msg: >-\n      You have set the variable nexus_package in your playbook.\n      Starting from version 2.2.0 of this role, this is not compatible\n      with the new nexus latest version detection feature and is not\n      supported anymore. Please use the nexus_version variable only.\n  when: nexus_package is defined\n\n# @deprecated: purge was refactored to nexus_purge\n# @version: 2.2.3\n- name: Broken compatibility => purge var moved to nexus_purge\n  fail:\n    msg: >-\n      You have set the purge variable to reset nexus.\n      Starting from version 2.2.3 of this role, this variable\n      has been renamed nexus_purge. Please fix the var name accordingly\n      on you command line call.\n  when: purge is defined\n\n# @deprecated: public_hostname was renamed to nexus_public_hostname\n# @version: 2.3.0\n- name: Variable refactoring - public_hostname is now nexus_public_hostname\n  fail:\n    msg: >-\n      Version 2.3.0 of this role introduced a variable name change: public_hostname was renamed to\n      nexus_public_hostname. We have detected that public_hostname is set in your vars ({{ public_hostname }})\n      and is different from nexus_public_hostname which still has its default value ({{ nexus_public_hostname }}).\n      Fix this by setting a correct value for nexus_public_hostname and remove public_hostname if\n      possible.\n  when: >-\n    public_hosname | default('') | length > 0\n    and\n    public_hostname != nexus_public_hostname\n    and\n    nexus_public_hostname == 'nexus.vm'\n\n# @deprecated: remote_url is the variable to configure a proxy repo. proxy_url is not supported\n# anymore to configure docker proxy repos\n# @version: 2.4.0\n- name: \"Coding standard: proxy_url is not supported anymore for docker proxy repos\"\n  fail:\n    msg: >-\n      proxy_url used to be the variable to configure docker proxy repositories in nexus3-oss role. Since version 2.3.0,\n      all repository configuration use the same standard var remote_url. Please review your configurations\n      in nexus_repos_docker_proxy and rename the variable accordingly.\n  when: >-\n    nexus_repos_docker_proxy\n    | json_query('[?proxy_url]')\n    | list\n    | length > 0\n\n# @requirement: new internal var _nexus_repos_global_List must be undefined\n# @version: 2.4.0\n- name: \"Make sure our internal var for repos is unset\"\n  when: _nexus_repos_global_list is defined\n  block:\n    - name: Print debug message about our internal var\n      vars:\n        error_msg: |-\n          _nexus_repos_global_list is set somewhere in your playbook/inventory.\n          This is an internal var that must be unset when the role starts executing.\n          Please unset _nexus_repos_global_list from your playbook/inventory\n      debug:\n        msg: \"{{ error_msg.split('\\n') }}\"\n\n    - name: Fail as we cannot run correctly with internal var set\n      fail:\n        msg: \"Unset _nexus_repos_global_list. See debug message above\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "faff44491d46900a413a12b49e20d04f86728905", "filename": "reference-architecture/gcp/ansible/playbooks/roles/ansible-gcp/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: check for service account\n  command: gcloud --project {{ gcloud_project }} iam service-accounts list --filter {{ service_account }}\n  register: service_account_output\n  changed_when: false\n\n- name: create new service account\n  command: gcloud --project {{ gcloud_project }} iam service-accounts create {{ service_account }} --display-name '{{ service_account_name }}'\n  register: service_account_created\n  when: service_account not in service_account_output.stdout\n\n- name: create service account key\n  command: gcloud --project {{ gcloud_project }} iam service-accounts keys create {{ inventory_dir }}/gce/hosts/project.json --iam-account {{ service_account_id }} creates={{ inventory_dir }}/gce/hosts/project.json\n\n- name: grant service account access to the gcp project\n  command: gcloud --project {{ gcloud_project }} projects add-iam-policy-binding {{ gcloud_project }} --member serviceAccount:{{ service_account_id }} --role roles/editor\n  when: service_account_created | changed\n\n- name: create config file for dynamic inventory\n  template:\n    src: gce.ini.j2\n    dest: '{{ inventory_dir }}/gce/hosts/gce.ini'\n\n- name: remove old secrets.py files\n  file:\n    path: '{{ item }}'\n    state: absent\n  with_items:\n    - '{{ inventory_dir }}/gce/hosts/secrets.py'\n    - '{{ inventory_dir }}/gce/hosts/secrets.pyc'\n    - '{{ inventory_dir }}/gce/hosts/secrets.pyo'\n"}, {"commit_sha": "c91b6076e3a957fb0a165131d0ff3b3b208ed419", "sha": "46d89aa68e3d2808b3448e1db064dcc073ade574", "filename": "tasks/section_06_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 6.1 Ensure the X Window system is not installed (Scored)\n    apt: name=xserver-xorg-core* purge=yes state=absent\n    when: remove_xserver == True\n    tags:\n      - section6\n      - section6.1\n\n  - name: 6.2 Ensure Avahi Server is not enabled (check) (Scored)\n    stat: path='/etc/init/avahi-daemon.conf'\n    register: avahi_stat\n    tags:\n      - section6\n      - section6.2\n\n  - name: 6.2 Ensure Avahi Server is not enabled (Scored)\n    lineinfile: >\n        line='#start on (filesystem'\n        state=present\n        regexp='start on \\(filesystem'\n        dest=/etc/init/avahi-daemon.conf\n    when: avahi_stat.stat.exists == True\n    tags:\n      - section6\n      - section6.2\n\n  - name: 6.3 Ensure print server is not enabled (check) (Not Scored)\n    stat: path='/etc/init/cups.conf'\n    register: cups_stat\n    tags:\n      - section6\n      - section6.3\n\n  - name: 6.3 Ensure print server is not enabled (Not Scored)\n    lineinfile: >\n        line='#start on (filesystem'\n        state=present\n        regexp='start on \\(filesystem'\n        dest=/etc/init/cups.conf\n    when: cups_stat.stat.exists == True\n    tags:\n      - section6\n      - section6.3\n\n  - name: 6.4.1 Ensure DHCP Server is not enabled (check) (Scored)\n    stat: path='/etc/init/isc-dhcp-server.conf'\n    register: dhcp_stat\n    tags:\n      - section6\n      - section6.4\n      - section6.4.1\n\n  - name: 6.4.1 Ensure DHCP Server is not enabled (Scored)\n    lineinfile: >\n        line='#start on runlevel [2345]'\n        state=present\n        regexp='start on runlevel'\n        dest=/etc/init/isc-dhcp-server.conf\n    when: dhcp_stat.stat.exists == True\n    tags:\n      - section6\n      - section6.4\n      - section6.4.1\n\n  - name: 6.4.2 Ensure DHCP Server is not enabled (check) (Scored)\n    stat: path='/etc/init/isc-dhcp-server6.conf'\n    register: dhcp6_stat\n    tags:\n      - section6\n      - section6.4\n      - section6.4.2\n\n  - name: 6.4.2 Ensure DHCP Server is not enabled (Scored)\n    lineinfile: >\n        line='#start on runlevel [2345]'\n        state=present\n        regexp='start on runlevel'\n        dest=/etc/init/isc-dhcp-server6.conf\n    when: dhcp6_stat.stat.exists == True\n    tags:\n      - section6\n      - section6.4\n      - section6.4.2\n\n  - name: 6.5.1 Configure Network Time Protocol (install) (NTP) (Scored)\n    apt: name=ntp state=present\n    tags:\n      - section6\n      - section6.5\n      - section6.5.1\n\n  - name: 6.5.2 Configure Network Time Protocol (restrict4) (NTP) (Scored)\n    lineinfile: >\n        dest='/etc/ntp.conf'\n        line='restrict -4 default kod nomodify notrap nopeer noquery'\n        regexp='^restrict -4 default'\n        state=present\n    tags:\n      - section6\n      - section6.5\n      - section6.5.2\n\n  - name: 6.5.3 Configure Network Time Protocol (restrict6) (NTP) (Scored)\n    lineinfile: >\n        dest='/etc/ntp.conf'\n        line='restrict -6 default kod nomodify notrap nopeer noquery'\n        regexp='^restrict -6 default'\n        state=present\n    tags:\n      - section6\n      - section6.5\n      - section6.5.3\n\n  - name: 6.5.4 Configure Network Time Protocol (server check) (NTP) (Scored)\n    command: 'grep \"^server\" /etc/ntp.conf'\n    register: ntp_server_rc\n    changed_when: False\n    always_run: True\n    tags:\n      - section6\n      - section6.5\n      - section6.5.4\n\n  - name: 6.5.4 Configure Network Time Protocol (server add) (NTP) (Scored)\n    lineinfile: >\n        dest='/etc/ntp.conf'\n        line='server 0.fr.pool.ntp.org'\n        state=present\n    when: ntp_server_rc.rc == 1\n    tags:\n      - section6\n      - section6.5\n      - section6.5.4\n\n  - name: 6.6 Ensure LDAP is not enabled (Not Scored)\n    apt: name=slapd purge=yes state=absent\n    tags:\n      - section6\n      - section6.6\n\n  - name: 6.7.1 Ensure NFS and RPC are not enabled (stat) (Not Scored)\n    stat: path=/etc/init/rpcbind-boot.conf\n    register: nfs_rpc_rc\n    tags:\n      - section6\n      - section6.7\n      - section6.7.1\n\n  - name: 6.7.2 Ensure NFS and RPC are not enabled (Not Scored)\n    lineinfile: >\n        dest=/etc/init/rpcbind-boot.conf\n        line='#start on virtual-filesystems and net-device-up IFACE=lo'\n        state=present\n        regexp='start on virtual-filesystems and net'\n    when: nfs_rpc_rc.stat.exists == True\n    tags:\n      - section6\n      - section6.7\n      - section6.7.2\n\n  - name: 6.7.2 Ensure NFS and RPC are not enabled (check) (Not Scored)\n    command: dpkg -S nfs-kernel-server\n    changed_when: False\n    failed_when: False\n    register: nfs_present\n    tags:\n      - section6\n      - section6.7\n      - section6.7.2\n\n  - name: 6.7.2 Ensure NFS and RPC are not enabled (rc.d) (Not Scored)\n    service: >\n        name=nfs-kernel-server\n        enabled=no\n    when: nfs_present is defined and nfs_present.rc == 0\n    register: nfs_service_result\n    failed_when: \"nfs_service_result|failed and 'service not found' not in nfs_service_result.msg\"\n    tags:\n      - section6\n      - section6.7\n      - section6.7.2\n\n  - name: 6.8-14 Ensure DNS,FTP,HTTP,IMAP,POP,Samba,Proxy,SNMP Servers are not enabled (Not Scored)\n    service: >\n        name={{ item }}\n        enabled=no\n    with_items:\n      - bind9\n      - vsftpd\n      - apache2\n      - dovecot\n      - smbd\n      - squid3\n      - snmpd\n    failed_when: False\n    tags:\n      - section6\n      - section6.8\n      - section6.9\n      - section6.10\n      - section6.11\n      - section6.12\n      - section6.13\n      - section6.14\n\n  - name: 6.15 Configure Mail Transfer Agent for Local-Only Mode (stat) (Scored)\n    stat: path=/etc/postfix/main.cf\n    register: postfix_main_cf\n    tags:\n      - section6\n      - section6.15\n\n  - name: 6.15 Configure Mail Transfer Agent for Local-Only Mode (Scored)\n    lineinfile: >\n        dest=/etc/postfix/main.cf\n        line='inet_interfaces = localhost'\n        state=present\n    when: postfix_main_cf.stat.exists == True\n    tags:\n      - section6\n      - section6.15\n\n  - name: 6.16 Ensure rsync service is not enabled (stat) (Scored)\n    stat: path=/etc/default/rsync\n    register: default_rsync\n    tags:\n      - section6\n      - section6.16\n\n  - name: 6.16 Ensure rsync service is not enabled (Scored)\n    lineinfile: >\n        dest='/etc/default/rsync'\n        regexp='^RSYNC_ENABLE'\n        line='RSYNC_ENABLE=false'\n    when: default_rsync.stat.exists == True\n    tags:\n      - section6\n      - section6.16\n\n  - name: 6.17 Ensure Biosdevname is not enabled (Scored)\n    apt: name=biosdevname purge=yes state=absent\n    tags:\n      - section6\n      - section6.17\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c772f6a5e7d8a00a6b57f7acdc1138fe0b91022e", "filename": "roles/load-balancers/manage-haproxy/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'enable and start service(s)'\n  service:\n    name: '{{ item }}'\n    enabled: yes\n    state: started\n  with_items:\n  - haproxy\n  become: True\n\n- name: 'restart rsyslog'\n  service:\n    name: rsyslog\n    state: restarted\n  become: True\n\n- name: 'reload haproxy'\n  service:\n    name: haproxy\n    state: reloaded\n  become: True\n\n- name: 'remove tmp new file'\n  file:\n    path: '{{ temp_new_file }}'\n    state: absent\n  become: True\n\n- name: 'cleanup temp dir'\n  file:\n    path: '{{ haproxy_temp_dir }}'\n    state: absent\n  delegate_to: localhost\n  run_once: True\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b4d6ed01cda6a89668e10ebbbab66969c4abf4de", "filename": "reference-architecture/vmware-ansible/playbooks/roles/create-vm-nfs/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Create NFS VM on vCenter\n  vmware_guest:\n    hostname: \"{{ vcenter_host }}\"\n    username: \"{{ vcenter_username }}\"\n    password: \"{{ vcenter_password }}\"\n    validate_certs: False\n    name: \"{{ item.value.guestname }}\"\n    cluster: \"{{ vcenter_cluster}}\"\n    datacenter: \"{{ vcenter_datacenter }}\"\n    resource_pool: \"{{ vcenter_resource_pool }}\"\n    template: \"{{vcenter_template_name}}\"\n    folder: \"/{{ vcenter_folder }}\"\n    annotation: \"{{ item.value.tag }}\"\n    state: poweredon\n    wait_for_ip_address: true\n    disk:\n    - size_gb: 60\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 40\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 40\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    networks:\n    - name: \"{{ vm_network }}\"\n      ip: \"{{ item.value.ip4addr }}\"\n      netmask: \"{{ vm_netmask }}\"\n      gateway: \"{{ vm_gw }}\"\n    customization:\n      domain: \"{{dns_zone}}\"\n      dns_servers:\n      - \"{{ vm_dns }}\"\n      dns_suffix: \"{{dns_zone}}\"\n      hostname: \"{{ item.value.guestname}}\"\n  with_dict: \"{{ host_inventory }}\"\n  when: \"'nfs' in item.value.guestname\"\n\n- name: Add NFS server to inventory\n  add_host: hostname=\"{{ item.value.guestname}}\" ansible_ssh_host=\"{{ item.value.ip4addr }}\" groups=\"{{ item.value.tag }}, nfs_group\"\n  with_dict: \"{{ host_inventory }}\"\n  when: \"'nfs' in item.value.guestname\"\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "8e6cf647d542978608e58cd2ed9336342061b4fe", "filename": "roles/registrator/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for registrator\nregistrator_image: \"gliderlabs/registrator:master\"\nregistrator_uri: \"consul://{{ ansible_default_ipv4.address }}:8500\"\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "417813e2a5ded6fba448818416bebf9fa988146e", "filename": "playbooks/provisioning/openstack/post-install.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: OSEv3\n  gather_facts: False\n  become: True\n  tasks:\n    - name: Save iptables rules to a backup file\n      when: openshift_use_flannel|default(False)|bool\n      shell: iptables-save > /etc/sysconfig/iptables.orig-$(date +%Y%m%d%H%M%S)\n\n# Enable iptables service on app nodes to persist custom rules (flannel SDN)\n# FIXME(bogdando) w/a https://bugzilla.redhat.com/show_bug.cgi?id=1490820\n- hosts: app\n  gather_facts: False\n  become: True\n  vars:\n    os_firewall_allow:\n      - service: dnsmasq tcp\n        port: 53/tcp\n      - service: dnsmasq udp\n        port: 53/udp\n  tasks:\n    - when: openshift_use_flannel|default(False)|bool\n      block:\n        - include_role:\n            name: openshift-ansible/roles/os_firewall\n        - include_role:\n            name: openshift-ansible/roles/lib_os_firewall\n        - name: set allow rules for dnsmasq\n          os_firewall_manage_iptables:\n            name: \"{{ item.service }}\"\n            action: add\n            protocol: \"{{ item.port.split('/')[1] }}\"\n            port: \"{{ item.port.split('/')[0] }}\"\n          with_items: \"{{ os_firewall_allow }}\"\n\n- hosts: OSEv3\n  gather_facts: False\n  become: True\n  tasks:\n    - name: Apply post-install iptables hacks for Flannel SDN (the best effort)\n      when: openshift_use_flannel|default(False)|bool\n      block:\n        - name: set allow/masquerade rules for for flannel/docker\n          shell: >-\n            (iptables-save | grep -q custom-flannel-docker-1) ||\n            iptables -A DOCKER -w\n            -p all -j ACCEPT\n            -m comment --comment \"custom-flannel-docker-1\";\n            (iptables-save | grep -q custom-flannel-docker-2) ||\n            iptables -t nat -A POSTROUTING -w\n            -o {{flannel_interface|default('eth1')}}\n            -m comment --comment \"custom-flannel-docker-2\"\n            -j MASQUERADE\n\n        # NOTE(bogdando) the rules will not be restored, when iptables service unit is disabled & masked\n        - name: Persist in-memory iptables rules (w/o dynamic KUBE rules)\n          shell: iptables-save | grep -v KUBE > /etc/sysconfig/iptables\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "292a0077eb916e8aaec3f9e14735b2465427a3a3", "filename": "reference-architecture/gcp/ansible/playbooks/create-inventory-file.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: create static inventory file\n  hosts: localhost\n  roles:\n  - inventory-file-creation\n  tasks:\n  - name: print message with the location of the inventory file\n    debug:\n      msg: Static inventory file created as ansible/static-inventory\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "29183b1467773e5d99aa8181e866e37d8f02a81f", "filename": "roles/activity-server/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# assume apache in admin group\n\n- name: Create xs-activity-server directory tree\n  file: path={{ item }}\n        mode=0755\n        owner=root\n        group=admin\n        state=directory\n  with_items:\n    - /library/xs-activity-server\n    - /library/xs-activity-server/activities\n    - /library/xs-activity-server/lang_templates\n    - /library/xs-activity-server/www.0\n    - /library/xs-activity-server/tmp\n\n# Wish synchronize worked, but it doesn't\n\n- name: Copy language templates\n  command: rsync -a {{ iiab_dir }}/roles/activity-server/files/lang_templates /library/xs-activity-server/\n\n- name: Copy default index files\n  copy: src={{ item }}\n        dest=/library/xs-activity-server/www.0\n        mode=0755\n        owner=root\n        group=root\n  with_fileglob:\n        - www.0/index.html.*\n\n- name: Point www to www.0 as default\n  file: src=/library/xs-activity-server/www.0\n        dest=/library/xs-activity-server/www\n        owner=root\n        group=admin\n        state=link\n\n- name: Chown language templates\n  file: path=/library/xs-activity-server/lang_templates\n        mode=0644\n        owner=root\n        group=admin\n        state=directory\n        recurse=yes\n\n# We should have a var for python site-packages directory\n\n- name: Create xs-activity-server python site-packages directory\n  file: path=/usr/lib/python2.7/site-packages/xs_activities\n        mode=0755\n        owner=root\n        group=root\n        state=directory\n\n- name: Install Python module\n  copy: src=xs_activities/__init__.py\n        dest=/usr/lib/python2.7/site-packages/xs_activities\n        mode=0644\n        owner=root\n        group=root\n\n- name: Copy scripts to /usr/bin\n  copy: src={{ item }}\n        dest=/usr/bin\n        mode=0755\n        owner=root\n        group=root\n  with_items:\n        - bin/xs-regenerate-activities\n        - bin/xs-check-activities\n\n# Do in ansible what was done in /etc/sysconfig/olpc-scripts/setup.d/xs-activity-server script\n\n- name: Copy xs-activity-server config file\n  template: src=xs-activity-server.conf\n            dest=/etc/{{ apache_config_dir }}\n            owner=root\n            group=root\n            mode=0644\n\n- name: enable mod_expires for debian\n  command: a2enmod expires\n  when: is_debuntu\n\n- name: create the link which enables the site\n  file: src=/etc/apache2/sites-available/xs-activity-server.conf\n        dest=/etc/apache2/sites-enabled/xs-activity-server.conf\n        state=link\n  when: activity_server_enabled and is_debuntu\n\n- name: delete the link which enables the site\n  file: src=/etc/apache2/sites-available/xs-activity-server.conf\n        dest=/etc/apache2/sites-enabled/xs-activity-server.conf\n        state=absent\n  when: not activity_server_enabled and is_debuntu\n\n\n- name: Copy xs-activity-server usbmount file\n  template: src=usbmount-60-xs-activity-server-installcontent\n        dest=/etc/usbmount/mount.d\n        owner=root\n        group=root\n        mode=0755\n\n# TODO: Fix multiview so it supports portal language menu\n#       For it only supports client's language code\n\n# TODO: Upload Activity via web interface\n#       and figure out what to do with olpc_activities.service\n\n# short term addition of link for upload-activity server\n# ln -sf /usr/share/xs-config/cfg/html/top/en/cntr_upl_activity.php {{ doc_root }}/upload_activity.php\n\n\n- name: Restart httpd\n  service: name={{ apache_service }}\n           enabled=yes\n           state=restarted\n\n- name: add xs-activity-server to service list\n  ini_file: dest='{{ service_filelist }}'\n            section=activity-server\n            option='{{ item.option }}'\n            value='{{ item.value }}'\n  with_items:\n    - option: name\n      value: \"Activity Server\"\n    - option: description\n      value: \"Download an Activity.\"\n    - option: path\n      value: /activities\n    - option: enabled\n      value: \"{{ xo_services_enabled }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "0731fc96c9c19f98d6dc3e8f05afe96a2ef2b112", "filename": "roles/osp/packstack-install/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: infra_osp_hosts\n  roles:\n    - packstack-install\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "07ad393b59ec9b4af7e2cc381a2d3cd5a3370806", "filename": "roles/storage-cns/defaults/main.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "cluster: \"openshift\"\naction: \"provision\"\n\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "bde310154e9c91a0db00e88cc42696d5b0ddddc4", "filename": "playbooks/templates/broctl.service.j2", "repository": "rocknsm/rock", "decoded_content": "[Unit]\nDescription=Bro Network Intrusion Detection System (NIDS)\nAfter=network.target\n\n[Service]\nType=forking\nUser={{ bro_user }}\nGroup={{ bro_group }}\nEnvironment=HOME={{ bro_data_dir }}/spool\nExecStart=/opt/bro/bin/broctl deploy\nExecStop=/opt/bro/bin/broctl stop\n\n[Install]\nWantedBy=multi-user.target\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "d8c4472ca1b65cea039252e137ff3b4ab5d3a555", "filename": "playbooks/roles", "repository": "redhat-cop/infra-ansible", "decoded_content": "../roles"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "0e26c994130e07079eaac5d1a5976bf5addabd8d", "filename": "roles/ovirt-engine-config/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n- name: Tune oVirt engine with engine-config, version specific\n  shell: \"engine-config -s {{ item.key }}='{{ item.value }}' --cver={{ item.version }}\"\n  with_items: \"{{ ovirt_engine_config | default([]) }}\"\n  notify:\n    - restart of ovirt-engine service\n    - check health status of page\n  when: ovirt_engine_config is defined and item.version != \"general\"\n  tags:\n    - skip_ansible_lint\n\n- name: Tune oVirt engine with engine-config, regardless version\n  shell: \"engine-config -s {{ item.key }}='{{ item.value }}'\"\n  notify:\n    - restart of ovirt-engine service\n    - check health status of page\n  with_items: \"{{ ovirt_engine_config | default([]) }}\"\n  when: ovirt_engine_config is defined and item.version == \"general\"\n  tags:\n    - skip_ansible_lint\n\n- name: Copy property file to engine\n  copy:\n    content: \"{{ ovirt_engine_config_property_file }}\"\n    dest: \"/tmp/ovirt_engine_config.properties\"\n  when: ovirt_engine_config_property_file is defined\n\n- name: Tune oVirt engine with engine-config; from property file\n  shell: \"engine-config --properties=/tmp/ovirt_engine_config.properties\"\n  notify:\n    - restart of ovirt-engine service\n    - check health status of page\n  when: ovirt_engine_config_property_file is defined\n  tags:\n    - skip_ansible_lint\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b5f5d9f545a6741f34f617a18f3c4c5ab3be1f2b", "filename": "roles/config-postgresql/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Install Containerized PostgreSQL\n  include_tasks: install_containerized.yml\n  when: mode == \"containerized\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c704e1cc4f6227fb3d60cfcc4deefe369dcfaea9", "filename": "playbooks/provision-nfs-server/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- import_playbook: ../prep.yml\n  when:\n  - rhsm_register|default(False)\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../osp/manage-user-network.yml\n  when:\n  - hosting_infrastructure == 'openstack'\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../osp/provision-osp-instance.yml\n  when:\n  - hosting_infrastructure == 'openstack'\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../rhsm.yml\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: nfs-server.yml\n  tags:\n  - 'always'\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "8a4c82c550b1b8e54879e2d2814f230c686f8475", "filename": "ops/playbooks/restore_ucp.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- name: Install UCP via restore\n  hosts: ucp_main\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - ../group_vars/backups\n    - includes/internal_vars.yml\n\n  vars:\n    UCPIP:  \"{{groups.ucp_main[0]}}.{{domain_name}}\"\n    UCPUSER: \"{{ucp_username}}\"\n    UCPPASSWORD: \"{{ucp_password}}\"\n    JOINING_IP:  \"{{ hostvars[groups.ucp_main[0]]['ip_addr'] | ipaddr('address')}}:2377\"\n  \n  pre_tasks:\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n\n  environment:\n    - \"{{ env }}\"\n\n  roles:\n    - role: hpe.openports\n      hpe_openports_ports: \"{{ internal_ucp_ports }}\"\n\n  tasks:\n\n#\n# configure passwordless ssh to our ansible box\n#\n    - name: Register key\n      stat: path=/root/.ssh/id_rsa\n      register: key\n    - name: Create keypairs\n      shell: ssh-keygen -t rsa -f /root/.ssh/id_rsa -N ''\n      when: key.stat.exists == False\n    - name: Fetch all public ssh keys\n      shell: cat ~/.ssh/id_rsa.pub\n      register: ssh_keys\n    - name: Deploy keys on localhost\n      authorized_key: user=root key=\"{{ item }}\"\n      delegate_to: localhost\n      with_items:\n        - \"{{ ssh_keys.stdout }}\"\n\n#\n# attempts to restore only UCP don't work, even when restoring the swarm data (which is not recommended) \n#\n    - set_fact:\n#        restore_ucp_only: \"{{ restore_ucp_only | default ('false') }}\"\n        restore_ucp_only: \"false\"\n\n    - name: Creates directory\n      file:\n        path: /root/scripts\n        state: directory\n      when: inventory_hostname in groups.ucp_main\n\n    - name: Copy restore script to target\n      template:\n        src: ../templates/restore_ucp.sh.j2\n        dest: /root/scripts/restore_ucp.sh\n        mode: 0744\n      when: inventory_hostname in groups.ucp_main\n\n\n    - name: restore UCP\n      shell: /root/scripts/restore_ucp.sh \n      register: res\n      when: inventory_hostname in groups.ucp_main and ucp_instance == \".none.\"\n\n    - name: Retrieve a token for the UCP API\n      uri:\n        url: \"https://{{ UCPIP }}/auth/login\"\n        headers:\n          Content-Type: application/json\n        method: POST\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        body: '{\"username\":\"{{ UCPUSER }}\",\"password\":\"{{ UCPPASSWORD }}\"}'\n      register: resp\n      until: resp.status == 200\n      retries: 20\n      delay: 5\n\n    - name: Remember the API's token\n      set_fact:\n        auth_token:  \"{{resp.json.auth_token}}\"\n\n    - name: Is the node already in the swarm\n      uri:\n        url: 'https://{{ UCPIP }}/nodes/{{ inventory_hostname }}.{{ domain_name }}'\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200,404\n        body_format: json\n        validate_certs: no\n      delegate_to: localhost\n      register: resp\n      until: resp.status == 200 and resp.json.Spec.Role == \"manager\" and resp.json.Status.State == \"ready\"\n      delay: 10\n      retries:  \"{{ 1 + ( 180  / 10 ) | int }}\"\n\n\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "fb68bc2303ee031ca5a697db4fd414e0a839dd51", "filename": "ops/playbooks/create_main_ucp.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- name: Create First UCP Instance\n  hosts: ucp_main\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - includes/internal_vars.yml\n\n  vars:\n    san_fqdns: \"{% for host in groups['ucp'] %}--san {{ host }}.{{ domain_name }} {% endfor %} {% if ucp_lb_fqdn | length > 0 %}--san {{ ucp_lb_fqdn }} {% endif %}\"\n    san_ips: \"{% for host in groups['ucp'] %} --san {{ hostvars[host].ip_addr | ipaddr('address') }} {% endfor %} {% if ucp_lb_ipv4 | length > 0 %} --san {{ ucp_lb_ipv4 }} {% endif %}\"\n    san_all_formatted: \"{{ san_fqdns }} {{ san_ips }}\"\n\n  environment:\n    - \"{{ env }}\"\n    - UCP_ADMIN_USER: \"{{ ucp_username }}\"\n    - UCP_ADMIN_PASSWORD: \"{{ ucp_password }}\"\n    - UCP_LICENSE:  \"{{ lookup('file','{{ license_file }}') }}\"\n\n  roles:\n    - role: hpe.openports\n      hpe_openports_ports: \"{{ internal_ucp_ports }}\"\n\n  tasks:\n\n#\n# Is UCP installed in the cluster\n#\n    - name: See if there is an UCP instance running\n      include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n\n    - name: Copy the license\n      copy: src=\"{{ license_file }}\" dest=\"/tmp/license.lic\"\n      when: ucp_instance == \".none.\"\n\n#\n# Load the certificates\n#\n    - include_tasks: includes/load_certificates.yml\n\n#\n# Copy user supplied certificates (if any)\n#\n    - name: \"Create volume  {{ ucp_certificates_volume_name }} if ucp_certs_dir is defined and UCP is not installed\" \n      command: docker volume create {{ ucp_certificates_volume_name }}\n      when: ucp_instance == \".none.\" and ucp_certs_dir is defined\n\n    - name: \"Copy user supplied certificates for UCP\" \n      copy:\n        src: \"{{ ucp_certs_dir }}/\"\n        dest: \"/root/ucp_certs/\"\n      when: ucp_instance == \".none.\" and ucp_certs_dir is defined\n\n    - name: \"Copying certificates to volume  {{ ucp_certificates_volume_name }}\"\n      command:  docker run --rm  -v/root/ucp_certs:/src -v{{ ucp_certificates_volume_name }}:/data alpine sh -c 'cp /src/* /data && ls /data'\n      when: ucp_instance == \".none.\" and ucp_certs_dir is defined\n      register: copy_certs\n\n    - debug: var=copy_certs\n      when: ucp_instance == \".none.\" and ucp_certs_dir is defined and _debug is defined\n\n#\n# Install UCP if needed\n#\n\n    - name: Install swarm leader and first UCP node\n      command: |\n         docker run --rm --env UCP_ADMIN_USER --env UCP_ADMIN_PASSWORD\n           --name ucp\n           -v /var/run/docker.sock:/var/run/docker.sock \n           -v /tmp/license.lic:/config/docker_subscription.lic\n           docker/ucp:{{ ucp_version }} install \n             --host-address {{ ip_addr | ipaddr('address') }}  \n             --pod-cidr {{ k8s_pod_cidr | default ('192.168.0.0/16') }}\n          {{ san_all_formatted }}\n          {{ external_server_cert_switch }}\n      vars:\n        - external_server_cert_switch: \"{% if ucp_certs_dir is defined %}--external-server-cert{% else %}{% endif %}\"\n      register: output\n      when: ucp_instance == \".none.\"\n\n    - debug:\n        var: output\n      when: _debug is defined and ucp_instance == \".none.\"\n\n    - debug: \n        msg: \"An UCP instance was found at {{ ucp_instance }}.{{ domain_name }}\"\n      when: ucp_instance != \".none.\" \n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "c5123e6adeab9f87271d2a428a9fe9be5ddb53bc", "filename": "reference-architecture/vmware-ansible/playbooks/ocp-configure.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  # Group systems\n  - instance-groups\n\n- hosts: single_master\n  gather_facts: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - instance-groups\n  - storage-class-configure\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "003be4ec1c8ff3a731c1722ff16f8194389a0f91", "filename": "roles/config-pxe/tasks/pxe.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Ensure necessary directories exists\"\n  file:\n    path: \"{{ item }}\"\n    state: directory\n  with_items:\n  - \"{{ tftpserver_root_dir }}/pxelinux\"\n  - \"{{ tftpserver_root_dir }}/pxelinux.cfg\"\n\n- name: \"Copy in necessary PXE files for regular BIOS use\"\n  copy: \n    src: \"{{ item }}\"\n    dest:  \"{{ tftpserver_root_dir }}/{{ item | basename }}\"\n    remote_src: True\n  with_items:\n  - \"/usr/share/syslinux/pxelinux.0\"\n  - \"/usr/share/syslinux/memdisk\"\n  - \"/usr/share/syslinux/vesamenu.c32\"\n  - \"/usr/share/syslinux/menu.c32\"\n\n# Note: the Ansible 'copy' module doesn't support resursive copy of remote src directories as-is, hence the 'command'\n- name: \"Copy in necessary PXE files for UEFI use\"\n  shell: \"cp -r {{ uefi_source }}/* {{ tftpserver_root_dir }}/pxelinux\"\n\n- name: \"Workaround for buggy UEFI requiring lowercase\"\n  copy: \n    src: \"{{ tftpserver_root_dir }}/pxelinux/BOOTX64.EFI\"\n    dest: \"{{ tftpserver_root_dir }}/pxelinux/bootx64.efi\"\n    remote_src: True\n\n- name: \"Ensure the hosted OS directories exists\"\n  file:\n    path: \"{{ tftpserver_root_dir }}/{{ item.destination }}\"\n    state: directory\n  with_items:\n  - \"{{ pxe_entries }}\"\n\n# Note: the Ansible 'copy' module doesn't support resursive copy of directories as-is, hence the 'command'\n- name: \"Copy in the necessary files for the hosted OS directories\"\n  shell: \"cp -r {{ item.source }}/* {{ tftpserver_root_dir }}/{{ item.destination }}\"\n  with_items:\n  - \"{{ pxe_entries }}\"\n\n- name: \"Populate the pxelinux.cfg default file\"\n  template:\n    src: pxelinux_cfg.j2\n    dest: \"{{ tftpserver_root_dir }}/pxelinux.cfg/default\"\n\n- name: \"Populate the grub.cfg (UEFI) default file\"\n  template:\n    src: pxelinux_uefi.j2\n    dest: \"{{ tftpserver_root_dir }}/pxelinux/grub.cfg\"\n\n- name: \"Generate target specific UEFI grub.cfg file\"\n  include_tasks: pxe-target.yml\n  loop_control:\n    loop_var: target_entry\n  with_items:\n  - \"{{ pxe_targets }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ea7c2e0535b95fac5641fd7e8efd5f0a82747b8a", "filename": "roles/manage-sshd-config/tasks/sshd-update.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: \"Update sshd_config with provided values\" \n  lineinfile:\n    path: /etc/ssh/sshd_config\n    backup: yes\n    regexp: '^{{ item.key }}'\n    line: '{{ item.key }} {{ item.value }}'\n  notify: 'reload sshd'\n  with_dict: \"{{ update_sshd_config | default({}) }}\"\n\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "174f872aa5db177e1f7a36a8aec491a673f86ef7", "filename": "archive/roles/cicd/files/jenkins/hudson.tasks.Maven.xml", "repository": "redhat-cop/casl-ansible", "decoded_content": "<?xml version='1.0' encoding='UTF-8'?>\n<hudson.tasks.Maven_-DescriptorImpl>\n  <installations>\n    <hudson.tasks.Maven_-MavenInstallation>\n      <name>maven</name>\n      <home>/usr/share/maven</home>\n      <properties/>\n    </hudson.tasks.Maven_-MavenInstallation>\n  </installations>\n</hudson.tasks.Maven_-DescriptorImpl>"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "050e3e0ebcfc6e12cb7692e87b246c1d3e0522ba", "filename": "playbooks/site.yml", "repository": "rocknsm/rock", "decoded_content": "#!/usr/bin/env ansible-playbook\n- import_playbook: generate-defaults.yml\n- import_playbook: deploy-rock.yml\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "60df753fe3f51feb2c3083cd0d2cf25e67a2e454", "filename": "roles/client/tasks/systems/CentOS.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- set_fact:\n    prerequisites:\n      - epel-release\n    configs_prefix: /etc/strongswan/\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "67898e0ce18d2cc7407e91479796e297a4630e45", "filename": "reference-architecture/vmware-ansible/playbooks/roles/vmware-guest-setup/handlers/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: restart chronyd\n  service: name=chronyd state=restarted\n\n- name: restart networking\n  service: name=networking state=restarted\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "619f137aec669be593bdde9188bab52f5b4fbb32", "filename": "roles/dns/config-dns-server/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: dns-servers\n  roles:\n  - role: dns/config-dns-server\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "37e94bf6963098df4382a19c4006c7df262cf801", "filename": "roles/idm/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- hosts: idm\n  become: yes\n\n  roles:\n    - idm\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "e8bec03f75f0487404fbfea02a0dcffb4355050e", "filename": "archive/roles/cicd/tasks/nexus.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n  \n- name: Download Nexus Artifact\n  get_url:\n    url: \"{{ nexus_url }}\"\n    dest: \"{{ nexus_local_archive }}\"\n  tags: nexus\n \n- name: Create Nexus User\n  user:\n    name: \"{{ nexus_user }}\"\n    shell: \"/bin/bash\"\n    home: \"{{ nexus_home_dir }}\"\n    createhome: no\n    state: present\n  tags: nexus\n \n- name: Extract Nexus Artifact\n  unarchive: \n    src: \"{{ nexus_local_archive }}\"\n    dest: \"{{ nexus_base_dir }}\"\n    copy: no\n    creates: \"{{ nexus_install_dir }}\"\n    group: \"{{ nexus_group }}\"\n    owner: \"{{ nexus_user }}\"\n  tags: nexus\n\n- name: Create Nexus Symbolic Link\n  file:\n    src: \"{{ nexus_install_dir }}\"\n    dest: \"{{ nexus_home_dir }}\"\n    state: link \n    group: \"{{ nexus_group }}\"\n    owner: \"{{ nexus_user }}\"\n  tags: nexus\n\n- name: Add NEXUS_HOME Variable\n  lineinfile: \n    dest: /etc/profile\n    regexp: \"^export NEXUS_HOME=/usr/local/nexus-{{nexus_version}}\"\n    line: \"export NEXUS_HOME=/usr/local/nexus-{{nexus_version}}\"\n  tags: nexus\n  \n- name: Create Nexus PID Directory\n  file: \n    path: \"{{nexus_pid_dir}}\"\n    state: directory\n    group: \"{{ nexus_group }}\"\n    owner: \"{{ nexus_user }}\"\n  tags: nexus\n  \n- name: Create Nexus Work Configuration Directory\n  file: \n    path: \"{{nexus_sonatype_work_dir}}/nexus/conf\"\n    state: directory\n    group: \"{{ nexus_group }}\"\n    owner: \"{{ nexus_user }}\"\n  tags: nexus\n  \n- name: Copy Work Nexus Configuration File\n  copy: \n    src: nexus/nexus.xml\n    dest: \"{{nexus_sonatype_work_dir}}/nexus/conf/\"\n    group: \"{{ nexus_group }}\"\n    owner: \"{{ nexus_user }}\"\n  notify:\n  - restart nexus\n  tags: nexus\n  \n- name: Custom Nexus Configuration File Settings\n  lineinfile:\n    dest: \"{{ nexus_home_dir }}/bin/nexus\"\n    regexp: \"{{ item.regex }}\"\n    line: \"{{ item.replace }}\"\n    backup: yes\n    state: present\n  with_items:\n  - { regex: \"^NEXUS_HOME=\\\"..\\\"\", replace: \"NEXUS_HOME=\\\"{{ nexus_home_dir }}\\\"\" }\n  - { regex: \"^#PIDDIR=\\\".\\\"\", replace: \"PIDDIR=\\\"{{ nexus_home_dir }}/bin/jsw/linux-x86-64\\\"\" }\n  - { regex: \"^#RUN_AS_USER=\", replace: \"RUN_AS_USER=\\\"{{ nexus_user }}\\\"\" }\n  notify:\n  - restart nexus\n  tags: nexus\n  \n- name: Copy Nexus Service File\n  copy: \n    src: nexus/nexus.service\n    dest: /etc/systemd/system/\n    mode: 0664\n  notify:\n  - reload systemd\n  - restart nexus\n  tags: nexus\n  \n- name: Open Firewall for Nexus\n  firewalld:\n    port: 8081/tcp\n    zone: public\n    permanent: yes\n    immediate: yes\n    state: enabled\n  tags: nexus\n  \n- name: Enable Nexus Service\n  service: \n    name: nexus\n    enabled: true\n  tags: nexus\n  \n  \n# TODO: Need to handle custom configurations"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "372ed8c23454af038000b4731432a6e4ccc5a3aa", "filename": "roles/dns/manage-dns-zones/tasks/named/keys.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Setup Domain Keys configuration\n  vars:\n    named_views: \"{{ dns_data.views }}\"\n  template:\n    src: named/domain-keys.j2\n    dest: /etc/named/named.conf.domain-keys\n    owner: named\n    group: named\n    mode: 0660\n  notify: restart named\n\n- import_tasks: generate_keys.yml\n  run_once: true\n  delegate_to: \"{{ ansible_play_hosts | first }}\"\n\n- name: Setup key files for nsupdate\n  template:\n    src: named/domain-key.j2\n    dest: /var/named/{{ item.item }}.key\n    owner: named\n    group: named\n    mode: 0660\n  with_items:\n    - \"{{ hostvars[ansible_play_hosts | first].nsupdate_keys_captured.results }}\"\n  notify: restart named\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "062f543ad496a6af82f9967d661f1b723bc54798", "filename": "reference-architecture/vmware-ansible/playbooks/roles/docker-storage-setup/defaults/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ndocker_dev: \"/dev/sdb\"\ndocker_vg: \"docker-vol\"\ndocker_data_size: \"95%VG\"\ndocker_dm_basesize: \"3G\"\ncontainer_root_lv_name: \"dockerlv\"\ncontainer_root_lv_mount_path: \"/var/lib/docker\"\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "32838ad94901e927f0a06e2ee43edbc3ab3e9608", "filename": "roles/registrator/meta/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\ngalaxy_info:\n  author: Alberto Garcia\n  description:\n  company: Capgemini\n  # Some suggested licenses:\n  # - BSD (default)\n  # - MIT\n  # - GPLv2\n  # - GPLv3\n  # - Apache\n  # - CC-BY\n  license: license (MIT)\n  min_ansible_version: 1.2\n  #\n  # Below are all platforms currently available. Just uncomment\n  # the ones that apply to your role. If you don't see your\n  # platform on this list, let us know and we'll get it added!\n  #\n  platforms:\n  #- name: EL\n  #  versions:\n  #  - all\n  #  - 5\n  #  - 6\n  #  - 7\n  #- name: GenericUNIX\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Fedora\n  #  versions:\n  #  - all\n  #  - 16\n  #  - 17\n  #  - 18\n  #  - 19\n  #  - 20\n  #- name: SmartOS\n  #  versions:\n  #  - all\n  #  - any\n  #- name: opensuse\n  #  versions:\n  #  - all\n  #  - 12.1\n  #  - 12.2\n  #  - 12.3\n  #  - 13.1\n  #  - 13.2\n  #- name: Amazon\n  #  versions:\n  #  - all\n  #  - 2013.03\n  #  - 2013.09\n  #- name: GenericBSD\n  #  versions:\n  #  - all\n  #  - any\n  #- name: FreeBSD\n  #  versions:\n  #  - all\n  #  - 8.0\n  #  - 8.1\n  #  - 8.2\n  #  - 8.3\n  #  - 8.4\n  #  - 9.0\n  #  - 9.1\n  #  - 9.1\n  #  - 9.2\n  - name: Ubuntu\n    versions:\n  #  - all\n  #  - lucid\n  #  - maverick\n  #  - natty\n  #  - oneiric\n  #  - precise\n  #  - quantal\n  #  - raring\n  #  - saucy\n     - trusty\n  #- name: SLES\n  #  versions:\n  #  - all\n  #  - 10SP3\n  #  - 10SP4\n  #  - 11\n  #  - 11SP1\n  #  - 11SP2\n  #  - 11SP3\n  #- name: GenericLinux\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Debian\n  #  versions:\n  #  - all\n  #  - etch\n  #  - lenny\n  #  - squeeze\n  #  - wheezy\n  #\n  # Below are all categories currently available. Just as with\n  # the platforms above, uncomment those that apply to your role.\n  #\n  categories:\n  - cloud\n  #- cloud:ec2\n  #- cloud:gce\n  #- cloud:rax\n  #- clustering\n  #- database\n  #- database:nosql\n  #- database:sql\n  #- development\n  #- monitoring\n  #- networking\n  #- packaging\n  - system\n  #- web\ndependencies: []\n  # List your role dependencies here, one per line. Only\n  # dependencies available via galaxy should be listed here.\n  # Be sure to remove the '[]' above if you add dependencies\n  # to this list.\n"}, {"commit_sha": "8d4956fcd97d78caa57ee3e5a36e9c44a23ab2a6", "sha": "68d88d87ae0a6ed66d5bde5f57276de72c31e240", "filename": "tasks/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Set distribution and python facts\n  set_fact:\n    _docker_os_dist: \"{{ ansible_distribution }}\"\n    _docker_os_dist_release: \"{{ ansible_distribution_release }}\"\n    _docker_os_dist_major_version: \"{{ ansible_distribution_major_version }}\"\n    _docker_os_arch: \"amd64\"\n    _docker_os_dist_check: yes\n    _docker_python3: \"{{ ansible_python_version is version('3', '>=') }}\"\n  tags: [\"always\"]\n\n- name: Reinterpret distribution facts for Linux Mint 18\n  set_fact:\n    _docker_os_dist: \"Ubuntu\"\n    _docker_os_dist_release: \"xenial\"\n    _docker_os_dist_major_version: 16\n  when:\n    _docker_os_dist == \"Linux Mint\" and\n    _docker_os_dist_major_version | int == 18\n  tags: [\"always\"]\n\n- name: Reinterpret distribution facts for Debian 10 (Buster) due to bug\n  set_fact:\n    _docker_os_dist: \"Debian\"\n    _docker_os_dist_release: \"buster\"\n    _docker_os_dist_major_version: 10\n  when:\n    - _docker_os_dist == \"Debian\"\n    - _docker_os_dist_release == \"buster\" or (ansible_lsb is defined\n        and ansible_lsb.codename is defined and ansible_lsb.codename == \"buster\")\n  tags: [\"always\"]\n\n- name: OS release info\n  raw: cat /etc/os-release\n  check_mode: no\n  changed_when: no\n  register: _docker_os_release_info\n  tags: [\"always\"]\n\n- name: Print OS release information\n  debug:\n    var: _docker_os_release_info\n    verbosity: 1\n  tags: [\"always\"]\n\n- name: Print LSB information\n  debug:\n    var: ansible_lsb\n    verbosity: 1\n  when:\n    - ansible_lsb is defined\n  tags: [\"always\"]\n\n- name: Reinterpret distribution facts for Raspbian\n  set_fact:\n    _docker_os_arch: \"armhf\"\n  when:\n    - _docker_os_release_info.stdout is search('raspbian')\n  tags: [\"always\"]\n\n- name: Reset role variables\n  set_fact:\n    docker_systemd_service_config_tweaks: []\n    docker_service_envs: {}\n  tags: [\"always\"]\n\n- name: Temporary handling of deprecated variable docker_enable_ce_edge (#54)\n  set_fact:\n    docker_channel: edge\n  when:\n    - docker_enable_ce_edge is defined\n    - docker_enable_ce_edge\n  tags: [\"always\"]\n\n- name: Temporary handling of deprecated variable docker_pkg_name\n  set_fact:\n    docker_version: \"{{ docker_pkg_name | regex_replace('^docker-ce.(.+)$', '\\\\1') }}\"\n  when: docker_pkg_name is match('docker-ce' + docker_os_pkg_version_separator[_docker_os_dist])\n  tags: [\"always\"]\n\n- name: Print interpreted distribution information\n  debug:\n    msg: \"distribution={{ _docker_os_dist }}, release={{ _docker_os_dist_release }}, major_version={{ _docker_os_dist_major_version }}\"\n    verbosity: 1\n  tags: [\"always\"]\n\n- name: Compatibility and distribution checks\n  include_tasks: checks.yml\n  when: docker_do_checks | bool\n  tags: [\"always\"]\n\n- name: Install and configure Docker CE\n  block:\n    - name: Network access disabled\n      debug:\n        msg: \"Tasks requiring network access will be skipped!\"\n      when: not docker_network_access | bool\n\n    - name: Remove Docker versions before Docker CE\n      include_tasks: remove-pre-docker-ce.yml\n      when:\n        - docker_remove_pre_ce | bool\n        - docker_network_access | bool\n      tags: [\"install\"]\n\n    - name: Setup Docker package repositories\n      include_tasks: setup-repository.yml\n      tags: [\"install\"]\n\n    - name: Install Docker\n      include_tasks: install-docker.yml\n      when: docker_network_access | bool\n      tags: [\"install\"]\n\n    - name: Configure audit logging\n      include_tasks: setup-audit.yml\n      tags: [\"configure\"]\n\n    - name: Apply workarounds for bugs and/or tweaks\n      include_tasks: bug-tweaks.yml\n      tags: [\"configure\"]\n\n    - name: Configure Docker\n      include_tasks: configure-docker.yml\n      tags: [\"configure\"]\n\n    - name: Postinstall tasks\n      include_tasks: postinstall.yml\n      when: docker_network_access | bool\n      tags: [\"install\", \"postinstall\"]\n  when: not docker_remove | bool\n\n- name: Remove Docker CE and related configuration\n  include_tasks: remove-docker.yml\n  when: docker_remove | bool\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "225dd44b9fc5b3abff7e9c68ff9e91d505cdd5f0", "filename": "ops/playbooks/roles/hpe.openports/README.md", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "Role Name\n=========\n\nA brief description of the role goes here.\n\nRequirements\n------------\n\nAny pre-requisites that may not be covered by Ansible itself or the role should be mentioned here. For instance, if the role uses the EC2 module, it may be a good idea to mention in this section that the boto package is required.\n\nRole Variables\n--------------\n\nA description of the settable variables for this role should go here, including any variables that are in defaults/main.yml, vars/main.yml, and any variables that can/should be set via parameters to the role. Any variables that are read from other roles and/or the global scope (ie. hostvars, group vars, etc.) should be mentioned here as well.\n\nDependencies\n------------\n\nA list of other roles hosted on Galaxy should go here, plus any details in regards to parameters that may need to be set for other roles, or variables that are used from other roles.\n\nExample Playbook\n----------------\n\nIncluding an example of how to use your role (for instance, with variables passed in as parameters) is always nice for users too:\n\n    - hosts: servers\n      roles:\n         - { role: username.rolename, x: 42 }\n\nLicense\n-------\n\nBSD\n\nAuthor Information\n------------------\n\nAn optional section for the role authors to include contact information, or a website (HTML is not allowed).\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "a4821ec576278c0411617cdfe2b2827d26a2d879", "filename": "roles/rhsm-unregister/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- block:\n    - name: Is the host already registered?\n      command: \"subscription-manager list\"\n      register: subscribed\n      ignore_errors: yes\n\n    - name: Unregister host\n      redhat_subscription:\n        state: absent\n      when: \"'Subscribed' in subscribed.stdout\"\n      ignore_errors: yes\n\n    - name: Check for sat config file\n      stat: path=/etc/rhsm/rhsm.conf.kat-backup\n      register: sat_cfg\n\n    - name: Remove satellite configuration if using RH CDN\n      command: \"mv -f /etc/rhsm/rhsm.conf.kat-backup /etc/rhsm/rhsm.conf\"\n      when: rhsm_user is defined and rhsm_user and sat_cfg.stat.exists == True\n      ignore_errors: yes\n\n    - name: Remove satellite SSL if using RH CDN\n      command: \"rpm -e $(rpm -qa katello-ca-consumer*)\"\n      when: rhsm_user is defined and rhsm_user and sat_cfg.stat.exists == True\n      ignore_errors: yes\n\n  when: ansible_distribution == \"RedHat\"\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "68874fd6bb8e1792cd2f40d94bc86a48c01810d2", "filename": "tasks/nexus_purge.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- name: Make sure nexus is stopped\n  debug:\n    msg: \"trigger nexus stop\"\n  changed_when: true\n  notify:\n    - nexus-service-stop\n\n- meta: flush_handlers\n\n- name: get target path of current installed nexus version\n  command: 'readlink {{ nexus_installation_dir }}/nexus-latest'\n  register: nexus_readlink_latest_call\n  failed_when: false\n  changed_when: false\n  check_mode: no\n\n- name: \"Purge Nexus\"\n  file:\n    path: \"{{ item }}\"\n    state: absent\n  with_items:\n    - \"{{ nexus_data_dir }}\"\n    - \"{{ nexus_readlink_latest_call.stdout | default(omit) }}\"\n    - \"{{ nexus_restore_log }}\"\n    - \"{{ nexus_installation_dir }}/nexus-latest\"\n    # - \"{{ nexus_backup_dir }}\" # Optional\n\n- name: \"remove nexus package if present\"\n  package:\n    name: nexus\n    state: absent\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4ddf1ccf0ac40e0ae530ba0b6d63666e52af4e2d", "filename": "roles/dns/manage-dns-zones/tasks/named/process-one-zone.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Set Zone state\n  set_fact:\n    zone_state: \"{{ (zone.state is defined) | ternary(zone.state, view.state | default('present')) }}\"\n\n- name: Remove Zone files if state is 'absent'\n  file:\n    path: \"{{ item }}\"\n    state: absent\n  ignore_errors: True\n  with_items:\n    - \"/var/named/static/{{ view.name + '-' + zone.dns_domain }}.db\"\n  when:\n    - zone_state == 'absent'\n\n- name: Prepare Zone Files\n  vars:\n    zone_dns_domain: \"{{ zone.dns_domain }}\"\n  template:\n    src: named/zone.db.j2\n    dest: \"/var/named/static/{{ view.name + '-' + zone.dns_domain }}.db\"\n    owner: named\n    group: named\n    mode: 0660\n  notify: restart named\n  when:\n    - zone.named is defined\n    - zone.named|bool != False\n    - zone_state == 'present'\n\n- name: Prepare the zone config content\n  vars:\n    zone_type: \"{{ zone.type | default('master') }}\"\n    zone_dns_domain: \"{{ zone.dns_domain }}\"\n    zone_forwarders: \"{{ zone.forwarders | default([]) }}\"\n    view_name: \"{{ view.name }}\"\n  template:\n    src: named/zone-config.j2\n    dest: \"{{ dns_zone_temp_config_dir }}/{{ view.name }}/0002-{{ zone.dns_domain }}.cfg\"\n    owner: named\n    group: named\n    mode: 0660\n  when:\n    - zone.named is defined\n    - zone.named|bool != False\n    - zone_state == 'present'\n\n- name: \"Set flag that a zone was processed\"\n  set_fact:\n    processed_zones: True\n  when:\n    - zone.named is defined\n    - zone.named|bool != False\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "54a257f4ba4e9b23fadb7bfc30bc4ae260af6303", "filename": "roles/elgg/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# Assume we only get here if elgg_install: True\n# Assume mysql is running\n\n- name: Download current version from our site\n  shell: wget {{ iiab_download_url }}/elgg-{{ elgg_version }}.zip -c -P {{ downloads_dir }}\n  args:\n    creates: \"{{ downloads_dir }}/elgg-{{ elgg_version }}.zip\"\n  when: internet_available\n\n- name: Determine if software is already expanded\n  stat:\n    path: \"/opt/elgg-{{ elgg_version }}/index.php\"\n  register: elgg\n\n# use unzip and shell until unarchive works again\n# unarchive: dest=/opt/\n#            src={{ downloads_dir }}/elgg-{{ elgg_version }}.zip\n\n- name: Expand it to our location unless already done\n  shell: \"/usr/bin/unzip -o {{ downloads_dir }}/elgg-{{ elgg_version }}.zip -d /opt\"\n  when: elgg.stat.exists is defined and not elgg.stat.exists\n\n- name: Create a link to the versioned elgg-* folder\n  file:\n    src: \"./elgg-{{ elgg_version }}\"\n    dest: /opt/elgg\n    owner: \"{{ apache_user }}\"\n    group: \"{{ apache_user }}\"\n    state: link\n    force: true\n\n# use template to fix up settings in engine/settings.php with our variables substituted\n# into engine/settings.example.php\n# note this will overwrite any manual settings\n- name: Substitute our parameters in engine/settings.example.php\n  template:\n    src: \"settings.php.j2\"\n    dest: \"/opt/{{ elgg_xx }}/elgg-config/settings.php\"\n    owner: \"{{ apache_user }}\"\n    group: \"{{ apache_user }}\"\n\n# The name of this file changed from 1.9 to 1.10.\n- name: Copy default .htaccess to the root directory of Elgg tree\n  copy:\n    src: \"/opt/{{ elgg_xx }}/vendor/elgg/elgg/install/config/htaccess.dist\"\n    dest: \"/opt/{{ elgg_xx }}/.htaccess\"\n    mode: 0644\n    owner: \"{{ apache_user }}\"\n    group: \"{{ apache_user }}\"\n\n#regexp='^#RewriteBase'\n- name: Modify .htaccess to have RewriteBase as our directory\n  lineinfile:\n    backup: no\n    dest: \"/opt/{{ elgg_xx }}/.htaccess\"\n    state: present\n    insertafter: '^#RewriteBase'\n    line: \"RewriteBase {{ elgg_url }}/\"\n\n- name: Change permissions on engine directory so Apache can write\n  file:\n    path: /opt/elgg/engine/\n    owner: \"{{ apache_user }}\"\n    mode: 0755\n    state: directory\n\n- name: Create an upload directory that Apache can write in or Elgg\n  file:\n    path: \"{{ elgg_upload_path }}\"\n    state: directory\n    owner: \"{{ apache_user }}\"\n\n- name: Change ownership\n  file:\n    path: \"/opt/elgg-{{ elgg_version }}\"\n    owner: \"{{ apache_user }}\"\n    group: \"{{ apache_user }}\"\n    recurse: yes\n    state: directory\n\n- name: Create a MySQL database for Elgg - can be run more than once\n  mysql_db:\n    name: \"{{ dbname }}\"\n  register: create_elgg_database\n\n- name: Create a user to access the Elgg database - can be run more than once\n  mysql_user:\n    name: \"{{ dbuser }}\"\n    host: \"{{ item }}\"\n    password: \"{{ dbpassword }}\"\n    priv: \"{{ dbname }}.*:ALL\"\n  with_items:\n        - 127.0.0.1\n        - ::1\n        - localhost\n\n- name: Create file to load database\n  template:\n    src: \"elggdb.sql.j2\"\n    dest: \"/tmp/elggdb.sql\"\n\n# elggdb.sql obtained with mysqldump --skip-add-drop-table elggdb > elggdb.sql\n# tar up a mysqldump of freshly installed database and use it in the install to avoid the startup\n# form, which worries me a lot. (/var/lib/mysql/elggdb)\n\n- name: Load Elgg database dump\n  mysql_db:\n    name: \"{{ dbname }}\"\n    state: import\n    target: /tmp/elggdb.sql\n  when: create_elgg_database.changed\n\n- name: Remove database dump after load\n  file:\n    name: /tmp/elggdb.sql\n    state: absent\n\n- name: Install config file for Elgg in Apache\n  template:\n    src: elgg.conf\n    dest: \"/etc/{{ apache_config_dir }}/elgg.conf\"\n\n- name: Enable Elgg for debuntu (will already be enabled above for redhat)\n  file:\n    src: /etc/apache2/sites-available/elgg.conf\n    dest: /etc/apache2/sites-enabled/elgg.conf\n    state: link\n  when: elgg_enabled and is_debuntu\n\n- name: Disable Elgg - remove config file for Elgg in Apache (debuntu)\n  file:\n    path: /etc/apache2/sites-enabled/elgg.conf\n    state: absent\n  when: not elgg_enabled and is_debuntu\n\n- name: Disable Elgg - remove config file for Elgg in Apache (redhat)\n  file:\n    dest: \"/etc/{{ apache_config_dir }}/elgg.conf\"\n    state: absent\n  when: not elgg_enabled and is_redhat\n\n- name: Add 'elgg' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: elgg\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: Elgg\n    - option: description\n      value: '\"Elgg is an award-winning social networking engine, delivering the building blocks that enable businesses, schools, universities and associations to create their own fully-featured social networks and applications.\"'\n    - option: path\n      value: /opt/elgg\n    - option: enabled\n      value: \"{{ elgg_enabled }}\"\n\n- name: Restart Apache, so it picks up the new aliases\n  service:\n    name: \"{{ apache_service }}\"\n    state: restarted\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5501299f24fd600076d296da33911f831f17070f", "filename": "roles/ansible/tower/manage-job-templates/tests/inventory/group_vars/tower.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nansible_tower:\n  admin_password: \"admin01\"\n  job_templates:\n  - name: \"Job 1\"\n    description: \"My Job 1\"\n    inventory: \"Inventory1\"\n    project: \"Project1\"\n    playbook: \"playbooks/prep.yml\"\n    credential: \"Cred1\"\n    extra_vars: \"---\\\\nhello: world\\\\n\"\n    ask_variables_on_launch: true\n    permissions:\n      teams:\n      - name: team1\n        role: Execute\n      users:\n      - name: user1\n        role: Execute\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5ced7e6e42e16bd8f751eb5501d9781edd0ccf8f", "filename": "roles/config-redis/tasks/firewall.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Check if firewalld is installed\n  command: systemctl status firewalld\n  register: firewalld_status\n  failed_when: false\n  changed_when: false\n\n- name: Check if iptables is installed\n  command: systemctl status iptables\n  register: iptables_status\n  failed_when: false\n  changed_when: false\n\n- name: Open port in firewalld\n  firewalld:\n    port: \"{{ redis_host_port }}/tcp\"\n    permanent: true\n    state: enabled\n  when: firewalld_status.rc == 0\n  notify:\n  - restart firewalld\n\n- name: Ensure iptables is correctly configured\n  lineinfile:\n    insertafter: \"^-A INPUT .* --dport {{ redis_host_port }} .* ACCEPT\"\n    state: present\n    dest: /etc/sysconfig/iptables\n    regexp: \"^-A INPUT .* --dport {{ redis_port }} .* ACCEPT\"\n    line: \"-A INPUT -p TCP -m state --state NEW -m TCP --dport {{ redis_port }} -j ACCEPT\"\n  when: iptables_status.rc == 0 and firewalld_status.rc != 0\n  notify:\n  - restart iptables\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "43be2395c3120a87143b54f57ffde227dde7c621", "filename": "reference-architecture/azure-ansible/3.6/ansibledeployocp/playbooks/prepare.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  connection: local\n  gather_facts: no\n  become: true\n  roles:\n    - { role: prepare, tags: ['prepare'] }\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "f8850308291b7fa2ea8c3f0019e2096c00207282", "filename": "roles/homepage/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Create home directory\n  file:\n    path: \"{{ doc_root }}/home\"\n    owner: \"{{ apache_user }}\"\n    group: \"{{ apache_user }}\"\n    mode: 0755\n    state: directory\n\n- name: Install admin homepage into apache2\n  template:\n    src: iiab-homepage.conf\n    dest: \"/etc/{{ apache_config_dir }}/iiab-homepage.conf\"\n\n- name: Enable the home page\n  file:\n    src: \"/etc/{{ apache_config_dir }}/iiab-homepage.conf\"\n    dest: /etc/apache2/sites-enabled/iiab-homepage.conf\n    state: link\n  when: is_debuntu\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "9130a32a57b6c9fb6e66a9776f0f7564b70cd368", "filename": "tasks/section_09_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 9.1.1.1 Check that cron conf file exists (check) (Scored)\n    stat: path=/etc/init/cron.conf\n    register: cron_conf_stat\n    tags:\n      - section9\n      - section9.1\n      - section9.1.1\n      - section9.1.1.1\n\n  - name: 9.1.1.2 Enable cron Daemon (Scored)\n    service: >\n        name=cron\n        state=started\n        enabled=yes\n    when: not cron_conf_stat.stat.exists\n    tags:\n      - section9\n      - section9.1\n      - section9.1.1\n      - section9.1.1.2\n\n  - name: 9.1.2 Set User/Group Owner and Permission on /etc/crontab (Scored)\n    file: path='/etc/crontab' owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.2\n\n  - name: 9.1.3 Set User/Group Owner and Permission on /etc/cron.hourly (Scored)\n    file: path=/etc/cron.hourly owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.3\n\n  - name: 9.1.4 Set User/Group Owner and Permission on /etc/cron.daily (Scored)\n    file: path=/etc/cron.daily owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.4\n\n  - name: 9.1.5 Set User/Group Owner and Permission on /etc/cron.weekly(Scored)\n    file: path=/etc/cron.weekly owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.5\n\n  - name: 9.1.6 Set User/Group Owner and Permission on /etc/cron.monthly(Scored)\n    file: path=/etc/cron.monthly owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.6\n\n  - name: 9.1.7 Set User/Group Owner and Permission on /etc/cron.d (Scored)\n    file: path=/etc/cron.d owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.7\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (remove deny) (Scored)\n    file: path={{ item }} state=absent\n    with_items:\n        - /etc/cron.deny\n        - /etc/at.deny\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (stat cron allow) (Scored)\n    stat: path=/etc/cron.allow\n    register: cron_allow_path\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (preparation create cron) (Scored)\n    shell: touch /etc/cron.allow\n    when: cron_allow_path.stat.exists == False\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (create cron allow) (Scored)\n    file: path=/etc/cron.allow owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (stat at allow) (Scored)\n    stat: path=/etc/at.allow\n    register: at_allow_path\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (preparation create at) (Scored)\n    shell: touch /etc/at.allow\n    when: at_allow_path.stat.exists == False\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (create at allow) (Scored)\n    file: path=/etc/at.allow owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.2.1 Set Password Creation Requirement Parameters Using pam_cracklib (install) (Scored)\n    apt: name=libpam-cracklib state=present\n    when: use_pam_cracklib == True\n    tags:\n      - section9\n      - section9.2\n      - section9.2.1\n\n  - name: 9.2.1 Set Password Creation Requirement Parameters Using pam_cracklib (Scored)\n    lineinfile: >\n        dest='/etc/pam.d/common-password'\n        regexp=\"pam_cracklib.so\"\n        line=\"password required pam_cracklib.so retry=3 minlen=14 dcredit=-1 ucredit=-1 ocredit=-1 lcredit=-1\"\n        state=present\n    when: use_pam_cracklib == True\n    tags:\n      - section9\n      - section9.2\n      - section9.2.1\n\n#Note for section 9.2.2:\n#If a user has been locked out because they have reached the maximum consecutive failure count\n#defined by denied= in the pam_tally2.so module, the user can be unlocked by issuing the command\n#/sbin/pam_tally2 -u username --reset\n#This command sets the failed count to 0, effectively unlocking the user\n  - name: 9.2.2 Set Lockout for Failed Password Attempts (Not Scored)\n    lineinfile: >\n        dest='/etc/pam.d/login'\n        regexp='pam_tally2'\n        line=\"auth required pam_tally2.so onerr=fail audit silent deny=5 unlock_time=900\"\n        state=present\n    tags:\n      - section9\n      - section9.2\n      - section9.2.2\n\n  - name: 9.2.3 Limit Password Reuse (Scored)\n    lineinfile: >\n        dest='/etc/pam.d/common-password'\n        regexp='remember=5'\n        line=\"password sufficient pam_unix.so remember=5\"\n        state=present\n    tags:\n      - section9\n      - section9.2\n      - section9.2.3\n\n  - name: 9.3 Check if ssh is installed (check) (Not Scored)\n    stat: path='/etc/ssh/sshd_config'\n    register: ssh_config_file\n    tags:\n      - section9\n      - section9.3\n      - section9.3.1\n\n  - include: section_09_level1_03.yml\n    when: ssh_config_file.stat.exists == True\n\n  - name: 9.4 Restrict root Login to System Console (stat securetty) (Not Scored)\n    stat: path=/etc/securetty\n    register: securetty_file\n    tags:\n      - section9\n      - section9.4\n\n  - name: 9.4 Restrict root Login to System Console (Not Scored)\n    debug: msg='*** Check /etc/securetty for console allowed for root access ***'\n    when: securetty_file.stat.exists == True\n    tags:\n      - section9\n      - section9.4\n\n  - name: 9.5.1 Restrict Access to the su Command (Scored)\n    lineinfile: >\n        dest='/etc/pam.d/su'\n        line='auth            required        pam_wheel.so use_uid'\n        state=present\n    tags:\n      - section9\n      - section9.5\n      - section9.5.1\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "eff0aac9b1c721a1ff0a2a9eeaf6c204978e87ad", "filename": "tasks/delete_repo_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include_tasks: call_script.yml\n  vars:\n    script_name: delete_repo\n    args:\n      name: \"{{ item }}\"\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "dbba6a62ed59bf2832b9720dbb0725fb0821717f", "filename": "roles/elasticsearch/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n- name: \"Running step {{ es_step }}\"\n  include: \"{{ es_step }}.yml\"\n...\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "d5e818f325191f456eff2c6f083dd0ffbac95e11", "filename": "roles/dns/manage-dns-records/tasks/nsupdate/nsupdate-server.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- include_tasks: process-records.yml\n  with_items:\n    - \"{{ dns.1.nsupdate }}\"\n  when:\n    - dns.1.nsupdate is defined\n  loop_control:\n    loop_var: nsupdate\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "76181e5d7e8c712b001665e481f53ffb6a122d94", "filename": "roles/docket/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# handlers file for rocknsm.docket\n- name: Cleanup csr on docket host\n  file:\n    path: \"{{ docket_x509_key }}.csr\"\n    state: absent\n  when: inventory_hostname in groups['docket'] | bool\n\n- name: Cleanup csr on sensor hosts\n  file:\n    path: \"{{ steno_certs_dir }}/{{ hostvars[item].inventory_hostname }}.csr\"\n    state: absent\n  loop: \"{{ groups['docket'] }}\"\n  when: inventory_hostname in groups['stenographer'] | bool\n\n- name: Restart redis\n  service:\n    name: redis\n    state: restarted\n  when: rock_services | selectattr('name', 'equalto', 'docket') | map(attribute='enabled') | bool\n\n- name: Seed random key\n  lineinfile:\n    path: /etc/docket/prod.yml\n    regexp: 'XX_NOT_A_SECRET_XX'\n    line: \"SECRET_KEY: {{ docket_secret }}\"\n    state: present\n\n- name: Restart docket celery services\n  service:\n    name: \"{{ item }}\"\n    state: restarted\n  loop:\n    - docket-celery-io\n    - docket-celery-query\n  when: rock_services | selectattr('name', 'equalto', 'docket') | map(attribute='enabled') | bool\n\n- name: Restart docket uwsgi\n  service:\n    name: docket\n    state: restarted\n  when: rock_services | selectattr('name', 'equalto', 'docket') | map(attribute='enabled') | bool\n\n- name: Restart lighttpd\n  service:\n    name: lighttpd\n    state: restarted\n  when: rock_services | selectattr('name', 'equalto', 'docket') | map(attribute='enabled') | bool\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "e6452979500d46f533348ade6889f9eabd4e0e77", "filename": "roles/dockerbench/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for docker bench\n- name: checkout docker bench git repo\n  git: \n    repo: \"{{ dockerbench_repo }}\"\n    dest: \"{{ dockerbench_dest }}\"\n    accept_hostkey: true\n  when: dockerbench_run_test\n\n- name: install docker bench threshold script\n  sudo: yes\n  template:\n    src: docker-bench-warn.sh.j2\n    dest: /usr/local/bin/docker-bench-warn.sh\n    mode: 0755\n  when: dockerbench_run_test\n\n- name: run docker bench script\n  command: /usr/local/bin/docker-bench-warn.sh\n  sudo: yes\n  args:\n    chdir: \"{{ dockerbench_dest }}\"\n  when: dockerbench_run_test\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "6d7266b35ac0526451c1520ef820a4173560a0f6", "filename": "roles/zookeeper/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for zookeeper\n- name: restart zookeeper\n  service:\n    name: zookeeper\n    state: restarted\n  sudo: yes\n\n- name: start zookeeper\n  service:\n    name: zookeeper\n    state: started\n  sudo: yes\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "d81f61303e8261800493632a8cec60038c82012c", "filename": "tasks/configure-Debian.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n\nhttpd_package_name: \"apache2\"\nhttpd_config_dir: \"/etc/{{ httpd_package_name }}/sites-enabled\"\ncertificate_file_dest: \"/etc/ssl/certs\"\ncertificate_key_dest: \"/etc/ssl/private\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "9372008fd3ffbb0d4d6b76894d685c523dd45b44", "filename": "roles/kolibri/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Create Linux user {{ kolibri_user }} and add it to groups {{ apache_user }}, disk\n  user:\n    name: \"{{ kolibri_user }}\"\n    groups:\n      - \"{{ apache_user }}\"\n      - disk\n    state: present\n    shell: /bin/false\n    system: yes\n    create_home: no\n\n- name: Create /library/kolibri to store data and configuration files\n  file:\n    path: \"{{ item }}\"\n    owner: \"{{ kolibri_user }}\"\n    group: \"{{ apache_user }}\"\n    mode: 0755\n    state: directory\n  with_items:\n    - \"{{ kolibri_home }}\"\n\n- name: Install kolibri using pip on all OS's\n  pip:\n    name: kolibri\n    state: latest\n    extra_args: --no-cache-dir\n  when: internet_available\n\n- name: Create kolibri systemd service unit file\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    mode: \"{{ item.mode }}\"\n    owner: root\n    group: root\n  with_items:\n    - { src: 'kolibri.service.j2', dest: '/etc/systemd/system/kolibri.service', mode: '0644' }\n\n- name: Ask systemd to reread unit files (daemon-reload)\n  systemd:\n    daemon_reload: yes\n\n- name: Set kolibri default language\n  shell: export KOLIBRI_HOME=\"{{ kolibri_home }}\" && \"{{ kolibri_exec_path }}\" language setdefault \"{{ kolibri_language }}\"\n  ignore_errors: yes\n  when: kolibri_provision\n\n- name: Create kolibri default facility name, admin account and language\n  shell: >\n    export KOLIBRI_HOME=\"{{ kolibri_home }}\" &&\n    \"{{ kolibri_exec_path }}\" manage provisiondevice --facility \"{{ kolibri_facility }}\"\n    --superusername \"{{ kolibri_admin_user }}\" --superuserpassword \"{{ kolibri_admin_password }}\"\n    --preset \"{{ kolibri_preset }}\" --language_id \"{{ kolibri_language }}\" --verbosity 0 --noinput\n  ignore_errors: yes\n  when: kolibri_provision\n\n- name: Change /library/kolibri directory permissions\n  file:\n    path: \"{{ kolibri_home }}\"\n    owner: \"{{ kolibri_user }}\"\n    group: \"{{ apache_user }}\"\n    recurse: yes\n\n- name: Enable kolibri service\n  service:\n    name: kolibri\n    enabled: yes\n    state: restarted\n  when: kolibri_enabled\n\n- name: Disable kolibri service\n  service:\n    name: kolibri\n    enabled: no\n    state: stopped\n  when: not kolibri_enabled\n\n- name: Add 'kolibri' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: kolibri\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: kolibri\n    - option: description\n      value: '\"Kolibri is an open-source educational platform specially designed to provide offline access to a wide range of quality, openly licensed educational contents in low-resource contexts like rural schools, refugee camps, orphanages, and also in non-formal school programs.\"'\n    - option: kolibri_url\n      value: \"{{ kolibri_url }}\"\n    - option: kolibri_path\n      value: \"{{ kolibri_path }}\"\n    - option: kolibri_port\n      value: \"{{ kolibri_http_port }}\"\n    - option: enabled\n      value: \"{{ kolibri_enabled }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "357bbd658bf01bde31a66581240190b523fb535e", "filename": "reference-architecture/vmware-ansible/playbooks/ocp-upgrade.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  connection: local\n  gather_facts: yes\n  become: no\n  vars_files:\n  - vars/main.yaml\n  roles:\n  # Group systems\n  - instance-groups\n\n- include: minor-update.yaml\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "f45373e4758ce617f091824aec4e41d23825844a", "filename": "playbooks/openshift/start-cluster.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- import_playbook: start-aws.yml\n  when:\n  - hosting_infrastructure == 'aws'\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "6f1e354e3679e6cbead2070cd118e90e62ddc274", "filename": "playbooks/roles/sensor-common/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- include_tasks: \"{{ method }}.yml\"\n...\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "65dd2b71e61c2f0066c2d553390f87d3ba042fd2", "filename": "playbooks/gce/openshift-cluster/tasks/launch_instances.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Launch instance(s)\n  gce:\n    instance_names: \"{{ instances|join(',') }}\"\n    machine_type: \"{{ gce_machine_type | default(deployment_vars[deployment_type].machine_type, true) }}\"\n    image: \"{{ gce_machine_image | default(deployment_vars[deployment_type].image, true) }}\"\n    service_account_email: \"{{ lookup('env', 'gce_service_account_email_address') }}\"\n    pem_file: \"{{ lookup('env', 'gce_service_account_pem_file_path') }}\"\n    project_id: \"{{ lookup('env', 'gce_project_id') }}\"\n    zone: \"{{ lookup('env', 'zone') }}\"\n    network: \"{{ lookup('env', 'network') }}\"\n    subnetwork: \"{{ lookup('env', 'subnetwork') | default(omit, True) }}\"\n    # unsupported in 1.9.+\n    #service_account_permissions: \"datastore,logging-write\"\n    tags:\n      - created-by-{{ lookup('env', 'LOGNAME') | regex_replace('[^a-z0-9]+', '') | default(cluster, true) }}\n      - environment-{{ cluster_env }}\n      - clusterid-{{ cluster_id }}\n      - host-type-{{ type }}\n      - sub-host-type-{{ g_sub_host_type }}\n    metadata:\n      startup-script: |\n        #!/bin/bash\n        echo \"Defaults:{{ deployment_vars[deployment_type].ssh_user }} !requiretty\" > /etc/sudoers.d/99-{{ deployment_vars[deployment_type].ssh_user }}\n\n  when: instances |length > 0\n  register: gce\n\n- set_fact:\n    node_label:\n      # There doesn't seem to be a way to get the region directly, so parse it out of the zone.\n      region: \"{{ gce.zone | regex_replace('^(.*)-.*$', '\\\\\\\\1') }}\"\n      type: \"{{ g_sub_host_type }}\"\n  when: instances |length > 0 and type == \"node\"\n\n- set_fact:\n    node_label:\n      # There doesn't seem to be a way to get the region directly, so parse it out of the zone.\n      region: \"{{ gce.zone | regex_replace('^(.*)-.*$', '\\\\\\\\1') }}\"\n      type: \"{{ type }}\"\n  when: instances |length > 0 and type != \"node\"\n\n- name: Add new instances to groups and set variables needed\n  add_host:\n    hostname: \"{{ item.name }}\"\n    ansible_ssh_host: \"{{ item.public_ip }}\"\n    ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n    ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    groups: \"{{ item.tags | oo_prepend_strings_in_list('tag_') | join(',') }}\"\n    gce_public_ip: \"{{ item.public_ip }}\"\n    gce_private_ip: \"{{ item.private_ip }}\"\n    openshift_node_labels: \"{{ node_label }}\"\n  with_items: \"{{ gce.instance_data | default([], true) }}\"\n\n- name: Wait for ssh\n  wait_for: port=22 host={{ item.public_ip }}\n  with_items: \"{{ gce.instance_data | default([], true) }}\"\n\n- name: Wait for user setup\n  command: \"ssh -o StrictHostKeyChecking=no -o PasswordAuthentication=no -o ConnectTimeout=10 -o UserKnownHostsFile=/dev/null {{ hostvars[item.name].ansible_ssh_user }}@{{ item.public_ip }} echo {{ hostvars[item.name].ansible_ssh_user }} user is setup\"\n  register: result\n  until: result.rc == 0\n  retries: 30\n  delay: 5\n  with_items: \"{{ gce.instance_data | default([], true) }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5fa8bf2e44b33b95983184cccc820e97ba270e16", "filename": "roles/dns/config-dns-server/tests/inventory/group_vars/forward-server.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nnamed_config:\n  recursion: 'yes'\n  dnssec_enable: 'yes'\n  dnssec_validation: 'yes'\n  dnssec_lookaside: 'no'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "40b1ab4ceaeb88d91cae0312c0ee5cf04032238b", "filename": "playbooks/provision-satellite-server/configure-satellite-server.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: satellite-server\n  pre_tasks:\n  - import_tasks: generate-lvm-list.yml\n  roles:\n  - role: config-lvm\n  - role: config-satellite\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "3dce63848495ee05a5d2826c1f17e1c2606aa259", "filename": "playbooks/common.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- name: Check the system\n  raw: uname -a\n  register: OS\n\n- name: Ubuntu pre-tasks\n  include: ubuntu.yml\n  when: '\"Ubuntu\" in OS.stdout'\n\n- name: FreeBSD pre-tasks\n  include: freebsd.yml\n  when: '\"FreeBSD\" in OS.stdout'\n\n- name: Ensure the algo ssh key exist on the server\n  authorized_key:\n    user: \"{{ ansible_ssh_user }}\"\n    state: present\n    key: \"{{ lookup('file', '{{ SSH_keys.public }}') }}\"\n  tags: [ 'always' ]\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "0582f42582c529fe97969da35c886a7fcd636758", "filename": "reference-architecture/rhv-ansible/playbooks/roles/gdeployer/handlers/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: run gdeploy\n  command: '/usr/bin/gdeploy -c /root/gdeploy.conf'\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "9f09f367ebbde05080c9a514dcfc006c01ddb98b", "filename": "roles/stenographer/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# handlers file for stenographer\n\n- name: Start stenographer service\n  service:\n    name: stenographer\n    state: \"{{ 'started' if rock_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | bool else 'stopped' }}\"\n\n- name: Start stenographer per interface\n  service:\n    name: \"stenographer@{{ item }}\"\n    state: \"{{ 'started' if enable_stenographer else 'stopped' }}\"\n  loop: \"{{ stenographer_monitor_interfaces }}\"\n\n- name: Restart stenographer per interface\n  service:\n    name: \"stenographer@{{ item }}\"\n    state: restarted\n  loop: \"{{ stenographer_monitor_interfaces }}\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "eebbe788f852d18843b7d90d25464f898841ea3b", "filename": "playbooks/roles/docket/tasks/install.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: docket | install rocknsm repo\n  yum_repository:\n    file: rocknsm\n    name: \"{{ item.name }}\"\n    enabled: yes\n    description: \"{{ item.name }}\"\n    baseurl: \"{{ item.baseurl }}\"\n    repo_gpgcheck: 1\n    gpgcheck: \"{{ item.gpgcheck }}\"\n    gpgkey:\n      - file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-pkgcloud-2_1\n      - file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-2\n    sslverify: 1\n    sslcacert: /etc/pki/tls/certs/ca-bundle.crt\n    metadata_expire: 300\n    state: present\n  with_items:\n    - { name: \"rocknsm_2_1\", gpgcheck: yes, baseurl: \"https://packagecloud.io/rocknsm/2_1/el/7/$basearch\" }\n    - { name: \"rocknsm_2_1-source\", gpgcheck: no, baseurl: \"https://packagecloud.io/rocknsm/2_1/el/7/SRPMS\" }\n  when: \"{{ inventory_hostname in groups['docket'] and docket_install == 'yumrepo' }}\"\n\n- name: docket | install packages\n  yum:\n    name:\n      - docket\n      - lighttpd\n    state: present\n  when: inventory_hostname in groups['docket']\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "c9fb0d8ea0204c98015b30e6b9342481f7a7cf45", "filename": "roles/2-common/tasks/xo.yml", "repository": "iiab/iiab", "decoded_content": "- name: XO Server specific tasks\n  command: echo Starting XO.yml\n\n- name: Disable sleep\n  command: touch /etc/powerd/flags/inhibit-suspend\n  args:\n    creates: /etc/powerd/flags/inhibit-suspend\n\n- name: Disable sleep on lid closing\n  lineinfile:\n    dest: /etc/powerd/powerd.conf\n    regexp: '^config_SLEEP_WHEN_LID_CLOSED'\n    line: 'config_SLEEP_WHEN_LID_CLOSED=\"no\"'\n    state: present\n    backup: yes\n\n- name: Keep yum cache\n  ini_file:\n    dest: /etc/yum.conf\n    section: main\n    option: keepcache\n    value: 1\n  when: not installing\n\n- name: Keep docs when installing packages\n  lineinfile:\n    backup: yes\n    dest: /etc/rpm/macros.imgcreate\n    regexp: '^%_excludedocs'\n    state: absent\n\n- name: Pre-install packages\n  package:\n    name: \"{{ item }}\"\n    state: latest\n  with_items:\n    - usbmount\n    - man\n    - man-db\n    - man-pages\n\n- name: Re-install packages\n  shell: yum -y reinstall sed libidn grep which util-linux wget gnupg2 groff gnash yum\n  when: not osbuilder is defined\n\n- name: Configure networkmanager plugin\n  ini_file:\n    dest: /etc/NetworkManager/NetworkManager.conf\n    section: main\n    option: plugins\n    value: ifcfg-rh,keyfile\n\n- name: Check for modem config file\n  stat:\n    path: /etc/NetworkManager/system-connections/\"Sugar Modem Connection\"\n  register: config\n\n- name: Change failure and interval settings for modem connection\n  ini_file:\n    dest: /etc/NetworkManager/system-connections/\"Sugar Modem Connection\"\n    section: ppp\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n    backup: yes\n    mode: 0600\n  with_items:\n    - { option: 'lcp-echo-failure', value: '5' }\n    - { option: 'lcp-echo-interval', value: '30' }\n  when: config.stat.exists\n\n- name: Create bigger rwtab\n  lineinfile:\n    backup: yes\n    dest: /etc/sysconfig/readonly-root\n    regexp: '^RW_OPTIONS'\n    line: 'RW_OPTIONS=\"-o size=4M -o nr_inodes=2048\"'\n    state: present\n\n- name: Remove dhcpd entry from /etc/rwtab\n  lineinfile:\n    backup: yes\n    dest: /etc/rwtab\n    regexp: '^empty.*/var/lib/dhcpd'\n    state: absent\n\n- name: Remove php entry from /etc/rwtab\n  lineinfile:\n    backup: yes\n    dest: /etc/rwtab\n    regexp: '^empty.*/var/lib/php'\n    state: absent\n\n- name: Persist /etc/hosts between reboots\n  lineinfile:\n    backup: yes\n    dest: /etc/statetab.d/olpc\n    regexp: '^/etc/hosts'\n    state: absent\n\n\n- name: Disable /var/log tmpfs\n  lineinfile:\n    backup: yes\n    dest: /etc/fstab\n    regexp: '^varlog.*'\n    state: absent\n\n- name: Enlarge the /tmp directory so that url_get does not error out\n  lineinfile:\n    backup: yes\n    dest: /etc/fstab\n    regexp: '^/tmp*'\n    line: '/tmp            /tmp            tmpfs         rw,size=600m 0 0'\n\n- name: Disable graphical login\n  file:\n    src: /lib/systemd/system/multi-user.target\n    dest: /etc/systemd/system/default.target\n    state: link\n  register: disabled_login\n\n- name: Remove custom profile settings\n  file:\n    path: /etc/profile.d/zzz_olpc.sh\n    state: absent\n\n- name: Download substitute software for i386 on FC18 XO1.5\n  get_url:\n    url: \"{{ iiab_download_url }}/{{ item }}\"\n    dest: \"{{ downloads_dir }}/{{ item }}\"\n    timeout: \"{{ download_timeout }}\"\n  with_items:\n    - hostapd_8188_i386\n  when: wifi_id == \"tplink_WM725M\" and xo_model == \"XO-1.5\" and internet_available\n  tags:\n    - xo\n\n- name: Put the substitute in place\n  copy:\n    src: \"{{ downloads_dir }}/hostapd_8188_i386\"\n    dest: /usr/sbin/hostapd\n    backup: yes\n    mode: 0775\n    owner: root\n    group: root\n  when: wifi_id == \"tplink_WM725M\" and xo_model == \"XO-1.5\"\n\n- name: Reboot system\n  command: /sbin/reboot\n  when: disabled_login.changed and not installing\n  ignore_errors: yes\n  async: 300\n  poll: 120\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c11ae1736e37e38c0a87b72ef9c9b24f7a0ef561", "filename": "roles/keepalived/tasks/keepalived-config.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Populate the keepalived.conf file'\n  template:\n    src: keepalived_conf.j2\n    dest: '/etc/keepalived/keepalived.conf'\n  notify: 'restart keepalived'\n\n  \n"}, {"commit_sha": "1818facd0a58a2b42203a403130b71825b960653", "sha": "1b42a49f96b8fea2292faca248f222d1c00b4b65", "filename": "tasks/section_09_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 9.1.1.1 Check that cron conf file exists (check) (Scored)\n    stat: path=/etc/init/cron.conf\n    register: cron_conf_stat\n    tags:\n      - section9\n      - section9.1\n      - section9.1.1\n      - section9.1.1.1\n\n  - name: 9.1.1.2 Enable cron Daemon (Scored)\n    service: >\n        name=cron\n        state=started\n        enabled=yes\n    when: not cron_conf_stat.stat.exists\n    tags:\n      - section9\n      - section9.1\n      - section9.1.1\n      - section9.1.1.2\n\n  - name: 9.1.2 Set User/Group Owner and Permission on /etc/crontab (Scored)\n    file: path='/etc/crontab' owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.2\n\n  - name: 9.1.3 Set User/Group Owner and Permission on /etc/cron.hourly (Scored)\n    file: path=/etc/cron.hourly owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.3\n\n  - name: 9.1.4 Set User/Group Owner and Permission on /etc/cron.daily (Scored)\n    file: path=/etc/cron.daily owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.4\n\n  - name: 9.1.5 Set User/Group Owner and Permission on /etc/cron.weekly(Scored)\n    file: path=/etc/cron.weekly owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.5\n\n  - name: 9.1.6 Set User/Group Owner and Permission on /etc/cron.monthly(Scored)\n    file: path=/etc/cron.monthly owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.6\n\n  - name: 9.1.7 Set User/Group Owner and Permission on /etc/cron.d (Scored)\n    file: path=/etc/cron.d owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.7\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (remove deny) (Scored)\n    file: path={{ item }} state=absent\n    with_items:\n        - /etc/cron.deny\n        - /etc/at.deny\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (stat cron allow) (Scored)\n    stat: path=/etc/cron.allow\n    register: cron_allow_path\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (preparation create cron) (Scored)\n    shell: touch /etc/cron.allow\n    when: cron_allow_path.stat.exists == False\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (create cron allow) (Scored)\n    file: path=/etc/cron.allow owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (stat at allow) (Scored)\n    stat: path=/etc/at.allow\n    register: at_allow_path\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (preparation create at) (Scored)\n    shell: touch /etc/at.allow\n    when: at_allow_path.stat.exists == False\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (create at allow) (Scored)\n    file: path=/etc/at.allow owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.2.1 Set Password Creation Requirement Parameters Using pam_cracklib (install) (Scored)\n    apt: name=libpam-cracklib state=present\n    when: use_pam_cracklib == True\n    tags:\n      - section9\n      - section9.2\n      - section9.2.1\n\n  - name: 9.2.1 Set Password Creation Requirement Parameters Using pam_cracklib (Scored)\n    lineinfile: >\n        dest='/etc/pam.d/common-password'\n        regexp=\"pam_cracklib.so\"\n        line=\"password required pam_cracklib.so retry=3 minlen=14 dcredit=-1 ucredit=-1 ocredit=-1 lcredit=-1\"\n        state=present\n    when: use_pam_cracklib == True\n    tags:\n      - section9\n      - section9.2\n      - section9.2.1\n\n#Note for section 9.2.2:\n#If a user has been locked out because they have reached the maximum consecutive failure count\n#defined by denied= in the pam_tally2.so module, the user can be unlocked by issuing the command\n#/sbin/pam_tally2 -u username --reset\n#This command sets the failed count to 0, effectively unlocking the user\n  - name: 9.2.2 Set Lockout for Failed Password Attempts (Not Scored)\n    lineinfile: >\n        dest='/etc/pam.d/login'\n        regexp='pam_tally2'\n        line=\"auth required pam_tally2.so onerr=fail audit silent deny=5 unlock_time=900\"\n        state=present\n    tags:\n      - section9\n      - section9.2\n      - section9.2.2\n\n  - name: 9.2.3 Limit Password Reuse (Scored)\n    lineinfile: >\n        dest='/etc/pam.d/common-password'\n        regexp='remember=5'\n        line=\"password sufficient pam_unix.so remember=5\"\n        state=present\n    tags:\n      - section9\n      - section9.2\n      - section9.2.3\n\n  - name: 9.3 Check if ssh is installed (check) (Not Scored)\n    stat: path='/etc/ssh/sshd_config'\n    register: ssh_config_file\n    tags:\n      - section9\n      - section9.3\n      - section9.3.1\n\n\n  - name: 9.3.1 Set SSH Protocol to 2 (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^Protocol'\n        state=present\n        line='Protocol 2'\n    when: ssh_config_file.stat.exists == True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.1\n\n  - name: 9.3.2 Set LogLevel to INFO (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^LogLevel'\n        state=present\n        line='LogLevel INFO'\n    when: ssh_config_file.stat.exists == True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.2\n\n  - name: 9.3.3 Set Permissions on /etc/ssh/sshd_config (Scored)\n    file: path='/etc/ssh/sshd_config' owner=root group=root mode=600\n    when: ssh_config_file.stat.exists == True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.3\n\n#Regroups sections 9.3.4 9.3.7 9.3.8 9.3.9 9.3.10\n  - name: 9.3.{4,7,8,9,10} Disable some SSH options (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^{{ item }}'\n        line='{{ item }} no'\n        state=present\n    with_items:\n      - X11Forwarding\n      - HostbasedAuthentication\n      - PermitRootLogin\n      - PermitEmptyPasswords\n      - PermitUserEnvironment\n    tags:\n      - section9\n      - section9.3\n      - section9.3.4\n      - section9.3.7\n      - section9.3.8\n      - section9.3.9\n      - section9.3.10\n\n  - name: 9.3.5 Set SSH MaxAuthTries to 4 or Less (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^MaxAuthTries'\n        line='MaxAuthTries 4'\n        state=present\n    tags:\n      - section9\n      - section9.3\n      - section9.3.5\n\n  - name: 9.3.6 Set SSH IgnoreRhosts to Yes (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^IgnoreRhosts'\n        line='IgnoreRhosts yes'\n        state=present\n    tags:\n      - section9\n      - section9.3\n      - section9.3.6\n\n  - name: 9.3.11 Use Only Approved Cipher in Counter Mode (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^Ciphers'\n        line='Ciphers aes128-ctr,aes192-ctr,aes256-ctr'\n        state=present\n    tags:\n      - section9\n      - section9.3\n      - section9.3.11\n\n  - name: 9.3.12.1 Set Idle Timeout Interval for User Login (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^ClientAliveInterval'\n        line='ClientAliveInterval 300'\n        state=present\n    tags:\n      - section9\n      - section9.3\n      - section9.3.12\n      - section9.3.12.1\n\n  - name: 9.3.12.2 Set Idle Timeout Interval for User Login (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^ClientAliveCountMax'\n        line='ClientAliveCountMax 0'\n        state=present\n    tags:\n      - section9\n      - section9.3\n      - section9.3.12\n      - section9.3.12.2\n\n  - name: 9.3.13.1 Limit Access via SSH (Scored)\n    shell: grep AllowUsers /etc/ssh/sshd_config\n    register: allow_rc\n    failed_when: allow_rc.rc == 1\n    changed_when: False\n    ignore_errors: True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.13\n      - section9.3.13.1\n\n  - name: 9.3.13.2 Limit Access via SSH (Scored)\n    shell: grep AllowGroups /etc/ssh/sshd_config\n    register: allow_rc\n    failed_when: allow_rc.rc == 1\n    changed_when: False\n    ignore_errors: True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.13\n      - section9.3.13.2\n\n  - name: 9.3.13.3 Limit Access via SSH (Scored)\n    shell: grep DenyUsers /etc/ssh/sshd_config\n    register: allow_rc\n    failed_when: allow_rc.rc == 1\n    changed_when: False\n    ignore_errors: True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.13\n      - section9.3.13.3\n\n  - name: 9.3.13.4 Limit Access via SSH (Scored)\n    shell: grep DenyGroups /etc/ssh/sshd_config\n    register: allow_rc\n    failed_when: allow_rc.rc == 1\n    changed_when: False\n    ignore_errors: True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.13\n      - section9.3.13.4\n\n  - name: 9.3.14 Set SSH Banner (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^Banner'\n        line='Banner /etc/issue.net'\n        state=present\n    tags:\n      - section9\n      - section9.3\n      - section9.3.14\n\n  - name: 9.4 Restrict root Login to System Console (stat securetty) (Not Scored)\n    stat: path=/etc/securetty\n    register: securetty_file\n    tags:\n      - section9\n      - section9.4\n\n  - name: 9.4 Restrict root Login to System Console (Not Scored)\n    debug: msg='*** Check /etc/securetty for console allowed for root access ***'\n    when: securetty_file.stat.exists == True\n    tags:\n      - section9\n      - section9.4\n\n  - name: 9.5.1 Restrict Access to the su Command (Scored)\n    lineinfile: >\n        dest='/etc/pam.d/su'\n        line='auth            required        pam_wheel.so use_uid'\n        state=present\n    tags:\n      - section9\n      - section9.5\n      - section9.5.1\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "0ea32f381be058633f8abb0360a4e279d79276b0", "filename": "tasks/create_repo_rubygems_hosted_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_rubygems_hosted\n    args: \"{{ _nexus_repos_rubygems_defaults|combine(item) }}\""}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "258334a6b0ebe0d7f278b323c91d8faccf863635", "filename": "roles/openstack-stack/tasks/cleanup.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n\n- name: cleanup temp files\n  file:\n    path: \"{{ stack_template_pre.path }}\"\n    state: absent\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b64617915c512acf09b65bdf1fcc198f2919da9d", "filename": "playbooks/manage-lb/lb-vms.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Generate and activate the HAproxy configuration'\n  hosts: lb_vms\n  tasks:\n  - import_role:\n      name: load-balancers/manage-haproxy\n      tasks_from: install\n    tags:\n    - 'never'\n    - 'install'\n  - import_role:\n      name: load-balancers/manage-haproxy\n      tasks_from: generate-config\n    tags:\n    - 'always'\n  - import_role:\n      name: load-balancers/manage-haproxy\n      tasks_from: activate-config\n    tags:\n    - 'always'\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "cff353bc456d82940d31e3ab2e07084c72f3261a", "filename": "roles/manage-jira/tasks/create_permission_scheme.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Create Jira Permission Scheme\n  uri:\n    url: \"{{ atlassian.jira.url }}/rest/api/2/permissionscheme\"\n    method: POST\n    user: \"{{ atlassian.jira.username }}\"\n    password: \"{{ atlassian.jira.password }}\"\n    return_content: yes\n    force_basic_auth: yes\n    body_format: json\n    header:\n      - Accept: 'application/json'\n      - Content-Type: 'application/json'\n    body: \"{{ lookup('template','permissionScheme.json.j2') }}\"\n    status_code: 201\n  register: permissionScheme\n\n- name: Set fact for Permission Scheme ID\n  set_fact:\n    PermissionScheme: \"{{ permissionScheme.json.id }}\"\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "e0a173462fdced388f07754319f21ad3120e63d0", "filename": "roles/ovirt-engine-cleanup/defaults/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_engine_dwh: True\novirt_engine_type: 'ovirt-engine'\novirt_engine_version: '4.0'\n\novirt_engine_db_host: 'localhost'\novirt_engine_db_port: 5432\novirt_engine_db_name: 'engine'\novirt_engine_db_user: 'engine'\novirt_engine_db_password: 'AqbXg4dpkbcVRZwPbY8WOR'\n\novirt_engine_dwh_db_host: 'localhost'\novirt_engine_dwh_db_port: 5432\novirt_engine_dwh_db_name: 'ovirt_engine_history'\novirt_engine_dwh_db_user: 'ovirt_engine_history'\novirt_engine_dwh_db_password: '37xmBKECANQGm0z3SfylMp'"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f9f1dd048c26fa0f1c20a9fbf24fd82c6595f274", "filename": "roles/user-management/list-users-by-group/tests/inventory/group_vars/manage-users-host.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nusers:\n- user_name: user1\n  first_name: user\n  last_name: one\n  email: user1@example.com\n- user_name: user2\n  first_name: user\n  last_name: two\n  email: user2@example.com\n- user_name: user3\n  first_name: user\n  last_name: three\n  email: user3@example.com\n- user_name: user4\n  first_name: user\n  last_name: four\n  email: user4@example.com\n- user_name: user5\n  first_name: user\n  last_name: five\n  email: user5@example.com\n\nuser_groups:\n- name: group1\n  members:\n  - user1\n  - user2\n  - user3\n- name: group2\n  members:\n  - user2\n  - user4\n  - user5\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "5cfb310d053674a796f2a4f5b3cf1f8a5f8b1ad5", "filename": "roles/calibre/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# 1. INSTALL THE LATEST CALIBRE 3.X+ (calibre, calibredb, calibre-server etc) ON ALL OS'S\n\n- name: Check if /usr/bin/calibre exists\n  stat:\n    path: \"/usr/bin/calibre\"\n  register: calib_executable\n\n- name: Install Calibre via OS's package installer (IF /usr/bin/calibre MISSING)\n  package:\n    name: \"{{ item }}\"\n    state: latest\n  with_items:\n    - calibre\n    - calibre-bin\n  when: internet_available and (not calib_executable.stat.exists)\n\n- name: Install Calibre experimental .debs IF calibre_via_debs (AND /usr/bin/calibre WAS MISSING)\n  include_tasks: debs.yml\n  when: calibre_via_debs and (not calib_executable.stat.exists)\n\n- name: Install Calibre via calibre-installer.py IF calibre_via_python (AND /usr/bin/calibre WAS MISSING)\n  include_tasks: py-installer.yml\n  when: calibre_via_python and (not calib_executable.stat.exists)\n\n# SEE calibre_via_python's value vars/default_vars.yml, vars/ubuntu-18.yml &\n# vars/raspbian-9.yml: try to AVOID Python installer on Raspbian since its\n# .deb's (http://raspbian.raspberrypi.org/raspbian/pool/main/c/calibre/)\n# are updated within about 10 days of Calibre's quasi-monthly releases!\n#\n# BUT IF ABSOLUTELY NEC: (SEE roles/calibre/tasks/debs.yml)\n# - run testing branch for RPi: scripts/calibre-install-latest-rpi.sh\n# - run testing branch for Ubuntu 16.04: scripts/calibre-install-latest.sh\n# - run unstable branch for Debian etc: scripts/calibre-install-unstable.sh\n\n- name: Create calibre-serve.service and calibre.conf (IF /usr/bin/calibre WAS MISSING)\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: root\n    group: root\n    mode: \"{{ item.mode }}\"\n    backup: no\n# register: calibre_config\n  with_items:\n    - { src: 'calibre-serve.service.j2', dest: '/etc/systemd/system/calibre-serve.service', mode: '0644'}\n    - { src: 'calibre.conf', dest: '/etc/{{ apache_config_dir }}', mode: '0644'}\n  when: (not calib_executable.stat.exists)\n\n- name: Force systemd to reread configs (IF /usr/bin/calibre WAS MISSING)\n  systemd:\n    daemon_reload: yes\n  when: (not calib_executable.stat.exists)\n# when: calibre_config.changed\n\n# 2. STOP CALIBRE SERVICE IF IT EXISTS (REQUIRED FOR DB ACTIVITY...AND IF not calibre_enabled)\n\n#- name: Check if Calibre systemd service exists\n#  stat:\n#    path: /etc/systemd/system/calibre-serve.service\n#  register: calibre_svc\n\n- name: Stop Calibre service -- calibre-server by Kovid Goyal\n# systemd:\n  service:\n    name: calibre-serve\n    state: stopped\n    #enabled: no\n# register: command_result    # gist.github.com/tyrells/0a79681de339237cb04c\n#  failed_when: false          # Never Fail during \"systemctl stop calibre-serve\" (even if service doesn't exist!)\n# when: calibre_svc.stat.exists\n\n# 3. CREATE USER DATABASE\n\n- name: Create /library/calibre (mandatory since Calibre 3.x)\n  file:\n    path: \"{{ calibre_dbpath }}\"\n    state: directory\n    #mode: 0755\n\n- name: Copy template userdb to /library/calibre/users.sqlite (IF /usr/bin/calibre WAS MISSING)\n  copy:\n    src: /opt/iiab/iiab/roles/calibre/templates/users.sqlite\n    dest: \"{{ calibre_userdb }}\"\n    owner: root\n    group: root\n    mode: 0644\n  when: (not calib_executable.stat.exists)\n\n# 4. CREATE CONTENT DATABASE WITH A SAMPLE BOOK (REQUIRED AS OF CALIBRE 3.x)\n\n- name: Check if /library/calibre/metadata.db exists\n  stat:\n    path: \"{{ calibre_dbpath }}/metadata.db\"\n  register: calibre_db\n\n- name: Create database (required since Calibre 3.x) with a sample book\n  include_tasks: create-db.yml\n  when: not calibre_db.stat.exists\n\n# 5. WRAP UP: ENABLE CALIBRE SERVICE, http://box/books ETC\n\n# http://box:8080 & http://box:8080/mobile WORK BUT OTHER URL'S LIKE http://box/books ARE A MESS (BOOKS RARELY DISPLAY)\n- name: Create calibre.conf link for UNTESTED http://box/books etc (debuntu)\n  file:\n    src: /etc/apache2/sites-available/calibre.conf\n    dest: /etc/apache2/sites-enabled/calibre.conf\n    state: link\n  when: calibre_enabled and is_debuntu\n\n- name: Remove calibre.conf link if disabled (debuntu)\n  file:\n    dest: /etc/apache2/sites-enabled/calibre.conf\n    state: absent\n  when: (not calibre_enabled) and is_debuntu\n\n- name: Enable Calibre service -- runs calibre-server by Kovid Goyal\n  service:\n    name: calibre-serve\n    enabled: yes\n    state: started\n  when: calibre_enabled\n  #async: 900\n  #poll: 5\n\n- name: Forcing apache to reread configs\n  service:\n    name: \"{{ apache_service }}\"\n    state: reloaded\n\n- name: Add 'calibre' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: calibre\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n  - option: name\n    value: Calibre\n  - option: description\n    value: '\"Calibre is an extremely popular personal library system for e-books.\"'\n  - option: url\n    value: \"{{ calibre_src_url }}\"\n  - option: database\n    value: \"{{ calibre_dbpath }}\"\n  - option: port\n    value: \"{{ calibre_port }}\"\n  - option: enabled\n    value: \"{{ calibre_enabled }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "119df9c7dbdf824a5c6379f9b53dc3bfdaf5d8e8", "filename": "playbooks/aws/openshift-cluster/cluster_hosts.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ng_all_hosts: \"{{ groups['tag_clusterid_' ~ cluster_id] | default([])\n                 | intersect(groups['tag_environment_' ~ cluster_env] | default([])) }}\"\n\ng_etcd_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type_etcd'] | default([])) }}\"\n\ng_lb_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type_lb'] | default([])) }}\"\n\ng_nfs_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type_nfs'] | default([])) }}\"\n\ng_glusterfs_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type-glusterfs'] | default([])) }}\"\n\ng_master_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type_master'] | default([])) }}\"\n\ng_new_master_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type_new_master'] | default([])) }}\"\n\ng_node_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type_node'] | default([])) }}\"\n\ng_new_node_hosts: \"{{ g_all_hosts | intersect(groups['tag_host-type_new_node'] | default([])) }}\"\n\ng_infra_hosts: \"{{ g_node_hosts | intersect(groups['tag_sub-host-type_infra'] | default([])) }}\"\n\ng_compute_hosts: \"{{ g_node_hosts | intersect(groups['tag_sub-host-type_compute'] | default([])) }}\"\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "784755e471f37e62cdec644058d15eba17d4268d", "filename": "meta/main.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\nallow_duplicates: no\n\n\ngalaxy_info:\n  author: Aur\u00e9lien Wailly\n  description: Ansible role to meet CIS (Center for Internet Security) requirements on ubuntu\n  license: GPLv2\n  min_ansible_version: 1.9.0.1\n  platforms:\n    - name: Ubuntu\n      versions:\n        14.04\n  categories:\n    #- cloud\n    #- cloud:ec2\n    #- cloud:gce\n    #- cloud:rax\n    #- clustering\n    #- database\n    #- database:nosql\n    #- database:sql\n    #- development\n    - monitoring\n    #- networking\n    #- packaging\n    - system\n    #- web\n\n\n\ndependencies: []\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "acbbbad8a28c834cc2676f1104630ae051290791", "filename": "roles/install-mongodb/tasks/install_mongodb.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: 'Add mongodb repo 3.0 and up'\n  yum_repository:\n    name: \"mongodb-org-{{ mongodb_ver }}\"\n    description: \"MongoDB Repository\"\n    baseurl: https://repo.mongodb.org/yum/{{ os_family }}/{{ os_ver }}/mongodb-org/{{ mongodb_ver }}/x86_64/\n    gpgcheck: yes\n    enabled: yes\n    gpgkey: https://www.mongodb.org/static/pgp/server-{{ mongodb_ver }}.asc\n    file: mongodb-org-{{ mongodb_ver }}\n  when: mongodb_ver|float() >= 3.0\n\n- name: 'Add mongodb repo before 3.0'\n  yum_repository:\n    name: \"mongodb-org-{{ mongodb_ver }}\"\n    description: \"MongoDB {{ mongodb_ver }} Repository\"\n    baseurl: http://downloads-distro.mongodb.org/repo/{{ os_family }}/os/x86_64/\n    gpgcheck: no\n    enabled: yes\n    file: mongodb-org-{{ mongodb_ver }}\n  when: mongodb_ver|float() < 3.0\n\n- name: 'Install mongodb-org'\n  package:\n    name: mongodb-org\n    state: latest\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c3a5ece29b637416ffc765058f0518948a02366f", "filename": "roles/config-clair/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Restart Clair Service\n  systemd:\n    name: \"{{ clair_name }}\"\n    enabled: yes\n    state: restarted\n    daemon_reload: yes\n\n- name: restart firewalld\n  service:\n    name: firewalld\n    state: restarted\n\n- name: restart iptables\n  service:\n    name: iptables\n    state: restarted"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "e3980da0665ad34261efe126bbceec489b94773c", "filename": "playbooks/roles/bro/vars/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# vars file for bro"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "0a2a50d5817af9d50549627d500588c3c0edd5b0", "filename": "roles/config-idm-server/vars/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n# vars file for idm"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "aa54811a7b551de57906414e49fcebb38aa25b7d", "filename": "playbooks/templates/pulledpork.conf.j2", "repository": "rocknsm/rock", "decoded_content": "# Config file for pulledpork\n# Be sure to read through the entire configuration file\n# If you specify any of these items on the command line, it WILL take \n# precedence over any value that you specify in this file!\n{# I'm making the assumption here that a user will only configure\n   pulledpork if they have snort or suricata installed. This will \n   currently break otherwise. We could address this usecase fairly\n   easily if there is a need \n#}\n{% if with_suricata %}\n{% set engine_basepath = \"/etc/suricata\" %}\n{% elif with_snort %}\n{% set engine_basepath = \"/etc/snort\" %}\n{% endif %}\n\n#######\n#######  The below section defines what your oinkcode is (required for \n#######  VRT rules), defines a temp path (must be writable) and also \n#######  defines what version of rules that you are getting (for your \n#######  snort version and subscription etc...)\n####### \n\n# You can specify one or as many rule_urls as you like, they \n# must appear as http://what.site.com/|rulesfile.tar.gz|1234567.  You can specify\n# each on an individual line, or you can specify them in a , separated list\n# i.e. rule_url=http://x.y.z/|a.tar.gz|123,http://z.y.z/|b.tar.gz|456\n# note that the url, rule file, and oinkcode itself are separated by a pipe |\n# i.e. url|tarball|123456789, \n#rule_url=https://www.snort.org/reg-rules/|snortrules-snapshot.tar.gz|<oinkcode>\n# NEW Community ruleset:\n#rule_url=https://snort.org/downloads/community/|community-rules.tar.gz|Community\n# NEW For IP Blacklisting! Note the format is urltofile|IPBLACKLIST|<oinkcode>\n# This format MUST be followed to let pulledpork know that this is a blacklist\n#rule_url=http://talosintelligence.com/feeds/ip-filter.blf|IPBLACKLIST|open\n# URL for rule documentation! (slow to process)\n#rule_url=https://www.snort.org/reg-rules/|opensource.gz|<oinkcode>\n# THE FOLLOWING URL is for emergingthreats downloads, note the tarball name change!\n# and open-nogpl, to avoid conflicts.\n#rule_url=https://rules.emergingthreats.net/|emerging.rules.tar.gz|open-nogpl\n# THE FOLLOWING URL is for etpro downloads, note the tarball name change!\n# and the et oinkcode requirement!\n#rule_url=https://rules.emergingthreatspro.com/|etpro.rules.tar.gz|<et oinkcode>\n# NOTE above that the VRT snortrules-snapshot does not contain the version\n# portion of the tarball name, this is because PP now automatically populates\n# this value for you, if, however you put the version information in, PP will\n# NOT populate this value but will use your value!\n{% for rule in pulledpork_rules if (rule.test is undefined or rule.test) %}\nrule_url={{ rule.url }}|{{ rule.file }}|{{ rule.apikey }}\n{% endfor %}\n# Specify rule categories to ignore from the tarball in a comma separated list\n# with no spaces.  There are four ways to do this:\n# 1) Specify the category name with no suffix at all to ignore the category\n#    regardless of what rule-type it is, ie: netbios\n# 2) Specify the category name with a '.rules' suffix to ignore only gid 1\n#    rulefiles located in the /rules directory of the tarball, ie: policy.rules\n# 3) Specify the category name with a '.preproc' suffix to ignore only\n#    preprocessor rules located in the /preproc_rules directory of the tarball,\n#    ie: sensitive-data.preproc\n# 4) Specify the category name with a '.so' suffix to ignore only shared-object\n#    rules located in the /so_rules directory of the tarball, ie: netbios.so\n# The example below ignores dos rules wherever they may appear, sensitive-\n# data preprocessor rules, p2p so-rules (while including gid 1 p2p rules),\n# and netbios gid-1 rules (while including netbios so-rules):\n# ignore = dos,sensitive-data.preproc,p2p.so,netbios.rules\n# These defaults are reasonable for the VRT ruleset with Snort 2.9.0.x.\nignore=deleted.rules,experimental.rules,local.rules\n# IMPORTANT, if you are NOT yet using 2.8.6 then you MUST comment out the\n# previous ignore line and uncomment the following!\n# ignore=deleted,experimental,local,decoder,preprocessor,sensitive-data\n\n# What is our temp path, be sure this path has a bit of space for rule \n# extraction and manipulation, no trailing slash\ntemp_path=/tmp\n\n#######\n#######  The below section is for rule processing.  This section is \n#######  required if you are not specifying the configuration using\n#######  runtime switches.  Note that runtime switches do SUPERSEED \n#######  any values that you have specified here!\n#######\n\n# What path you want the .rules file containing all of the processed \n# rules? (this value has changed as of 0.4.0, previously we copied \n# all of the rules, now we are creating a single large rules file\n# but still keeping a separate file for your so_rules!\nrule_path={{ engine_basepath }}/rules/pulledpork.rules\n# What path you want the .rules files to be written to, this is UNIQUE\n# from the rule_path and cannot be used in conjunction, this is to be used with the\n# -k runtime flag, this can be set at runtime using the -K flag or specified\n# here.  If specified here, the -k option must also be passed at runtime, however\n# specifying -K <path> at runtime forces the -k option to also be set\n# out_path=/usr/local/etc/snort/rules/\n\n# If you are running any rules in your local.rules file, we need to\n# know about them to properly build a sid-msg.map that will contain your\n# local.rules metadata (msg) information.  You can specify other rules\n# files that are local to your system here by adding a comma and more paths...\n# remember that the FULL path must be specified for EACH value.\n# local_rules=/path/to/these.rules,/path/to/those.rules\nlocal_rules={{ engine_basepath }}/rules/local.rules\n\n# Where should I put the sid-msg.map file?\nsid_msg={{ engine_basepath }}/sid-msg.map\n\n# New for by2 and more advanced msg mapping.  Valid options are 1 or 2\n# specify version 2 if you are running barnyard2.2+.  Otherwise use 1\nsid_msg_version=1\n\n# Where do you want me to put the sid changelog?  This is a changelog \n# that pulledpork maintains of all new sids that are imported\nsid_changelog=/var/log/sid_changes.log\n# this value is optional\n\n#######\n#######  The below section is for so_rule processing only.  If you don't\n#######  need to use them.. then comment this section out!\n#######  Alternately, if you are not using pulledpork to process \n#######  so_rules, you can specify -T at runtime to bypass this altogether\n#######\n\n# What path you want the .so files to actually go to *i.e. where is it\n# defined in your snort.conf, needs a trailing slash\n#sorule_path=''\n\n# Path to the snort binary, we need this to generate the stub files\nsnort_path={{ \"/sbin/suricata\" if with_suricata else \"/usr/sbin/snort\" }}\n\n# We need to know where your snort.conf file lives so that we can\n# generate the stub files\n#config_path={{ engine_basepath }}/{{ \"suricata.yaml\" if with_suricata else \"snort.conf\" }}\n\n##### Deprecated - The stubs are now  categorically written to the  single rule file!\n# sostub_path=/usr/local/etc/snort/rules/so_rules.rules\n\n# Define your distro, this is for the precompiled shared object libs!\n# Valid Distro Types:\n# Debian-6-0, Ubuntu-10-4\n# Ubuntu-12-04, Centos-5-4\n# FC-12, FC-14, RHEL-5-5, RHEL-6-0\n# FreeBSD-8-1, FreeBSD-9-0, FreeBSD-10-0\n# OpenBSD-5-2, OpenBSD-5-3\n# OpenSUSE-11-4, OpenSUSE-12-1\n# Slackware-13-1\n#distro=''\n\n#######  This next section is optional, but probably pretty useful to you.\n#######  Please read thoroughly!\n\n# If you are using IP Reputation and getting some public lists, you will probably\n# want to tell pulledpork where your blacklist file lives, PP automagically will\n# de-dupe any duplicate IPs from different sources.\nblack_list={{ engine_basepath }}/rules/iplists/default.blacklist\n\n# IP Reputation does NOT require a full snort HUP, it introduces a concept whereby\n# the IP list can be reloaded while snort is running through the use of a control\n# socket.  Please be sure that you built snort with the following optins:\n# -enable-shared-rep and --enable-control-socket.  Be sure to read about how to\n# configure these!  The following option tells pulledpork where to place the version\n# file for use with control socket ip list reloads!\n# This should be the same path where your black_list lives!\nIPRVersion={{ engine_basepath }}/rules/iplists\n\n# The following option tells snort where the snort_control tool is located.\nsnort_control={{ \"/usr/bin/snort_control\" if with_snort else \"\" }}\n\n# What do you want to backup and archive?  This is a comma separated list\n# of file or directory values.  If a directory is specified, PP will recurse\n# through said directory and all subdirectories to archive all files.\n# The following example backs up all snort config files, rules, pulledpork\n# config files, and snort shared object binary rules.\n# backup=/usr/local/etc/snort,/usr/local/etc/pulledpork,/usr/local/lib/snort_dynamicrules/\n\n# what path and filename should we use for the backup tarball?\n# note that an epoch time value and the .tgz extension is automatically added\n# to the backup_file name on completeion i.e. the written file is:\n# pp_backup.1295886020.tgz\n# backup_file=/tmp/pp_backup\n\n# Where do you want the signature docs to be copied, if this is commented \n# out then they will not be copied / extracted.  Note that extracting them \n# will add considerable runtime to pulledpork.\n# docs=/path/to/base/www\n\n# The following option, state_order, allows you to more finely control the order\n# that pulledpork performs the modify operations, specifically the enablesid\n# disablesid and dropsid functions.  An example use case here would be to\n# disable an entire category and later enable only a rule or two out of it.\n# the valid values are disable, drop, and enable.\n# state_order=disable,drop,enable\n\n\n# Define the path to the pid files of any running process that you want to\n# HUP after PP has completed its run.\n# pid_path=/var/run/snort.pid,/var/run/barnyard.pid,/var/run/barnyard2.pid\n# and so on...\n# pid_path=/var/run/snort_eth0.pid\n\n# This defines the version of snort that you are using, for use ONLY if the \n# proper snort binary is not on the system that you are fetching the rules with\n# This value MUST contain all 4 minor version\n# numbers. ET rules are now also dependant on this, verify supported ET versions\n# prior to simply throwing rubbish in this variable kthx!\n#\n# Suricata users - set this to 'suricata-3.x.x' to process rule files\n# for suricata, this mimics the -S flag on the command line.\n{% if with_suricata %}\nsnort_version=suricata\n{% else %}\nsnort_version=2.9.0.0\n{% endif %}\n\n\n# Here you can specify what rule modification files to run automatically.\n# simply uncomment and specify the apt path.\n# enablesid=/usr/local/etc/snort/enablesid.conf\n# dropsid=/usr/local/etc/snort/dropsid.conf\ndisablesid=/etc/pulledpork/disablesid.conf\n# modifysid=/usr/local/etc/snort/modifysid.conf\n\n# What is the base ruleset that you want to use, please uncomment to use\n# and see the README.RULESETS for a description of the options.  \n# Note that setting this value will disable all ET rulesets if you are \n# Running such rulesets\n# ips_policy=security\n\n####### Remember, a number of these values are optional.. if you don't \n####### need to process so_rules, simply comment out the so_rule section\n####### you can also specify -T at runtime to process only GID 1 rules.\n\nversion=0.7.2\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "a184869c89534f7f297df64416002d5926156a17", "filename": "tasks/checks/distribution-checks-CentOS.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Fail if unsupported CentOS version\n  fail:\n    msg: \"CentOS 7 or later is required!\"\n  when: _docker_os_dist_major_version | int < 7\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "ec920927140a279793f7c6f4911d002f702effcd", "filename": "roles/client/tasks/systems/Fedora.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- set_fact:\n    prerequisites:\n      - libselinux-python\n    configs_prefix: /etc/strongswan/\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "7c982dbd2e5d3134f4f17a3d69f1ab99c0eba697", "filename": "roles/network/tasks/create_ifcfg.yml", "repository": "iiab/iiab", "decoded_content": "- name: Stop 'Wired WAN connection'\n  shell: nmcli dev disconnect {{ discovered_wan_iface }}\n  ignore_errors: True\n  changed_when: False\n  when: discovered_wan_iface != \"none\" and not has_WAN and has_ifcfg_gw == \"none\"\n\n# set user_wan_iface: <device> for static\n# use wan_* for static info\n- name: Supply ifcfg-WAN file\n  template: src=network/ifcfg-WAN.j2\n            dest=/etc/sysconfig/network-scripts/ifcfg-WAN\n  when: iiab_wan_iface != \"none\" and not has_WAN  and has_ifcfg_gw == \"none\"\n\n- name: Now setting ifcfg-WAN True after creating file\n  set_fact:\n    has_WAN: True\n  when: iiab_wan_iface != \"none\" and has_ifcfg_gw == \"none\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "88c621245354fecf81a0790b4625b26ef4c4c5f9", "filename": "playbooks/templates/es_cleanup.sh.j2", "repository": "rocknsm/rock", "decoded_content": "#!/bin/bash\n\nindexes=(bro suricata)\nes_uri=\"http://localhost:9200\"\n\n#Clean out old marvel indexes, only keeping the current index.\nfor i in $(curl -sSL ${es_uri}/_stats/indexes\\?pretty\\=1 | grep marvel | grep -Ev 'es-data|kibana' | grep -vF \"$(date +%m.%d)\" | awk '{print $1}' | sed 's/\\\"//g' 2>/dev/null); do\n  curl -sSL -XDELETE ${es_uri}/$i > /dev/null 2>&1\ndone\n\n#Cleanup TopBeats indexes from 5 days ago.\n#curl -sSL -XDELETE \"http://127.0.0.1:9200/topbeat-$(date -d '5 days ago' +%Y.%m.%d)\" 2>&1\n\nfor item in ${indexes[*]}; do\n  #Delete Logstash indexes from 60 days ago.\n  curl -sSL -XDELETE \"${es_uri}/${item}-$(date -d '{{ elastic_delete_interval }} days ago' +%Y.%m.%d)\" 2>&1\n  #Close Logstash indexes from 15 days ago.\n  curl -sSL -XPOST \"${es_uri}/${item}-$(date -d '{{ elastic_close_interval }} days ago' +%Y.%m.%d)/_close\" 2>&1\ndone\n\n#Make sure all indexes have replicas off\ncurl -sSL -XPUT 'localhost:9200/_all/_settings' -d '\n{\n    \"index\" : {\n        \"number_of_replicas\" : 0\n    }\n}' > /dev/null 2>&1\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "c85b9283ba5483a17934ecb0abf7805a91182a79", "filename": "roles/suricata/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n- name: Install packages\n  yum:\n    name:\n      - suricata\n      - PyYAML\n    state: present\n\n- name: Set monitor interface config\n  template:\n    src: templates/ifcfg-monif.j2\n    dest: /etc/sysconfig/network-scripts/ifcfg-{{ item }}\n    mode: 0644\n    owner: root\n    group: root\n    force: true\n  loop: \"{{ rock_monifs }}\"\n\n- name: Configure local ifup script\n  template:\n    src: templates/ifup-local.j2\n    dest: /sbin/ifup-local\n    mode: 0755\n    owner: root\n    group: root\n    force: true\n  notify: Configure monitor interfaces\n\n- name: Create suricata data directory\n  file:\n    path: \"{{ suricata_data_dir }}/\"\n    mode: 0755\n    owner: \"{{ suricata_user }}\"\n    group: \"{{ suricata_group }}\"\n    state: directory\n    setype: var_log_t\n\n- name: Remove suricata sysconfig file\n  file:\n    path: /etc/sysconfig/suricata\n    state: absent\n\n- name: Install suricata service files\n  copy:\n    src: \"suricata.service\"\n    dest: \"/etc/systemd/system/suricata.service\"\n    mode: 0644\n    owner: root\n    group: root\n\n- name: Setup suricata tmpfiles\n  copy:\n    src: \"suricata.tmpfiles\"\n    dest: \"/etc/tmpfiles.d/suricata.conf\"\n    mode: 0644\n    owner: root\n    group: root\n\n- name: Install suricata overrides\n  template:\n    src: templates/suricata_overrides.yaml.j2\n    dest: /etc/suricata/rocknsm-overrides.yaml\n    mode: 0640\n    owner: \"root\"\n    group: \"{{ suricata_group }}\"\n\n- name: Create IP reputation config directory\n  file:\n    path: /etc/suricata/rules/iplists\n    state: directory\n    owner: root\n    group: root\n    mode: 0755\n\n- name: Create directories for suricata-update\n  file:\n    path: \"{{ suricata_var_dir }}/{{ item }}\"\n    state: directory\n    owner: \"{{ suricata_user }}\"\n    group: \"{{ suricata_group }}\"\n    mode: 0755\n    recurse: \"yes\"\n  loop:\n    - rules\n    - update\n\n- name: Set suricata overrides include\n  lineinfile:\n    dest: /etc/suricata/suricata.yaml\n    line: \"include: rocknsm-overrides.yaml\"\n    state: present\n\n- name: Discover facts about data mount\n  set_fact:\n    rock_mounts:\n      mount: \"{{ item.mount }}\"\n      device: \"{{ item.device }}\"\n      size_total: \"{{ item.size_total }}\"\n  loop:\n    \"{{ ansible_mounts }}\"\n  when: (default_mount is defined and item.mount == default_mount and rock_mounts is not defined)\n\n- name: Determining if quotas are enabled\n  shell: grep \"{{ default_mount }}\" /etc/fstab | awk /prjquota/\n  register: prjquota\n  changed_when: false\n\n- name: Create suricata quota project id\n  getent:\n    database: group\n    split: ':'\n    key: suricata\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Map suricata quota project id to name\n  lineinfile:\n    create: true\n    state: present\n    insertafter: EOF\n    path: /etc/projid\n    line: \"suricata:{{ getent_group.suricata[1] }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Define suricata quota project directories\n  lineinfile:\n    create: true\n    state: present\n    insertafter: EOF\n    path: /etc/projects\n    line: \"{{ getent_group.suricata[1] }}:{{ suricata_data_dir }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: set suricata weight\n  set_fact:\n    suricata_weight: \"{{ rock_services | selectattr('name', 'equalto', 'suricata') | map(attribute='quota_weight') | first }}\"\n  when: suricata_quota is not defined and (prjquota.stdout|length>0)\n\n- name: set suricata quota if not user defined\n  set_fact:\n    suricata_quota: \"{{ rock_mounts.size_total | int / xfs_quota_weight | int * suricata_weight | int }}\"\n  when: suricata_quota is not defined and (prjquota.stdout|length>0)\n\n- name: set suricata project quota\n  xfs_quota:\n    type: project\n    name: suricata\n    bhard: \"{{ suricata_quota }}\"\n    state: present\n    mountpoint: \"{{ rock_mounts.mount }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Enable and start suricata\n  service:\n    name: suricata\n    state: \"{{ 'started' if rock_services | selectattr('name', 'equalto', 'suricata') | map(attribute='enabled') | bool else 'stopped' }}\"\n    enabled: \"{{ rock_services | selectattr('name', 'equalto', 'suricata') | map(attribute='enabled') | bool }}\"\n\n- name: Configure logrotate for suricata logs\n  template:\n    src: templates/logrotate-suricata.j2\n    dest: /etc/logrotate.d/suricata\n    mode: 0644\n    owner: root\n    group: root\n\n- name: Create local rules source for offline install of suricata\n  command: /usr/bin/suricata-update add-source \"emerging-threats-offline\" \"file:///srv/rocknsm/support/emerging.rules-suricata.tar.gz\"\n  args:\n    creates: /var/lib/suricata/update/sources/emerging-threats-offline.yaml\n  when: \"rock_services | selectattr('name', 'equalto', 'suricata') | map(attribute='installed') and not rock_online_install\"\n  become: true\n  become_user: \"{{ suricata_user }}\"\n\n- name: Offline install of suricata rules\n  command: /usr/bin/suricata-update update --reload-command \"/usr/bin/systemctl kill -s USR2 suricata\"\n  args:\n    creates: /var/lib/suricata/rules/suricata.rules\n  when: \"rock_services | selectattr('name', 'equalto', 'suricata') | map(attribute='enabled') and not rock_online_install\"\n  become: true\n  become_user: \"{{ suricata_user }}\"\n\n- name: Update suricata-update source index\n  command: /usr/bin/suricata-update update-sources\n  args:\n    creates: /var/lib/suricata/update/cache/index.yaml\n    chdir: /var/lib/suricata\n  when: \"rock_services | selectattr('name', 'equalto', 'suricata') | map(attribute='enabled') and rock_online_install\"\n  become: true\n  become_user: \"{{ suricata_user }}\"\n\n- name: Explicitly enable ET rules for suricata-update online\n  command: /usr/bin/suricata-update enable-source et/open\n  args:\n    creates: /var/lib/suricata/update/sources/et-open.yaml\n    chdir: /var/lib/suricata\n  when: \"rock_services | selectattr('name', 'equalto', 'suricata') | map(attribute='enabled') and rock_online_install\"\n  become: true\n  become_user: \"{{ suricata_user }}\"\n\n- name: Suricata-update online rules pull\n  command: /usr/bin/suricata-update update --reload-command \"/usr/bin/systemctl kill -s USR2 suricata\"\n  args:\n    creates: /var/lib/suricata/rules/suricata.rules\n    chdir: /var/lib/suricata\n  when: \"rock_services | selectattr('name', 'equalto', 'suricata') | map(attribute='enabled') and rock_online_install\"\n  become: true\n  become_user: \"{{ suricata_user }}\"\n\n- name: Cron for suricata-update\n  cron:\n    name: \"suricata-update\"\n    cron_file: rocknsm_suricata-update\n    user: \"{{ suricata_user }}\"\n    hour: \"12\"\n    minute: \"0\"\n    job: /usr/bin/suricata-update update --reload-command \"/usr/bin/systemctl kill -s USR2 suricata\"\n         > /var/log/suricata-update.log 2>&1\n  when: \"rock_services | selectattr('name', 'equalto', 'suricata') | map(attribute='enabled')\"\n\n- name: Apply Logstash role\n  include_role:\n    name: logstash\n    apply:\n      delegate_to: \"{{ host }}\"\n      vars:\n        logstash_configs:\n          - { src: 'ls-input-suricata.j2', dest: 'logstash-100-input-kafka-suricata.conf' }\n  loop:\n    \"{{ groups['logstash'] }}\"\n  loop_control:\n    loop_var: host\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "8c45f031a33f47ecf4efa7bcce5df0e4988b8413", "filename": "reference-architecture/aws-ansible/playbooks/roles/gluster-instance-groups/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Add gluster instances to host group\n  add_host:\n    name: \"{{ hostvars[item].ec2_tag_Name }}\"\n    groups: crs, storage\n  with_items: \"{{ groups['tag_StorageType_crs'] }}\"\n  when:\n    - hostvars[item]['ec2_tag_aws_cloudformation_stack_name'] == \"{{ stack_name }}-{{ glusterfs_stack_name }}\"\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "f4f147ba603e923f433db23b81dd874a4284fcf7", "filename": "roles/registrator/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n\n# Install (docker-py) python package as is a docker module dependency.\n- pip: name=docker-py version=1.1.0\n\n# tasks file for docker registrator\n- name: run registrator container\n  docker:\n    name: registrator\n    image: \"{{ registrator_image }}\"\n    state: started\n    restart_policy: always\n    net: host\n    command: \"-internal {{ registrator_uri }}\"\n    hostname: \"{{ hostname }}\"\n    volumes:\n    - \"/var/run/docker.sock:/tmp/docker.sock\"\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "1415245ed4cdf9fccbc5dc59550d4a98e0e758a0", "filename": "roles/common/handlers/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: restart rsyslog\n  service: name=rsyslog state=restarted\n\n- name: restart ipfw\n  service: name=ipfw state=restarted\n\n- name: flush routing cache\n  shell: echo 1 > /proc/sys/net/ipv4/route/flush\n\n- name: restart systemd-networkd\n  systemd:\n    name: systemd-networkd\n    state: restarted\n    daemon_reload: true\n\n- name: restart loopback bsd\n  shell: >\n    ifconfig lo100 destroy || true &&\n    ifconfig lo100 create &&\n    ifconfig lo100 inet {{ local_service_ip }} netmask 255.255.255.255 &&\n    ifconfig lo100 inet6 FCAA::1/64; echo $?\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "2ed9ff3475b2d97773b5255a9125affa1510e3d8", "filename": "reference-architecture/vmware-ansible/playbooks/roles/create-vm-add-prod-ose/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Assign app memory for container_storage\n  set_fact:\n    app_memory: 32768\n  when: \"'cns' in container_storage\"\n\n- name: Assign app memory for container_storage\n  set_fact:\n    app_memory: 8192\n  when: \"'cns' not in container_storage\"\n\n- name: Create additional production VMs on vCenter\n  vmware_guest:\n    hostname: \"{{ vcenter_host }}\"\n    username: \"{{ vcenter_username }}\"\n    password: \"{{ vcenter_password }}\"\n    validate_certs: False\n    name: \"{{ item.value.guestname }}\"\n    cluster: \"{{ vcenter_cluster}}\"\n    datacenter: \"{{ vcenter_datacenter }}\"\n    resource_pool: \"{{ vcenter_resource_pool }}\"\n    template: \"{{vcenter_template_name}}\"\n    state: poweredon\n    wait_for_ip_address: True\n    folder: \"/{{ vcenter_folder }}\"\n    annotation: \"{{ item.value.tag }}\"\n    hardware:\n      memory_mb: \"{{ app_memory }}\"\n    networks:\n    - name: \"{{ vm_network }}\"\n      ip: \"{{ item.value.ip4addr }}\"\n      netmask: \"{{ vm_netmask }}\"\n      gateway: \"{{ vm_gw }}\"\n    customization:\n      domain: \"{{dns_zone}}\"\n      dns_servers:\n      - \"{{ vm_dns }}\"\n      dns_suffix: \"{{dns_zone}}\"\n      hostname: \"{{ item.value.guestname}}\"\n  with_dict: \"{{host_inventory}}\"\n  when: \"'master' in item.value.guestname or 'app' in item.value.guestname or 'infra' in item.value.guestname\"\n\n- name: Add additional production VMs to inventory\n  add_host: hostname=\"{{ item.value.guestname }}\" ansible_ssh_host=\"{{ item.value.ip4addr }}\" groups=\"{{ item.value.tag }}, production_group, new_nodes\"\n  with_dict: \"{{host_inventory}}\"\n  when: \"'master' in item.value.guestname or 'app' in item.value.guestname or 'infra' in item.value.guestname\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "cd21505a47e530a967e3c44bd2a772d1b8d08bd7", "filename": "roles/dns/manage-dns-records/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "4c385ac9e9377a25e10819a56ee52c0228cd2e35", "filename": "roles/kafka/vars/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# vars file for kafka\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "bb1bfe6b28adacba44ff9ebb5a8ee0b124b39824", "filename": "roles/config-quay-enterprise/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# Base Configurations\nquay_name: quay\nquay_service: \"{{ quay_name }}.service\"\nquay_server_hostname:\nquay_database_type: postgresql\n\n#Systemd\nsystemd_service_dir: /usr/lib/systemd/system\nsystemd_environmentfile_dir: /etc/sysconfig\n\n# Quay\nquay_image: quay.io/coreos/quay:v2.9.2\nquay_config_dir: /var/lib/quay/config\nquay_container_config_dir: /conf/stack\nquay_storage_dir: /var/lib/quay/storage\nquay_container_storage_dir: /datastorage\nquay_storage_selinux_relabel: True\n\n# External Databases\npostgresql_db_uri: \"postgresql://{{ quay_database_username }}:{{ quay_database_password }}@{{ quay_database_host }}:{{ quay_database_port | default('5432') }}/{{ quay_database_name }}\"\nmysql_db_uri: \"mysql+pymysql://{{ quay_database_username }}:{{ quay_database_password }}@{{ quay_database_host }}:{{ quay_database_port | default('3306') }}/{{ quay_database_name }}\"\n\n# Container Credentials\ncontainer_credentials_file: /root/.docker/config.json\ncontainer_credentials_file_content: {}\nquay_registry_server: quay.io\nquay_registry_auth:\nquay_registry_email:\n\n# Port Configurations\nquay_host_http_port: 80\nquay_container_http_port: 80\nquay_host_https_port: 443\nquay_container_https_port: 443\n\n# SSL\nquay_ssl_enable: True\nquay_ssl_key_file: \"\"\nquay_ssl_cert_file: \"\"\nquay_ssl_generate_city: Raleigh\nquay_ssl_generate_state: NC\nquay_ssl_generate_country: US\nquay_ssl_generate_organization: Red Hat\nquay_ssl_generate_organizational_unit: CoP\nquay_ssl_generate_days_validity: 365\nquay_ssl_local_tmp_dir: \"/tmp\"\nquay_ssl_delete_generated_cert: True\n\n# Clair\nquay_clair_enable: False\nquay_clair_endpoint: \"\"\n\n# Builder\nquay_builder_enable: False\n\n# Superuser Configuration\nquay_superuser_username: \"\"\nquay_superuser_password: \"\"\nquay_superuser_email: \"\""}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "cc940ae35bc28d3e87a3dbd72b9a8b7e4c127b07", "filename": "ops/playbooks/config_storage_driver.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- name: Config local drives for Docker hosts\n  hosts: docker\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment:\n    - \"{{ env }}\"\n\n  tasks:\n\n    - include_tasks: includes/storage_driver_devicemapper.yml\n      when: docker_storage_driver is defined and docker_storage_driver == 'devicemapper'\n\n    - include_tasks: includes/storage_driver_overlay2.yml\n      when: docker_storage_driver is not defined or docker_storage_driver == 'overlay2'\n\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "99ba9d6c797f2342d6fe19c7a03a1c73480eae0d", "filename": "ops/playbooks/install_docker.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- name: Install Docker (Redhat)\n  hosts: docker\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  environment: \"{{ env }}\"\n\n  tasks:\n\n    # see http://success.docker.com/article/node-using-swap-memory-instead-of-host-memory\n    - name: Set vm.swapiness\n      sysctl:\n        name: vm.swappiness\n        value: 0\n        state: present\n\n    # http://success.docker.com/article/node-using-swap-memory-instead-of-host-memory\n    - name: set sysctl.vm.overcommit_memory\n      sysctl:\n        name: vm.overcommit_memory\n        value: 1\n        state: present\n\n    # http://success.docker.com/article/fix-ucp-calico-for-rp-filter\n    - name: set net.ipv4.conf.all.rp_filter\n      sysctl:\n        name: net.ipv4.conf.all.rp_filter\n        value: 1\n        state: present\n\n    - name: Install dependencies\n      yum:\n        name: \"{{ packages }}\"\n        state: latest\n      vars:\n        packages:\n        - policycoreutils-python \n        - libseccomp \n        - libtool-ltdl \n        - yum-utils\n        - NetworkManager-glib \n        - nm-connection-editor\n        - libsemanage-python \n        - policycoreutils-python\n\n    - name: Enable extras RHEL repository\n      shell: yum-config-manager --enable rhel-7-server-extras-rpms\n\n    - name: Set Docker url\n      lineinfile:\n        path: /etc/yum/vars/dockerurl\n        create: yes\n        backup: yes\n        state: present\n        line: \"{{ docker_ee_url }}/rhel\" \n\n    - name: Set OS version\n      lineinfile:\n        path: /etc/yum/vars/dockerosversion\n        create: yes\n        backup: yes\n        state: present\n        line: \"{{ rhel_version }}\"\n\n    - name: Add Docker repository\n      command: yum-config-manager --add-repo {{ docker_ee_url }}/rhel/docker-ee.repo\n\n    - name: Install Docker\n      yum:\n        name: '{{ docker_ee_version | default(\"docker-ee\") }}'\n#        enablerepo: docker-ee-test-18.01\n        state: 'latest' \n        update_cache: yes\n\n    - name: Ensure Docker is started\n      systemd:\n        name: docker\n        enabled: yes\n        state: started\n\n######################################################################################\n#\n# Play2: Install Docker on Windows\n#\n######################################################################################\n- name: Install Docker (Windows)\n  hosts: win_worker\n  gather_facts: false\n  connection: local\n  user: remote\n  become: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  environment: \"{{ env }}\"\n\n  vars:\n    proxy_switch: \"{% if  env.http_proxy is defined  %}-Proxy http://{{ env.http_proxy  }}{% endif %}\"\n\n  tasks:\n\n  - name: Is Docker running\n    win_shell: |\n      $svc=(get-service -name docker)\n      if ( $svc -ne $null )\n      {\n        $svc.status\n      }\n    register: status\n\n  - block: # Docker is not running\n    #\n    # Apply Windows updates if wanted\n    #\n    - name: Configure Proxy for use by Windows Updates\n      win_shell: |\n        netsh winhttp set proxy http://{{ env.http_proxy }}\n      when: env.http_proxy is defined\n\n    - name: See if Windows Updates are wanted or not\n      set_fact:\n        windows_update: \"{{ windows_update | default(true) }}\"\n\n    - block:\n\n      - name: Download and install Windows updates\n        win_updates:\n        register: update_result\n\n      - name: Reboot VM after Updates if required\n        win_reboot:\n          post_reboot_delay: 30\n        when: update_result.reboot_required\n\n      when: windows_update is defined and windows_update\n\n    - name: Configure proxy IE\n      win_shell: |\n        $regKey=\"HKCU:\\Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\"\n        Set-ItemProperty -path $regKey ProxyEnable -value 1\n        Set-ItemProperty -path $regKey ProxyServer -value {{ env.http_proxy }}\n      when: env.http_proxy is defined\n\n    - name: Evaluate Path of Powershell Profile\n      win_shell: |\n        split-path $profile\n      register: res\n\n    - name: Create Profile Directory\n      win_file:\n        path: \"{{ res.stdout_lines[0] }}\"\n        state: directory\n\n    - name: Populate the Powershell profile file\n      win_template:\n        src: ../templates/windows/ps_profile.ps1.j2\n        dest: 'C:\\Users\\Administrator\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1'\n      when: env.http_proxy is defined\n\n    - name: Configure Proxy for Docker Daemon\n      win_shell: |\n        [Environment]::SetEnvironmentVariable(\"HTTP_PROXY\", \"http://{{ env.http_proxy }}/\", [EnvironmentVariableTarget]::Machine)\n      when: env.http_proxy is defined\n\n    #\n    # wanted to get rid of the proxy switch thanks the regsitry edit above but it does not work\n    #\n    - name: Install Nuget Provider\n      win_shell: |\n        Install-PackageProvider -Name NuGet -MinimumVersion 2.8.5.201 -Force {{ proxy_switch }}\n      register: res\n    - debug: var=res\n      when: _debug is defined\n\n    # \n    # could not make win_psmodule work consistently behinda proxy\n    #\n    - name: Install DockerMsftProvider Powershell Module\n      win_shell: |\n        Install-Module DockerMsftProvider -Force {{ proxy_switch }}\n      register: res\n    - debug: var=res\n      when: _debug is defined\n\n    - name: Install Docker Package\n      win_shell: Install-Package Docker -ProviderName DockerMsftProvider -Force {{ proxy_switch }}\n      register: res\n\n    - name: Check if Windows reboot is required\n      win_shell: |\n        (Install-WindowsFeature Containers).RestartNeeded\n      register: res\n    - debug: var=res\n      when: _debug is defined\n\n    - name: Reboot VM after Docker package installation\n      win_reboot:\n      when: res.stdout_lines[0] == \"Yes\"\n\n    - name: Start Docker Service\n      win_service:\n        name: Docker\n        start_mode: auto\n        state: restarted\n\n    - wait_for_connection:\n        delay: 120\n        timeout: 300\n      register: res\n\n    - debug: var=res\n      when: _debug is defined\n\n    - name: Install required docker swarm ucp-agent-win image \n      win_shell: |\n        docker image pull docker/ucp-agent-win:{{ ucp_version }}\n\n    - name: Install required docker swarm ucp-dsinfo-win image\n      win_shell: |\n        docker image pull docker/ucp-dsinfo-win:{{ ucp_version }}\n\n    - name: Run Windows node setup script\n      win_shell: |\n        docker run --rm docker/ucp-agent-win:{{ ucp_version }} windows-script | \n        powershell -noprofile -noninteractive -command 'Invoke-Expression -Command $input'\n      register: res\n    - debug: var=res\n      when: _debug is defined\n\n    - name: Test Docker Installation\n      win_shell: docker container run hello-world:nanoserver\n      register: docker_cmd\n\n    - name: Fail Playbook if Docker Test Fails\n      fail:\n      when: docker_cmd.rc != 0\n\n    # idempotency not implemented, we don't touch the target if the service was installed\n    when: status.stdout == \"\" # Docker was installedO\n\n  - block: # Docker was running\n    - debug: msg=\"Docker Service is installed, not making any change to the system\"\n    when: status.stdout_lines != \"\"\n\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "84401e3d3322807c46bae430a6efeac64fd382a5", "filename": "roles/registrator/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for registrator\nregistrator_image: \"gliderlabs/registrator:master\"\nregistrator_uri: \"consul://{{ ansible_default_ipv4.address }}:8500\"\nhostname: \"{{ ansible_all_ipv4_addresses }}\"\n\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "d336e253039efc864e0ce9f5132acefe4c46ace1", "filename": "tasks/Win32NT/fetch/security-fetch/security-winfetch-local.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Copy security policy artifact to destination\n  win_copy:\n    src: '{{ java_unlimited_policy_transport_local }}'\n    dest: '{{ java_download_path }}'\n  register: policy_file_downloaded\n  retries: 5\n  delay: 2\n  until: policy_file_downloaded is succeeded\n\n- name: Downloaded artifact\n  set_fact:\n    security_policy_java_artifact: '{{ policy_file_downloaded.dest }}'\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "e75396acbbb102293cb6de23f10e42992ee9fbb7", "filename": "roles/mediawiki/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "mediawiki_major_version: \"1.31\"\nmediawiki_minor_version: \"0\"\nmediawiki_version: \"{{ mediawiki_major_version  }}.{{ mediawiki_minor_version }}\"\n\nmediawiki_download_base_url: \"https://releases.wikimedia.org/mediawiki/{{ mediawiki_major_version }}\"\nmediawiki_src: \"mediawiki-{{ mediawiki_version }}.tar.gz\"\n\nmediawiki_db_name: iiab_mediawiki\nmediawiki_db_user: iiab_mediawiki_user\nmediawiki_db_user_password: changeme\n\nmediawiki_admin_user: Admin\nmediawiki_admin_user_password: changeme\n\nmediawiki_site_name: Community Wiki\n\nmediawiki_install: True\nmediawiki_enabled: True\n\nmediawiki_install_path: \"{{ content_base }}\"\nmediawiki_abs_path: \"{{ mediawiki_install_path }}/mediawiki-{{ mediawiki_version }}\"\n\nmediawiki_url: /mediawiki\nmediawiki_full_url: \"http://{{ iiab_hostname }}.{{ iiab_domain }}/{{ mediawiki_url }}\"\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "6af122256b3d34204c4f0fd6180dfb4de1e5333d", "filename": "roles/ovirt-engine-remote-dwh/defaults/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_engine_dwh: True\n\novirt_engine_dwh_db_user: 'ovirt_engine_history'\novirt_engine_dwh_db_name: 'ovirt_engine_history'\novirt_engine_dwh_db_port: 5432\n\novirt_engine_db_user: 'engine'\novirt_engine_db_password: 'AqbXg4dpkbcVRZwPbY8WOR'\novirt_engine_db_name: 'engine'\novirt_engine_db_port: 5432\n\novirt_engine_ssh_port: 22\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "c7aa5476b96a89733be6b069e4013ca679a828e6", "filename": "roles/ovirt-collect-logs/tasks/db.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n- name: Dump engine database\n  shell: \"su - postgres -c 'pg_dump engine > {{ ovirt_collect_logs_tmp_dir }}/engine_db.sql'\"\n  ignore_errors: true\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "7a69f0c0cbf15f578cea3c6db7f1e7578d24a101", "filename": "playbooks/templates/easy-rsa-vars.j2", "repository": "rocknsm/rock", "decoded_content": "# easy-rsa parameter settings\n\n# NOTE: If you installed from an RPM,\n# don't edit this file in place in\n# /usr/share/openvpn/easy-rsa --\n# instead, you should copy the whole\n# easy-rsa directory to another location\n# (such as /etc/openvpn) so that your\n# edits will not be wiped out by a future\n# OpenVPN package upgrade.\n\n# This variable should point to\n# the top level of the easy-rsa\n# tree.\nexport EASY_RSA=\"`pwd`\"\n\n#\n# This variable should point to\n# the requested executables\n#\nexport OPENSSL=\"openssl\"\nexport PKCS11TOOL=\"pkcs11-tool\"\nexport GREP=\"grep\"\n\n\n# This variable should point to\n# the openssl.cnf file included\n# with easy-rsa.\nexport KEY_CONFIG=`$EASY_RSA/whichopensslcnf $EASY_RSA`\n\n# Edit this variable to point to\n# your soon-to-be-created key\n# directory.\n#\n# WARNING: clean-all will do\n# a rm -rf on this directory\n# so make sure you define\n# it correctly!\nexport KEY_DIR=\"$EASY_RSA/keys\"\n\n# Issue rm -rf warning\necho NOTE: If you run ./clean-all, I will be doing a rm -rf on $KEY_DIR\n\n# PKCS11 fixes\nexport PKCS11_MODULE_PATH=\"dummy\"\nexport PKCS11_PIN=\"dummy\"\n\n# Increase this to 2048 if you\n# are paranoid.  This will slow\n# down TLS negotiation performance\n# as well as the one-time DH parms\n# generation process.\nexport KEY_SIZE=2048\n\n# In how many days should the root CA key expire?\nexport CA_EXPIRE=3650\n\n# In how many days should certificates expire?\nexport KEY_EXPIRE=3650\n\n# These are the default values for fields\n# which will be placed in the certificate.\n# Don't leave any of these fields blank.\nexport KEY_COUNTRY=\"US\"\nexport KEY_PROVINCE=\"MO\"\nexport KEY_CITY=\"St Louis\"\nexport KEY_ORG=\"RockNSM\"\nexport KEY_EMAIL=\"info@rocknsm.io\"\nexport KEY_OU=\"NSM Ninjas\"\n\n# X509 Subject Field\nexport KEY_NAME=\"EasyRSA\"\n\n# PKCS11 Smart Card\n# export PKCS11_MODULE_PATH=\"/usr/lib/changeme.so\"\n# export PKCS11_PIN=1234\n\n# If you'd like to sign all keys with the same Common Name, uncomment the KEY_CN export below\n# You will also need to make sure your OpenVPN server config has the duplicate-cn option set\n# export KEY_CN=\"CommonName\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "f60f22b283744696e238ae9867bdc33e892b02f9", "filename": "dev/playbooks/config_rsyslog.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: docker,logger\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Open required ports for rsyslog\n      command: firewall-cmd --permanent --zone=public --add-port=514/tcp --add-port=514/udp\n      when: inventory_hostname in groups.logger\n\n    - name: Reload firewalld configuration\n      command: firewall-cmd --reload\n      when: inventory_hostname in groups.logger\n\n    - name: Install dependencies\n      yum:\n        name: rsyslog\n        state: latest\n\n    - name: Configure logger server\n      copy: src=../files/rsyslog.conf dest=/etc/rsyslog.conf\n      when: inventory_hostname in groups.logger\n      notify: Restart Rsyslog\n\n    - name: Allow docker nodes to send logs\n      template: src=../templates/rsyslog.conf.j2 dest=/etc/rsyslog.conf\n      when: inventory_hostname in groups.docker\n      notify: Restart Rsyslog\n\n  handlers:\n    - name: Restart Rsyslog\n      systemd:\n        name: rsyslog\n        state: restarted\n\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "3e144d314d083a762d664048e08c19baf8676d14", "filename": "tasks/section_04_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 4.1.1 Restrict Core Dumps (Scored)\n    lineinfile: dest='/etc/security/limits.conf' line=\"* hard core 0\" state=present\n    tags:\n      - section4\n      - section4.1\n      - section4.1.1\n\n  - name: 4.1.2 Restrict Core Dumps (Scored)\n    sysctl: >\n        name=fs.suid_dumpable\n        value=0\n        state=present\n    when: restrict_core_dumps == True\n    tags:\n      - section4\n      - section4.1\n      - section4.1.2\n\n  - name: 4.1.4 Restrict Core Dumps (stat file) (Scored)\n    stat: path='/etc/init/apport.conf'\n    register: apport_present\n    tags:\n      - section4\n      - section4.1\n      - section4.1.4\n\n  - name: 4.1.5 Restrict Core Dumps (Scored)\n    lineinfile: >\n        dest='/etc/init/apport.conf'\n        line='#start on runlevel [2345]'\n        state=present\n        regexp='start on runlevel'\n    when: apport_present.stat.exists == True\n    tags:\n      - section4\n      - section4.1\n      - section4.1.3\n\n  - name: 4.1.6 Restrict Core Dumps (stat file) (Scored)\n    stat: path='/etc/init/whoopsie.conf'\n    register: whoopsie_present\n    tags:\n      - section4\n      - section4.1\n      - section4.1.4\n\n  - name: 4.1.7 Restrict Core Dumps (Scored)\n    lineinfile: >\n        dest='/etc/init/whoopsie.conf'\n        line='#start on runlevel [2345]'\n        state=present\n        regexp='start on runlevel'\n    when: whoopsie_present.stat.exists == True\n    tags:\n      - section4\n      - section4.1\n      - section4.1.4\n\n  - name: 4.2 Enable XD/NX Support on 32-bit x86 Systems (read dmesg) (Not Scored)\n    shell: 'dmesg | grep NX'\n    register: nx_result\n    failed_when: nx_result.rc == 1\n    changed_when: False\n    ignore_errors: True\n    tags:\n      - section4\n      - section4.2\n\n  - name: 4.3 Enable Randomized Virtual Memory Region Placement (Scored)\n    sysctl: >\n       name=kernel.randomize_va_space\n       value=2\n       state=present\n    when: enable_aslr\n    tags:\n      - section4\n      - section4.3\n\n  - name: 4.4 Disable Prelink (check) (Scored)\n    stat: path=/usr/sbin/prelink\n    register: prelink_rc\n    tags:\n      - section4\n      - section4.4\n\n  - name: 4.4 Disable Prelink (restore) (Scored)\n    command: '/usr/sbin/prelink -ua'\n    when: prelink_rc.stat.exists == True\n    tags:\n      - section4\n      - section4.4\n\n  - name: 4.4 Disable Prelink (remove) (Scored)\n    apt: purge=yes name='prelink' state=absent\n    when: prelink_rc.stat.exists == True\n    tags:\n      - section4\n      - section4.4\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "11f267be5dbfc951862caac6bded685dfbb40b7d", "filename": "ops/playbooks/roles/hpe.haproxy/vars/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "---\n# vars file for hpe.haproxy\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "ce0486feb1b5792f8a702c69e59efeda3dad1f43", "filename": "roles/owncloud/tasks/F18.yml", "repository": "iiab/iiab", "decoded_content": "- name: Remove /etc/owncloud to avoid confusion as we use the config in {{ owncloud_prefix }}/owncloud/config/\n  file: path=/etc/owncloud\n        state=absent\n\n# but we use the tar file to get the latest version; really only benefits the xo4 on fedora 18\n- name: Get the owncloud software\n  get_url: url=\"{{ owncloud_dl_url }}\"/{{ owncloud_src_file }}  dest={{ downloads_dir }}/{{ owncloud_src_file }}\n  when: internet_available \n\n- name: Copy it to permanent location /opt\n  unarchive: src={{ downloads_dir }}/{{ owncloud_src_file }}  dest=/opt/\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "56d351e28a79f333da4a9fb5070d243e70f34da6", "filename": "roles/config-postgresql/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\nmode: containerized\n\npostgresql_name: postgresql\npostgresql_service: \"{{ postgresql_name }}.service\"\n\n#Systemd\nsystemd_service_dir: /usr/lib/systemd/system\nsystemd_environmentfile_dir: /etc/sysconfig\n\n# Postgresql\npostgresql_image: registry.access.redhat.com/rhscl/postgresql-96-rhel7:latest\npostgresql_storage_dir: /var/lib/{{ postgresql_name }}\npostgresql_container_storage_dir: /var/lib/pgsql/data\npostgresql_database: sampledb\npostgresql_container_port: 5432\npostgresql_host_port: 5432\n\n# These Values will be randomaly generated if not defined\n#postgresql_username: postgresql\n#postgresql_password: postgresql\n#postgresql_admin_user: postgresqladmin\n#postgresql_admin_password: postgresqladmin\n\npostgresql_db_uri: \"postgresql://{{ postgresql_admin_user }}:{{ postgresql_admin_password }}@{{ database_service }}/{{ postgresql_database }}\"\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "5a6c88324e8f2023600359b78cca88a5a114f477", "filename": "roles/ovirt-engine-cleanup/meta/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "galaxy_info:\n  author: Lukas Bednar\n  description: generates answer file for engine-cleanup and execute it.\n  company: Red Hat\n\n  # If the issue tracker for your role is not on github, uncomment the\n  # next line and provide a value\n  issue_tracker_url: https://github.com/rhevm-qe-automation/ovirt-ansible/issues\n\n  # Some suggested licenses:\n  # - BSD (default)\n  # - MIT\n  # - GPLv2\n  # - GPLv3\n  # - Apache\n  # - CC-BY\n  license: GPLv3\n\n  min_ansible_version: 1.2\n\n  # Optionally specify the branch Galaxy will use when accessing the GitHub\n  # repo for this role. During role install, if no tags are available,\n  # Galaxy will use this branch. During import Galaxy will access files on\n  # this branch. If travis integration is cofigured, only notification for this\n  # branch will be accepted. Otherwise, in all cases, the repo's default branch\n  # (usually master) will be used.\n  #github_branch:\n\n  #\n  # Below are all platforms currently available. Just uncomment\n  # the ones that apply to your role. If you don't see your\n  # platform on this list, let us know and we'll get it added!\n  #\n  platforms:\n    - name: EL\n      versions:\n        - all\n\n  galaxy_tags:\n    # List tags for your role here, one per line. A tag is\n    # a keyword that describes and categorizes the role.\n    # Users find roles by searching for tags. Be sure to\n    # remove the '[]' above if you add tags to this list.\n    #\n    # NOTE: A tag is limited to a single word comprised of\n    # alphanumeric characters. Maximum 20 tags per role.\n    - installer\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a0190e66c7152567180a84db90dc9dae1cbc2999", "filename": "roles/dhcp/tasks/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: 'Pre-Req: Install required packages '\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - firewalld\n  - python-firewall\n  - libsemanage-python\n  - dhcp\n  notify: 'enable and start dhcp services'\n\n- name: 'Start firewalld service'\n  service:\n    name: firewalld\n    enabled: yes\n    state: started\n\n- name: 'Pre-Req: Open Firewall for dhcp use'\n  firewalld:\n    service: \"{{item}}\"\n    permanent: yes\n    state: enabled\n    immediate: yes\n  with_items:\n  - dhcp\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "6d399362cfebd51fde7bf2cbc398d1d4a65c0d8b", "filename": "tasks/section_02.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - include: section_02_level1.yml\n    tags:\n      - section02\n      - level1\n\n  - include: section_02_level2.yml\n    tags:\n      - section02\n      - level2"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "38f7070419ac7825c52248e71c9ac8de43ee3c29", "filename": "roles/6-generic-apps/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# Generic Apps\n\n- name: ...IS BEGINNING ====================================\n  command: echo\n\n- name: DOKUWIKI\n  include_role:\n    name: dokuwiki\n  when: dokuwiki_install\n  tags: dokuwiki\n\n- name: MEDIAWIKI\n  include_role:\n    name: mediawiki\n  when: mediawiki_install\n  tags: mediawiki\n\n- name: ELGG\n  include_role:\n    name: elgg\n  when: elgg_install\n  tags: elgg\n\n- name: EJABBERD\n  include_role:\n    name: ejabberd\n  when: ejabberd_install\n  tags: ejabberd\n\n- name: NEXTCLOUD\n  include_role:\n    name: nextcloud\n  when: nextcloud_install\n  tags: nextcloud\n\n- name: OWNCLOUD\n  include_role:\n    name: owncloud\n  when: owncloud_install\n  tags: owncloud\n\n- name: WORDPRESS\n  include_role:\n    name: wordpress\n  when: wordpress_install\n  tags: wordpress\n\n- name: Recording STAGE 6 HAS COMPLETED ====================\n  lineinfile:\n    dest: /etc/iiab/iiab.env\n    regexp: '^STAGE=*'\n    line: 'STAGE=6'\n    state: present\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "0936e1f9196bb2a2acd404285715fa7265dd290f", "filename": "roles/network/tasks/detected_redhat.yml", "repository": "iiab/iiab", "decoded_content": "- name: Checking for ifcfg-WAN file - Can Fail\n  stat: path=/etc/sysconfig/network-scripts/ifcfg-WAN\n  when: not first_run\n  register: has_ifcfg_WAN\n  ignore_errors: True\n\n- name: Setting ifcfg-WAN True\n  set_fact:\n    has_WAN: True\n  when: not first_run and has_ifcfg_WAN.stat.exists\n\n# DETECT -- gateway and wireless - Can Fail\n- name: Get a list of slaves from previous config - Can Fail\n  shell: \"egrep -rn BRIDGE=br0 /etc/sysconfig/network-scripts/ifcfg-* | gawk -F'[-:]' '{print $3}'\"\n  when: not first_run\n  register: ifcfg_slaves\n  ignore_errors: True\n  changed_when: False\n\n# returns list of paths\n- name: Find gateway config based on device - Can Fail\n  shell: \"egrep -rn {{ device_gw }} /etc/sysconfig/network-scripts/ifcfg* | gawk -F ':' '{print $1}'\"\n  when: not first_run and device_gw != \"none\"\n  register: ifcfg_gw_device\n  ignore_errors: True\n  changed_when: False\n\n# last match wins\n- name: Setting has ifcfg gw based on device if found\n  set_fact:\n    has_ifcfg_gw: \"{{ item|trim }}\"\n  when: ifcfg_gw_device.stdout_lines is defined and item|trim != \"\" and item|trim != \"/etc/sysconfig/network-scripts/ifcfg-LAN\"\n  with_items:\n      - \"{{ ifcfg_gw_device.stdout_lines }}\"\n  ignore_errors: True\n\n# returns path\n- name: Find active gateway config based on macaddress - Can Fail\n  shell: \"egrep -irn {{ ansible_default_ipv4.macaddress }} /etc/sysconfig/network-scripts/ifcfg* | gawk -F ':' '{print $1}' | head -n 1\"\n  when: ansible_default_ipv4.gateway is defined\n  register: ifcfg_gw_mac\n  ignore_errors: True\n  changed_when: False\n\n- name: Set has ifcfg gw based on macaddress if found\n  set_fact:\n    has_ifcfg_gw: \"{{ ifcfg_gw_mac.stdout|trim }}\"\n  when: ifcfg_gw_mac is defined and ifcfg_gw_mac.changed and ifcfg_gw_mac.stdout != \"\"\n\n# could use something else - Can Fail\n- name: Find WiFi gateway config if present - Can Fail\n  shell: egrep -rn ESSID /etc/sysconfig/network-scripts/ifcfg* | gawk -F ':' '{print $1}' | gawk -F '/' '{print $5}'\n  register: ifcfg_WAN_wifi\n  ignore_errors: True\n\n#returns file name\n- name: Setting has_wifi_gw based on ESSID if found - Can Fail\n  set_fact:\n    has_wifi_gw: \"{{ item|trim }}\"\n  when: ifcfg_WAN_wifi.changed and item|trim != \"\"\n  with_items:\n      - \"{{ ifcfg_WAN_wifi.stdout_lines }}\"\n\n- name: Finding device for WiFi AP gateway - Can Fail\n  shell: egrep -rn DEVICE /etc/sysconfig/network-scripts/{{ has_wifi_gw }} |  gawk -F '=' '{print $2}'\n  when: has_wifi_gw != \"none\" and has_ifcfg_gw != \"none\"\n  register: AP_device\n  ignore_errors: True\n\n- name: Setting WiFi device\n  set_fact:\n    ap_device: \"{{ AP_device.stdout }}\"\n  when: AP_device.stdout is defined and AP_device.stdout != \"\"\n\n#unused\n#- name: Get a list of ifcfg files to delete - Can Fail\n#  shell: \"ls -1 /etc/sysconfig/network-scripts/ifcfg-* | grep -v -e ifcfg-lo  -e ifcfg-WAN -e {{ has_wifi_gw }}\"\n#  register: ifcfg_files\n#  changed_when: False\n#  ignore_errors: True\n#  when: num_lan_interfaces >= \"1\" or iiab_wireless_lan_iface != \"none\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5355de1b3aea09d95dd334892205e0d316dafdd9", "filename": "roles/dns/manage-dns-zones/tasks/route53/loop-records.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: \"Loop through zone records\"\n  include_tasks: empty-zone.yml\n  with_subelements:\n    - \"{{ r53_zone.ResourceRecordSets }}\"\n    - ResourceRecords\n  loop_control:\n    loop_var: r53_record\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "d7e54cb142eb911e4957936664a427c6f3d02a1a", "filename": "playbooks/roles/kafka/vars/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# vars file for kafka"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9117b3795fa83a9fcbfff293bef4234dea94df1e", "filename": "roles/config-mysql/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Install Containerized MySQL\n  include_tasks: install_containerized.yml\n  when: mode == \"containerized\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "435139849c06389d14682eaeca54e1da61cb00b2", "filename": "playbooks/openstack/openshift-cluster/files/heat_stack_server.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "heat_template_version: 2014-10-16\n\ndescription: OpenShift cluster server\n\nparameters:\n\n  name:\n    type: string\n    label: Name\n    description: Name\n\n  cluster_env:\n    type: string\n    label: Cluster environment\n    description: Environment of the cluster\n\n  cluster_id:\n    type: string\n    label: Cluster ID\n    description: Identifier of the cluster\n\n  type:\n    type: string\n    label: Type\n    description: Type master or node\n\n  subtype:\n    type: string\n    label: Sub-type\n    description: Sub-type compute or infra for nodes, default otherwise\n    default: default\n\n  key_name:\n    type: string\n    label: Key name\n    description: Key name of keypair\n\n  image:\n    type: string\n    label: Image\n    description: Name of the image\n\n  flavor:\n    type: string\n    label: Flavor\n    description: Name of the flavor\n\n  net:\n    type: string\n    label: Net ID\n    description: Net resource\n\n  net_name:\n    type: string\n    label: Net name\n    description: Net name\n\n  subnet:\n    type: string\n    label: Subnet ID\n    description: Subnet resource\n\n  secgrp:\n    type: comma_delimited_list\n    label: Security groups\n    description: Security group resources\n\n  floating_network:\n    type: string\n    label: Floating network\n    description: Network to allocate floating IP from\n\noutputs:\n\n  name:\n    description: Name of the server\n    value: { get_attr: [ server, name ] }\n\n  private_ip:\n    description: Private IP of the server\n    value:\n      get_attr:\n        - server\n        - addresses\n        - { get_param: net_name }\n        - 0\n        - addr\n\n  floating_ip:\n    description: Floating IP of the server\n    value:\n      get_attr:\n        - server\n        - addresses\n        - { get_param: net_name }\n        - 1\n        - addr\n\nresources:\n\n  server:\n    type: OS::Nova::Server\n    properties:\n      name:      { get_param: name }\n      key_name:  { get_param: key_name }\n      image:     { get_param: image }\n      flavor:    { get_param: flavor }\n      networks:\n        - port:  { get_resource: port }\n      user_data: { get_resource: config }\n      user_data_format: RAW\n      metadata:\n        environment: { get_param: cluster_env }\n        clusterid: { get_param: cluster_id }\n        host-type: { get_param: type }\n        sub-host-type:    { get_param: subtype }\n\n  port:\n    type: OS::Neutron::Port\n    properties:\n      network: { get_param: net }\n      fixed_ips:\n        - subnet: { get_param: subnet }\n      security_groups: { get_param: secgrp }\n\n  floating-ip:\n    type: OS::Neutron::FloatingIP\n    properties:\n      floating_network: { get_param: floating_network }\n      port_id: { get_resource: port }\n\n  config:\n    type: OS::Heat::CloudConfig\n    properties:\n      cloud_config:\n        disable_root: true\n\n        hostname: { get_param: name }\n\n        system_info:\n          default_user:\n            name: openshift\n            sudo: [\"ALL=(ALL) NOPASSWD: ALL\"]\n\n        write_files:\n          - path: /etc/sudoers.d/00-openshift-no-requiretty\n            permissions: 440\n            # content: Defaults:openshift !requiretty\n            # Encoded in base64 to be sure that we do not forget the trailing newline or\n            # sudo will not be able to parse that file\n            encoding: b64\n            content: RGVmYXVsdHM6b3BlbnNoaWZ0ICFyZXF1aXJldHR5Cg==\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "8f5d07aa747dbe35fa4ae50c8ecf24ff3d49ae8c", "filename": "roles/ovirt-engine-remote-db/vars/default.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\npostgres_service_name: \"postgresql\"\npostgres_server: \"postgresql-server\"\npostgres_data_dir: \"/var/lib/pgsql/data\"\npostgres_config_file: \"/var/lib/pgsql/data/postgresql.conf\"\npostgres_setup_cmd: \"/usr/bin/initdb  -D /var/lib/pgsql/data\""}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a146d9ccd3a24ffd588e70b93d343299a47e5aba", "filename": "roles/osp/admin-user/tasks/roles.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Grant access for account {{ role.name }}\"\n  shell: >\n    source {{ admin_keystonerc_file }};\n    openstack role add \\\n      --user \"{{ role.name }}\" \\\n      --user-domain \"{{ role.domain }}\" \\\n      --project \"{{ item.0.name }}\" \\\n      --project-domain \"{{ item.0.domain }}\" \\\n      \"{{ item.1 }}\"\n  with_subelements:\n  - \"{{ role.projects }}\"\n  - roles\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "9137748e915cdb58b04f4d1c9a5118018697bd67", "filename": "roles/weave/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for weave\nweave_bridge: \"10.2.0.1/16\"\nweave_server_group: weave_servers\nweave_docker_subnet: \"\n    {%- for host in groups[weave_server_group] -%}\n      {%- if host == 'default' or host == inventory_hostname or host == ansible_fqdn or host in ansible_all_ipv4_addresses -%}\n        10.2.{{ loop.index }}.0/24\n      {%- endif -%}\n    {%- endfor -%}\n\"\nweave_launch_peers: \"\n  {%- set weave_peers = [] -%}\n  {%- for host in groups[weave_server_group] -%}\n    {%- if host != inventory_hostname and host not in weave_peers -%}\n      {% do weave_peers.append(hostvars[host]['ansible_eth0']['ipv4']['address']) %}\n    {%- endif -%}\n  {%- endfor -%}\n  {{ weave_peers|join(' ') }}\n\"\nweave_docker_opts: \"--bridge=weave --fixed-cidr={{ weave_docker_subnet }} --dns 172.17.42.1 --dns 8.8.8.8 --dns-search service.{{ consul_domain }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c0f2fcfe6bac86aeb66d8ce27c06a2373a94259c", "filename": "roles/config-versionlock/tasks/prereq-Fedora.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install required packages\"\n  package:\n    name: \"{{ item }}\"\n    state: installed\n  with_items:\n  - python2-dnf-plugins-extras-versionlock\n  - python3-dnf-plugins-extras-versionlock\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "31b34488ecb01d6738c4d1a59126870506c4e209", "filename": "roles/dnsmasq/vars/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# vars file for dnsmasq\n"}, {"commit_sha": "218cdc58f9fe9d7ece7d43e5f100fe9631fde5cc", "sha": "b439d6d73c57be076b76d2cd0c82ea4916366bb4", "filename": "tasks/install.yml", "repository": "fubarhouse/ansible-role-golang", "decoded_content": "---\n- name: \"Go-Lang | Define shell exports\"\n  set_fact:\n    shell_exports:\n    - regex: \"export GOROOT={{ GOROOT }}\"\n      lineinfile: \"export GOROOT={{ GOROOT }}\"\n    - regex: \"export GOPATH={{ GOPATH }}/bin\"\n      lineinfile: \"export GOPATH={{ GOPATH }}/bin\"\n    - regex: \"export PATH=$PATH:{{ GOPATH }}/bin\"\n      lineinfile: \"export PATH=$PATH:{{ GOPATH }}/bin\"\n  when: shell_exports is undefined\n\n- name: \"Go-Lang | Get distribution\"\n  become: yes\n  become_user: root\n  get_url:\n    url: \"{{ go_url }}\"\n    dest: \"{{ go_temporary_dir }}/go{{ go_version }}.{{ GOOS }}-{{ GOARCH }}.tar.gz\"\n    validate_certs: no\n\n- name: \"Go-Lang | Empty destination directory\"\n  become: yes\n  become_user: root\n  file:\n    path: \"{{ GOROOT }}\"\n    state: absent\n\n- name: \"Go-Lang | Ensure directory is writable\"\n  become: yes\n  become_user: root\n  file:\n    path: \"{{ GOROOT }}\"\n    state: directory\n    owner: \"{{ fubarhouse_user }}\"\n    mode: 0755\n    recurse: true\n\n- name: \"Go-Lang | Unpack distribution\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  unarchive:\n    src: \"{{ go_temporary_dir }}/go{{ go_version }}.{{ GOOS }}-{{ GOARCH }}.tar.gz\"\n    dest: \"{{ go_temporary_dir }}\"\n    copy: \"no\"\n\n- name: \"Go-Lang | Moving to installation directory\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  synchronize:\n    src: \"{{ go_temporary_dir }}/go/\"\n    dest: \"{{ GOROOT }}\"\n    delete: yes\n    recursive: yes\n  delegate_to: \"{{ inventory_hostname }}\"\n  when: ansible_ssh_user is undefined\n\n- name: \"Go-Lang | Moving to installation directory\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell: \"cp -rf {{ go_temporary_dir }}/go/* {{ GOROOT }}/\"\n  when: ansible_ssh_user is defined\n\n- name: \"Go-Lang | Remove temporary data\"\n  become: yes\n  become_user: root\n  file:\n    path: \"{{ go_temporary_dir }}/go/\"\n    state: absent\n\n- name: \"Go-Lang | Ensure shell profiles are available\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  file:\n    path: \"{{ fubarhouse_user_dir }}/{{ item }}\"\n    state: touch\n  with_items: \"{{ shell_profiles }}\"\n  failed_when: false\n  when: shell_exports is defined\n\n- name: \"Go-Lang | Ensure shell profiles are configured\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  lineinfile:\n    dest: \"{{ fubarhouse_user_dir }}/{{ item[0] }}\"\n    regexp: \"{{ item[1].regex }}\"\n    line: \"{{ item[1].lineinfile }}\"\n    state: present\n  with_nested:\n  - \"{{ shell_profiles }}\"\n  - \"{{ shell_exports }}\"\n  ignore_errors: yes\n  when: shell_exports is defined\n\n- name: \"Go-Lang | Verify version\"\n  shell: \"export GOROOT={{ GOROOT }}; export GOPATH={{ GOPATH }}/bin; {{ GOPATH }}/go version\"\n  register: go_version_output\n  failed_when: '\"{{ go_version }}\" not in \"{{ go_version_output.stdout }}\"'\n\n- name: \"Go-Lang | Restart shell\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell: \"source {{ fubarhouse_user_dir }}/{{ item }}\"\n  with_items: \"{{ shell_profiles }}\"\n  failed_when: false\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7b879f8cd4ab0c6974a33e633a5d2df1bb2fa315", "filename": "playbooks/provision-idm-server/README.md", "repository": "redhat-cop/infra-ansible", "decoded_content": "# IdM Server playbook\n\nThis playbooks runs through the steps to provision a VM, set it up, configure DNS records, and install IdM.\nCurrently it is configured to provision OpenStack resources, but other providers can easily be added.\n\n\n## Prerequisites\n\nFor hosting infrastructure, you will need one of the two:\n- a set of running instance(s)\n- a IaaS that allow for provisioning through these playbooks\n\nYou must have a working DNS server which accepts \"nsupdate\" connections for the IdM VMs forward and reverse DNS records to be added/updated to the existing DNS zones. For this, you will need the DNS zone key names, key secrets, and key algorithms.\n\n## Example\n\n### Inventory\n\nPlease see the **sample** inventory in the inventory area:\n\n- [IdM-server](../../inventory/idm-server/README.md)\n\n\nYou will need to modify this sample inventory to fit your desired configuration, including information from your DNS server such as the key names, secrets, and more.\n\n\n### Playbook execution\n\nDepending on how this is being hosted, the initial may need the `tags='install'` set to ensure all necessary software is installed:\n\n```bash\n> ansible-playbook -i inventory main.yml --tags='install'\n```\n\nAny consecutive runs can be done without the 'install' tag to speed up execution:\n```bash\n> ansible-playbook -i inventory main.yml\n```\n\nLicense\n-------\n\nApache License 2.0\n\n\nAuthor Information\n------------------\n\nRed Hat Community of Practice & staff of the Red Hat Open Innovation Labs.\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "5f05ec60b668039b6e68c22f68384debc9d3072e", "filename": "playbooks/roles/sensor-defaults/defaults/main.yml", "repository": "rocknsm/rock", "decoded_content": "%YAML 1.1\n---\nrock_version: 2.1.0\nrock_online_install: true\nrock_sysctl_file: /etc/sysctl.d/10-ROCK.conf\nrock_data_dir: /data\nrocknsm_dir: /opt/rocknsm\nrock_data_user: root\nrock_data_group: root\nrock_monifs: \"{{ ansible_interfaces | difference(['lo', ansible_default_ipv4.interface | default('lo') ])| list }}\"\nrock_hostname: \"{{ inventory_hostname_short }}\"\nrock_fqdn: \"{{ inventory_hostname }}\"\nrock_mgmt_nets: [ \"0.0.0.0/0\" ]\nrock_cache_dir: /srv/rocknsm/support\npulledpork_rules:\n  - { url: \"https://snort.org/downloads/community/\", file: \"community-rules.tar.gz\", apikey: \"Community\", test: \"{{not with_suricata and with_snort}}\" }\n  - { url: \"https://www.snort.org/reg-rules/\", file: \"snortrules-snapshot.tar.gz\", apikey: \"796f26a2188c4c953ced38ff3ec899d8ae543350\", test: \"{{not with_suricata and with_snort}}\" }\n  - { url: \"https://rules.emergingthreats.net/\", file: \"emerging.rules.tar.gz\", apikey: \"open-nogpl\" }\n  - { url: \"http://talosintel.com/feeds/ip-filter.blf\", file: \"IPBLACKLIST\", apikey: \"open\" }\n\n#### Retention Configuration ####\nelastic_close_interval: 15\nelastic_delete_interval: 60\nkafka_retention: 168\nsuricata_retention: 3\nbro_log_retention: 0\nbro_stats_retention: 0\n\n# Feature options - Don't flip these unless you know what you're doing\n# These control if the service is installed\nwith_stenographer: true\nwith_docket: true\nwith_bro: true\nwith_suricata: true\nwith_snort: false\nwith_pulledpork: true\nwith_logstash: true\nwith_elasticsearch: true\nwith_kibana: true\nwith_filebeat: true\nwith_zookeeper: true\nwith_kafka: true\nwith_nginx: true\nwith_lighttpd: true\nwith_fsf: true\n\n# Feature options - Don't flip these unless you know what you're doing\n# These control if the systemd service is enabled\nenable_stenographer: false\nenable_docket: false\nenable_bro: true\nenable_suricata: true\nenable_snort: false\nenable_pulledpork: true\nenable_logstash: true\nenable_elasticsearch: true\nenable_kibana: true\nenable_filebeat: true\nenable_zookeeper: true\nenable_kafka: true\nenable_nginx: true\nenable_lighttpd: true\nenable_fsf: false\n\nrocknsm_package_list:\n  - java-1.8.0-openjdk-headless\n  - jq\n  - GeoIP\n  - GeoIP-update\n  - tcpreplay\n  - tcpdump\n  - bats\n  - policycoreutils-python\n  - htop\n  - vim\n  - git\n  - tmux\n  - nmap-ncat\n  - logrotate\n  - perl-LWP-Protocol-https\n  - perl-Sys-Syslog\n  - perl-Crypt-SSLeay\n  - perl-Archive-Tar\n\n\nhttp_tls_crt: /etc/pki/tls/certs/http_tls_crt.pem\nhttp_tls_pub: /etc/pki/tls/certs/http_tls_pub.pem\nhttp_tls_key: /etc/pki/tls/private/http_tls_key.pem\nhttp_tls_combined: /etc/pki/tls/private/httpd-combined.pem\nhttp_tls_dhparams: /etc/pki/tls/misc/http_tls_dhparams.pem\n\ndocket_web_pemfile: \"{{ http_tls_combined }}\"\ndocket_web_dhparams: \"{{ http_tls_dhparams }}\"\n\nepel_baseurl: http://download.fedoraproject.org/pub/epel/$releasever/$basearch/\nepel_gpgurl:  https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7\nelrepo_baseurl: http://elrepo.org/linux/kernel/el7/x86_64/\nelrepo_gpgurl: https://www.elrepo.org/RPM-GPG-KEY-elrepo.org\nelastic_baseurl: https://artifacts.elastic.co/packages/5.x/yum\nelastic_gpgurl: https://artifacts.elastic.co/GPG-KEY-elasticsearch\npulledpork_release: 0.7.2\npulledpork_url: \"https://github.com/shirkdog/pulledpork/archive/{{ pulledpork_release }}.tar.gz\"\npulledpork_filename: \"pulledpork-{{ pulledpork_release }}.tar.gz\"\npulledpork_engine_basepath: \"/etc/{{ \\\"suricata\\\" if with_suricata else \\\"snort\\\" }}\"\n\nrocknsm_baseurl: https://packagecloud.io/rocknsm/2/el/7/$basearch\nrocknsm_gpgurl: https://packagecloud.io/rocknsm/2/gpgkey\nrocknsm_local_baseurl: file:///srv/rocknsm\nbro_user: bro\nbro_group: bro\nbro_data_dir: \"{{ rock_data_dir }}/bro\"\nbro_prefix: /usr\nbro_sysconfig_dir: /etc/bro\nbro_site_dir: /usr/share/bro/site\nbro_cpu: \"{{ (ansible_processor_vcpus // 2) if (ansible_processor_vcpus <= 16) else 8 }}\"\nbro_rockscripts_repo: https://github.com/rocknsm/rock-scripts.git\nbro_rockscripts_branch: master\nbro_rockscripts_filename: \"rock-scripts_{{ bro_rockscripts_branch | replace('/', '-') }}.tar.gz\"\nrock_dashboards_repo: https://github.com/rocknsm/rock-dashboards.git\nrock_dashboards_branch: master\nrock_dashboards_url: \"https://github.com/rocknsm/rock-dashboards/archive/{{ rock_dashboards_branch }}.tar.gz\"\nrock_dashboards_filename: \"rock-dashboards_{{ rock_dashboards_branch | replace('/', '-') }}.tar.gz\"\nrock_dashboards_version: 2.0\nstenographer_user: stenographer\nstenographer_group: stenographer\nstenographer_data_dir: \"{{ rock_data_dir }}/stenographer\"\nsuricata_user: suricata\nsuricata_group: suricata\nsuricata_data_dir: \"{{ rock_data_dir }}/suricata\"\npulled_pork_repo: https://github.com/shirkdog/pulledpork.git\npulled_pork_oinkcode: 796f26a2188c4c953ced38ff3ec899d8ae543350\nfsf_user: fsf\nfsf_group: fsf\nfsf_data_dir: \"{{ rock_data_dir }}/fsf\"\nfsf_archive_dir: \"{{ fsf_data_dir }}/archive\"\nfsf_client_logfile: \"{{ fsf_data_dir }}/client.log\"\nkafka_user: kafka\nkafka_group: kafka\nkafka_data_dir: \"{{ rock_data_dir }}/kafka\"\nkafka_config_path: /etc/kafka/server.properties\nes_user: elasticsearch\nes_group: elasticsearch\nes_data_dir: \"{{ rock_data_dir }}/elasticsearch\"\nes_log_dir: /var/log/elasticsearch\nes_cluster_name: rocknsm\nes_node_name: \"{{ rock_hostname }}\"\nes_mem: \"{{ (ansible_memtotal_mb // 1024 // 2) if (ansible_memtotal_mb // 1024) < 64 else 31 }}\"\nes_url: \"http://localhost:9200\"\nes_memlock_override: |\n  [Service]\n  LimitMEMLOCK=infinity\nlogstash_user: logstash\nlogstash_group: logstash\n...\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "316cf9e6bc350c96f8f99165b8c62b70016668b4", "filename": "roles/dns/config-dns-server/tests/inventory/group_vars/dns-server.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nnamed_config:\n  recursion: 'no'\n  dnssec_enable: 'yes'\n  dnssec_validation: 'yes'\n  dnssec_lookaside: 'no'\n  allow_transfer:\n    - 192.168.48.21\n    - 192.168.48.22\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "9816fea1fc1ae0bac6f39be39af0248a097fe8d1", "filename": "roles/cloud-lightsail/tasks/venv.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Clean up the environment\n  file:\n    dest: \"{{ lightsail_venv }}\"\n    state: absent\n  when: clean_environment\n\n- name: Install requirements\n  pip:\n    name:\n      - boto>=2.5\n      - boto3\n    state: latest\n    virtualenv: \"{{ lightsail_venv }}\"\n    virtualenv_python: python2.7\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "6086cd98763240b4760616a80323272d687e6582", "filename": "roles/activity-server/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "activity_server_enabled: False\nactivity_server_install: True\n\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "dff68da9ad46fb5cc41a1b5e6dec270cd093276b", "filename": "roles/openshift-management/tasks/prune-images.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: Prune Images\n  shell: oc adm prune images --keep-tag-revisions={{ openshift_prune_images_tag_revisions }}  --keep-younger-than={{ openshift_prune_images_keep_younger }} --confirm\n  environment:\n    KUBECONFIG: \"{{ kubeconfig }}\""}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "d2212e53496b7cd96c79644924dd8122f14fd0d8", "filename": "roles/router-scaleup/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Gather facts\n  openshift_facts:\n    role: common\n\n- name: use the default project\n  shell: \"{{ openshift.common.client_binary }} project default\"\n\n- name: Count the infrastructure nodes\n  shell: \"{{ openshift.common.client_binary }}  get nodes --show-labels | grep role=infra -c\"\n  register: nodes\n  when: node_type == \"infra\"\n\n- name: Scale the router\n  shell: \"{{ openshift.common.client_binary }} scale dc/router --replicas={{ nodes.stdout }}\"\n  when: node_type == \"infra\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "caea144f23cc4b3bcbe55cf62884e59a9335c997", "filename": "roles/config-quay-builder/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Restart Quay Builder Service\n  systemd:\n    name: \"{{ quay_builder_service }}\"\n    enabled: yes\n    state: restarted\n    daemon_reload: yes"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c5e35b2f1a6274af4545f02621af9143e84a913d", "filename": "playbooks/osp/install-osp-cluster.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_playbook: ../infra-hosts.yml\n\n- hosts: infra_osp_hosts\n  roles:\n  - { role: config-software-src, when: '\"controller\" in osp_roles' }\n\n- hosts: infra_osp_hosts\n  roles:\n  - role: osp/packstack-install\n  - role: osp/packstack-post\n\n- import_playbook: update-osp-cluster-admin.yml\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "166061904535dd54f2b34f056dc1b0564500d657", "filename": "roles/scm/github.com/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Concatenate the users\n  set_fact:\n    list_of_users: \"{{ users | join(',') }}\"\n\n- name: Create new Github Team in {{ github_org_name }}, and add users {{ users }}\n  uri:\n    url: '{{ github_api_teams }}'\n    headers:\n      Accept: application/vnd.github.hellcat-preview+json\n    method: POST\n    user: '{{ github_api_username }}'\n    password: '{{ github_api_password }}'\n    force_basic_auth: yes\n    status_code: 201\n    body_format: json\n    body: '{\n             \"name\": \"{{ team.name }}\",\n             \"description\": \"{{ team.description | default(team.name) }}\",\n             \"permission\": \"{{ team.permission | default(\"admin\") }}\",\n             \"privacy\": \"{{ team.privacy | default(\"closed\") }}\",\n             \"maintainers\": [ {{ list_of_users }} ]\n           }'\n  register: teams_json_response\n\n- name: Add Repo to existing Github Organization\n  uri:\n    url: '{{ github_api_repos }}'\n    headers:\n      Accept: application/json\n    method: POST\n    user: '{{ github_api_username }}'\n    password: '{{ github_api_password }}'\n    force_basic_auth: yes\n    status_code: 201\n    body_format: json\n    body: '{\n             \"name\": \"{{ item.repo_name }}\",\n             \"description\": \"{{ item.description }}\",\n             \"homepage\": \"https://github.com\",\n             \"private\": {{ item.private_repo_bool | default(\"false\") }},\n             \"has_issues\": {{ item.has_issues | default(\"true\") }},\n             \"has_projects\": {{ item.has_projects | default(\"false\") }},\n             \"has_wiki\": {{ item.has_wiki | default(\"true\") }},\n             \"team_id\": {{ teams_json_response.json.id }},\n             \"auto_init\": {{ item.auto_init | default(\"false\") }}\n           }'\n  with_items: \"{{ repos }}\"\n  register: repos_json_response\n\n- name: Add a new deploy key to the GitHub repositories using basic authentication\n  github_deploy_key:\n    organization: \"{{ github_org_name }}\"\n    repo: \"{{ item.repo_name }}\"\n    name: \"deploy-key-{{ item.repo_name }}\"\n    key: \"{{ item.deploy_key_location }}\"\n    read_only: \"{{ item.deploy_key_read_only | default('no') }}\"\n    username: \"{{ github_api_username }}\"\n    password: \"{{ github_api_password }}\"\n    otp: \"{{ github_api_otp | default('123456') }}\"\n  with_items: \"{{ repos }}\"\n  register: deploy_keys_json_response\n\n- name: \"Create a temporary directory to use\"\n  tempfile:\n    state: directory\n  register: tmp_dir\n  notify: github.com cleanup temp\n\n- name: Seed Repo with another repo\n  git:\n    repo: '{{ item.seed_repo_url }}'\n    dest: '{{ tmp_dir.path }}/{{ item.repo_name }}'\n    version: '{{ item.seed_repo_version | default(\"HEAD\") }}'\n  with_items: '{{ repos }}'\n\n- name: Set seed repo to new locations\n  command: >\n    git remote set-url origin https://{{ github_api_username }}:{{ github_api_password }}@github.com/{{ github_org_name }}/{{ item.repo_name }}.git\n  args:\n    chdir: \"{{ tmp_dir.path }}/{{ item.repo_name }}\"\n  with_items: \"{{ repos }}\"\n\n- name: Push repos up to new locations\n  command: >\n    git push origin master\n  args:\n    chdir: \"{{ tmp_dir.path }}/{{ item.repo_name }}\"\n  with_items: \"{{ repos }}\"\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "faea03a28e08d10e2d9ad7e101a41dba2814fca1", "filename": "handlers/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# handlers file for ansible-role-docker-ce\n\n- name: restart docker\n  become: yes\n  service:\n    name: docker\n    state: restarted\n  tags: [\"install\", \"configure\"]\n\n- name: reload docker\n  become: yes\n  service:\n    name: docker\n    state: reloaded\n  tags: [\"install\", \"configure\"]\n\n# Workaround because systemd cannot be used: https://github.com/ansible/ansible/issues/22171\n- name: restart auditd\n  become: yes\n  command: service auditd restart\n  args:\n    warn: no\n  tags: [\"install\", \"configure\"]\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "63e4e8f4b630f6174c5d9d6b671fe00dac5a95fb", "filename": "roles/manage-confluence-space/tasks/copy_confluence_content.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Get Content from Source\n  uri:\n    url: '{{ confluence_source_url }}/wiki/rest/api/content/{{ confluence_space_content.id }}?expand=body.storage,history,space,container.history,container.version,version,ancestors'\n    method: GET\n    user: '{{ confluence_source_username }}'\n    password: '{{ confluence_source_password }}'\n    force_basic_auth: yes\n    status_code: 200\n    return_content: yes\n  register: content_json\n\n- name: Map ancestor ids if any\n  set_fact: \n    content_ancestors: \"{{ content_json.json.ancestors | map(attribute='id') | list | map('extract', id_mapping) | list }}\"\n\n- name: Pick only the last ancestor\n  set_fact:\n    content_ancestors: \"{{ [-1] | map('extract', content_ancestors) | list }}\"\n  when: content_ancestors|length > 0\n\n- name: Get the current page version\n  uri:\n    url: '{{ confluence_destination_url }}/wiki/rest/api/space/{{ atlassian.confluence.destination.key }}/content?expand=homepage,version'\n    method: GET\n    user: '{{ confluence_destination_username }}'\n    password: '{{ confluence_destination_password }}'\n    force_basic_auth: yes\n    status_code: 200\n    return_content: yes\n  register: homepage_content\n  no_log: false\n\n- name: set homepage at destination\n  block:\n    - set_fact:\n        homepage:\n          body:\n            storage:\n              value: \"{{ content_json.json.body.storage.value }}\"\n              representation: \"storage\"\n          title: \"{{ atlassian.confluence.destination.name }}\"\n          type: \"{{ content_json.json.type }}\"\n          space:\n            key: \"{{ atlassian.confluence.destination.key }}\"\n          version:\n            number: \"{{ homepage_content.json['page']['results'][0]['version']['number'] + 1 }}\"\n    - uri:\n        url: '{{ confluence_destination_url }}/wiki/rest/api/content/{{ destination_homepage_id }}'\n        method: PUT\n        user: '{{ confluence_destination_username }}'\n        password: '{{ confluence_destination_password }}'\n        force_basic_auth: yes\n        status_code: 200\n        body_format: json\n        body: \"{{  homepage | to_json }}\"\n        return_content: yes\n      register: homepage_content_json\n\n    - set_fact:\n       id_mapping: \"{{ id_mapping|combine({ content_json.json.id : { 'id' : homepage_content_json.json.id }}) }}\"\n  when: content_json.json.id == source_homepage_id\n\n- name: set payload for create contect\n  set_fact:\n    payload:\n      id: \"{{ content_json.json.id }}\"\n      title: \"{{ content_json.json.title }}\"\n      type: \"{{ content_json.json.type }}\"\n      space:\n        key: \"{{ atlassian.confluence.destination.key }}\"\n      body:\n        storage:\n          value: \"{{ content_json.json.body.storage.value }}\"\n          representation: \"storage\"\n\n- name: if page is child add ancestors\n  set_fact:\n    payload: \"{{ payload | combine( {'ancestors': content_ancestors} )}}\"\n  when: content_json.json.ancestors|length > 1\n\n- name: Create Content at Destination Site\n  uri:\n    url: '{{ confluence_destination_url }}/wiki/rest/api/content'\n    method: POST\n    user: '{{ confluence_destination_username }}'\n    password: '{{ confluence_destination_password }}'\n    force_basic_auth: yes\n    status_code: 200\n    body_format: json\n    body: \"{{  payload |  to_json }}\"\n    return_content: yes\n  register: new_content_json\n\n- name: Append New id to old id mapping\n  set_fact:\n    id_mapping: \"{{ id_mapping|combine({ content_json.json.id : { 'id' : new_content_json.json.id }}) }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "3ca4f302e134343195b84d40002b8f5a604755ea", "filename": "roles/dhcp/tests/vars.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# vars file for dhcp-config\ndhcp_host_entries:\n- host: test1\n  fqdn: test1.example.com\n  hw_addr: e0:07:1b:ec:fd:ec\n  ip_addr: 192.168.10.123\n\n- host: test2\n  fqdn: test2.example.com\n  hw_addr: e0:07:1b:fd:a5:70\n  ip_addr: 192.168.11.124\n\n\n#\n# Define the subnet entries\n#\ndhcp_subnet_entries:\n- subnet: 192.168.11.0\n  range: 192.168.11.50 192.168.11.99\n  subnet_mask: 255.255.255.0\n  router: 192.168.11.1\n  broadcast: 192.168.11.255\n  dns: 8.8.8.8, 8.8.4.4\n  domain_search: example.com \n  domain_name: example.com\n  next_server: 192.168.11.33\n\n- subnet: 192.168.12.0\n  range: 192.168.12.50 192.168.12.99\n  subnet_mask: 255.255.255.0\n  router: 192.168.12.1\n  broadcast: 192.168.12.255\n  dns: 8.8.8.8, 8.8.4.4\n  domain_search: example.com \n  domain_name: example.com\n  next_server: 192.168.12.33\n\n- subnet: 192.168.100.0\n  range: 192.168.100.50 192.168.100.99\n  subnet_mask: 255.255.255.0\n  router: 192.168.100.1\n  broadcast: 192.168.100.255\n  dns: 8.8.8.8, 8.8.4.4\n  domain_search: example.com \n  domain_name: example.com\n  next_server: 192.168.100.33\n\ndhcp_group_entries:\n- group:\n  title: '# Test group 1'\n  hosts:\n  - desc: '# Blade #1 - interface \"ens1f0\"'\n    name: node1\n    hw_addr: 00:0a:f7:57:e1:f0\n    ip_addr: 192.168.11.121\n    fqdn: node1.example.com\n\n  - desc: '# Blade #2 - interface \"ens1f0\"'\n    name: node2\n    hw_addr: 00:0a:f7:57:dd:80\n    ip_addr: 192.168.11.122\n    fqdn: node4.example.com\n\n- group: \n  title: '#Test group 2 '\n  hosts:\n  - desc: '# Blade #3 - interface \"ens1f0\"'\n    name: node3\n    hw_addr: 00:0a:f7:57:dd:80\n    ip_addr: 192.168.11.123\n    fqdn: node5.example.com\n\n  - desc: '# Blade #4 - interface \"ens1f0\"'\n    name: node4\n    hw_addr: 00:0a:f7:57:de:54\n    ip_addr: 192.168.11.124\n    fqdn: node6.example.com\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ff1a1c7f845b541d0b66e212a2cacc9043616c05", "filename": "roles/manage-confluence-space/tasks/prepare_confluence_vars.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Verify source and destination dictionaries exist\n  fail:\n    msg: \"This role requires that the following dictionaries exist: atlassian.confluence.source, atlassian.confluence.destination\"\n  when:\n    - (atlassian.confluence.source|trim == \"\") or (atlassian.confluence.destination|trim == \"\")\n\n- name: Set Confluence Credentials\n  set_fact:\n    confluence_source_username: \"{{ atlassian.confluence.source.username | default(atlassian.username) }}\"\n    confluence_source_password: \"{{ atlassian.confluence.source.password | default(atlassian.password) }}\"\n    confluence_destination_username: \"{{ atlassian.confluence.destination.username | default(atlassian.username) }}\"\n    confluence_destination_password: \"{{ atlassian.confluence.destination.password | default(atlassian.password) }}\"\n\n- name: Set Confluence URL\n  set_fact:\n    confluence_source_url: \"{{ atlassian.confluence.source.url | default(atlassian.url) }}\"\n    confluence_destination_url: \"{{ atlassian.confluence.destination.url | default(atlassian.url) }}\"\n\n- name: Fail when Confluence credentials are not set\n  fail:\n    msg: \"This role requires the following variables to be set: confluence_source_username, confluence_source_password, confluence_destination_username, confluence_destination_password.\"\n  when:\n    - (confluence_source_username|trim == \"\") or (confluence_source_password|trim == \"\") or\n      (confluence_destination_username|trim == \"\") or (confluence_destination_password|trim == \"\")\n\n- name: Fail when Confluence url is not set\n  fail:\n    msg: \"This role requires the following variables to be set: confluence_source_url, confluence_destination_url.\"\n  when:\n    - (confluence_source_url|trim == \"\") or (confluence_destination_url|trim == \"\")\n"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "8d4ee0f8eb09139e668c2226e5d597523a580031", "filename": "tasks/system/not-supported.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Warn on unsupported platform\"\n  fail:\n    msg: \"This role does not support '{{ ansible_os_family }}' platform.\\\n          Please contact support@lean-delivery.com\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6ade7095ad23fd7655c3262e547cc876e6dfcd9c", "filename": "reference-architecture/aws-ansible/playbooks/roles/cloudformation-infra/files/add-cns-storage-iops.json", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "{\n  \"AWSTemplateFormatVersion\": \"2010-09-09\",\n  \"Parameters\": {\n    \"KeyName\": {\n      \"Type\": \"AWS::EC2::KeyPair::KeyName\"\n    },\n    \"Route53HostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"PublicHostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"AmiId\": {\n      \"Type\": \"AWS::EC2::Image::Id\"\n    },\n    \"InstanceType\": {\n      \"Type\": \"String\",\n      \"Default\": \"m4.2xlarge\"\n    },\n    \"NodeRootVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"30\"\n    },\n    \"NodeDockerVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"NodeDockerVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"GlusterVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"GlusterVolSize\": {\n      \"Type\": \"Number\",\n      \"Default\": \"500\"\n    },\n    \"Iops\": {\n      \"Type\": \"Number\"\n    },\n    \"NodeUserData\": {\n      \"Type\": \"String\"\n    },\n    \"NodeEmptyVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"NodeEmptyVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"NodeRootVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"PublicHostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"NodeInstanceProfile\": {\n      \"Type\": \"String\"\n    },\n    \"NodeType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gluster\"\n    },\n    \"GlusterNodeDns1\": {\n      \"Type\": \"String\"\n    },\n    \"GlusterNodeDns2\": {\n      \"Type\": \"String\"\n    },\n    \"GlusterNodeDns3\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet1\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet2\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet3\": {\n      \"Type\": \"String\"\n    },\n    \"NodeSg\": {\n      \"Type\": \"String\"\n    }\n  },\n  \"Resources\": {\n    \"Route53Records\": {\n      \"Type\": \"AWS::Route53::RecordSetGroup\",\n      \"DependsOn\": [\n        \"GlusterNode1\",\n        \"GlusterNode2\",\n        \"GlusterNode3\"\n      ],\n      \"Properties\": {\n        \"HostedZoneName\": { \"Ref\": \"Route53HostedZone\" },\n        \"RecordSets\": [\n          {\n            \"Name\":  {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns1\"},{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t            \"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"GlusterNode1\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\":  {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns2\"},{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t            \"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"GlusterNode2\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\":  {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns3\"},{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t            \"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"GlusterNode3\", \"PrivateIp\"] }]\n          }\n        ]\n      }\n    },\n    \"GlusterNode1\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InstanceType\"},\n\t  \"SecurityGroupIds\": [{\"Ref\": \"NodeSg\"}],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet1\"},\n          \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns1\"},{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"provision\",\n              \"Value\": \"node\"\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"storage\" \n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdd\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"GlusterVolSize\"},\n              \"VolumeType\": {\"Ref\": \"GlusterVolType\"},\n              \"Iops\": {\"Ref\": \"Iops\"}\n            }\n          }\n         ]\n     }\n   },\n    \"GlusterNode2\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InstanceType\"},\n\t  \"SecurityGroupIds\": [{\"Ref\": \"NodeSg\"}],\n\t  \"SecurityGroupIds\": [{\"Ref\": \"NodeSg\"}],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet2\"},\n          \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns2\"},{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"provision\",\n              \"Value\": \"node\"\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"storage\" \n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdd\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"GlusterVolSize\"},\n              \"VolumeType\": {\"Ref\": \"GlusterVolType\"},\n              \"Iops\": {\"Ref\": \"Iops\"}\n            }\n          }\n         ]\n     }\n   },\n    \"GlusterNode3\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InstanceType\"},\n\t  \"SecurityGroupIds\": [{\"Ref\": \"NodeSg\"}],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet3\"},\n          \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns3\"},{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"provision\",\n              \"Value\": \"node\"\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": \"storage\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdd\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"GlusterVolSize\"},\n              \"VolumeType\": {\"Ref\": \"GlusterVolType\"},\n              \"Iops\": {\"Ref\": \"Iops\"}\n            }\n          }\n         ]\n     }\n   }\n }\n}\n"}, {"commit_sha": "bf6e08dcb2440421477b6536ff6a8d11adc2be17", "sha": "4259b433d60d91ad26218840c86ef00445302aea", "filename": "roles/zookeeper/meta/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\ngalaxy_info:\n  author: Graham Taylor\n  description:\n  company: Capgemini\n  # Some suggested licenses:\n  # - BSD (default)\n  # - MIT\n  # - GPLv2\n  # - GPLv3\n  # - Apache\n  # - CC-BY\n  license: license (MIT)\n  min_ansible_version: 1.2\n  #\n  # Below are all platforms currently available. Just uncomment\n  # the ones that apply to your role. If you don't see your\n  # platform on this list, let us know and we'll get it added!\n  #\n  platforms:\n  #- name: EL\n  #  versions:\n  #  - all\n  #  - 5\n  #  - 6\n  #  - 7\n  #- name: GenericUNIX\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Fedora\n  #  versions:\n  #  - all\n  #  - 16\n  #  - 17\n  #  - 18\n  #  - 19\n  #  - 20\n  #- name: SmartOS\n  #  versions:\n  #  - all\n  #  - any\n  #- name: opensuse\n  #  versions:\n  #  - all\n  #  - 12.1\n  #  - 12.2\n  #  - 12.3\n  #  - 13.1\n  #  - 13.2\n  #- name: Amazon\n  #  versions:\n  #  - all\n  #  - 2013.03\n  #  - 2013.09\n  #- name: GenericBSD\n  #  versions:\n  #  - all\n  #  - any\n  #- name: FreeBSD\n  #  versions:\n  #  - all\n  #  - 8.0\n  #  - 8.1\n  #  - 8.2\n  #  - 8.3\n  #  - 8.4\n  #  - 9.0\n  #  - 9.1\n  #  - 9.1\n  #  - 9.2\n  - name: Ubuntu\n    versions:\n  #  - all\n  #  - lucid\n  #  - maverick\n  #  - natty\n  #  - oneiric\n  #  - precise\n  #  - quantal\n  #  - raring\n  #  - saucy\n     - trusty\n  #- name: SLES\n  #  versions:\n  #  - all\n  #  - 10SP3\n  #  - 10SP4\n  #  - 11\n  #  - 11SP1\n  #  - 11SP2\n  #  - 11SP3\n  #- name: GenericLinux\n  #  versions:\n  #  - all\n  #  - any\n  #- name: Debian\n  #  versions:\n  #  - all\n  #  - etch\n  #  - lenny\n  #  - squeeze\n  #  - wheezy\n  #\n  # Below are all categories currently available. Just as with\n  # the platforms above, uncomment those that apply to your role.\n  #\n  categories:\n  - cloud\n  #- cloud:ec2\n  #- cloud:gce\n  #- cloud:rax\n  #- clustering\n  #- database\n  #- database:nosql\n  #- database:sql\n  #- development\n  #- monitoring\n  #- networking\n  #- packaging\n  - system\n  #- web\ndependencies:\n  - handlers\n  # List your role dependencies here, one per line. Only\n  # dependencies available via galaxy should be listed here.\n  # Be sure to remove the '[]' above if you add dependencies\n  # to this list.\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "e424ce8b36255dc6f705d981182f112d0d012663", "filename": "playbooks/templates/fsf-server-config.j2", "repository": "rocknsm/rock", "decoded_content": "#!/usr/bin/env python\n#\n# Basic configuration attributes for scanner. Used as default\n# unless the user overrides them. \n#\n\nSCANNER_CONFIG = { 'LOG_PATH' : '{{ fsf_data_dir }}',\n                   'YARA_PATH' : '/var/lib/yara-rules/rules.yara',\n                   'EXPORT_PATH' : '{{ fsf_archive_dir }}',\n                   'TIMEOUT' : 60,\n                   'PID_PATH': '/run/fsf/fsf.pid',\n                   'MAX_DEPTH' : 10,\n\t\t   'ACTIVE_LOGGING_MODULES': ['rockout', 'scan_log'] }\n\nSERVER_CONFIG = { 'IP_ADDRESS' : \"localhost\",\n                  'PORT' : 5800 }\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "11e16c7374bcbe5157af7f2d272587f7625999eb", "filename": "roles/kubevirt/defaults/main.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "namespace: \"kube-system\"\ndocker_prefix: \"kubevirt\"\ndocker_tag: \"latest\"\ncluster: \"openshift\"\naction: \"provision\"\nmanifest_version: \"release\"\nkubevirt_manifest_url: \"https://raw.githubusercontent.com/kubevirt/kubevirt/master/manifests\"\nkubevirt_template_dir: \"{{ role_path }}/templates\"\n\ndev_template_resources:\n  - \"offline-vm\"\n  - \"rbac.authorization.k8s\"\n  - \"replicase-resource\"\n  - \"virt-controller\"\n  - \"virt-handler\"\n  - \"vm-resource\"\n  - \"vmpreset-resource\"\n"}, {"commit_sha": "c91b6076e3a957fb0a165131d0ff3b3b208ed419", "sha": "652bdc840882bcd10fa0b521a560196b0e4c1134", "filename": "tasks/main.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - include: section_01.yml\n    tags: section01\n\n  - include: section_02.yml\n    tags: section02\n\n  - include: section_03.yml\n    tags: section03\n\n  - include: section_04.yml\n    tags: section04\n\n  - include: section_05.yml\n    tags: section05\n\n  - include: section_06.yml\n    tags: section06\n\n  - include: section_07.yml\n    tags: section07\n\n  - include: section_08.yml\n    tags: section08\n\n  - include: section_09.yml\n    tags: section09\n\n  - include: section_10.yml\n    tags: section10\n\n  - include: section_11.yml\n    tags: section11\n\n  - include: section_12.yml\n    tags: section12\n\n  - include: section_13.yml\n    tags: section13\n\n"}, {"commit_sha": "584d564218d6e2e63d3ecca157daf817fe4d533c", "sha": "c2df7fccef67ba7c0eb8ed178042d6677bbfdac4", "filename": "tasks/setup_replace_user.yml", "repository": "mikolak-net/ansible-raspi-config", "decoded_content": "---\n- name: Create user {{raspi_config_replace_user['name']}}\n  user: name={{raspi_config_replace_user['name']}}\n  changed_when: True #to force handler call\n  notify:\n    - remove default user\n- name: Add your login key to {{raspi_config_replace_user['name']}}\n  authorized_key: user={{raspi_config_replace_user['name']}} key=\"{{ lookup('file', raspi_config_replace_user['path_to_ssh_key']) }}\"\n- name: Add {{raspi_config_replace_user['name']}} to sudoers\n  lineinfile:\n  args:\n    dest: /etc/sudoers\n    line: \"{{raspi_config_replace_user['name']}} ALL=(ALL) NOPASSWD: ALL\""}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "90dd17706e524b980a208c38d19d1d2c0a19a898", "filename": "roles/network/tasks/named.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install named packages (debuntu)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n   - bind9\n   - bind9utils\n  when: is_debuntu\n  tags:\n    - download\n\n- name: Install named packages (OS's that are not debuntu)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n   - bind\n   - bind-utils\n  when: not is_debuntu\n  tags:\n    - download\n\n# or we have to change the serial number in the config files.\n- name: Stop named before copying files\n  service:\n    name: \"{{ dns_service }}\"\n    state: stopped\n  when: first_run and is_debuntu\n\n- name: Set folder permission\n  file:\n    path: \"{{ item }}\"\n    owner: \"{{ dns_user }}\"\n    group: root\n    mode: 0755\n    state: directory\n  with_items:\n    - /var/named-iiab\n    - /var/named-iiab/data\n    - /etc/sysconfig/olpc-scripts/domain_config.d\n\n- name: Configure named\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: \"{{ item.owner }}\"\n    group: root\n    mode: \"{{ item.mode }}\"\n  with_items:\n    - { src: 'roles/network/templates/named/named-iiab.conf.j2', dest: '/etc/named-iiab.conf', owner: \"root\", mode: '0644' }\n    - { src: 'roles/network/templates/named/named.j2', dest: '/etc/sysconfig/named', owner: \"root\", mode: '0644' }\n    - { src: 'roles/network/templates/named/named', dest: '/etc/sysconfig/olpc-scripts/domain_config.d/named', owner: \"root\", mode: '0644' }\n    - { src: 'roles/network/templates/named/localdomain.zone', dest: '/var/named-iiab/localdomain.zone', owner: \"{{ dns_user }}\", mode: '0644' }\n    - { src: 'roles/network/templates/named/localhost.zone', dest: '/var/named-iiab/localhost.zone', owner: \"{{ dns_user }}\", mode: '0644' }\n    - { src: 'roles/network/templates/named/named.broadcast', dest: '/var/named-iiab/named.broadcast', owner: \"{{ dns_user }}\", mode: '0644' }\n    - { src: 'roles/network/templates/named/named.ip6.local', dest: '/var/named-iiab/named.ip6.local', owner: \"{{ dns_user }}\", mode: '0644' }\n    - { src: 'roles/network/templates/named/named.local', dest: '/var/named-iiab/named.local', owner: \"{{ dns_user }}\", mode: '0644' }\n    - { src: 'roles/network/templates/named/named.rfc1912.zones', dest: '/var/named-iiab/named.rfc1912.zones', owner: \"{{ dns_user }}\", mode: '0644' }\n    - { src: 'roles/network/templates/named/named.root', dest: '/var/named-iiab/named.root', owner: \"{{ dns_user }}\", mode: '0644' }\n    - { src: 'roles/network/templates/named/named.root.hints', dest: '/var/named-iiab/named.root.hints', owner: \"{{ dns_user }}\", mode: '0644' }\n    - { src: 'roles/network/templates/named/named.zero', dest: '/var/named-iiab/named.zero', owner: \"{{ dns_user }}\", mode: '0644' }\n    - { src: 'roles/network/templates/named/school.external.zone.db', dest: '/var/named-iiab/school.external.zone.db', owner: \"{{ dns_user }}\", mode: '0644' }\n    - { src: 'roles/network/templates/named/school.internal.zone.16.in-addr.db.j2', dest: '/var/named-iiab/school.internal.zone.16.in-addr.db', owner: \"{{ dns_user }}\", mode: '0644' }\n    - { src: 'roles/network/templates/named/school.internal.zone.32.in-addr.db.j2', dest: '/var/named-iiab/school.internal.zone.32.in-addr.db', owner: \"{{ dns_user }}\", mode: '0644' }\n    - { src: 'roles/network/templates/named/school.internal.zone.48.in-addr.db.j2', dest: '/var/named-iiab/school.internal.zone.48.in-addr.db', owner: \"{{ dns_user }}\", mode: '0644' }\n# the following two files are not writeable by named, but bind 9.4 cannot discover that fact correctly\n    - { src: 'roles/network/templates/named/school.internal.zone.db', dest: '/var/named-iiab/school.internal.zone.db', owner: \"root\", mode: '0644' }\n    - { src: 'roles/network/templates/named/school.local.zone.db', dest: '/var/named-iiab/school.local.zone.db', owner: \"root\", mode: '0644' }\n    - { src: 'roles/network/templates/named/school.internal.zone.in-addr.db.j2', dest: '/var/named-iiab/school.internal.zone.in-addr.db', owner: \"{{ dns_user }}\", mode: '0644' }\n    - { src: 'roles/network/templates/named/dummy', dest: '/var/named-iiab/data/dummy', owner: \"{{ dns_user }}\", mode: '0644' }\n    - { src: 'roles/network/templates/named/named.blackhole', dest: '/var/named-iiab/named.blackhole', owner: \"{{ dns_user }}\", mode: '0644' }\n\n- name: Substitute our unit file which uses $OPTIONS from sysconfig\n  template:\n    src: \"roles/network/templates/named/{{ dns_service }}.service\"\n    dest: \"/etc/systemd/system/{{ dns_service }}.service\"\n    mode: 0644\n\n- name: The dns-jail redirect requires the named.blackhole, disabling recursion\n#        in named-iiab.conf, and the redirection of 404 error documents to /\n  template:\n    src: roles/network/templates/named/dns-jail.conf\n    dest: \"/etc/{{ apache_config_dir }}/\"\n  when: dns_jail_enabled\n\n- name: Separate enabling required (debuntu)\n  file:\n    src: \"/etc/{{ apache_config_dir }}/dns-jail.conf\"\n    path: \"/etc/{{ apache_service }}/sites-enabled/dns-jail.conf\"\n    state: link\n  when: is_debuntu and dns_jail_enabled\n\n- name: Separate disabling required (debuntu)\n  file: \n    path: \"/etc/{{ apache_service }}/sites-enabled/dns-jail.conf\"\n    state: absent\n  when: is_debuntu and not dns_jail_enabled\n\n- name: Separate enabling/disabling required (OS's that are not debuntu)\n  file:\n    path: \"/etc/{{ apache_config_dir }}/dns-jail.conf\"\n    state: absent\n  when: not is_debuntu and not dns_jail_enabled\n\n- name: Start named after copying files\n  service:\n    name: \"{{ dns_service }}\"\n    state: started\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "259464b4f5e532371e2dc26e532e3baa466ca08d", "filename": "roles/ssh_tunneling/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- block:\n    - name: Ensure that the sshd_config file has desired options\n      blockinfile:\n        dest: /etc/ssh/sshd_config\n        marker: '# {mark} ANSIBLE MANAGED BLOCK ssh_tunneling_role'\n        block: |\n          Match Group algo\n              AllowTcpForwarding local\n              AllowAgentForwarding no\n              AllowStreamLocalForwarding no\n              PermitTunnel no\n              X11Forwarding no\n      notify:\n        - restart ssh\n\n    - name: Ensure that the algo group exist\n      group: name=algo state=present\n\n    - name: Ensure that the jail directory exist\n      file:\n        path: /var/jail/\n        state: directory\n        mode: 0755\n        owner: root\n        group: \"{{ root_group|default('root') }}\"\n\n    - name: Ensure that the SSH users exist\n      user:\n        name: \"{{ item }}\"\n        groups: algo\n        home: '/var/jail/{{ item }}'\n        createhome: yes\n        generate_ssh_key: false\n        shell: /bin/false\n        state: present\n        append: yes\n      with_items: \"{{ users }}\"\n      tags: update-users\n\n    - name: The authorized keys file created\n      authorized_key:\n        user: \"{{ item }}\"\n        key: \"{{ lookup('file', 'configs/' + IP_subject_alt_name + '/pki/public/' + item + '.pub') }}\"\n        state: present\n        manage_dir: true\n        exclusive: true\n      with_items: \"{{ users }}\"\n      tags: update-users\n\n    - name: Generate SSH fingerprints\n      shell: ssh-keyscan {{ IP_subject_alt_name }} 2>/dev/null\n      register: ssh_fingerprints\n\n    - name: Fetch the known_hosts file\n      local_action:\n        module: template\n        src: known_hosts.j2\n        dest: configs/{{ IP_subject_alt_name }}/known_hosts\n      become: no\n\n    - name: Build the client ssh config\n      local_action:\n        module: template\n        src: ssh_config.j2\n        dest: configs/{{ IP_subject_alt_name }}/{{ item }}.ssh_config\n        mode: 0600\n      become: false\n      tags: update-users\n      with_items: \"{{ users }}\"\n\n    - name: Get active users\n      getent:\n        database: group\n        key: algo\n        split: ':'\n      tags: update-users\n\n    - name: Delete non-existing users\n      user:\n        name: \"{{ item }}\"\n        state: absent\n        remove: yes\n        force: yes\n      when: item not in users\n      with_items: \"{{ getent_group['algo'][2].split(',') }}\"\n      tags: update-users\n  rescue:\n    - debug: var=fail_hint\n      tags: always\n    - fail:\n      tags: always\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "944236bcb45c3e66a6d92df917e292249d15630e", "filename": "playbooks/roles/stenographer/defaults/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# defaults file for stenographer\nstenographer_packagename: stenographer\nstenographer_user: stenographer\nstenographer_group: stenographer\nstenographer_monitor_interfaces: [eth0]\nenable_stenographer: true\naction: deploy\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "dcb28b999d3435400ccf64a3f5e17c9de1310cee", "filename": "roles/dns/config-dns-server/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: restart named\n  service:\n    name: named\n    state: restarted\n\n- name: reload named\n  service:\n    name: named\n    state: reloaded\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "b42fdd8b7d0047682fa5f329dc83d7893187f0a7", "filename": "examples/playbooks/provision_hypervisor.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# example playbook for provisioning hypervisors\n# use with examples/inventory/provision_hypervisor.inventory\n- hosts: hypervisors\n  remote_user: root\n  roles:\n    - {role: ovirt-common}\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "53b5dc3a9815c5adf833057e4b5f562fe13e29c6", "filename": "roles/bro/defaults/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# defaults file for bro\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "43857b2020dd4616192bc77a5e34db068f145c02", "filename": "archive/playbooks/registry/byo.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- hosts: registry\n  roles:\n    - role: registry\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "105ae3bedcce0752d1b1df27abc46a97059c9fe6", "filename": "tasks/create_repo_npm_hosted_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_npm_hosted\n    args: \"{{ _nexus_repos_npm_defaults|combine(item) }}\""}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "ce3166b20f2efd195a219a3cf10da07685372fdc", "filename": "roles/nextcloud/tasks/nextcloud_enabled.yml", "repository": "iiab/iiab", "decoded_content": "# This should go in computed_network.yml, but here for now\n- name: Compute Nextcloud listen ip addr for nextcloud.conf\n  set_fact:\n    nextcloud_required_ip: \"{{ ansible_default_ipv4.network }}/{{ ansible_default_ipv4.netmask }}\"\n  when: ansible_default_ipv4.network is defined\n\n- name: Enable Nextcloud by copying template to httpd config\n  template:\n    src: nextcloud.conf.j2\n    dest: \"/etc/{{ apache_config_dir }}/nextcloud.conf\"\n    owner: root\n    group: root\n    mode: 0644\n  when: nextcloud_enabled\n\n- name: Enable Nextcloud (debuntu)\n  file:\n    path: /etc/apache2/sites-enabled/nextcloud.conf\n    src: /etc/apache2/sites-available/nextcloud.conf\n    state: link\n  when: nextcloud_enabled and is_debuntu\n\n- name: Remove the config file if not nextcloud_enabled (redhat)\n  file:\n    path: \"/etc/{{ apache_config_dir }}/nextcloud.conf\"\n    state: absent\n  when: not nextcloud_enabled and is_redhat\n\n- name: Restart Apache, so it picks up the new aliases\n  service:\n    name: \"{{ apache_service }}\"\n    state: restarted\n\n# the install wizard does not succeed if already installed\n- name: Determine if Nextcloud is installed\n  shell: >\n    sudo -u {{ apache_user }} php\n    '{{ nextcloud_prefix }}/nextcloud/occ' status |\n    gawk '/installed:/ { print $3 }'\n  register: returned\n\n- name: Run Nextcloud initial install wizard\n  shell: >\n    cd {{ nextcloud_prefix }}/nextcloud;\n    sudo -u {{ apache_user }} php occ maintenance:install\n    --database \"mysql\"\n    --database-name \"{{ nextcloud_dbname }}\"\n    --database-user \"{{ nextcloud_dbuser }}\"\n    --database-pass \"{{ nextcloud_dbpassword }}\"\n    --admin-user \"{{ nextcloud_admin_user }}\"\n    --admin-pass \"{{ nextcloud_admin_password }}\"\n  when: nextcloud_enabled and returned.stdout == \"false\"\n\n- name: Allow access from all hosts and ips\n  command: php '{{ nextcloud_prefix }}/nextcloud/occ' config:system:set trusted_domains 1 --value=*\n  become: true\n  become_user: \"{{ apache_user }}\"\n  when: nextcloud_enabled and returned.stdout == \"false\"\n\n- name: Determine if Nextcloud user exists already\n  shell: >\n    sudo -u {{ apache_user }} php\n    '{{ nextcloud_prefix }}/nextcloud/occ' user:list |\n    grep {{ nextcloud_user }} | wc | cut -d' ' -f1\n  register: returned_count\n\n# nextcloud wants to make users rather than just mysql users and not done\n- name: Create the default user\n  shell: >\n    su -s /bin/sh {{ apache_user }} -c\n    'OC_PASS={{ nextcloud_user_password }};\n    php {{ nextcloud_prefix }}/nextcloud/occ user:add\n    --password-from-env --display-name={{ nextcloud_user }}\n    --group=\"users\" {{ nextcloud_user }}'\n  when: nextcloud_enabled and returned_count == \"0\"\n\n- name: Remove Rewrite URL\n  lineinfile:\n    regexp: \"overwrite.cli.url\"\n    state: absent\n    dest: \"{{ nextcloud_prefix }}/nextcloud/config/config.php\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "eb70ae2ecf52b8aec1869de04083bce4fd62ecac", "filename": "ops/playbooks/roles/ucp/meta/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\nallow_duplicates: no\ndependencies:\n  - role: hpe.openports\n    hpe_openports_ports: \"{{ ucp_role_ports }}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "8509bba5b83f683d50fc1b72f9ed1c1a9f2f1ab1", "filename": "roles/2-common/tasks/iiab-startup.yml", "repository": "iiab/iiab", "decoded_content": "- name: Does /usr/libexec/iiab-startup.sh exist?\n  stat:\n    path: /usr/libexec/iiab-startup.sh\n  register: startup_script\n\n- name: Copy template script to /usr/libexec/iiab-startup.sh\n  template:\n    src: iiab-startup.sh\n    dest: /usr/libexec/\n    mode: 0755\n  when: not startup_script.stat.exists\n\n- name: Copy iiab-startup.service to {{ systemd_location }}\n  template:\n    src: iiab-startup.service\n    dest: \"{{ systemd_location }}\"\n  when: not startup_script.stat.exists\n\n- name: Enable & restart the systemd service after daemon-reload\n  # shell: systemctl daemon-reload\n  # shell: systemctl restart iiab-startup.service\n  # shell: systemctl enable iiab-startup.service\n  systemd:\n    name: iiab-startup\n    daemon_reload: yes\n    enabled: yes\n    state: restarted\n  when: not startup_script.stat.exists\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7f87ca8a2d9dba6ac66d37b0338d67927156b581", "filename": "roles/osp/packstack-post/tasks/cinder.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# This is a workaround for iptables config \"bug\" whereas the IPs are wrong for\n# iSCSI traffic (i.e.: using the wrong interface)\n- name: \"Update iptables config to use correct interfaces\"\n  replace:\n    path: \"/etc/sysconfig/iptables\"\n    regexp: '-A INPUT -s {{ hostvars[item].mgmt_net_ip }}/32 -p tcp -m multiport --dports 3260 -m comment --comment \"001 cinder incoming cinder_{{ hostvars[item].mgmt_net_ip }}\" -j ACCEPT'\n    replace: '-A INPUT -s {{ hostvars[item].storage_net_ip }}/32 -p tcp -m multiport --dports 3260 -m comment --comment \"001 cinder incoming cinder_{{ hostvars[item].storage_net_ip }}\" -j ACCEPT'\n  with_items:\n  - \"{{ ansible_play_hosts }}\"\n  notify:\n  - 'restart iptables'\n\n- name: \"Configure Cinder\"\n  ini_file:\n    path: \"/etc/cinder/cinder.conf\"\n    section: \"{{ item.0.section }}\"\n    option: \"{{ item.1.option }}\"\n    value: \"{{ item.1.value }}\"\n  with_subelements:\n  - \"{{ cinder_config }}\"\n  - params\n  notify:\n  - 'restart openstack-cinder-api'\n  - 'restart openstack-cinder-backup'\n  - 'restart openstack-cinder-scheduler'\n  - 'restart openstack-cinder-volume'\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "1c2ebc923d09dfba3ab049f8a55c616d6c58bd53", "filename": "roles/disconnected-git/handlers/main.yaml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: restart httpd\n  service: \n    name: httpd\n    state: restarted"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "c379572ba2b94e96b87253a49cb2e4e9538f86d4", "filename": "roles/ovirt-collect-logs/tasks/engine.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n\n- name: Prepare directory structure\n  file:\n    src: \"{{ item }}\"\n    dest: \"{{ ovirt_collect_logs_tmp_dir }}/{{ item }}\"\n    state: directory\n  with_items:\n    - \"etc\"\n    - \"httpd\"\n\n- name: ovirt-requests-logs files\n  shell: ls /var/log/httpd/ovirt-requests-log*\n  register: ovirt_requests_logs\n  ignore_errors: yes\n\n- name: Link ovirt-engine logs\n  file:\n    src: \"{{ item.src }}\"\n    dest: \"{{ ovirt_collect_logs_tmp_dir }}/{{ item.dest }}\"\n    state: link\n  with_items:\n    -\n      src: \"/var/log/ovirt-engine\"\n      dest: \"ovirt-engine-logs\"\n    -\n      src: \"/var/lib/ovirt-engine\"\n      dest: \"ovirt-engine-data\"\n    -\n      src: \"/etc/ovirt-engine\"\n      dest: \"etc/ovirt-engine\"\n    -\n      src: \"/etc/ovirt-engine-setup.conf.d\"\n      dest: \"etc/ovirt-engine-setup.conf.d\"\n    -\n      src: \"/etc/ovirt-host-deploy.conf.d\"\n      dest: \"etc/ovirt-host-deploy.conf.d\"\n    -\n      src: \"/etc/ovirt-vmconsole\"\n      dest: \"etc/ovirt-vmconsole\"\n  ignore_errors: true\n\n- name: Link ovirt-requests logs\n  file:\n    src: \"{{ item }}\"\n    dest: \"{{ ovirt_collect_logs_tmp_dir }}/httpd/{{ item|replace('/var/log/httpd/', '')}}\"\n    state: link\n  with_items: \"{{ ovirt_requests_logs.stdout_lines }}\"\n  ignore_errors: true\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b72652d70c05753e5eaec28b63c2c30c3817e366", "filename": "roles/config-linux-desktop/config-gnome/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install, configure and enable Gnome\"\n  include_tasks: \"{{ distro_file }}\"\n  with_first_found:\n  - files:\n    - gnome-{{ ansible_distribution }}.yml\n    skip: true\n  loop_control:\n    loop_var: distro_file\n  when:\n  - gnome_install|default(False)\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4b79ca68c652f44d2e665eff5be7cf547ee7e074", "filename": "roles/manage-confluence-space/tasks/create_confluence_space.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Create Confluence Space\n  uri:\n    url: '{{ confluence_destination_url }}/wiki/rest/api/space'\n    method: POST\n    user: '{{ confluence_destination_username }}'\n    password: '{{ confluence_destination_password }}'\n    force_basic_auth: yes\n    status_code: 200\n    body_format: json\n    body: \"{{ lookup('template', 'space.j2') }}\"\n  register: space_content\n\n- name: set fact\n  set_fact:\n    destination_homepage_id: \"{{ space_content.json['homepage']['id']}}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "4021764e9f66d4360d3b518dcda48e167ef387d1", "filename": "roles/samba/handlers/main.yml", "repository": "iiab/iiab", "decoded_content": "---\n- name: restart {{ smb_service }}\n  command: service {{ smb_service }} restart\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "68e45f989ef842e6e52988cfeb339939d563a84b", "filename": "archive/roles/secure-registry/meta/main.yaml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n dependencies:\n   - { role: openshift_common }\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "d3443acdffdf8f75137cc044ed0179fca59756ab", "filename": "reference-architecture/gcp/ansible/playbooks/library/redhat_subscription.py", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#!/usr/bin/python\n\n# James Laska (jlaska@redhat.com)\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\nANSIBLE_METADATA = {'status': ['preview'],\n                    'supported_by': 'core',\n                    'version': '1.0'}\n\nDOCUMENTATION = '''\n---\nmodule: redhat_subscription\nshort_description: Manage registration and subscriptions to RHSM using the C(subscription-manager) command\ndescription:\n    - Manage registration and subscription to the Red Hat Subscription Management entitlement platform using the C(subscription-manager) command\nversion_added: \"1.2\"\nauthor: \"Barnaby Court (@barnabycourt)\"\nnotes:\n    - In order to register a system, subscription-manager requires either a username and password, or an activationkey.\nrequirements:\n    - subscription-manager\noptions:\n    state:\n        description:\n          - whether to register and subscribe (C(present)), or unregister (C(absent)) a system\n        required: false\n        choices: [ \"present\", \"absent\" ]\n        default: \"present\"\n    username:\n        description:\n            - access.redhat.com or Sat6  username\n        required: False\n        default: null\n    password:\n        description:\n            - access.redhat.com or Sat6 password\n        required: False\n        default: null\n    server_hostname:\n        description:\n            - Specify an alternative Red Hat Subscription Management or Sat6 server\n        required: False\n        default: Current value from C(/etc/rhsm/rhsm.conf) is the default\n    server_insecure:\n        description:\n            - Enable or disable https server certificate verification when connecting to C(server_hostname)\n        required: False\n        default: Current value from C(/etc/rhsm/rhsm.conf) is the default\n    rhsm_baseurl:\n        description:\n            - Specify CDN baseurl\n        required: False\n        default: Current value from C(/etc/rhsm/rhsm.conf) is the default\n    autosubscribe:\n        description:\n            - Upon successful registration, auto-consume available subscriptions\n        required: False\n        default: False\n    activationkey:\n        description:\n            - supply an activation key for use with registration\n        required: False\n        default: null\n    org_id:\n        description:\n            - Organization ID to use in conjunction with activationkey\n        required: False\n        default: null\n        version_added: \"2.0\"\n    environment:\n        description:\n            - Register with a specific environment in the destination org. Used with Red Hat Satellite 6.x or Katello\n        required: False\n        default: null\n        version_added: \"2.2\"\n    pool:\n        description:\n            - Specify a subscription pool name to consume.  Regular expressions accepted.\n        required: False\n        default: '^$'\n    consumer_type:\n        description:\n            - The type of unit to register, defaults to system\n        required: False\n        default: null\n        version_added: \"2.1\"\n    consumer_name:\n        description:\n            - Name of the system to register, defaults to the hostname\n        required: False\n        default: null\n        version_added: \"2.1\"\n    consumer_id:\n        description:\n            - References an existing consumer ID to resume using a previous registration for this system. If the  system's identity certificate is lost or corrupted, this option allows it to resume using its previous identity and subscriptions. The default is to not specify a consumer ID so a new ID is created.\n        required: False\n        default: null\n        version_added: \"2.1\"\n    force_register:\n        description:\n            -  Register the system even if it is already registered\n        required: False\n        default: False\n        version_added: \"2.2\"\n'''\n\nEXAMPLES = '''\n# Register as user (joe_user) with password (somepass) and auto-subscribe to available content.\n- redhat_subscription:\n    state: present\n    username: joe_user\n    password: somepass\n    autosubscribe: true\n\n# Same as above but with pulling existing system data.\n- redhat_subscription:\n    state: present\n    username: joe_user\n    password: somepass\n    consumer_id: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n\n# Register with activationkey (1-222333444) and consume subscriptions matching\n# the names (Red hat Enterprise Server) and (Red Hat Virtualization)\n- redhat_subscription:\n    state: present\n    activationkey: 1-222333444\n    pool: '^(Red Hat Enterprise Server|Red Hat Virtualization)$'\n\n# Update the consumed subscriptions from the previous example (remove the Red\n# Hat Virtualization subscription)\n- redhat_subscription:\n    state: present\n    activationkey: 1-222333444\n    pool: '^Red Hat Enterprise Server$'\n\n# Register as user credentials into given environment (against Red Hat\n# Satellite 6.x), and auto-subscribe to available content.\n- redhat_subscription:\n    state: present\n    username: joe_user\n    password: somepass\n    environment: Library\n    autosubscribe: yes\n'''\n\nimport os\nimport re\nimport types\nimport ConfigParser\nimport shlex\n\n\nclass RegistrationBase(object):\n    def __init__(self, module, username=None, password=None):\n        self.module = module\n        self.username = username\n        self.password = password\n\n    def configure(self):\n        raise NotImplementedError(\"Must be implemented by a sub-class\")\n\n    def enable(self):\n        # Remove any existing redhat.repo\n        redhat_repo = '/etc/yum.repos.d/redhat.repo'\n        if os.path.isfile(redhat_repo):\n            os.unlink(redhat_repo)\n\n    def register(self):\n        raise NotImplementedError(\"Must be implemented by a sub-class\")\n\n    def unregister(self):\n        raise NotImplementedError(\"Must be implemented by a sub-class\")\n\n    def unsubscribe(self):\n        raise NotImplementedError(\"Must be implemented by a sub-class\")\n\n    def update_plugin_conf(self, plugin, enabled=True):\n        plugin_conf = '/etc/yum/pluginconf.d/%s.conf' % plugin\n        if os.path.isfile(plugin_conf):\n            cfg = ConfigParser.ConfigParser()\n            cfg.read([plugin_conf])\n            if enabled:\n                cfg.set('main', 'enabled', 1)\n            else:\n                cfg.set('main', 'enabled', 0)\n            fd = open(plugin_conf, 'rwa+')\n            cfg.write(fd)\n            fd.close()\n\n    def subscribe(self, **kwargs):\n        raise NotImplementedError(\"Must be implemented by a sub-class\")\n\n\nclass Rhsm(RegistrationBase):\n    def __init__(self, module, username=None, password=None):\n        RegistrationBase.__init__(self, module, username, password)\n        self.config = self._read_config()\n        self.module = module\n\n    def _read_config(self, rhsm_conf='/etc/rhsm/rhsm.conf'):\n        '''\n            Load RHSM configuration from /etc/rhsm/rhsm.conf.\n            Returns:\n             * ConfigParser object\n        '''\n\n        # Read RHSM defaults ...\n        cp = ConfigParser.ConfigParser()\n        cp.read(rhsm_conf)\n\n        # Add support for specifying a default value w/o having to standup some configuration\n        # Yeah, I know this should be subclassed ... but, oh well\n        def get_option_default(self, key, default=''):\n            sect, opt = key.split('.', 1)\n            if self.has_section(sect) and self.has_option(sect, opt):\n                return self.get(sect, opt)\n            else:\n                return default\n\n        cp.get_option = types.MethodType(get_option_default, cp, ConfigParser.ConfigParser)\n\n        return cp\n\n    def enable(self):\n        '''\n            Enable the system to receive updates from subscription-manager.\n            This involves updating affected yum plugins and removing any\n            conflicting yum repositories.\n        '''\n        RegistrationBase.enable(self)\n        self.update_plugin_conf('rhnplugin', False)\n        self.update_plugin_conf('subscription-manager', True)\n\n    def configure(self, **kwargs):\n        '''\n            Configure the system as directed for registration with RHSM\n            Raises:\n              * Exception - if error occurs while running command\n        '''\n        args = ['subscription-manager', 'config']\n\n        # Pass supplied **kwargs as parameters to subscription-manager.  Ignore\n        # non-configuration parameters and replace '_' with '.'.  For example,\n        # 'server_hostname' becomes '--system.hostname'.\n        for k,v in kwargs.items():\n            if re.search(r'^(system|rhsm)_', k):\n                args.append('--%s=%s' % (k.replace('_','.'), v))\n\n        self.module.run_command(args, check_rc=True)\n\n    @property\n    def is_registered(self):\n        '''\n            Determine whether the current system\n            Returns:\n              * Boolean - whether the current system is currently registered to\n                          RHSM.\n        '''\n        # Quick version...\n        if False:\n            return os.path.isfile('/etc/pki/consumer/cert.pem') and \\\n                   os.path.isfile('/etc/pki/consumer/key.pem')\n\n        args = ['subscription-manager', 'identity']\n        rc, stdout, stderr = self.module.run_command(args, check_rc=False)\n        if rc == 0:\n            return True\n        else:\n            return False\n\n    def register(self, username, password, autosubscribe, activationkey, org_id,\n                 consumer_type, consumer_name, consumer_id, force_register, environment):\n        '''\n            Register the current system to the provided RHSM or Sat6 server\n            Raises:\n              * Exception - if error occurs while running command\n        '''\n        args = ['subscription-manager', 'register']\n\n        # Generate command arguments\n        if activationkey:\n            args.extend(['--activationkey', activationkey])\n            if org_id:\n                args.extend(['--org', org_id])\n        else:\n            if autosubscribe:\n                args.append('--autosubscribe')\n            if username:\n                args.extend(['--username', username])\n            if password:\n                args.extend(['--password', password])\n            if consumer_type:\n                args.extend(['--type', consumer_type])\n            if consumer_name:\n                args.extend(['--name', consumer_name])\n            if consumer_id:\n                args.extend(['--consumerid', consumer_id])\n            if force_register:\n                args.extend(['--force'])\n            if environment:\n                args.extend(['--environment', environment])\n\n        rc, stderr, stdout = self.module.run_command(args, check_rc=True)\n\n    def unsubscribe(self, serials=None):\n        '''\n            Unsubscribe a system from subscribed channels\n            Args:\n              serials(list or None): list of serials to unsubscribe. If\n                                     serials is none or an empty list, then\n                                     all subscribed channels will be removed.\n            Raises:\n              * Exception - if error occurs while running command\n        '''\n        items = []\n        if serials is not None and serials:\n            items = [\"--serial=%s\" % s for s in serials]\n        if serials is None:\n            items = [\"--all\"]\n\n        if items:\n            args = ['subscription-manager', 'unsubscribe'] + items\n            rc, stderr, stdout = self.module.run_command(args, check_rc=True)\n        return serials\n\n    def unregister(self):\n        '''\n            Unregister a currently registered system\n            Raises:\n              * Exception - if error occurs while running command\n        '''\n        args = ['subscription-manager', 'unregister']\n        rc, stderr, stdout = self.module.run_command(args, check_rc=True)\n\n    def subscribe(self, regexp):\n        '''\n            Subscribe current system to available pools matching the specified\n            regular expression\n            Raises:\n              * Exception - if error occurs while running command\n        '''\n\n        # Available pools ready for subscription\n        available_pools = RhsmPools(self.module)\n\n        subscribed_pool_ids = []\n        for pool in available_pools.filter(regexp):\n            pool.subscribe()\n            subscribed_pool_ids.append(pool.get_pool_id())\n        return subscribed_pool_ids\n\n    def update_subscriptions(self, regexp):\n        changed=False\n        consumed_pools = RhsmPools(self.module, consumed=True)\n        pool_ids_to_keep = [p.get_pool_id() for p in consumed_pools.filter(regexp)]\n\n        serials_to_remove=[p.Serial for p in consumed_pools if p.get_pool_id() not in pool_ids_to_keep]\n        serials = self.unsubscribe(serials=serials_to_remove)\n\n        subscribed_pool_ids = self.subscribe(regexp)\n\n        if subscribed_pool_ids or serials:\n            changed=True\n        return {'changed': changed, 'subscribed_pool_ids': subscribed_pool_ids,\n                'unsubscribed_serials': serials}\n\n\n\nclass RhsmPool(object):\n    '''\n        Convenience class for housing subscription information\n    '''\n\n    def __init__(self, module, **kwargs):\n        self.module = module\n        for k,v in kwargs.items():\n            setattr(self, k, v)\n\n    def __str__(self):\n        return str(self.__getattribute__('_name'))\n\n    def get_pool_id(self):\n        return getattr(self, 'PoolId', getattr(self, 'PoolID'))\n\n    def subscribe(self):\n        args = \"subscription-manager subscribe --pool %s\" % self.get_pool_id()\n        rc, stdout, stderr = self.module.run_command(args, check_rc=True)\n        if rc == 0:\n            return True\n        else:\n            return False\n\n\nclass RhsmPools(object):\n    \"\"\"\n        This class is used for manipulating pools subscriptions with RHSM\n    \"\"\"\n    def __init__(self, module, consumed=False):\n        self.module = module\n        self.products = self._load_product_list(consumed)\n\n    def __iter__(self):\n        return self.products.__iter__()\n\n    def _load_product_list(self, consumed=False):\n        \"\"\"\n            Loads list of all available or consumed pools for system in data structure\n\n            Args:\n                consumed(bool): if True list consumed  pools, else list available pools (default False)\n        \"\"\"\n        args = \"subscription-manager list\"\n        if consumed:\n            args += \" --consumed\"\n        else:\n            args += \" --available\"\n        rc, stdout, stderr = self.module.run_command(args, check_rc=True)\n\n        products = []\n        for line in stdout.split('\\n'):\n            # Remove leading+trailing whitespace\n            line = line.strip()\n            # An empty line implies the end of a output group\n            if len(line) == 0:\n                continue\n            # If a colon ':' is found, parse\n            elif ':' in line:\n                (key, value) = line.split(':',1)\n                key = key.strip().replace(\" \", \"\")  # To unify\n                value = value.strip()\n                if key in ['ProductName', 'SubscriptionName']:\n                    # Remember the name for later processing\n                    products.append(RhsmPool(self.module, _name=value, key=value))\n                elif products:\n                    # Associate value with most recently recorded product\n                    products[-1].__setattr__(key, value)\n                # FIXME - log some warning?\n                #else:\n                    # warnings.warn(\"Unhandled subscription key/value: %s/%s\" % (key,value))\n        return products\n\n    def filter(self, regexp='^$'):\n        '''\n            Return a list of RhsmPools whose name matches the provided regular expression\n        '''\n        r = re.compile(regexp)\n        for product in self.products:\n            if r.search(product._name):\n                yield product\n            if r.search(product.PoolID):\n                yield product\n\n\ndef main():\n\n    # Load RHSM configuration from file\n    rhsm = Rhsm(None)\n\n    module = AnsibleModule(\n                argument_spec = dict(\n                    state = dict(default='present', choices=['present', 'absent']),\n                    username = dict(default=None, required=False),\n                    password = dict(default=None, required=False, no_log=True),\n                    server_hostname = dict(default=rhsm.config.get_option('server.hostname'), required=False),\n                    server_insecure = dict(default=rhsm.config.get_option('server.insecure'), required=False),\n                    rhsm_baseurl = dict(default=rhsm.config.get_option('rhsm.baseurl'), required=False),\n                    autosubscribe = dict(default=False, type='bool'),\n                    activationkey = dict(default=None, required=False),\n                    org_id = dict(default=None, required=False),\n                    environment = dict(default=None, required=False, type='str'),\n                    pool = dict(default='^$', required=False, type='str'),\n                    consumer_type = dict(default=None, required=False),\n                    consumer_name = dict(default=None, required=False),\n                    consumer_id = dict(default=None, required=False),\n                    force_register = dict(default=False, type='bool'),\n                )\n            )\n\n    rhsm.module = module\n    state = module.params['state']\n    username = module.params['username']\n    password = module.params['password']\n    server_hostname = module.params['server_hostname']\n    server_insecure = module.params['server_insecure']\n    rhsm_baseurl = module.params['rhsm_baseurl']\n    autosubscribe = module.params['autosubscribe'] == True\n    activationkey = module.params['activationkey']\n    org_id = module.params['org_id']\n    environment = module.params['environment']\n    pool = module.params['pool']\n    consumer_type = module.params[\"consumer_type\"]\n    consumer_name = module.params[\"consumer_name\"]\n    consumer_id = module.params[\"consumer_id\"]\n    force_register = module.params[\"force_register\"]\n\n    # Ensure system is registered\n    if state == 'present':\n\n        # Check for missing parameters ...\n        if not (activationkey or username or password):\n            module.fail_json(msg=\"Missing arguments, must supply an activationkey (%s) or username (%s) and password (%s)\" % (activationkey, username, password))\n        if not activationkey and not (username and password):\n            module.fail_json(msg=\"Missing arguments, If registering without an activationkey, must supply username or password\")\n\n        # Register system\n        if rhsm.is_registered and not force_register:\n            if pool != '^$':\n                try:\n                    result = rhsm.update_subscriptions(pool)\n                except Exception:\n                    e = get_exception()\n                    module.fail_json(msg=\"Failed to update subscriptions for '%s': %s\" % (server_hostname, e))\n                else:\n                    module.exit_json(**result)\n            else:\n                module.exit_json(changed=False, msg=\"System already registered.\")\n        else:\n            try:\n                rhsm.enable()\n                rhsm.configure(**module.params)\n                rhsm.register(username, password, autosubscribe, activationkey, org_id,\n                             consumer_type, consumer_name, consumer_id, force_register,\n                             environment)\n                subscribed_pool_ids = rhsm.subscribe(pool)\n            except Exception:\n                e = get_exception()\n                module.fail_json(msg=\"Failed to register with '%s': %s\" % (server_hostname, e))\n            else:\n                module.exit_json(changed=True,\n                                 msg=\"System successfully registered to '%s'.\" % server_hostname,\n                                 subscribed_pool_ids=subscribed_pool_ids)\n    # Ensure system is *not* registered\n    if state == 'absent':\n        if not rhsm.is_registered:\n            module.exit_json(changed=False, msg=\"System already unregistered.\")\n        else:\n            try:\n                rhsm.unsubscribe()\n                rhsm.unregister()\n            except Exception:\n                e = get_exception()\n                module.fail_json(msg=\"Failed to unregister: %s\" % e)\n            else:\n                module.exit_json(changed=True, msg=\"System successfully unregistered from %s.\" % server_hostname)\n\n\n# import module snippets\nfrom ansible.module_utils.basic import *\n\nif __name__ == '__main__':\n    main()\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "f95484ae23d20cf16cb3e7edbc81ded04c16c3a0", "filename": "roles/haproxy/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for haproxy\n- name: \"assures {{ consul_template_dir }} dirs exists\"\n  file:\n    path: \"{{ consul_template_dir }}/{{ item.path }}\"\n    state: directory\n  with_items:\n    - { path: 'config' }\n    - { path: 'templates' }\n  tags:\n    - haproxy\n\n- name: upload template config files\n  template:\n    src: consul.cfg.j2\n    dest: \"{{ consul_template_dir }}/config/consul.cfg\"\n    mode: 0644\n  sudo: yes\n  tags:\n    - haproxy\n\n- name: upload static config files\n  copy:\n    src: \"{{ item.src }}\"\n    dest: \"{{ consul_template_dir }}/{{ item.dst }}\"\n    mode: 0644\n  sudo: yes\n  with_items:\n    - { src: haproxy.cfg, dst: 'config/haproxy.cfg' }\n    - { src: haproxy.tmpl, dst: 'templates/haproxy.tmpl' }\n  tags:\n    - haproxy\n\n- name: run haproxy container\n  docker:\n    name: haproxy\n    image: \"{{ haproxy_image }}\"\n    state: started\n    net: host\n    restart_policy: always\n    ports:\n      - \"80:80\"\n      - \"34180:34180\"\n    env:\n      HAPROXY_DOMAIN: \"{{ haproxy_domain }}\"\n      CONSUL_TEMPLATE_VERSION: \"{{ consul_template_version }}\"\n      CONSUL_LOGLEVEL: \"{{ consul_template_loglevel }}\"\n      CONSUL_CONNECT: \"{{ consul_backend }}\"\n      CONSUL_CONFIG: \"/config\"\n      SERVICE_NAME: haproxy\n    volumes:\n    - \"{{ consul_template_dir }}/config:/config\"\n    - \"{{ consul_template_dir }}/templates:/templates\"\n  tags:\n    - haproxy\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "06842fd7d609d0a6d8a32e7546b345d83919ec9d", "filename": "roles/ovirt-collect-logs/vars/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_collect_logs_tmp_dir: \"/var/tmp/ovirt-logs-{{ ovirt_collect_logs_from_system }}\"\novirt_collect_logs_archive: \"/var/tmp/ovirt-logs-{{ ovirt_collect_logs_from_system }}.tar.gz\"\novirt_collect_logs_shell_commands:\n  rpm-list: \"rpm -qa | sort -f\"\n  yum-list: \"yum list installed\"\n  services: \"systemctl -t service --failed --no-legend | awk '{print $1}'\n            | xargs -r -n1 journalctl -u\"\n  iptables: \"iptables -L\"\n  lsof: \"lsof -P\"\n  pstree: \"pstree -p\"\n  sysctl: \"sysctl -a\"\n  netstat: \"netstat -lnp\"\n  lsmod: \"lsmod\"\n  lspci: \"lspci\"\n  memory_usage: \"ps -e -orss=,args= | sort  -b -k1,1n | tac\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "affb5711720270c3aac159362b089dffec39ec72", "filename": "playbooks/openstack/openshift-cluster/terminate.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Terminate instance(s)\n  hosts: localhost\n  become: no\n  connection: local\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  tasks:\n  - add_host:\n      name: \"{{ item }}\"\n      groups: oo_hosts_to_terminate\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    with_items: \"{{ (groups['meta-environment_' ~ cluster_env]|default([])) | intersect(groups['meta-clusterid_' ~ cluster_id ]|default([])) }}\"\n\n- name: Unsubscribe VMs\n  hosts: oo_hosts_to_terminate\n  vars_files:\n  - vars.yml\n  roles:\n  - role: rhel_unsubscribe\n    when: deployment_type in ['atomic-enterprise', 'enterprise', 'openshift-enterprise'] and\n          ansible_distribution == \"RedHat\" and\n          lookup('oo_option', 'rhel_skip_subscription') | default(rhsub_skip, True) |\n            default('no', True) | lower in ['no', 'false']\n\n- hosts: localhost\n  become: no\n  connection: local\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  tasks:\n  - name: Delete the OpenStack Stack\n    command: 'heat stack-delete openshift-ansible-{{ cluster_id }}-stack'\n    register: stack_delete_result\n    changed_when: stack_delete_result.rc == 0\n    failed_when: stack_delete_result.rc != 0 and 'could not be found' not in stack_delete_result.stdout\n\n  - name: Wait for the completion of the OpenStack Stack deletion\n    shell: 'heat stack-show openshift-ansible-{{ cluster_id }}-stack | awk ''$2 == \"stack_status\" {print $4}'''\n    when: stack_delete_result.changed\n    register: stack_show_result\n    until: stack_show_result.stdout != 'DELETE_IN_PROGRESS'\n    retries: 60\n    delay: 5\n    failed_when: '\"Stack not found\" not in stack_show_result.stderr and\n                   stack_show_result.stdout != \"DELETE_COMPLETE\"'\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "7e642b2a6a29962445d73ab1a445970c8def283c", "filename": "ops/playbooks/scale_workers.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- name: Join Workers (Linux)\n  hosts: worker\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - includes/internal_vars.yml\n\n  pre_tasks:\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n\n  roles:\n    - role: worker\n      ARG_UCP_IP:        \"{{ ucp_instance }}.{{domain_name}}\"\n      ARG_UCP_USER:      \"{{ ucp_username }}\"\n      ARG_UCP_PASSWORD:  \"{{ ucp_password }}\"\n      ARG_ADVERTIZE_IP:  \"{{ ucp_instance }}.{{ domain_name }}:2377\"\n      worker_role_ports: \"{{ internal_worker_ports }}\"\n      worker_join_delay: 180\n\n###############################################################\n#\n# Play 2: Join Windows Workers\n#\n###############################################################\n- name: Join Workers (Windows)\n  hosts: win_worker\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  pre_tasks:\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n\n  roles:\n    - role: windows_worker\n      ARG_UCP_IP:        \"{{ ucp_instance }}.{{domain_name}}\"\n      ARG_UCP_USER:      \"{{ ucp_username }}\"\n      ARG_UCP_PASSWORD:  \"{{ ucp_password }}\"\n      ARG_ADVERTIZE_IP:  \"{{ ucp_instance }}.{{ domain_name }}:2377\"\n      worker_join_delay: 180\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "c57ea67cd90f1b410fc256d4dee0e98441eed518", "filename": "roles/docker/defaults/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ndefault_docker_storage_block_device: \"/dev/vdb\"\ndefault_docker_storage_volume_group: \"docker_vg\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6d6cab7b45f5fa5971a4647aa6013c591260f0bc", "filename": "roles/openshift-emptydir-quota/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Modify the node configuration\n  replace:\n    dest: /etc/origin/node/node-config.yaml\n    regexp: '^(.*)perFSGroup: (\\s+.*)?$'\n    replace: '\\1 perFSGroup: {{ node_local_quota_per_fsgroup }}\\2'\n    backup: yes\n  notify:\n  - restart openshift-node\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5849dcbe3afc3994a15438779ec86b13802977b8", "filename": "roles/osp/packstack-install/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'restart mariadb'\n  service:\n    name: 'mariadb'\n    state: restarted\n\n- name: 'restart openstack-nova-compute'\n  service:\n    name: 'openstack-nova-compute'\n    state: restarted\n\n- name: 'restart libvirtd'\n  service:\n    name: 'libvirtd'\n    state: restarted\n\n- name: 'restart iptables'\n  service:\n    name: 'iptables'\n    state: restarted\n\n- name: 'restart keystone'\n  service:\n    name: 'httpd'\n    state: restarted\n\n- name: 'restart openstack-cinder-api'\n  service:\n    name: 'openstack-cinder-api'\n    state: restarted\n\n- name: 'restart openstack-cinder-backup'\n  service:\n    name: 'openstack-cinder-backup'\n    state: restarted\n\n- name: 'restart openstack-cinder-scheduler'\n  service:\n    name: 'openstack-cinder-scheduler'\n    state: restarted\n\n- name: 'restart openstack-cinder-volume'\n  service:\n    name: 'openstack-cinder-volume'\n    state: restarted\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "a043490835952cd9d1a288d274979f80e2fd2a66", "filename": "roles/kubevirt/tasks/provision.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- name: Login As Super User\n  command: \"oc login -u {{ admin_user }} -p {{ admin_password }}\"\n  when: cluster==\"openshift\"\n        and admin_user is defined\n        and admin_password is defined\n\n- name: Check if {{ namespace }} exists\n  shell: kubectl get ns | grep -w {{ namespace }} | awk '{ print $1 }'\n  register: ns\n\n- name: Create {{ namespace }} namespace\n  shell: kubectl create namespace {{ namespace }}\n  when: ns.stdout != \"{{ namespace }}\"\n\n- name: Add Privileged Policy\n  command: \"oc adm policy add-scc-to-user privileged -z {{ item }} -n {{ namespace }}\"\n  with_items:\n    - kubevirt-privileged\n    - kubevirt-controller\n    - kubevirt-infra  # For KubeVirt v0.2.0\n  when: cluster==\"openshift\"\n\n- name: Add Hostmount-anyuid Policy\n  command: \"oc adm policy add-scc-to-user hostmount-anyuid -z kubevirt-infra -n {{ namespace }}\"\n  when: cluster==\"openshift\"\n\n- name: Check for kubevirt.yml template in {{ kubevirt_template_dir }}\n  stat:\n    path: \"{{ kubevirt_template_dir }}/kubevirt.yml\"\n  register: byo_template\n\n- name: Use kubevirt.yml template from {{ kubevirt_template_dir }}\n  set_fact:\n    manifest_version: \"release\"\n  when: byo_template.stat.exists == True\n\n- include_tasks: \"{{ manifest_version }}.yml\"\n\n- name: Create KubeVirt Resources\n  command: \"kubectl apply -f /tmp/{{ item }}.yml\"\n  with_items:\n    - \"{{ dev_template_resources }}\"\n  when: manifest_version==\"dev\"\n\n- name: Create KubeVirt Resources\n  command: kubectl apply -f /tmp/kubevirt.yml\n  when: manifest_version==\"release\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c7e9512d0efd2975b100ab8bc52e5cc518b0161e", "filename": "roles/ansible/tower/manage-projects/tests/inventory/group_vars/tower.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nansible_tower:\n  admin_password: \"admin01\"\n  projects:\n  - name: \"Project1\"\n    description: \"My Project\"\n    scm_type: \"git\"\n    scm_url: \"https://github.com/redhat-cop/infra-ansible.git\"\n    scm_branch: \"master\"\n    organization: \"Default\"\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "e4584024eae780a4eca1e34c98b5a2cdcb18646f", "filename": "roles/config-selinux/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Configure SELinux\"\n  selinux:\n    state: \"{{ target_state | default('enforcing') }}\"\n    policy: \"{{ (target_state == 'disabled') | ternary(omit, target_policy) }}\"\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "7955fd79455461e840332fcbda0fc0fb9b81c373", "filename": "roles/manage-jira/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- include_tasks: create_project_category.yml\n \n- include_tasks: create_permission_scheme.yml\n\n- include_tasks: create_project.yml\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "a56cb98c85fbfadbe74a4121fc372f5d32180a68", "filename": "roles/osm/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "osm_install: True\nosm_enabled: False\nosm_path: \"\"\nosm_venv: /usr/local/osm/\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "fc75fac60b3068a2f15308b1520a6122c3b37961", "filename": "ops/playbooks/includes/storage_driver_overlay2.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n    - name: Check for partitions on disk\n      parted:\n        state: info\n        device: \"{{ disk2 }}\"\n        number: 1\n      register: DiskInfo\n\n    - set_fact:\n        partPresent: \"{{ DiskInfo.partitions[0] is defined }}\"\n\n    - name: Create partition on second disk\n      parted:\n        label: gpt\n        part_type: primary\n        device: \"{{ disk2 }}\"\n        flags: [ lvm ]\n        state: present\n        number: 1\n        part_start: 0%\n        part_end: 100%\n      when: partPresent == false\n        \n    - name: Create Docker VG\n      lvg:\n        vg: docker\n        pvs: \"{{ disk2_part }}\"\n      when: partPresent == false\n\n    - name: Create lvm logical volume\n      lvol:\n        lv: var_lib_docker\n        opts: --wipesignatures y\n        vg: docker\n        size: 100%VG\n#      when: partPresent == false\n\n    - name: Create XFS partition\n      filesystem:\n        dev: /dev/docker/var_lib_docker\n        opts: -n ftype=1\n        fstype: xfs\n\n    - name: Create /etc/docker directory\n      file:\n        path: /etc/docker\n        state: directory\n\n    - name: Config Docker daemon\n      template: src=../templates/daemon.overlay2.json.j2 dest=/etc/docker/daemon.json\n\n    - name: Mount data drive in /var/lib/docker\n      mount:\n        src: /dev/docker/var_lib_docker\n        fstype: xfs\n        path: /var/lib/docker\n        state: mounted\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "560774ff3c9635de287fdd68036df80a9495c009", "filename": "roles/sshd/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Disable root login with password\n  lineinfile: dest=/etc/ssh/sshd_config\n              regexp='^PermitRootLogin'\n              line='PermitRootLogin without-password'\n              state=present\n#TODO: use handler to reload ssh\n\n- name: Enable sshd\n  service: name={{ sshd_service }}\n           enabled=yes\n           state=started\n  when: sshd_enabled\n\n- name: Disable sshd\n  service: name={{ sshd_service }}\n           enabled=no\n           state=stopped\n  when: not sshd_enabled\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "e420fc46cc0d0bdcd124527ecb49e19afd8ed601", "filename": "playbooks/roles/sensor-common/tasks/configure-time.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# timedatectl.yml - configure ntp\n- name: Install Chrony\n  yum:\n    name: chrony\n    state: installed\n\n- name: Enable and start chrony\n  service:\n    name: chronyd\n    enabled: yes\n    state: started\n\n- name: Set system timezone\n  command: /usr/bin/timedatectl set-timezone UTC\n  when: ansible_date_time.tz != \"UTC\"\n\n- name: Check if RTC set to UTC\n  shell: timedatectl | awk '/RTC in local/ { print $5 }'\n  changed_when: false\n  register: chrony_local_utc\n\n- name: Set system hardware clock to UTC\n  command: /usr/bin/timedatectl set-local-rtc no\n  when: chrony_local_utc == 'yes'\n\n- name: Check if NTP is enabled\n  shell: timedatectl | awk '/NTP enabled/ { print $3 }'\n  changed_when: false\n  register: chrony_ntp_enabled\n\n- name: Set NTP enabled\n  command: /usr/bin/timedatectl set-ntp yes\n  when: chrony_ntp_enabled == 'no'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ac6ca1be8d1094c2dc6095993397571be7169c4d", "filename": "roles/nfs-server/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Reload NFS\"\n  command: exportfs -a\n\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "ba69dfbb30b4f4e9448c76ce51a8afdf8cba2d28", "filename": "archive/roles/registry/defaults/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\ndefault_auth_layer: false\ndefault_certificate_path: \"/etc/nginx/ssl\"\ndefault_certificate_subject: \"/C=US/ST=NC/L=Raleigh/O=Example, Inc/OU=Web/CN=example.com\"\ndefault_domain_name: registry.example.com\ndefault_nginx_repo_url: \"http://nginx.org/packages/rhel/7/noarch/RPMS/nginx-release-rhel-7-0.el7.ngx.noarch.rpm\"\n"}, {"commit_sha": "45971be8249cc4627ef8ddfacf55a661b7fc13ca", "sha": "0ece5d0ee3b5e4aaac46f2363fc5037339eea25f", "filename": "tasks/setup-audit.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Ensure auditd is installed\n  package:\n    name: auditd\n    state: present\n  become: true\n  when: _docker_os_dist == \"Ubuntu\" or\n        _docker_os_dist == \"Debian\"\n\n- name: Copy Docker audit rules\n  copy:\n    src: files/etc/audit/rules.d/docker.rules\n    dest: /etc/audit/rules.d/docker.rules\n  become: yes\n  notify: restart auditd\n  when: docker_enable_audit | bool\n\n- name: Ensure Docker audit rules are removed\n  file:\n    path: /etc/audit/rules.d/docker.rules\n    state: absent\n  become: yes\n  notify: restart auditd\n  when: not docker_enable_audit | bool\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "b18d8db4c3510222139fd9a9e1c39a880fe31272", "filename": "ops/playbooks/roles/worker/defaults/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n\n#\n# allow some time for a worker node to join the UCP swarm \n#\nworker_join_delay: 60\n\nworker_role_ports:\n  - 80/tcp\n  - 443/tcp\n  - 2377/tcp\n  - 4789/tcp\n  - 4789/udp\n  - 7946/tcp\n  - 7946/udp\n  - 12376/tcp\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "06ae0ee9f81f520b25a52b46c592d9dfeeb1ac75", "filename": "roles/cloud-lightsail/defaults/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\nlightsail_venv: \"{{ playbook_dir }}/configs/.venvs/aws\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "d42484e050b7208881189d489b77002ef56eba3c", "filename": "reference-architecture/vmware-ansible/playbooks/roles/storage-class-configure/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Copy cloud provider storage class file\n  template:\n    src: cloud-provider-storage-class.yaml.j2\n    dest: ~/cloud-provider-storage-class.yaml\n\n- name: Copy cloud provider storage class file to single master\n  fetch:\n    src: ~/cloud-provider-storage-class.yaml\n    dest: ~/cloud-provider-storage-class.yaml\n    flat: yes\n\n- name: Switch to default project\n  command: oc project default\n\n- name: Check to see if storage class is already created\n  command: \"oc get storageclass\"\n  register: storage_class\n\n- name: Create storage class\n  command: \"oc create -f ~/cloud-provider-storage-class.yaml\"\n  when: \"'{{ vcenter_datastore }}' not in storage_class.stdout\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "e4c9fdffb69bf8e48204f5166d67609ce32e5d20", "filename": "roles/subscription-manager/tasks/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: \"Initialize rhsm_password variable if vars_prompt was used\"\n  set_fact:\n    rhsm_password: \"{{ hostvars.localhost.rhsm_password }}\"\n  when:\n    - rhsm_password is not defined or rhsm_password is none or rhsm_password|trim == ''\n\n- name: \"Initializing Subscription Manager authentication method\"\n  set_fact:\n    rhsm_authentication: false\n\n# 'rhsm_activationkey' will take precedence even if 'rhsm_username' and 'rhsm_password' are also set\n- name: \"Setting Subscription Manager Activation Key Fact\"\n  set_fact:\n    rhsm_authentication: \"key\"\n  when:\n    - rhsm_activationkey is defined\n    - rhsm_activationkey is not none\n    - rhsm_activationkey|trim != ''\n    - not rhsm_authentication\n\n# If 'rhsm_username' and 'rhsm_password' are set but not 'rhsm_activationkey', set 'rhsm_authentication' to password\n- name: \"Setting Subscription Manager Username and Password Fact\"\n  set_fact:\n    rhsm_authentication: \"password\"\n  when:\n    - rhsm_username is defined\n    - rhsm_username is not none\n    - rhsm_username|trim != ''\n    - rhsm_password is defined\n    - rhsm_password is not none\n    - rhsm_password|trim != ''\n    - not rhsm_authentication\n\n- name: \"Initializing registration status\"\n  set_fact:\n    registered: false\n\n- name: \"Checking subscription status (a failure means it is not registered and will be)\"\n  command: \"/usr/bin/subscription-manager status\"\n  ignore_errors: yes\n  changed_when: no\n  register: check_if_registered\n\n- name: \"Set registration fact if system is already registered\"\n  set_fact:\n    registered: true\n  when: check_if_registered.rc == 0\n\n- name: \"Cleaning any old subscriptions\"\n  command: \"/usr/bin/subscription-manager clean\"\n  when:\n    - not registered\n    - rhsm_authentication is defined\n  register: cleaningsubs_result\n  until: cleaningsubs_result.rc == 0\n  retries: 10\n  delay: 1\n\n- name: \"Install Satellite certificate\"\n  command: \"rpm -Uvh --force http://{{ rhsm_satellite }}/pub/katello-ca-consumer-latest.noarch.rpm\"\n  when:\n    - not registered\n    - rhsm_satellite is defined\n    - rhsm_satellite is not none\n    - rhsm_satellite|trim != ''\n\n- name: \"Register to Satellite using activation key\"\n  command: \"/usr/bin/subscription-manager register --activationkey={{ rhsm_activationkey }} --org='{{ rhsm_org }}'\"\n  when:\n    - not registered\n    - rhsm_authentication == 'key'\n    - rhsm_satellite is defined\n    - rhsm_satellite is not none\n    - rhsm_satellite|trim != ''\n  register: register_key_result\n  until: register_key_result.rc == 0\n  retries: 10\n  delay: 1\n\n# This can apply to either Hosted or Satellite\n- name: \"Register using username and password\"\n  command: \"/usr/bin/subscription-manager register --username={{ rhsm_username }} --password={{ rhsm_password }}\"\n  no_log: true\n  when:\n    - not registered\n    - rhsm_authentication == \"password\"\n    - rhsm_org is not defined or rhsm_org is none or rhsm_org|trim == ''\n  register: register_userpw_result\n  until: register_userpw_result.rc == 0\n  retries: 10\n  delay: 1\n\n# This can apply to either Hosted or Satellite\n- name: \"Register using username, password and organization\"\n  command: \"/usr/bin/subscription-manager register --username={{ rhsm_username }} --password={{ rhsm_password }} --org={{ rhsm_org }}\"\n  no_log: true\n  when:\n    - not registered\n    - rhsm_authentication == \"password\"\n    - rhsm_org is defined\n    - rhsm_org is not none\n    - rhsm_org|trim != ''\n  register: register_userpworg_result\n  until: register_userpworg_result.rc == 0\n  retries: 10\n  delay: 1\n\n- name: \"Auto-attach to Subscription Manager Pool\"\n  command: \"/usr/bin/subscription-manager attach --auto\"\n  when:\n    - not registered\n    - rhsm_pool is undefined or rhsm_pool is none or rhsm_pool|trim == ''\n  register: autoattach_result\n  until: autoattach_result.rc == 0\n  retries: 10\n  delay: 1\n\n- name: \"Attach to a specific pool\"\n  command: \"/usr/bin/subscription-manager attach --pool={{ rhsm_pool }}\"\n  when:\n    - rhsm_pool is defined\n    - rhsm_pool is not none\n    - rhsm_pool|trim != ''\n    - not registered\n  register: attachpool_result\n  until: attachpool_result.rc == 0\n  retries: 10\n  delay: 1\n\n- name: \"Disable all repositories\"\n  command: \"/usr/bin/subscription-manager repos --disable=*\"\n  when:\n    - not registered\n    - rhsm_repos is defined\n    - rhsm_repos is not none\n    - rhsm_repos|trim != ''\n\n- name: \"Enable specified repositories\"\n  command: \"/usr/bin/subscription-manager repos --enable={{ item }}\"\n  with_items: \"{{ rhsm_repos }}\"\n  when:\n    - not registered\n    - rhsm_repos is defined\n    - rhsm_repos is not none\n    - rhsm_repos|trim != ''\n  register: enablerepos_result\n  until: enablerepos_result.rc == 0\n  retries: 10\n  delay: 1\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a903180a20d57df6ea8bd93b38156da7d7cec407", "filename": "roles/rhsm/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Testing RHSM functional for Satellite 6 integration\n  hosts: test-sat6\n  roles:\n  - role: rhsm \n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "e73ecf7eb6cd30bccb0cc60f0ef9efe04b3cbcec", "filename": "roles/ejabberd_xs/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install ejabberd packages\n  package: name={{ item }}\n           state=present\n  with_items:\n   - ejabberd-2.1.11\n  tags:\n    - download\n  when: not is_debuntu\n\n# need to use lineinfile and better regexp\n- name: Disable updating ejabberd on CentOS\n  shell: sed -i -e '/^enabled=/a exclude=ejabberd' {{ item }}\n  with_items:\n    - /etc/yum.repos.d/CentOS-Base.repo\n    - /etc/yum.repos.d/CentOS-CR.repo\n    - /etc/yum.repos.d/CentOS-fasttrack.repo\n    - /etc/yum.repos.d/CentOS-Vault.repo\n  when: ejabberd_xs_install and is_centos\n\n- name: Disable updating ejabberd on Fedora\n  shell: sed -i -e '/^enabled=/a exclude=ejabberd' {{ item }}\n  with_items:\n    - /etc/yum.repos.d/fedora.repo\n    - /etc/yum.repos.d/fedora-updates.repo\n    - /etc/yum.repos.d/fedora-updates-testing.repo\n  when: ejabberd_xs_install and ansible_distribution == \"Fedora\"\n\n- name: Configure ejabberd\n  template: backup=yes\n            src={{ item.src }}\n            dest={{ item.dest }}\n            owner=root\n            group=root\n            mode={{ item.mode }}\n  with_items:\n    - { src: 'ejabberd-xs.cfg.j2', dest: '/etc/ejabberd/ejabberd-xs.cfg' , mode: '0644' }\n    - { src: 'ejabberdctl.cfg.j2', dest: '/etc/ejabberd/ejabberdctl-xs.cfg', mode: '0644' }\n    - { src: 'ejabberd-xs', dest: '/etc/sysconfig/ejabberd-xs', mode: '0755' }\n#    - { src: 'ejabberd-domain-config', dest: '/etc/sysconfig/olpc-scripts/domain_config.d/ejabberd', mode: '0755'}\n#    - { src: 'ejabberd', dest: '/etc/sysconfig/olpc-scripts/domain_config.d/ejabberd' , mode: '0755' }\n    - { src: 'ejabberd-xs.service.j2', dest: '/etc/systemd/system/ejabberd-xs.service', mode: '0755' }\n    - { src: 'xs-ejabberd-srg', dest: '/usr/bin/xs-ejabberd-srg' , mode: '0755' }\n    - { src: '10-ejabberdmoodle', dest: '/etc/sudoers.d/10-ejabberdmoodle', mode: '0440' }\n    - { src: 'ejabberd.tmpfiles', dest: '/etc/tmpfiles.d/ejabberd.conf', mode: '0640' }\n  register: ejabberd_config\n  when: not is_debuntu\n\n- name: Put the startup script in place - non debian\n  template: src='ejabberd-xs.init'\n            dest='/usr/libexec/ejabberd-xs'\n  when: not is_debuntu\n\n- name: Remove ejabberd_domain if domain changes\n  file: path=/etc/sysconfig/ejabberd_domain_name\n        state=absent\n  when: ejabberd_config.changed and ejabberd_config is defined and not is_debuntu\n\n- name: Enable ejabberd service\n  file: src=/etc/systemd/system/ejabberd-xs.service\n        dest=/etc/systemd/system/multi-user.target.wants/ejabberd-xs.service\n        owner=root\n        group=root\n        state=link\n  when: not is_debuntu and ejabberd_xs_enabled\n\n- name: Start ejabberd service\n  service: name=ejabberd-xs\n           state=restarted\n           enabled=yes\n  when: ejabberd_config.changed and ejabberd_xs_enabled and not is_debuntu\n\n- name: Wait for ejabberd service start\n  wait_for: port=5280\n            delay=15\n            state=started\n            timeout=300\n  when: ejabberd_config.changed and ejabberd_xs_enabled\n\n#- name: Create online group\n#  shell: ejabberdctl srg_create Online \"schoolserver\" Online \"Online_Users\" Online\n#  when: ejabberd_config.changed and not is_debuntu and ejabberd_xs_enabled\n\n#- name: Add all users to online group\n#  shell: ejabberdctl srg_user_add '@online@' \"schoolserver\" Online \"schoolserver\"\n#  when: ejabberd_config.changed and not is_debuntu and ejabberd_xs_enabled\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "0cfaf3657cc8d710a9e5588b006d54f41fbc9ddc", "filename": "roles/config-idm-server/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n# defaults file for idm\n\nidm_principal: admin\n\n"}, {"commit_sha": "c91b6076e3a957fb0a165131d0ff3b3b208ed419", "sha": "5a03ce9abd50bd9d9028e72fa82a250b7834a51a", "filename": "tasks/section_04_level2.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 4.5 Activate AppArmor (install) (Scored)\n    apt: >\n        name=apparmor\n        state=present\n    when: use_apparmor == True\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Activate AppArmor (start) (Scored)\n    service: >\n        name=apparmor\n        state=started\n    when: use_apparmor == True\n    tags:\n      - section4\n      - section4.5\n      \n  - name: 4.5 Activate AppArmor (fix profiles) (Scored)\n    service: >\n        name=rsyslog\n        state=restarted\n    when: use_apparmor == True\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Activate AppArmor (Scored)\n    command: apparmor_status\n    register: aa_status_lines\n    failed_when: '\"0 profiles are loaded\" in aa_status_lines.stdout_lines or \"0 processes are in complain mode.\" not in aa_status_lines.stdout_lines or \"0 processes are unconfined but have a profile defined.\" not in aa_status_lines.stdout_lines'\n        # - '\"0 processes are unconfined but have a profile defined.\" not in aa_status_lines.stdout_lines'\n    changed_when: False\n    when: use_apparmor == True\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Activate AppArmor (enforce install) (Scored)\n    apt: >\n        name=apparmor-utils\n        state=present\n    when: use_apparmor == True\n    tags:\n      - section4\n      - section4.5\n\n  - name: 4.5 Activate AppArmor (enforce) (Scored)\n    #shell: 'aa-enforce /etc/apparmor.d/*'\n    shell: for profile in /etc/apparmor.d/*; do aa-enforce $profile; done\n    register: aaenforce_rc\n    failed_when: aaenforce_rc.rc == 1\n    changed_when: False\n    when: use_apparmor == True\n    tags:\n      - section4\n      - section4.5\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "0c6f15c4095e5b2127554f0c1aa8155e2904aefc", "filename": "playbooks/provision-dns-server/configure-dns-server.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: dns-server\n  roles:\n  - role: dns/config-dns-server\n  - role: dns/manage-dns-zones\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "ef06fea2727eb222b2873b0b3a233a6371d11998", "filename": "tasks/setup_privilege_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: setup_privilege\n    args: \"{{ _nexus_privilege_defaults|combine(item) }}\""}, {"commit_sha": "45971be8249cc4627ef8ddfacf55a661b7fc13ca", "sha": "5046dc11257e6e710565aa2ca3e03ec66865a0f8", "filename": "tasks/distribution-checks.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Fail if unsupported CentOS/RedHat version\n  fail:\n    msg: \"CentOS/RedHat 7 or later is required!\"\n  when: (_docker_os_dist == \"CentOS\" or _docker_os_dist == \"RedHat\") and\n        _docker_os_dist_major_version < '7'\n\n- name: Fail if unsupported Fedora version\n  fail:\n    msg: \"Fedora 24 or later is required!\"\n  when: _docker_os_dist == \"Fedora\" and\n        _docker_os_dist_major_version < '24'\n\n- name: Fail if unsupported Ubuntu version\n  fail:\n    msg: \"Ubuntu 14 or later is required!\"\n  when: _docker_os_dist == \"Ubuntu\" and\n        _docker_os_dist_major_version < '14'\n\n- name: Fail if unsupported Debian version\n  fail:\n    msg: \"Debian 8 (jessie) or later is required!\"\n  when: _docker_os_dist == \"Debian\" and\n        _docker_os_dist_major_version < '8'\n\n- name: Fail if this roles does not support the distribution\n  fail:\n    msg: \"Distribution {{ _docker_os_dist }} is not supported by this role!\"\n  when: _docker_os_dist != \"Fedora\" and\n        _docker_os_dist != \"CentOS\" and\n        _docker_os_dist != \"RedHat\" and\n        _docker_os_dist != \"Ubuntu\" and\n        _docker_os_dist != \"Debian\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "cbf6672ba05d96cc088a81363d2f9b7a0d248750", "filename": "archive/roles/cicd-common/tasks/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: Setting CICD Common Facts\n  set_fact:\n    cicd_storage_disk_volume: \"{{ cicd_storage_disk_volume | default(default_cicd_storage_disk_volume) }}\"\n    cicd_openstack_security_groups: \"{{ cicd_openstack_security_groups | default(default_cicd_openstack_security_groups) }}\"\n    cicd_openstack_flavor_name: \"{{ cicd_openstack_flavor_name | default(default_cicd_openstack_flavor_name) }}\"\n    cicd_openstack_image_name: \"{{ cicd_openstack_image_name | default(default_cicd_openstack_image_name) }}\"\n    cicd_openstack_storage_size: \"{{ cicd_openstack_storage_size | default(default_cicd_openstack_storage_size) }}\"\n    cicd_instance_count: \"{{ cicd_instance_count | default(default_cicd_instance_count) }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "3a35911cfe0f4d2bc0a00e9ed913ffb0eb24efb6", "filename": "reference-architecture/vmware-ansible/playbooks/roles/heketi-configure/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- stat: path=~/.bashrc\n  register: bashrc_file\n\n- name: Add heketi_cli_server ENV var\n  lineinfile:\n    dest: ~/.bashrc\n    line: \"export HEKETI_CLI_SERVER=http://{{ ansible_default_ipv4.address }}:8080\"\n\n- name: Add heketi_cli_user ENV var\n  lineinfile:\n    dest: ~/.bashrc\n    line: \"export HEKETI_CLI_USER=admin\"\n\n- name: Add heketi_cli_password ENV var\n  lineinfile:\n    dest: ~/.bashrc\n    line: \"export HEKETI_CLI_KEY={{ admin_key }}\"\n\n- name: source bashrc\n  action: shell source ~/.bashrc\n\n- name: Copy topology config file to single_crs\n  template:\n    src: ../topology.json\n    dest: ~/topology.json\n\n- name: load topology file\n  command: \"heketi-cli topology load --json=topology.json\"\n  register: load_top\n\n- name: store heketi secret\n  shell: \"echo -n {{ admin_key }} | base64\"\n  register: stored_secret\n\n- name: assign secret to a variable for use in template\n  set_fact:\n    heketi_secret: \"{{ stored_secret.stdout }}\"\n\n- name: Copy heketi secret config file\n  template:\n    src: heketi-secret.yaml.j2\n    dest: ~/heketi-secret.yaml\n\n- name: Fetch heketi secret to copy to master\n  fetch:\n    src: ~/heketi-secret.yaml\n    dest: ~/heketi-secret.yaml\n    flat: yes\n\n- name: Copy storage-crs config file\n  template:\n    src: storage-crs.json.j2\n    dest: ~/storage-crs.json\n\n- name: Fetch storage-crs to copy to master\n  fetch:\n    src: ~/storage-crs.json\n    dest: ~/storage-crs.json\n    flat: yes\n"}, {"commit_sha": "c91b6076e3a957fb0a165131d0ff3b3b208ed419", "sha": "7d7892e9531f26330476956bda3f90fe622354c7", "filename": "tasks/section_07_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 7.1.1 Disable IP Forwarding (Scored)\n    sysctl: >\n      name=net.ipv4.ip_forward\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.1\n      - section7.1.1\n\n  - name: 7.1.2.1 Disable Send Packet Redirects (Scored)\n    sysctl: >\n      name=net.ipv4.conf.all.send_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.1\n      - section7.1.2\n      - section7.1.2.1\n\n  - name: 7.1.2.2 Disable Send Packet Redirects (Scored)\n    sysctl: >\n      name=net.ipv4.conf.default.send_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.1\n      - section7.1.2\n      - section7.1.2.2\n\n  - name: 7.2.1.1 Disable Source Routed Packet Acceptance (Scored)\n    sysctl: >\n      name=net.ipv4.conf.all.accept_source_route\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.1\n      - section7.2.1.1\n\n  - name: 7.2.1.2 Disable Source Routed Packet Acceptance (Scored)\n    sysctl: >\n      name=net.ipv4.conf.default.accept_source_route\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.1\n      - section7.2.1.2\n\n  - name: 7.2.2.1 Disable ICMP Redirect Acceptance (Scored)\n    sysctl: >\n      name=net.ipv4.conf.all.accept_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.2\n      - section7.2.2.1\n\n  - name: 7.2.2.2 Disable ICMP Redirect Acceptance (Scored)\n    sysctl: >\n      name=net.ipv4.conf.default.accept_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.2\n      - section7.2.2.2\n\n  - name: 7.2.3.1 Disable Secure ICMP Redirect Acceptance (Scored)\n    sysctl: >\n      name=net.ipv4.conf.all.secure_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.3\n      - section7.2.3.1\n\n  - name: 7.2.3.2 Disable Secure ICMP Redirect Acceptance (Scored)\n    sysctl: >\n      name=net.ipv4.conf.default.secure_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.3\n      - section7.2.3.2\n\n  - name: 7.2.4.1 Log Suspicious Packets (Scored)\n    sysctl: >\n      name=net.ipv4.conf.all.log_martians\n      value=1\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.4\n      - section7.2.4.1\n\n  - name: 7.2.4.2 Log Suspicious Packets (Scored)\n    sysctl: >\n      name=net.ipv4.conf.default.log_martians\n      value=1\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.4\n      - section7.2.4.2\n\n  - name: 7.2.5 Enable Ignore Broadcast Requests (Scored)\n    sysctl: >\n      name=net.ipv4.icmp_echo_ignore_broadcasts\n      value=1\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.5\n\n  - name: 7.2.6 Enable Bad Error Message Protection (Scored)\n    sysctl: >\n      name=net.ipv4.icmp_ignore_bogus_error_responses\n      value=1\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.6\n\n  - name: 7.2.7.1 Enable RFC-recommended Source Route Validation (Scored)\n    sysctl: >\n      name=net.ipv4.conf.all.rp_filter\n      value=1\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.7\n      - section7.2.7.1\n\n  - name: 7.2.7.2 Enable RFC-recommended Source Route Validation (Scored)\n    sysctl: >\n      name=net.ipv4.conf.default.rp_filter\n      value=1\n      state=present\n    tags:\n      - section7\n      - section7.2\n      - section7.2.7\n      - section7.2.7.2\n\n  - name: 7.2.8 Enable TCP SYN Cookies (Scored)\n    sysctl: >\n      name=net.ipv4.tcp_syncookies\n      value=1\n      state=present\n    when: enable_tcp_syncookies\n    tags:\n      - section7\n      - section7.2\n      - section7.2.8\n\n  - name: 7.3.1.1 Disable IPv6 Router Advertisements (Not Scored)\n    sysctl: >\n      name=net.ipv6.conf.all.accept_ra\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.3\n      - section7.3.1\n      - section7.3.1.1\n\n  - name: 7.3.1.2 Disable IPv6 Router Advertisements (Not Scored)\n    sysctl: >\n      name=net.ipv6.conf.default.accept_ra\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.3\n      - section7.3.1\n      - section7.3.1.2\n\n  - name: 7.3.2.1 Disable IPv6 Redirect Acceptance (Not Scored)\n    sysctl: >\n      name=net.ipv6.conf.all.accept_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.3\n      - section7.3.2\n      - section7.3.2.1\n\n  - name: 7.3.2.2 Disable IPv6 Redirect Acceptance (Not Scored)\n    sysctl: >\n      name=net.ipv6.conf.default.accept_redirects\n      value=0\n      state=present\n    tags:\n      - section7\n      - section7.3\n      - section7.3.2\n      - section7.3.2.2\n\n  - name: 7.3.3 Disable IPv6 (Not Scored)\n    sysctl: >\n      name={{ item }}\n      value=1\n      state=present\n    with_items:\n      - net.ipv6.conf.all.disable_ipv6\n      - net.ipv6.conf.default.disable_ipv6\n      - net.ipv6.conf.lo.disable_ipv6\n    when: disable_ipv6 == True\n    tags:\n      - section7\n      - section7.3\n      - section7.3.3\n\n  - name: 7.4.1 Install TCP Wrappers (Scored)\n    apt: name=tcpd state=present\n    tags:\n      - section7\n      - section7.4\n      - section7.4.1\n\n  - name: 7.4.2 Create /etc/hosts.allow (Not Scored)\n    debug: msg=\"*** Verify /etc/hosts.allow ***\"\n    tags:\n      - section7\n      - section7.4\n      - section7.4.2\n\n  - name: 7.4.3 Verify Permissions on /etc/hosts.allow (Scored)\n    file: >\n        path=/etc/hosts.allow\n        owner=root\n        group=root\n        mode=0644\n    tags:\n      - section7\n      - section7.4\n      - section7.4.3\n\n  - name: 7.4.4 Create /etc/hosts.deny (Not Scored)\n    debug: msg='*** Verify /etc/hosts.deny ***'\n    tags:\n      - section7\n      - section7.4\n      - section7.4.4\n\n  - name: 7.4.5 Verify Permissions on /etc/hosts.deny (Scored)\n    file: >\n        path=/etc/hosts.deny\n        owner=root\n        group=root\n        mode=0644\n    tags:\n      - section7\n      - section7.4\n      - section7.4.5\n\n  - name: 7.5.0 Check the presence of the file \"cis.conf\" under modprobe.d\n    stat: >\n        path=/etc/modprobe.d/CIS.conf\n    register: cis_conf_file\n    tags:\n      - section7\n      - section7.5\n\n  - name: 7.5.0 Create the file \"cis.conf\" under modprobe.d if doesn't exist\n    file: >\n        dest=/etc/modprobe.d/CIS.conf state=touch\n    when: not cis_conf_file.stat.exists\n    tags:\n      - section7\n      - section7.5\n\n  - name: 7.5.1-4 Disable DCCP, SCTP, RDS, TIPC (Not Scored)\n    lineinfile: >\n        dest=/etc/modprobe.d/CIS.conf\n        line='install {{ item }} /bin/true'\n        state=present\n    with_items:\n        - dccp\n        - sctp\n        - rds\n        - tipc\n    tags:\n      - section7\n      - section7.5\n      - section7.5.1\n      - section7.5.2\n      - section7.5.3\n      - section7.5.4\n\n  - name: 7.6 Deactivate Wireless Interfaces (Not Scored)\n    shell: 'lspci -k | grep -i wifi'\n    changed_when: False\n    register: lspci_wifi\n    failed_when: lspci_wifi.rc == 0\n    tags:\n      - section7\n      - section7.6\n\n  - name: 7.7 Ensure Firewall is active (install) (Scored)\n    apt: >\n        name=ufw\n        state=present\n    when: activate_ufw\n    tags:\n      - section7\n      - section7.7\n\n  - name: 7.7 Ensure Firewall is active (Scored)\n    service: >\n        name=ufw\n        state=started\n    when: activate_ufw\n    tags:\n      - section7\n      - section7.7\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "0b1f25eaa14b1f5d178c99f08148ae1c316b7218", "filename": "archive/roles/secure-registry/tasks/main.yaml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n  - name: Create temp directory for kubeconfig\n    command: mktemp -d /tmp/openshift-ansible-XXXXXX\n    register: mktemp\n    changed_when: False\n    delegate_to: \"{{ secure_registry_master_host }}\"\n    run_once: true\n\n  - name: Copy the admin client config(s)\n    command: >\n      cp {{ openshift_master_config_dir }}/admin.kubeconfig {{ mktemp.stdout }}/admin.kubeconfig\n    changed_when: False\n    delegate_to: \"{{ secure_registry_master_host }}\"\n    run_once: true\n\n  - name: \"Check that Openshift Docker Registry exists\"\n    command: >\n      {{ openshift.common.client_binary }} --config={{ mktemp.stdout }}/admin.kubeconfig get deploymentConfig {{ registry_dc }}\n       -n {{ openshift_registry_project }}\n    register: registry_exists\n    delegate_to: \"{{ secure_registry_master_host }}\"\n    run_once: true\n\n  - fail:\n      msg: \"No docker registry found in project default\"\n    when: registry_exists.rc != 0\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n  - fail:\n      msg: \"Both registry_certificate and registry_key must be set, or neither\"\n    when: (registry_certificate == \"\" and registry_key != \"\") or (registry_certificate != \"\" and registry_key == \"\")\n    delegate_to: \"{{ secure_registry_master_host }}\"\n    run_once: true\n\n  - name: \"Collect registry service info\"\n    command: >\n      {{ openshift.common.client_binary}} get service {{ docker_registry_dc_name}}\n       -n {{ openshift_registry_project }} -o yaml\n    register: registry_svc_info\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n  - set_fact:\n      registry_svc_ip: \"{{ (registry_svc_info.stdout | from_yaml).spec.clusterIP }}\"\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n  - set_fact:\n      registry_svc_ports: [\"{{item.port}}\"]\n    with_items: \"{{ (registry_svc_info.stdout | from_yaml).spec.ports}}\"\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n  - name: \"Check if registry is already secured\"\n    uri:\n      url: \"https://{{ registry_svc_ip }}:5000\"\n      method: \"GET\"\n      status_code: '200'\n      validate_certs: no\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n    register: secured\n    failed_when: false\n\n  - fail:\n      msg: \"The registry is already secured\"\n    when: secured.status == 200\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n  - name: \"Creating certificate...\"\n    command: \"{{ openshift.common.admin_binary}} ca create-server-cert --signer-cert={{ openshift_master_config_dir }}/ca.crt \\\n        --signer-key={{ openshift_master_config_dir }}/ca.key \\\n        --signer-serial={{ openshift_master_config_dir }}/ca.serial.txt \\\n        --hostnames='docker-registry.default.svc.cluster.local,{{registry_svc_ip}}' \\\n        --cert=/etc/secrets/registry.crt \\\n        --key=/etc/secrets/registry.key\"\n    when: registry_certificate == \"\" and registry_key == \"\"\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n    args:\n      creates: '/etc/secrets/registry.crt'\n    register: cert_created\n\n  - set_fact:\n      registry_certificate: '/etc/secrets/registry.crt'\n    when: registry_certificate == \"\" and registry_key == \"\"\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n  - set_fact:\n      registry_key: '/etc/secrets/registry.key'\n    when: registry_certificate == \"/etc/secrets/registry.crt\" and registry_key == \"\"\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n  - set_fact:\n      registry_ca: '/etc/origin/master/ca.crt'\n    when: registry_ca == \"\"\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n  - name: \"Cleaning up any previous attempts\"\n    command: >\n      {{openshift.common.client_binary}} delete secrets {{registry_secret_name}}\n       -n {{openshift_registry_project}}\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n    register: cleanup\n    failed_when: cleanup.rc != 0 and cleanup.rc != 1\n\n  - name: \"Creating registry secret\"\n    command: >\n      {{openshift.common.client_binary}} secrets new {{registry_secret_name}}\n       {{registry_certificate}} {{registry_key}} -n {{openshift_registry_project}}\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n  - name: \"Adding secrets to registry's service account\"\n    command: >\n      {{openshift.common.client_binary}} secrets add\n      serviceaccounts/{{registry_serviceaccount}} secrets/{{registry_secret_name}}\n       -n {{openshift_registry_project}}\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n  - name: \"Adding secrets to default account\"\n    command: >\n      {{openshift.common.client_binary}} secrets add serviceaccounts/default\n      secrets/{{registry_secret_name}} -n {{openshift_registry_project}}\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n  - name: \"Ceanup any previous runs\"\n    shell: \"{{openshift.common.client_binary}} get deploymentConfig/{{registry_dc}} -n {{openshift_registry_project}} -o yaml | grep {{registry_secret_name}}\"\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n    register: dc_cleanup\n    failed_when: dc_cleanup.rc != 0  and dc_cleanup.rc != 1\n\n\n  - name: \"Disable DeploymentConfig change triggger\"\n    command: \"{{openshift.common.client_binary}} patch deploymentConfig/{{registry_dc}} --api-version=v1 -p '{{registry_trigger_disable}}' -n {{openshift_registry_project}}\"\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n\n  - name: \"Add secret volumes to registry deploymentConfiguration\"\n    command: >\n      {{openshift.common.client_binary}} volume deploymentConfig/{{registry_dc}}\n      --add --type=secret --secret-name={{registry_secret_name}} -m /etc/secrets\n      --overwrite -n {{openshift_registry_project}}\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n    when: dc_cleanup.rc == 0\n\n  - name: \"Add secret volumes to registry deploymentConfiguration\"\n    command: >\n      {{openshift.common.client_binary}} volume deploymentConfig/{{registry_dc}}\n       --add --type=secret --secret-name={{registry_secret_name}} -m /etc/secrets\n       -n {{openshift_registry_project}}\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n    when: dc_cleanup.rc == 1\n\n  - name: \"Enable TLS\"\n    command: >\n      {{openshift.common.client_binary}} env deploymentConfig/{{registry_dc}}\n       REGISTRY_HTTP_TLS_CERTIFICATE={{registry_certificate}}\n        REGISTRY_HTTP_TLS_KEY={{registry_key}} --overwrite\n       -n {{openshift_registry_project}}\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n  - name: \"Updating liveness probe to HTTPS scheme\"\n    command: \"{{openshift.common.client_binary}} patch deploymentConfig/{{registry_dc}} --api-version=v1 -p '{{liveness_patch}}' -n {{openshift_registry_project}}\"\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n  - name: \"Updating readinessProbe to HTTPS scheme\"\n    command: \"{{openshift.common.client_binary}} patch deploymentConfig/{{registry_dc}} --api-version=v1 -p '{{readiness_patch}}' -n {{openshift_registry_project}}\"\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n    when: \"{{openshift.common.version_gte_3_2_or_1_2}}\"\n\n  - name: \"Create registry certificates directory for serviceIP\"\n    file: \"state=directory path=/etc/docker/certs.d/{{registry_svc_ip}}:{{item}} mode=755\"\n    with_items: \"{{registry_svc_ports}}\"\n\n  - name: \"Create registry certificates directory for kubernetes service\"\n    file: \"state=directory path=/etc/docker/certs.d/{{registry_dc}}.default.svc.cluster.local:{{item}} mode=755\"\n    with_items: \"{{registry_svc_ports}}\"\n\n  - name: Grab CA cert\n    slurp:\n      src: \"{{registry_ca}}\"\n    register: registry_ca_file\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n  - name: \"Copy registry CA cert to kubernetes service directory\"\n    copy:\n      content: \"{{ registry_ca_file.content | b64decode }}\"\n      dest: \"/etc/docker/certs.d/{{registry_dc}}.default.svc.cluster.local:{{item}}/ca.crt\"\n    with_items: \"{{registry_svc_ports}}\"\n\n  - name: \"Copy registry CA cert to registry IP directory\"\n    copy:\n      content: \"{{ registry_ca_file.content | b64decode }}\"\n      dest: \"/etc/docker/certs.d/{{registry_svc_ip}}:{{item}}/ca.crt\"\n    with_items: \"{{registry_svc_ports}}\"\n\n\n  - name: \"Remove --insecure flag from /etc/sysconfig/docker\"\n    lineinfile:\n      dest: /etc/sysconfig/docker\n      state: absent\n      regexp: '--insecure-registry=.*172\\.30\\.0\\.0/16'\n\n  - name: \"Remove INSECURE_REGISTRY flag from /etc/sysconfig/docker\"\n    lineinfile:\n      dest: /etc/sysconfig/docker\n      state: absent\n      regexp: 'INSECURE_REGISTRY=\\\"--insecure-registry 172\\.30\\.0\\.0/16\\\"'\n\n  - name: \"Reload daemon\"\n    command: \"systemctl daemon-reload\"\n\n  - name: \"Restart docker\"\n    service: \"name=docker state=restarted\"\n\n  - name: \"Re-enable DeploymentConfig change triggger\"\n    command: \"{{openshift.common.client_binary}} patch deploymentConfig/{{registry_dc}} --api-version=v1 -p '{{registry_trigger_enable}}' -n {{openshift_registry_project}}\"\n    run_once: true\n    delegate_to: \"{{ secure_registry_master_host }}\"\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a42fb3ddc5939231a417815303c87c5a58d0c3b8", "filename": "roles/user-management/manage-atlassian-users/tasks/add_user_to_groups.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Add user to groups\n  uri:\n    url: \"{{ atlassian.url }}/rest/api/2/group/user?groupname={{ item | urlencode }}\"\n    method: POST\n    user: '{{ atlassian.username }}'\n    password: '{{ atlassian.password }}'\n    force_basic_auth: yes\n    status_code: [201, 400]\n    body_format: json\n    body: \"{ 'name': '{{ atlassian_user.email.split(\\\"@\\\") | first }}' }\"\n    return_content: yes\n  with_items: '{{ atlassian_user.groups }}'\n  when: atlassian_user.groups|length > 0\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "96ac3b41ed9facabb7ec6586f0dea5e9d8a73a8d", "filename": "tasks/checks/distribution-checks-Ubuntu.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Fail if unsupported Ubuntu version\n  fail:\n    msg: \"Ubuntu 14 or later is required!\"\n  when: _docker_os_dist == \"Ubuntu\" and\n        _docker_os_dist_major_version | int < 14\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "45b3998a181551dd87ca694f58812b0aca998d28", "filename": "tasks/Linux/fetch/security-fetch/security-fetch-web.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Download security policy artifact from web\n  get_url:\n    url: '{{ java_unlimited_policy_url }}'\n    dest: '{{ java_download_path }}'\n  register: policy_file_downloaded\n  retries: 3\n  delay: 2\n  until: policy_file_downloaded is succeeded\n\n- name: Downloaded security policy artifact\n  set_fact:\n    security_policy_java_artifact: '{{ policy_file_downloaded.dest }}'\n"}, {"commit_sha": "4a9aaf0951e383c57077cf651b93e78eeea1b5ac", "sha": "388cb7be6d95bb5f450c3ef2be07e7b2ef528fe4", "filename": "tasks/install.yml", "repository": "geerlingguy/ansible-role-solr", "decoded_content": "---\n- name: Ensure dependencies are installed.\n  package:\n    name:\n      - lsof\n      - acl\n      - sudo\n    state: present\n\n- name: Run Solr installation script.\n  command: >\n    {{ solr_workspace }}/{{ solr_filename }}/bin/install_solr_service.sh\n    {{ solr_workspace }}/{{ solr_filename }}.tgz\n    -i {{ solr_install_dir }}\n    -d {{ solr_home }}\n    -u {{ solr_user }}\n    -s {{ solr_service_name }}\n    -p {{ solr_port }}\n    creates={{ solr_install_path }}/bin/solr\n  register: solr_install_script_result\n\n# Workaround for bug https://github.com/ansible/ansible-modules-core/issues/915.\n- name: Ensure solr is stopped (RHEL 7 workaround).\n  command: service {{ solr_service_name }} stop\n  when:\n    - ansible_os_family == 'RedHat'\n    - ansible_distribution_version.split(\".\")[0] == '7'\n    - solr_install_script_result.changed\n  failed_when: false\n  tags: ['skip_ansible_lint']\n\n- name: Run systemd daemon_reload (RHEL 7 workaround).\n  systemd:\n    name: solr\n    daemon_reload: true\n  when:\n    - ansible_os_family == 'RedHat'\n    - ansible_distribution_version.split(\".\")[0] == '7'\n    - solr_install_script_result.changed\n  tags: ['skip_ansible_lint']\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "37b0494e43015945e34470aebb48fef88dedaa14", "filename": "roles/config-ipa-client/tasks/prereq-Fedora.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install additional packages for IPA/IdM\"\n  package:\n    name: \"{{ item }}\"\n    state: latest\n  with_items:\n  - ipa-client\n  - libsss_sudo\n  - sssd-nfs-idmap\n  - libselinux-python\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "56f201f1838e4496f4834697d55b70a0986ad3eb", "filename": "roles/kubevirt/tasks/deprovision.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- name: Login As Super User\n  command: \"oc login -u {{ admin_user }} -p {{ admin_password }}\"\n  when: cluster==\"openshift\"\n        and admin_user is defined\n        and admin_password is defined\n\n- name: Check for kubevirt.yml template in {{ kubevirt_template_dir }}\n  stat:\n    path: \"{{ kubevirt_template_dir }}/kubevirt.yml\"\n  register: byo_template\n\n- name: Use kubevirt.yml template from {{ kubevirt_template_dir }}\n  set_fact:\n    manifest_version: \"release\"\n  when: byo_template.stat.exists == True\n\n- include_tasks: \"{{ manifest_version }}.yml\"\n\n- name: Delete KubeVirt Resources\n  command: \"kubectl delete -f /tmp/{{ item }}.yml --ignore-not-found=true\"\n  with_items:\n    - \"{{ dev_template_resources }}\"\n  when: manifest_version==\"dev\"\n\n- name: Delete KubeVirt Resources\n  command: kubectl delete -f /tmp/kubevirt.yml --ignore-not-found=true\n  when: manifest_version==\"release\"\n\n\n- name: Delete Privileged Policy\n  command: \"oc adm policy remove-scc-from-user privileged -z kubevirt-infra -n {{ namespace }}\"\n  when: cluster==\"openshift\"\n\n- name: Delete Hostmount-anyuid Policy\n  command: \"oc adm policy remove-scc-from-user hostmount-anyuid -z kubevirt-infra -n {{ namespace }}\"\n  when: cluster==\"openshift\"\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "5d0918f70d28050c111f03c290b0494e26734d8a", "filename": "roles/ovirt-guest-agent/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "- name: Install latest {{ ovirt_guest_agent_pkg_prefix }}-guest-agent\n  yum:\n    name: \"{{ item }}\"\n    state: latest\n  with_items:\n    - \"{{ ovirt_guest_agent_pkg_prefix }}-guest-agent\"\n  notify: enable and start {{ ovirt_guest_agent_pkg_prefix }}-guest-agent\n  tags:\n    - skip_ansible_lint\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "d5f972085bf4c60967330772a548c564598c64fd", "filename": "tasks/postinstall.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Reset internal variables for additional packages to be installed\n  set_fact:\n    _docker_additional_packages_os: []\n    _docker_additional_packages_pip: []\n\n- name: Set facts to install Docker SDK for Python\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + \\\n      docker_predefined_packages_pip[_docker_os_dist]['sdk'] }}\"\n  when:\n    - docker_sdk | bool\n\n- name: Set facts to install Docker Compose\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + \\\n      docker_predefined_packages_pip[_docker_os_dist]['compose'] }}\"\n  when:\n    - docker_compose | bool and not docker_compose_no_pip | bool\n\n- name: Set facts to install Docker Stack dependencies ('docker_stack')\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + \\\n      docker_predefined_packages_pip[_docker_os_dist]['stack'] }}\"\n  when:\n    - docker_stack | bool\n\n- name: Set facts with additional package to be installed\n  set_fact:\n    _docker_additional_packages_pip: \"{{ _docker_additional_packages_pip + docker_additional_packages_pip }}\"\n    _docker_additional_packages_os: \"{{ _docker_additional_packages_os + docker_additional_packages_os }}\"\n\n- name: Ensure required OS packages will be installed for PiP\n  block:\n    - name: Determine if pip/pip3 exists in path\n      become: true\n      shell: type {{ _docker_python3 | ternary('pip3', 'pip') }}\n      register: _docker_pip_cmd\n      changed_when: false\n      failed_when: false\n      check_mode: no\n      tags:\n        - skip_ansible_lint\n\n    - name: Set fact to install Python 2 PiP\n      set_fact:\n        _docker_additional_packages_os: \"{{ _docker_additional_packages_os + [docker_pip_package] }}\"\n      when:\n        - not _docker_python3 | bool\n        - _docker_pip_cmd.rc != 0\n\n    - name: Set fact to install Python 3 PiP\n      set_fact:\n        _docker_additional_packages_os: \"{{ _docker_additional_packages_os + [docker_pip3_package] }}\"\n      when:\n        - _docker_python3 | bool\n        - _docker_pip_cmd.rc != 0\n\n    - name: Set fact to install build libraries (CentOS/Fedora/RedHat)\n      set_fact:\n        _docker_additional_packages_os: \"{{ _docker_additional_packages_os + ['openssl-devel'] }}\"\n      when:\n        - (_docker_os_dist == \"CentOS\" or _docker_os_dist == \"Fedora\" or _docker_os_dist == \"RedHat\")\n\n    - name: Set fact to install build libraries [General] (Fedora)\n      set_fact:\n        _docker_additional_packages_os: \"{{ _docker_additional_packages_os + ['redhat-rpm-config','make','libffi-devel','gcc'] }}\"\n      when:\n        - _docker_os_dist == \"Fedora\"\n\n    - name: Set fact to install build libraries [Python 3] (Fedora)\n      set_fact:\n        _docker_additional_packages_os: \"{{ _docker_additional_packages_os + ['python3-devel'] }}\"\n      when:\n        - _docker_python3 | bool\n        - _docker_os_dist == \"Fedora\"\n\n    - name: Set fact to install build libraries (Debian/Ubuntu)\n      set_fact:\n        _docker_additional_packages_os: \"{{ _docker_additional_packages_os + ['libffi-dev', 'libssl-dev'] }}\"\n      when:\n        - (_docker_os_dist == \"Debian\" or _docker_os_dist == \"Ubuntu\")\n  when:\n    - _docker_additional_packages_pip | length > 0\n\n- name: Ensure python-pip-whl is present (Debian 8)\n  set_fact:\n    _docker_additional_packages_os: \"{{ _docker_additional_packages_os + ['python-pip-whl'] }}\"\n  when:\n    - _docker_additional_packages_pip | length > 0\n    - _docker_pip_cmd.rc != 0\n    - _docker_os_dist == \"Debian\"\n    - _docker_os_dist_major_version | int == 8\n\n- name: Ensure python-backports.ssl-match-hostname is present (Debian 10)\n  set_fact:\n    _docker_additional_packages_os: \"{{ _docker_additional_packages_os + ['python-backports.ssl-match-hostname'] }}\"\n  when:\n    - not _docker_python3 | bool\n    - _docker_additional_packages_pip | length > 0\n    - _docker_pip_cmd.rc != 0\n    - _docker_os_dist == \"Debian\"\n    - _docker_os_dist_major_version | int == 10\n\n- name: Ensure EPEL release repository is installed\n  become: true\n  package:\n    name: \"epel-release\"\n    state: present\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when:\n    - docker_setup_repos | bool\n    - _docker_os_dist == \"CentOS\"\n    - _docker_additional_packages_os | length > 0\n\n- name: Install additional packages (OS package manager)\n  become: true\n  package:\n    name: \"{{ item }}\"\n    state: present\n  loop: \"{{ _docker_additional_packages_os }}\"\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when: _docker_additional_packages_os | length > 0\n\n- name: Upgrade PiP\n  become: \"{{ docker_pip_sudo | bool }}\"\n  pip:\n    name: pip\n    state: forcereinstall\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when: docker_pip_upgrade | bool\n\n- name: Install additional packages (PiP)\n  become: \"{{ docker_pip_sudo | bool }}\"\n  pip:\n    name: \"{{ item }}\"\n    state: present\n    extra_args: \"{{ docker_pip_extra_args }}\"\n  loop: \"{{ _docker_additional_packages_pip }}\"\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when: _docker_additional_packages_pip | length > 0\n  environment:\n    PYTHONWARNINGS: ignore\n\n# https://github.com/docker/docker-py/issues/1502\n- name: Workaround for issue - No module named ssl_match_hostname (Python 2.7)\n  become: yes\n  command: cp -r /usr/local/lib/python2.7/dist-packages/backports/ssl_match_hostname/ /usr/lib/python2.7/dist-packages/backports\n  when:\n    - docker_x_ssl_match_hostname | bool\n    - _docker_additional_packages_pip | length > 0\n    - not _docker_python3 | bool\n\n- name: Stat /usr/bin/docker-compose\n  stat:\n    path: /usr/bin/docker-compose\n  register: _docker_compose_file\n  check_mode: no\n\n# Official installation of docker-compose (Linux): https://docs.docker.com/compose/install/#install-compose\n- name: Install docker-compose without PiP\n  block:\n    # Not using github_release:  https://github.com/ansible/ansible/issues/45391\n    - name: Get latest release of docker-compose\n      uri:\n        url: https://api.github.com/repos/docker/compose/releases/latest\n        body_format: json\n      register: _github_docker_compose\n      until: _github_docker_compose.status == 200\n      retries: 10\n      check_mode: no\n      when:\n        - docker_compose_no_pip_detect_version | bool\n\n    - name: Set detected docker-compose version\n      set_fact:\n        _docker_compose_version: \"{{ _github_docker_compose.json.tag_name }}\"\n      when:\n        - _github_docker_compose is defined\n        - _github_docker_compose.json is defined\n\n    - name: Set fixed docker-compose version\n      set_fact:\n        _docker_compose_version: \"{{ docker_compose_no_pip_version }}\"\n      when:\n        - not docker_compose_no_pip_detect_version | bool\n\n    - name: Fetch docker-compose SHA265 sum file\n      get_url:\n        url: \"https://github.com/docker/compose/releases/download/\\\n          {{ _docker_compose_version }}/docker-compose-{{ ansible_system }}-{{ ansible_architecture }}.sha256\"\n        dest: \"/tmp/ansible.docker-compose-sha256\"\n      register: _github_docker_compose_shasum_file\n      changed_when: false\n      until: _github_docker_compose_shasum_file.status_code == 200\n      retries: 10\n      check_mode: no\n\n    - name: Dump SHA256 file contents to variable\n      command: cat /tmp/ansible.docker-compose-sha256\n      register: _github_docker_compose_shasum\n      changed_when: false\n      check_mode: no\n\n    - name: Remove temporary file for SHA256 sum\n      file:\n        path: \"/tmp/ansible.docker-compose-sha256\"\n        state: absent\n      changed_when: false\n      check_mode: no\n\n    - name: Set SHA256 facts related to docker-compose\n      set_fact:\n        _docker_compose_checksum: \"sha256:{{ _github_docker_compose_shasum.stdout | \\\n          regex_replace('^([0-9a-zA-Z]*)[\\\\s\\\\t]+.+', '\\\\1') }}\"\n\n    # Use when moving to Ansible 2.7 as minimum version\n    # - name: Set SHA256 facts related to docker-compose (Ansible >= 2.7)\n    #   set_fact:\n    #     _docker_compose_checksum: \"sha256:https://github.com/docker/compose/releases/download/\\\n    #       {{ _github_docker_compose.json.tag_name }}/\\\n    #       docker-compose-{{ ansible_system }}-{{ ansible_architecture }}.sha256\"\n    #   when: ansible_version.full is version_compare('2.7', '>=')\n\n    - name: Install docker-compose {{ _docker_compose_version }} (Linux)\n      become: true\n      get_url:\n        url: \"https://github.com/docker/compose/releases/download/\\\n          {{ _docker_compose_version }}/docker-compose-{{ ansible_system }}-{{ ansible_architecture }}\"\n        checksum: \"{{ _docker_compose_checksum }}\"\n        dest: /usr/local/bin/docker-compose\n        mode: 0755\n\n    - name: Create symlink for docker-compose to work with sudo in some distributions\n      become: true\n      file:\n        src: /usr/local/bin/docker-compose\n        dest: /usr/bin/docker-compose\n        state: link\n  when:\n    - docker_compose | bool\n    - docker_compose_no_pip | bool\n    - not _docker_compose_file.stat.exists\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "9c6e6a5b7e7199e218e2425521f22cbfc4b417c7", "filename": "roles/common/tasks/ubuntu.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- block:\n  - name: Ubuntu | Install prerequisites\n    apt:\n      name: \"{{ item }}\"\n      update_cache: true\n    with_items:\n      - python2.7\n      - sudo\n\n  - name: Ubuntu | Configure defaults\n    alternatives:\n      name: python\n      link: /usr/bin/python\n      path: /usr/bin/python2.7\n      priority: 1\n    tags:\n      - update-alternatives\n  vars:\n    ansible_python_interpreter: /usr/bin/python3\n\n- name: Gather facts\n  setup:\n\n- name: Cloud only tasks\n  block:\n    - name: Install software updates\n      apt:\n        update_cache: true\n        install_recommends: true\n        upgrade: dist\n\n    - name: Check if reboot is required\n      shell: >\n        if [[ -e /var/run/reboot-required ]]; then echo \"required\"; else echo \"no\"; fi\n      args:\n        executable: /bin/bash\n      register: reboot_required\n\n    - name: Reboot\n      shell: sleep 2 && shutdown -r now \"Ansible updates triggered\"\n      async: 1\n      poll: 0\n      when: reboot_required is defined and reboot_required.stdout == 'required'\n      ignore_errors: true\n\n    - name: Wait until SSH becomes ready...\n      local_action:\n        module: wait_for\n        port: 22\n        host: \"{{ inventory_hostname }}\"\n        search_regex: OpenSSH\n        delay: 10\n        timeout: 320\n      when: reboot_required is defined and reboot_required.stdout == 'required'\n      become: false\n  when: algo_provider != \"local\"\n\n- name: Include unatteded upgrades configuration\n  import_tasks: unattended-upgrades.yml\n\n- name: Disable MOTD on login and SSHD\n  replace: dest=\"{{ item.file }}\" regexp=\"{{ item.regexp }}\" replace=\"{{ item.line }}\"\n  with_items:\n    - { regexp: '^session.*optional.*pam_motd.so.*', line: '# MOTD DISABLED', file: '/etc/pam.d/login' }\n    - { regexp: '^session.*optional.*pam_motd.so.*', line: '# MOTD DISABLED', file: '/etc/pam.d/sshd' }\n\n- name: Loopback for services configured\n  template:\n    src: 10-algo-lo100.network.j2\n    dest: /etc/systemd/network/10-algo-lo100.network\n  notify:\n    - restart systemd-networkd\n  tags:\n    - always\n\n- name: systemd services enabled and started\n  systemd:\n    name: \"{{ item }}\"\n    state: started\n    enabled: true\n    daemon_reload: true\n  with_items:\n    - systemd-networkd\n    - systemd-resolved\n  tags:\n    - always\n\n- meta: flush_handlers\n  tags:\n    - always\n\n- name: Check apparmor support\n  shell: apparmor_status\n  ignore_errors: yes\n  register: apparmor_status\n\n- set_fact:\n    apparmor_enabled: true\n  when: '\"profiles are in enforce mode\" in apparmor_status.stdout'\n\n- set_fact:\n    tools:\n      - git\n      - screen\n      - apparmor-utils\n      - uuid-runtime\n      - coreutils\n      - iptables-persistent\n      - cgroup-tools\n      - openssl\n    sysctl:\n      - item: net.ipv4.ip_forward\n        value: 1\n      - item: net.ipv4.conf.all.forwarding\n        value: 1\n      - item: net.ipv6.conf.all.forwarding\n        value: 1\n  tags:\n    - always\n\n- name: Install tools\n  package: name=\"{{ item }}\" state=present\n  with_items:\n    - \"{{ tools|default([]) }}\"\n  tags:\n    - always\n\n- name: Install headers\n  apt:\n    name: \"{{ item }}\"\n    state: present\n  when: install_headers\n  with_items:\n    - linux-headers-generic\n    - \"linux-headers-{{ ansible_kernel }}\"\n"}, {"commit_sha": "a10c5f4577e6e74feb1fadec4bcbab039b8b180a", "sha": "1221fb50b46231b5ddfd5524377442629d4be9a4", "filename": "tasks/remove-docker.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "# Best effort to remove Docker CE and related configuration\n\n- name: Stop Docker service\n  become: yes\n  service:\n    name: docker\n    state: stopped\n  ignore_errors: yes\n\n- name: Ensure Docker CE is removed (CentOS/Fedora/RedHat)\n  become: yes\n  package:\n    name: \"{{ item }}\"\n    state: absent\n  with_items: \"{{ docker_packages }}\"\n  register: _pkg_result\n  until: _pkg_result|succeeded\n  when: _docker_os_dist != \"Ubuntu\" and\n        _docker_os_dist != \"Debian\"\n\n- name: Ensure Docker CE is removed (Ubuntu/Debian)\n  become: yes\n  apt:\n    name: \"{{ item }}\"\n    state: absent\n    purge: yes\n  with_items: \"{{ docker_packages }}\"\n  register: _pkg_result\n  until: _pkg_result|succeeded\n  when: _docker_os_dist == \"Ubuntu\" or\n        _docker_os_dist == \"Debian\"\n\n- name: Remove network interface docker0\n  become: yes\n  command: ip link del docker0\n  args:\n    warn: no\n  ignore_errors: yes\n  changed_when: no\n  tags:\n    - skip_ansible_lint\n\n- name: Remove dockerd from alternatives configuration\n  become: yes\n  shell: alternatives --remove dockerd /usr/bin/dockerd-ce\n  ignore_errors: yes\n  changed_when: no\n  tags:\n    - skip_ansible_lint\n\n- name: Clean yum cache (CentOS/RedHat)\n  become: yes\n  command: yum clean all --enablerepo=\\*\n  args:\n    warn: no\n  changed_when: no\n  when: _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"RedHat\"\n\n- name: Clean dnf cache\n  become: yes\n  command: dnf clean all --enablerepo=\\*\n  args:\n    warn: no\n  changed_when: no\n  when: _docker_os_dist == \"Fedora\"\n\n- name: Clean apt cache\n  become: yes\n  command: apt-get clean\n  changed_when: no\n  when: _docker_os_dist == \"Ubuntu\" or\n        _docker_os_dist == \"Debian\"\n  tags:\n    - skip_ansible_lint\n\n- name: Remove repository docker specific repo file\n  become: yes\n  yum_repository:\n    name: docker-ce\n    file: docker-ce\n    state: absent\n  when: _docker_os_dist != \"Ubuntu\" and\n        _docker_os_dist != \"Debian\"\n\n- name: Ensure Docker CE and configuration files are removed\n  become: yes\n  file:\n    path: \"{{ item }}\"\n    state: absent\n  with_items:\n    # all distributions\n    - \"{{ docker_envs_dir[_docker_os_dist] }}/docker\"\n    - \"{{ docker_envs_dir[_docker_os_dist] }}/docker-envs\"\n    - /etc/audit/rules.d/docker.rules\n    # centos\n    - /etc/yum.repos.d/docker-ce.repo\n    - /etc/systemd/system/docker.service.d\n    - /etc/docker\n    - /usr/bin/dockerd\n    - /run/docker\n    # ubuntu/debian\n    - /etc/apt/sources.list.d/docker-ce.list\n\n- name: Ensure additional files and data directories are removed\n  become: yes\n  file:\n    path: \"{{ item }}\"\n    state: absent\n  with_items:\n    - /var/lib/docker\n    - /var/lib/docker-engine\n    - \"{{ docker_remove_additional }}\"\n  when: docker_remove_all | bool\n\n- name: Find Docker related diretories in package cache\n  become: yes\n  find:\n    paths: \"{{ _os_pkg_cache_dirs[_docker_os_dist] }}\"\n    file_type: directory\n    recurse: yes\n    patterns: \"docker-ce*\"\n  vars:\n    _os_pkg_cache_dirs:\n      RedHat: /var/cache/yum\n      CentOS: /var/cache/yum\n      Fedora: /var/cache/dnf\n  register: _remove_cache_dirs\n  when: _docker_os_dist == \"Fedora\" or\n        _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"RedHat\"\n\n- name: Remove dangeling files/directories in package cache\n  become: yes\n  file:\n    path: \"{{ item.path }}\"\n    state: absent\n  with_items: \"{{ _remove_cache_dirs.files }}\"\n  when: _docker_os_dist == \"Fedora\" or\n        _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"RedHat\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "00171b95063e622f201eddb340d6c58f57791ac1", "filename": "roles/config-software-src/tasks/prep.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - nfs-utils\n\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "d0cd3b6f1e8f0b37f0741c11faea7237dfd13cd9", "filename": "roles/openshift-applier/handlers/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: \"Clean-up temporary dir\"\n  file:\n    path: \"{{ item }}\"\n    state: absent\n  with_items:\n  - \"{{ tmp_inv_dir }}\"\n  - \"{{ tmp_dep_dir }}\"\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "f71f19ef60a61d3283eae1a556e6105ce6765649", "filename": "reference-architecture/vmware-ansible/playbooks/roles/nfs-server/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Install NFS server\n  package: name=nfs-utils use=yum state=present\n\n- name: configure NFS services\n  file: src=etc-sysconfig-nfs dest=/etc/sysconfig/nfs owner=root group=root mode=0644\n\n# figure if we need to reconfigure firewall if we run in a vApp\n# https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/nfs-serverconfig.html#s2-nfs-nfs-firewall-config\n\n- name: Start rpcbind\n  service: name=rpcbind state=started enabled=yes\n\n- name: Start nfs\n  service: name=nfs-server state=started enabled=yes\n\n- name: Stop firewalld\n  service: name=firewalld state=stopped enabled=no\n\n- name: Stop iptables\n  command: \"iptables -F\"\n  ignore_errors: true\n\n- name: Stop iptables\n  command: \"iptables-save\"\n  ignore_errors: true\n\n- name: Create openshift volume group\n  lvg: vg=openshift pvs=/dev/sdb\n\n- name: Create lvm volumes\n  lvol: vg=openshift lv=nfs size=95%FREE state=present shrink=no\n\n- name: Create local partition on lvm lv\n  filesystem:\n    fstype: xfs\n    dev: /dev/openshift/nfs\n\n- name: Make mounts owned by nfsnobody\n  file: path=/exports state=directory owner=nfsnobody group=nfsnobody mode=6775\n\n- name: Mount the partition\n  mount:\n    name: /exports\n    src: /dev/openshift/nfs\n    fstype: xfs\n    state: present\n\n- name: Remount new partition\n  command: \"mount -a\"\n\n- name: Make mounts owned by nfsnobody\n  file: path=/exports owner=nfsnobody group=nfsnobody mode=6775\n\n- name: Make directory and set ownership\n  file: path=/exports/registry state=directory owner=nfsnobody group=nfsnobody mode=0775\n\n- name: Make directory and set ownership\n  file: path=/exports/metrics state=directory owner=nfsnobody group=nfsnobody mode=0775\n\n- name: Export the directories\n  lineinfile:\n    dest: \"/etc/exports\"\n    line: \"/exports *(rw,sync,all_squash)\"\n  notify: restart nfs\n\n- name: Creates pv1 directory\n  file: path=\"/pv1\" state=directory owner=nfsnobody group=nfsnobody mode=0777\n\n- name: Export the directories\n  lineinfile:\n    dest: \"/etc/exports\"\n    line: \"/pv1 *(rw,sync,all_squash)\"\n  notify: restart nfs\n\n- name: Reload NFS exports\n  command: exportfs -ra\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "26c01c2e65b7ccbfde25a2004c811bc732b27bc8", "filename": "tasks/bug-tweaks/lvm-thinpool.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Ensure lvm2 is installed\n  become: true\n  package:\n    name: lvm2\n    state: present\n  register: _pkg_result\n  until: _pkg_result is succeeded\n\n- name: Create LVM volume group\n  become: true\n  lvg:\n    pvs: '{{ pool.physical_volumes }}'\n    state: present\n    vg: '{{ pool.volume_group }}'\n  when: pool.physical_volumes|default(None)\n\n- name: Check if data volume exists\n  become: true\n  stat:\n    path: '/dev/mapper/{{ pool.volume_group }}-{{ pool.name }}'\n  ignore_errors: true\n  register: _volume\n\n- name: Create data volume\n  become: true\n  lvol:\n    lv: '{{ pool.name }}'\n    size: '{{ pool.data_size }}'\n    vg: '{{ pool.volume_group }}'\n  register: _datavolume_created\n  when: not _volume.stat.exists\n\n- name: Create meta data volume\n  become: true\n  lvol:\n    lv: '{{ pool.name }}meta'\n    size: '{{ pool.metadata_size }}'\n    vg: '{{ pool.volume_group }}'\n  when: _datavolume_created is changed\n\n- name: Convert data volume to thinpool\n  become: true\n  shell:\n    lvconvert\n        -y\n        --zero n\n        -c 512K\n        --thinpool \"{{ pool.volume_group }}/{{ pool.name }}\"\n        --poolmetadata \"{{ pool.volume_group }}/{{ pool.name }}meta\"\n  when: _datavolume_created is changed\n  tags:\n    - skip_ansible_lint"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "a548c7c3d9c21e72aa27aba06c6fde57c9cd26bf", "filename": "roles/registry-scaleup/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Gather facts\n  openshift_facts:\n    role: common\n\n- name: use the default project\n  shell: \"{{ openshift.common.client_binary }} project default\"\n\n- name: Count the infrastructure nodes\n  shell: \"{{ openshift.common.client_binary }}  get nodes --show-labels | grep role=infra -c\"\n  register: nodes\n  when: node_type == \"infra\"\n\n- name: Scale the registry\n  shell: \"{{ openshift.common.client_binary }} scale dc/docker-registry --replicas={{ nodes.stdout }}\"\n  when: node_type == \"infra\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b2374341a75a92bf4c96fc1a7d8cf146a15a6216", "filename": "roles/git-server/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Install Packages\n  action: \"{{ ansible_pkg_mgr }} name={{ item }} state=present\"\n  with_items:\n    - git\n    - httpd\n    - firewalld\n    - libsemanage-python\n\n- name: Create Git User\n  user: name=\"{{ git_user }}\"\n\n- name: Create Git User Authorized Keys\n  authorized_key:\n    user: \"{{ git_user}}\"\n    key: \"{{ item }}\"\n  with_items:\n    - \"{{ git_user_authorized_keys | default('') }}\"\n\n- name: Configure Git HTTP Configuration\n  template:\n    src: \"{{ role_path }}/templates/git.conf.j2\"\n    dest: \"/etc/httpd/conf.d/git.conf\"\n    owner: root\n    group: root\n  notify: restart httpd\n\n- name: Enable Services\n  service: name={{ item }} enabled=yes state=started\n  with_items:\n    - firewalld\n    - httpd\n\n- name: HTTPD SELinux Configurations\n  seboolean:\n    name: httpd_can_network_connect\n    state: yes\n    persistent: yes\n\n- name: Open Firewall for HTTPD\n  firewalld: port=80/tcp permanent=yes state=enabled immediate=yes zone=public\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "9e7c034178179e4505c4b1b82ce9b5c5c9010e9e", "filename": "roles/manage-aws-infra/tasks/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- import_tasks: pre-reqs.yml\n\n- import_tasks: create-vpc.yml\n  when:\n    - aws_create_vpc\n    - operation == \"deploy\"\n\n- import_tasks: create-subnet.yml\n  when:\n    - operation == \"deploy\"\n\n- import_tasks: deploy-cluster.yml\n  when:\n    - operation == \"deploy\"\n\n- import_tasks: update_dns.yml\n  when:\n    - operation == \"deploy\"\n\n- import_tasks: start_stop_instances.yml\n  when: (operation == \"running\") or\n        (operation == \"stopped\")\n\n- import_tasks: remove_infra.yml\n  when:\n    - operation == \"absent\"\n\n- import_tasks: remove_vpc.yml\n  when:\n    - operation == \"absent\"\n    - delete_vpc\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "42ffcbda3b2251e5838e949dc8a66a3ef63170ee", "filename": "tasks/remove-docker.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "# Best effort to remove Docker CE and related configuration\n\n- name: Stop Docker service\n  become: yes\n  service:\n    name: docker\n    state: stopped\n  ignore_errors: yes\n\n- name: Ensure Docker CE is removed (CentOS/Fedora/RedHat)\n  become: yes\n  package:\n    name: \"{{ item }}\"\n    state: absent\n  loop: \"{{ docker_packages }}\"\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when: _docker_os_dist != \"Ubuntu\" and\n        _docker_os_dist != \"Debian\"\n\n- name: Ensure Docker CE is removed (Ubuntu/Debian)\n  become: yes\n  apt:\n    name: \"{{ item }}\"\n    state: absent\n    purge: yes\n  loop: \"{{ docker_packages }}\"\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when: _docker_os_dist == \"Ubuntu\" or\n        _docker_os_dist == \"Debian\"\n\n- name: Remove network interface docker0\n  become: yes\n  command: ip link del docker0\n  args:\n    warn: no\n  ignore_errors: yes\n  changed_when: no\n  tags:\n    - skip_ansible_lint\n\n- name: Remove dockerd from alternatives configuration\n  become: yes\n  shell: alternatives --remove dockerd /usr/bin/dockerd-ce\n  ignore_errors: yes\n  changed_when: no\n  tags:\n    - skip_ansible_lint\n\n- name: Clean yum cache (CentOS/RedHat)\n  become: yes\n  command: yum clean all --enablerepo=\\*\n  args:\n    warn: no\n  changed_when: no\n  when: _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"RedHat\"\n\n- name: Clean dnf cache\n  become: yes\n  command: dnf clean all --enablerepo=\\*\n  args:\n    warn: no\n  changed_when: no\n  when: _docker_os_dist == \"Fedora\"\n\n- name: Clean apt cache\n  become: yes\n  command: apt-get clean\n  changed_when: no\n  when: _docker_os_dist == \"Ubuntu\" or\n        _docker_os_dist == \"Debian\"\n  tags:\n    - skip_ansible_lint\n\n- name: Remove repository docker specific repo file\n  become: yes\n  yum_repository:\n    name: docker-ce\n    file: docker-ce\n    state: absent\n  when: _docker_os_dist != \"Ubuntu\" and\n        _docker_os_dist != \"Debian\"\n\n- name: Ensure Docker CE and configuration files are removed\n  become: yes\n  file:\n    path: \"{{ item }}\"\n    state: absent\n  loop:\n    # all distributions\n    - \"{{ docker_envs_dir[_docker_os_dist] }}/docker\"\n    - \"{{ docker_envs_dir[_docker_os_dist] }}/docker-envs\"\n    - /etc/audit/rules.d/docker.rules\n    # centos\n    - /etc/yum.repos.d/docker-ce.repo\n    - /etc/systemd/system/docker.service.d\n    - /etc/docker\n    - /usr/bin/dockerd\n    - /run/docker\n    # ubuntu/debian\n    - /etc/apt/sources.list.d/docker-ce.list\n\n- name: Ensure additional files and data directories are removed\n  become: yes\n  file:\n    path: \"{{ item }}\"\n    state: absent\n  loop:\n    - /var/lib/docker\n    - /var/lib/docker-engine\n    - \"{{ docker_remove_additional }}\"\n  when: docker_remove_all | bool\n\n- name: Find Docker related diretories in package cache\n  become: yes\n  find:\n    paths: \"{{ _os_pkg_cache_dirs[_docker_os_dist] }}\"\n    file_type: directory\n    recurse: yes\n    patterns: \"docker-ce*\"\n  vars:\n    _os_pkg_cache_dirs:\n      RedHat: /var/cache/yum\n      CentOS: /var/cache/yum\n      Fedora: /var/cache/dnf\n  register: _remove_cache_dirs\n  when: _docker_os_dist == \"Fedora\" or\n        _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"RedHat\"\n\n- name: Remove dangeling files/directories in package cache\n  become: yes\n  file:\n    path: \"{{ item.path }}\"\n    state: absent\n  loop: \"{{ _remove_cache_dirs.files }}\"\n  when: _docker_os_dist == \"Fedora\" or\n        _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"RedHat\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6d2af3d26794b4bece08513b18c52163e2f4fc64", "filename": "playbooks/openstack/openshift-cluster/update.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: no\n  tasks:\n  - include_vars: vars.yml\n  - include_vars: cluster_hosts.yml\n  - add_host:\n      name: \"{{ item }}\"\n      groups: l_oo_all_hosts\n    with_items: \"{{ g_all_hosts }}\"\n\n- hosts: l_oo_all_hosts\n  gather_facts: no\n  tasks:\n  - include_vars: vars.yml\n  - include_vars: cluster_hosts.yml\n\n- name: Populate oo_hosts_to_update group\n  hosts: localhost\n  connection: local\n  become: no\n  gather_facts: no\n  tasks:\n  - name: Evaluate oo_hosts_to_update\n    add_host:\n      name: \"{{ item }}\"\n      groups: oo_hosts_to_update\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    with_items: \"{{ g_all_hosts | default([]) }}\"\n\n- include: ../../common/openshift-cluster/update_repos_and_packages.yml\n\n- include: config.yml\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "29ee3f55b81ecfd97e52956566f6b3bd02d9f33d", "filename": "roles/common/tasks/facts.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- block:\n  - name: Generate password for the CA key\n    local_action:\n      module: shell\n        openssl rand -hex 16\n    register: CA_password\n\n  - name: Generate p12 export password\n    local_action:\n      module: shell\n        openssl rand 8 | python -c 'import sys,string; chars=string.ascii_letters + string.digits + \"_@\"; print \"\".join([chars[ord(c) % 64] for c in list(sys.stdin.read())])'\n    register: p12_password_generated\n    when: p12_password is not defined\n    tags: update-users\n  become: false\n\n- name: Define facts\n  set_fact:\n    p12_export_password: \"{{ p12_password|default(p12_password_generated.stdout) }}\"\n  tags: update-users\n\n- set_fact:\n    CA_password: \"{{ CA_password.stdout }}\"\n    IP_subject_alt_name: \"{{ IP_subject_alt_name }}\"\n\n- name: Set IPv6 support as a fact\n  set_fact:\n    ipv6_support: \"{% if ansible_default_ipv6['gateway'] is defined %}true{% else %}false{% endif %}\"\n  tags: always\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "756fd7ba7de61bbabc501dbbc0d8a9b98c00e396", "filename": "handlers/main.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- name: systemd-reload\n  systemd:\n    daemon-reload: yes\n    name: nexus.service\n    no_block: yes\n\n- name: nexus-service-restart\n  systemd:\n    name: nexus.service\n    state: restarted\n    no_block: yes\n\n- name: nexus-service-stop\n  systemd:\n    name: nexus.service\n    state: stopped\n    no_block: yes\n  when: nexus_systemd_service_file.stat.exists\n\n- name: wait-for-nexus\n  wait_for:\n    path: \"{{ nexus_data_dir }}/log/nexus.log\"\n    search_regex: \"Started Sonatype Nexus OSS .*\"\n    timeout: 1800\n\n- name: wait-for-nexus-port\n  wait_for:\n    port: \"{{ nexus_default_port }}\"\n    delay: 5\n\n- name: httpd-service-reload\n  systemd:\n    name: \"{{ httpd_package_name }}.service\"\n    state: reloaded\n    enabled: yes\n    no_block: yes\n\n- name: wait-for-httpd\n  wait_for:\n    port: 443\n    delay: 5\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "af7da385027297845baf1972193e98594cb2bede", "filename": "ops/playbooks/includes/get_ucp_version.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n\n    - name: Retrieve a token for the UCP API\n      uri:\n        url: \"https://{{ ucp_instance }}.{{ domain_name }}/auth/login\"\n        headers:\n          Content-Type: application/json\n        method: POST\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        body: '{\"username\":\"{{ ucp_username }}\",\"password\":\"{{ ucp_password }}\"}'\n      delegate_to: localhost\n      register: resp\n      until: resp.status == 200\n      retries: 20\n      delay: 5\n\n    - name: Remember the API's token\n      set_fact:\n        auth_token:  \"{{resp.json.auth_token}}\"\n\n    - name: Ping UCP\n      uri:\n        url: 'https://{{ ucp_instance }}.{{ domain_name }}/_ping'\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        validate_certs: no\n      delegate_to: localhost\n      register: resp\n\n    - set_fact:\n        detected_ucp_version=\"{{ resp.ucp_version.split (' ') | first}}\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "392dd0bd07f93ef98c671be420b72c59de6c25db", "filename": "ops/files/splunk/linux/SPLUNK_HOME/etc/apps/search/metadata/local.meta", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "[inputs/tcp%3A%2F%2F1514]\nowner = admin\nversion = 7.0.0\nmodtime = 1521819553.018276000\n\n[inputs/monitor%3A%2F%2F%2Fvar%2Flog%2Fsecure]\nowner = admin\nversion = 7.0.0\nmodtime = 1521819553.313503000\n\n[inputs/monitor%3A%2F%2F%2Fvar%2Flog%2Fmessages]\nowner = admin\nversion = 7.0.0\nmodtime = 1521819553.609134000\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "44b4b24806d3d5bdd231430386c499e9f3bc5330", "filename": "playbooks/roles/suricata/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# tasks file for suricata\n- import_tasks: \"{{ method }}.yml\"\n...\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "33c40088f1cf7b8fcdfa92e4fc1264a28bfdf577", "filename": "tasks/call_script.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- block:\n    - name: Calling Groovy script {{ script_name }}\n      uri:\n        url: \"{{ nexus_api_scheme }}://{{ nexus_api_hostname }}:{{ nexus_api_port }}\\\n          {{ nexus_api_context_path }}{{ nexus_rest_api_endpoint }}/{{ script_name }}/run\"\n        user: 'admin'\n        password: \"{{ current_nexus_admin_password }}\"\n        headers:\n          Content-Type: \"text/plain\"\n        method: POST\n        force_basic_auth: yes\n        validate_certs: \"{{ nexus_api_validate_certs }}\"\n        body: \"{{ args | to_json }}\"\n      register: script_run\n      failed_when: script_run | nexus_groovy_error | bool\n      changed_when: script_run | nexus_groovy_changed | bool\n\n    - name: Details about runned script if verbose mode is on\n      debug:\n        msg: \"{{ script_run | nexus_groovy_details }}\"\n        verbosity: 1\n      when: not ansible_check_mode\n\n  rescue:\n\n    - when: script_run | nexus_groovy_details == 'Global script failure'\n      block:\n\n        - name: Debug script result for global fail\n          debug:\n            var: script_run\n\n        - name: Global script failure at nexus level\n          fail:\n            msg: >-\n              Running the script {{ script_name }} failed at nexus level.\n              See the above debug output\n\n\n    - name: Debug script result for failed script actions\n      debug:\n        msg: \"{{ script_run | nexus_groovy_details }}\"\n\n    - name: Script action failure\n      fail:\n        msg: >-\n          The script {{ script_name }} returned at least one of its\n          actions has failed. See the degug message above for details\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "2e8ee3c777c72b5cfd6d861522f26cac83cdc118", "filename": "roles/config-httpd/tasks/seed.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Seed web server with content'\n  copy:\n    src: \"{{ httpd_seed_dir }}\"\n    dest: \"{{ html_document_root | default(default_document_root) }}\"\n  when:\n  - httpd_seed_dir is defined\n  - httpd_seed_dir|trim != \"\"\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "98d971b0a71421e5bef01e126ac9eedacf290da2", "filename": "roles/dns/test/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: localhost\n  roles:\n  - role: dns\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "b054cc9ec1c5025df800bda6ed3bb66b0cab94f8", "filename": "roles/cloud-gce/tasks/prompts.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- pause:\n    prompt: |\n      Enter the local path to your credentials JSON file\n      (https://support.google.com/cloud/answer/6158849?hl=en&ref_topic=6262490#serviceaccounts)\n  register: _gce_credentials_file\n  when:\n    - gce_credentials_file is undefined\n    - lookup('env','GCE_CREDENTIALS_FILE_PATH')|length <= 0\n\n- set_fact:\n    credentials_file_path: \"{{ gce_credentials_file | default(_gce_credentials_file.user_input|default(None)) | default(lookup('env','GCE_CREDENTIALS_FILE_PATH'), true) }}\"\n    ssh_public_key_lookup: \"{{ lookup('file', '{{ SSH_keys.public }}') }}\"\n\n- set_fact:\n    credentials_file_lookup: \"{{ lookup('file', '{{ credentials_file_path }}') }}\"\n\n- set_fact:\n    service_account_email: \"{{ credentials_file_lookup.client_email | default(lookup('env','GCE_EMAIL')) }}\"\n    project_id: \"{{ credentials_file_lookup.project_id | default(lookup('env','GCE_PROJECT')) }}\"\n\n- block:\n  - name: Get regions\n    gce_region_facts:\n      service_account_email: \"{{ credentials_file_lookup.client_email }}\"\n      credentials_file: \"{{ credentials_file_path  }}\"\n      project_id: \"{{ credentials_file_lookup.project_id }}\"\n    register: _gce_regions\n\n  - name: Set facts about the regions\n    set_fact:\n      gce_regions: >-\n        [{%- for region in _gce_regions.results.regions | sort(attribute='name') -%}\n          {% if region.status == \"UP\" %}\n            {% for zone in region.zones | sort(attribute='name') %}\n              {% if zone.status == \"UP\" %}\n                '{{ zone.name }}'\n              {% endif %}{% if not loop.last %},{% endif %}\n            {% endfor %}\n          {% endif %}{% if not loop.last %},{% endif %}\n        {%- endfor -%}]\n\n  - name: Set facts about the default region\n    set_fact:\n      default_region: >-\n        {% for region in gce_regions %}\n          {%- if region == \"us-east1-b\" %}{{ loop.index }}{% endif %}\n        {%- endfor %}\n\n  - pause:\n      prompt: |\n        What region should the server be located in?\n        (https://cloud.google.com/compute/docs/regions-zones/)\n          {% for r in gce_regions %}\n          {{ loop.index }}. {{ r }}\n          {% endfor %}\n\n        Enter the number of your desired region\n        [{{ default_region }}]\n    register: _gce_region\n  when: region is undefined\n\n- set_fact:\n    algo_region: >-\n      {% if region is defined %}{{ region }}\n      {%- elif _gce_region.user_input is defined and _gce_region.user_input != \"\" %}{{ gce_regions[_gce_region.user_input | int -1 ] }}\n      {%- else %}{{ gce_regions[default_region | int - 1] }}{% endif %}\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "98539eeab07c2ef468fde0bcb57bfc0bf0fff4c2", "filename": "roles/ansible/tower/manage-credential-types/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: tower\n  roles:\n  - role: ansible/tower/manage-credential-types"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "200efb1bda47f12e2c76849811a531abb0bab9f9", "filename": "playbooks/roles/sensor-common/tasks/install.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: Install packages\n  yum:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - python-pyOpenSSL\n    - firewalld\n    - chrony\n    - libselinux-python\n...\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "40424b3b006e8b04b0d7922ea776cdf83483515c", "filename": "reference-architecture/vmware-ansible/playbooks/roles/keepalived_haproxy/templates/firewall.sh.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#!/bin/bash\n{% for host in groups['haproxy_group'] %}\niptables -A INPUT -s {{ hostvars[host].ansible_default_ipv4 }} -j ACCEPT\n{% endfor %}\n\niptables-save > /etc/sysconfig/iptables\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "1f15aa4bf0d94065eefe3a81d344d8ee0cb51d3f", "filename": "playbooks/aws/openshift-cluster/terminate.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Terminate instance(s)\n  hosts: localhost\n  connection: local\n  become: no\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  tasks:\n  - add_host:\n      name: \"{{ item }}\"\n      groups: oo_hosts_to_terminate\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    with_items: \"{{ (groups['tag_clusterid_' ~ cluster_id] | default([])) | difference(['localhost']) }}\"\n\n- name: Unsubscribe VMs\n  hosts: oo_hosts_to_terminate\n  roles:\n  - role: rhel_unsubscribe\n    when: deployment_type in ['atomic-enterprise', 'enterprise', 'openshift-enterprise'] and\n          ansible_distribution == \"RedHat\" and\n          lookup('oo_option', 'rhel_skip_subscription') | default(rhsub_skip, True) |\n            default('no', True) | lower in ['no', 'false']\n\n- name: Terminate instances\n  hosts: localhost\n  connection: local\n  become: no\n  gather_facts: no\n  tasks:\n  - name: Remove tags from instances\n    ec2_tag:\n      resource: \"{{ hostvars[item]['ec2_id'] }}\"\n      region: \"{{ hostvars[item]['ec2_region'] }}\"\n      state: absent\n      tags:\n        environment: \"{{ hostvars[item]['ec2_tag_environment'] }}\"\n        clusterid: \"{{ hostvars[item]['ec2_tag_clusterid'] }}\"\n        host-type: \"{{ hostvars[item]['ec2_tag_host-type'] }}\"\n        sub_host_type: \"{{ hostvars[item]['ec2_tag_sub-host-type'] }}\"\n    with_items: \"{{ groups.oo_hosts_to_terminate }}\"\n    when: \"'oo_hosts_to_terminate' in groups\"\n\n  - name: Terminate instances\n    ec2:\n      state: absent\n      instance_ids: [\"{{ hostvars[item].ec2_id }}\"]\n      region: \"{{ hostvars[item].ec2_region }}\"\n    ignore_errors: yes\n    register: ec2_term\n    with_items: \"{{ groups.oo_hosts_to_terminate }}\"\n    when: \"'oo_hosts_to_terminate' in groups\"\n\n  # Fail if any of the instances failed to terminate with an error other\n  # than 403 Forbidden\n  - fail:\n      msg: \"Terminating instance {{ item.ec2_id }} failed with message {{ item.msg }}\"\n    when: \"'oo_hosts_to_terminate' in groups and item.has_key('failed') and item.failed\"\n    with_items: \"{{ ec2_term.results }}\"\n\n  - name: Stop instance if termination failed\n    ec2:\n      state: stopped\n      instance_ids: [\"{{ item.item.ec2_id }}\"]\n      region: \"{{ item.item.ec2_region }}\"\n    register: ec2_stop\n    when: \"'oo_hosts_to_terminate' in groups and item.has_key('failed') and item.failed\"\n    with_items: \"{{ ec2_term.results }}\"\n\n  - name: Rename stopped instances\n    ec2_tag: resource={{ item.item.item.ec2_id }} region={{ item.item.item.ec2_region }} state=present\n    args:\n      tags:\n        Name: \"{{ item.item.item.ec2_tag_Name }}-terminate\"\n    with_items: \"{{ ec2_stop.results }}\"\n    when: ec2_stop | changed\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "1b0d7f0bfcf898ada178e79cdf9a8d9c26b32df4", "filename": "roles/sugarizer/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "sugarizer_install: True\nsugarizer_enabled: False\n\nsugarizer_location: \"{{ iiab_base }}\"    # /opt/iiab\n\nsugarizer_version: sugarizer-1.0\nsugarizer_git_version: v1.0.1\n# PLEASE HELP MONITOR https://github.com/llaske/sugarizer/releases\n\nsugarizer_server_version: sugarizer-server-1.0\nsugarizer_server_git_version: v1.0.1    # \"master\" worked over July 11-14, 2018\n# PLEASE HELP MONITOR https://github.com/llaske/sugarizer-server/releases\n\n# Unused as of 2018-07-14\n# node_modules_exists: False\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "efbf9ee25b7b8e775bedf9c1dba06bdc53d46b36", "filename": "roles/docket/tasks/install.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: Configure RockNSM online repos\n  yum_repository:\n    file: rocknsm\n    name: \"{{ item.name }}\"\n    enabled: \"{{ rock_online_install }}\"\n    description: \"{{ item.name }}\"\n    baseurl: \"{{ item.baseurl }}\"\n    repo_gpgcheck: 1\n    gpgcheck: \"{{ item.gpgcheck }}\"\n    gpgkey:\n      - file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-pkgcloud-2_4\n      - file:///etc/pki/rpm-gpg/RPM-GPG-KEY-RockNSM-2\n    sslverify: 1\n    sslcacert: /etc/pki/tls/certs/ca-bundle.crt\n    metadata_expire: 300\n    cost: 750\n    state: present\n  loop:\n    - { name: \"rocknsm_2_4\", gpgcheck: true, baseurl: \"{{ rocknsm_baseurl }}\" }\n    - { name: \"rocknsm_2_4-source\", gpgcheck: false, baseurl: \"{{ rocknsm_srpm_baseurl }}\" }\n  when: docket_install == 'yumrepo'\n\n- name: Install packages\n  yum:\n    name:\n      - docket\n      - lighttpd\n    state: present\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "8b2f6721b3113b665688926630d91da3d0f20e8e", "filename": "tasks/Win32NT/fetch/fetch_checksum.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: 'Download artifact from {{ artifact_url }}'\n  win_get_url:\n    url: '{{ artifact_url }}'\n    dest: '{{ java_download_path }}\\{{ artifact_basename }}'\n    force: true\n\n- name: 'Get {{ checksum_alg }} checksum of file'\n  win_stat:\n    path: '{{ java_download_path }}\\{{ artifact_basename }}'\n    get_checksum: true\n    checksum_algorithm: '{{ checksum_alg }}'\n  register: artifact\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "a69c4b3113cd41d33f4a554fbec3c0c78d13a8b9", "filename": "tasks/section_06_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 6.1 Ensure the X Window system is not installed (Scored)\n    apt: name=xserver-xorg-core* purge=yes state=absent\n    when: remove_xserver == True\n    tags:\n      - section6\n      - section6.1\n\n  - name: 6.2 Ensure Avahi Server is not enabled (check) (Scored)\n    stat: path='/etc/init/avahi-daemon.conf'\n    register: avahi_stat\n    tags:\n      - section6\n      - section6.2\n\n  - name: 6.2 Ensure Avahi Server is not enabled (Scored)\n    lineinfile: >\n        line='#start on (filesystem'\n        state=present\n        regexp='start on \\(filesystem'\n        dest=/etc/init/avahi-daemon.conf\n    when: avahi_stat.stat.exists == True\n    tags:\n      - section6\n      - section6.2\n\n  - name: 6.3 Ensure print server is not enabled (check) (Not Scored)\n    stat: path='/etc/init/cups.conf'\n    register: cups_stat\n    tags:\n      - section6\n      - section6.3\n\n  - name: 6.3 Ensure print server is not enabled (Not Scored)\n    lineinfile: >\n        line='#start on (filesystem'\n        state=present\n        regexp='start on \\(filesystem'\n        dest=/etc/init/cups.conf\n    when: cups_stat.stat.exists == True\n    tags:\n      - section6\n      - section6.3\n\n  - name: 6.4.1 Ensure DHCP Server is not enabled (check) (Scored)\n    stat: path='/etc/init/isc-dhcp-server.conf'\n    register: dhcp_stat\n    tags:\n      - section6\n      - section6.4\n      - section6.4.1\n\n  - name: 6.4.1 Ensure DHCP Server is not enabled (Scored)\n    lineinfile: >\n        line='#start on runlevel [2345]'\n        state=present\n        regexp='start on runlevel'\n        dest=/etc/init/isc-dhcp-server.conf\n    when: dhcp_stat.stat.exists == True\n    tags:\n      - section6\n      - section6.4\n      - section6.4.1\n\n  - name: 6.4.2 Ensure DHCP Server is not enabled (check) (Scored)\n    stat: path='/etc/init/isc-dhcp-server6.conf'\n    register: dhcp6_stat\n    tags:\n      - section6\n      - section6.4\n      - section6.4.2\n\n  - name: 6.4.2 Ensure DHCP Server is not enabled (Scored)\n    lineinfile: >\n        line='#start on runlevel [2345]'\n        state=present\n        regexp='start on runlevel'\n        dest=/etc/init/isc-dhcp-server6.conf\n    when: dhcp6_stat.stat.exists == True\n    tags:\n      - section6\n      - section6.4\n      - section6.4.2\n\n  - name: 6.5.1 Configure Network Time Protocol (install) (NTP) (Scored)\n    apt: name=ntp state=present\n    tags:\n      - section6\n      - section6.5\n      - section6.5.1\n\n  - name: 6.5.2 Configure Network Time Protocol (restrict4) (NTP) (Scored)\n    lineinfile: >\n        dest='/etc/ntp.conf'\n        line='restrict -4 default kod nomodify notrap nopeer noquery'\n        regexp='^restrict -4 default'\n        state=present\n    tags:\n      - section6\n      - section6.5\n      - section6.5.2\n\n  - name: 6.5.3 Configure Network Time Protocol (restrict6) (NTP) (Scored)\n    lineinfile: >\n        dest='/etc/ntp.conf'\n        line='restrict -6 default kod nomodify notrap nopeer noquery'\n        regexp='^restrict -6 default'\n        state=present\n    tags:\n      - section6\n      - section6.5\n      - section6.5.3\n\n  - name: 6.5.4 Configure Network Time Protocol (server check) (NTP) (Scored)\n    command: 'grep \"^server\" /etc/ntp.conf'\n    register: ntp_server_rc\n    changed_when: False\n    always_run: True\n    tags:\n      - section6\n      - section6.5\n      - section6.5.4\n\n  - name: 6.5.4 Configure Network Time Protocol (server add) (NTP) (Scored)\n    lineinfile: >\n        dest='/etc/ntp.conf'\n        line='server 0.fr.pool.ntp.org'\n        state=present\n    when: ntp_server_rc.rc == 1\n    tags:\n      - section6\n      - section6.5\n      - section6.5.4\n\n  - name: 6.6 Ensure LDAP is not enabled (Not Scored)\n    apt: name=slapd purge=yes state=absent\n    tags:\n      - section6\n      - section6.6\n\n  - name: 6.7.1 Ensure NFS and RPC are not enabled (stat) (Not Scored)\n    stat: path=/etc/init/rpcbind-boot.conf\n    register: nfs_rpc_rc\n    tags:\n      - section6\n      - section6.7\n      - section6.7.1\n\n  - name: 6.7.2 Ensure NFS and RPC are not enabled (Not Scored)\n    lineinfile: >\n        dest=/etc/init/rpcbind-boot.conf\n        line='#start on virtual-filesystems and net-device-up IFACE=lo'\n        state=present\n        regexp='start on virtual-filesystems and net'\n    when: nfs_rpc_rc.stat.exists == True\n    tags:\n      - section6\n      - section6.7\n      - section6.7.2\n\n  - name: 6.7.2 Ensure NFS and RPC are not enabled (check) (Not Scored)\n    command: dpkg -S nfs-kernel-server\n    changed_when: False\n    failed_when: False\n    register: nfs_present\n    tags:\n      - section6\n      - section6.7\n      - section6.7.2\n\n  - name: 6.7.2 Ensure NFS and RPC are not enabled (rc.d) (Not Scored)\n    service: >\n        name=nfs-kernel-server\n        enabled=no\n    when: nfs_present is defined and nfs_present.rc == 0\n    register: nfs_service_result\n    failed_when: \"nfs_service_result|failed and 'service not found' not in nfs_service_result.msg\"\n    tags:\n      - section6\n      - section6.7\n      - section6.7.2\n\n  - name: 6.8-14 Ensure DNS,FTP,HTTP,IMAP,POP,Samba,Proxy,SNMP Servers are not enabled (Not Scored)\n    service: >\n        name={{ item }}\n        enabled=no\n    with_items:\n      - bind9\n      - vsftpd\n      - apache2\n      - dovecot\n      - smbd\n      - squid3\n      - snmpd\n    failed_when: False\n    tags:\n      - section6\n      - section6.8\n      - section6.9\n      - section6.10\n      - section6.11\n      - section6.12\n      - section6.13\n      - section6.14\n\n  - name: 6.15 Configure Mail Transfer Agent for Local-Only Mode (stat) (Scored)\n    stat: path=/etc/postfix/main.cf\n    register: postfix_main_cf\n    tags:\n      - section6\n      - section6.15\n\n  - name: 6.15 Configure Mail Transfer Agent for Local-Only Mode (Scored)\n    lineinfile: >\n        dest=/etc/postfix/main.cf\n        regexp='^inet_interfaces ='\n        line='inet_interfaces = localhost'\n        state=present\n    when: postfix_main_cf.stat.exists == True\n    tags:\n      - section6\n      - section6.15\n\n  - name: 6.16 Ensure rsync service is not enabled (stat) (Scored)\n    stat: path=/etc/default/rsync\n    register: default_rsync\n    tags:\n      - section6\n      - section6.16\n\n  - name: 6.16 Ensure rsync service is not enabled (Scored)\n    lineinfile: >\n        dest='/etc/default/rsync'\n        regexp='^RSYNC_ENABLE'\n        line='RSYNC_ENABLE=false'\n    when: default_rsync.stat.exists == True\n    tags:\n      - section6\n      - section6.16\n\n  - name: 6.17 Ensure Biosdevname is not enabled (Scored)\n    apt: name=biosdevname purge=yes state=absent\n    tags:\n      - section6\n      - section6.17\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "fb933cfbabc249897f35517d95ad1528b48a5170", "filename": "roles/kubevirt/tasks/dev.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "- name: Download KubeVirt Template\n  get_url:\n    url: \"{{ kubevirt_manifest_url }}/{{ manifest_version }}/{{ item }}.yaml.in\"\n    dest: \"{{ kubevirt_template_dir }}/{{ item }}.yml\"\n  with_items:\n    - \"{{ dev_template_resources }}\"\n\n- name: Render KubeVirt Yml\n  template:\n    src: \"{{ kubevirt_template_dir }}/{{ item }}.yml\"\n    dest: \"/tmp/{{ item }}.yml\"\n  with_items:\n    - \"{{ dev_template_resources }}\"\n\n- name: Remove downloaded templates\n  file:\n    path: \"{{ kubevirt_template_dir }}/{{ item }}.yml\"\n    state: absent\n  with_items:\n    - \"{{ dev_template_resources }}\"\n  when: byo_template.stat.exists == False\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "233d96586ca783942b6b08369b69cb73c900dc2e", "filename": "roles/dns/manage-dns-zones/tasks/named/process-zones.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Ensure the view directory exists\n  file:\n    path: \"{{ dns_zone_temp_config_dir }}/{{ view.name }}\"\n    state: directory\n\n- name: Set server config facts\n  set_fact:\n    view_recursion: \"{{ view.named.recursion }}\"\n  when:\n    - view.named is defined\n    - view.named.recursion is defined\n\n- name: Prepare the view pre-zones content\n  vars:\n    view_name: \"{{ view.name }}\"\n    view_recursion: \"{{ view_recursion | default(dns_data.named_global_config.recursion) }}\"\n  template:\n    src: named/view-config-1.j2\n    dest: \"{{ dns_zone_temp_config_dir }}/{{ view.name }}/0001-{{ view.name }}.cfg\"\n    owner: named\n    group: named\n    mode: 0660\n  when:\n    - view.state|default('present') == 'present'\n\n- name: Initialize flags\n  set_fact:\n    processed_zones: False\n\n- include_tasks: process-one-zone.yml\n  with_items:\n    -  \"{{ view.zones }}\"\n  loop_control:\n    loop_var: \"zone\"\n\n- name: Prepare the view post-zones content\n  vars:\n    view_forwarders: \"{{ view.default_forwarders | default(['127.0.0.1']) }}\"\n  template:\n    src: named/view-config-2.j2\n    dest: \"{{ dns_zone_temp_config_dir }}/{{ view.name }}/0003-{{ view.name }}.cfg\"\n    owner: named\n    group: named\n    mode: 0660\n  when:\n    - processed_zones|bool == True\n    - view.state|default('present') == 'present'\n\n- name: Assemble the complete view file\n  assemble:\n    src: \"{{ dns_zone_temp_config_dir }}/{{ view.name }}\"\n    dest: \"{{ dns_zone_temp_config_dir }}/view/{{ view.name }}.cfg\"\n  when:\n    - processed_zones|bool == True\n    - view.state|default('present') == 'present'\n\n- name: Remove temporary view if no zones were processed\n  file:\n    path: \"{{ dns_zone_temp_config_dir }}/{{ view.name }}\"\n    state: absent\n  when:\n    - processed_zones|bool == False\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "ea1a57ccbe2fb91afe6fcc476b3f59c4eaa31657", "filename": "ops/playbooks/docker_post_config.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- hosts: docker\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  vars:\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Create Docker service directory\n      file:\n        path: /etc/systemd/system/docker.service.d\n        state: directory\n\n    - name: Add proxy details\n      template: src=../templates/http-proxy.conf.j2 dest=/etc/systemd/system/docker.service.d/http-proxy.conf\n      when: env.http_proxy is defined or env.https_proxy is defined\n      notify: Restart Docker\n\n    - meta: flush_handlers\n\n    - name: Check if vsphere plugin is installed\n      shell: docker plugin ls | grep vsphere | wc -l\n      register: vsphere_installed\n\n    - name: Install vsphere plugin\n      command: docker plugin install --grant-all-permissions --alias vsphere vmware/vsphere-storage-for-docker:{{ vsphere_plugin_version }}\n      when: vsphere_installed.stdout == \"0\"\n\n  handlers:\n    - name: Restart Docker\n      systemd:\n        name: docker\n        state: restarted\n        daemon_reload: yes\n\n#####################################################################\n#\n# Play 2: Install vSPhere plugin on windows workers\n#\n#####################################################################\n- hosts: win_worker\n  gather_facts: false\n  connection: local\n  user: remote\n  become: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n#  environment: \"{{ env }}\"\n\n  tasks:\n\n  - name: Download vSphere Docker Volume Service Installer\n    win_get_url:\n      url: \"{{ windows_vdvs_ps }}\"\n      proxy_url: \"{{ proxy_url }}\"\n      dest: \"{{ windows_vdvs_directory }}install-vdvs.ps1\"\n    vars:\n      proxy_url: \"{% if env.http_proxy is defined %}http://{{ env.http_proxy }}{% endif %}\"\n\n  - name: Install Windows vSphere Docker Volume Service\n    win_shell: |\n      [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12\n      .\\install-vdvs.ps1 {{ windows_vdvs_path }}_{{ windows_vdvs_version }}.zip\n    args:\n      chdir: \"{{ windows_vdvs_directory }}\"\n\n  - name: Configure Windows vSphere Docker Volume Service\n    win_service:\n      name: vdvs\n      start_mode: auto\n      state: started\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "948357b992755347d93ab323da0b5846d8883471", "filename": "roles/stenographer/tasks/deploy.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- import_tasks: install.yml\n- import_tasks: config.yml\n...\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4fca5d1c34df0c8826de4e0cdc4d150b0c09f4e5", "filename": "roles/dns/manage-dns-records/tasks/nsupdate/process-records.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Manage DNS records for view: {{ dns.0.name }}, zone: {{ dns.1.dns_domain }}, server: {{ nsupdate.server }}\"\n  nsupdate:\n    server: \"{{ nsupdate.server }}\"\n    key_name: \"{{ nsupdate.key_name }}\"\n    key_secret: \"{{ nsupdate.key_secret }}\"\n    key_algorithm: \"{{ nsupdate.key_algorithm }}\"\n    zone: \"{{ dns.1.dns_domain }}\"\n    record: \"{{ item.record }}\"\n    value: \"{{ item.value | default(omit) }}\"\n    type: \"{{ item.type }}\"\n    ttl: \"{{ item.ttl | default(omit) }}\"\n    state: \"{{ item.state | default(present) }}\"\n  with_items:\n    - \"{{ dns.1.entries }}\"\n  when:\n    - dns.1.entries is defined\n  register: nsupdate_result\n  until: nsupdate_result | succeeded\n  retries: 10\n  delay: 1\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "16504370d7d8c7d4e0b1eec598cac382385275ca", "filename": "roles/openshift-applier/defaults/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\ntmp_inv_dir: ''\n\nfilter_tags: ''\n\ndefault_file_action: apply\ndefault_template_action: apply\n\nprovision: True\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "da6bbc09a633c4ef01809f988abefea2322b7278", "filename": "roles/notifications/md-to-html/tests/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nmarkdown_content: \"Hello, this is **bold** text\"\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "38adc7416ec92ac38216e58d16bf36bd09031f3c", "filename": "roles/cloud-azure/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- block:\n    - name: Build python virtual environment\n      import_tasks: venv.yml\n\n    - block:\n      - name: Include prompts\n        import_tasks: prompts.yml\n\n      - set_fact:\n          algo_region: >-\n            {% if region is defined %}{{ region }}\n            {%- elif _algo_region.user_input is defined and _algo_region.user_input != \"\" %}{{ azure_regions[_algo_region.user_input | int -1 ]['name'] }}\n            {%- else %}{{ azure_regions[default_region | int - 1]['name'] }}{% endif %}\n\n      - name: Create AlgoVPN Server\n        azure_rm_deployment:\n          state: present\n          deployment_name: \"AlgoVPN-{{ algo_server_name }}\"\n          template: \"{{ lookup('file', 'deployment.json') }}\"\n          secret: \"{{ secret }}\"\n          tenant: \"{{ tenant }}\"\n          client_id: \"{{ client_id }}\"\n          subscription_id: \"{{ subscription_id }}\"\n          resource_group_name: \"AlgoVPN-{{ algo_server_name }}\"\n          parameters:\n            AlgoServerName:\n              value: \"{{ algo_server_name }}\"\n            sshKeyData:\n              value: \"{{ lookup('file', '{{ SSH_keys.public }}') }}\"\n            location:\n              value: \"{{ algo_region }}\"\n            WireGuardPort:\n              value: \"{{ wireguard_port }}\"\n            vmSize:\n              value: \"{{ cloud_providers.azure.size }}\"\n            imageReferenceSku:\n              value: \"{{ cloud_providers.azure.image }}\"\n        register: azure_rm_deployment\n\n      - set_fact:\n          cloud_instance_ip: \"{{ azure_rm_deployment.deployment.outputs.publicIPAddresses.value }}\"\n          ansible_ssh_user: ubuntu\n      environment:\n        PYTHONPATH: \"{{ azure_venv }}/lib/python2.7/site-packages/\"\n  rescue:\n    - debug: var=fail_hint\n      tags: always\n    - fail:\n      tags: always\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "353eb23a1e76a15be8a342d5ee55af53b6dfcd08", "filename": "playbooks/openshift/pre-install.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n###############################\n# OpenShift Pre-Requisites\n\n# Ensure the 'docker_storage_block_device' is correctly populated for all nodes\n- hosts: nodes\n  tasks:\n    - set_fact:\n        docker_storage_block_device: \"{{ hostvars['localhost'].docker_storage_block_device }}\"\n\n# - subscribe hosts\n# - prepare docker\n# - other prep (install additional packages, etc.)\n#\n- hosts: OSEv3\n  roles:\n    - { role: openshift-ansible-contrib/roles/subscription-manager, when: hostvars.localhost.rhsm_register, tags: 'subscription-manager', ansible_sudo: true }\n    - { role: openshift-ansible-contrib/roles/docker, tags: 'docker' }\n    - { role: openshift-ansible-contrib/roles/openshift-prep, tags: 'openshift-prep' }\n\n# Ensure the CNS / glusterFS facts are correctly populated for all CNS nodes\n- hosts: glusterfs\n  tasks:\n    - set_fact:\n       glusterfs_devices:\n         - \"{{ hostvars['localhost'].cns_node_glusterfs_volume }}\"\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "79fc090508b373cbb94d2e3211f2f5d3c07aec89", "filename": "playbooks/provisioning/openstack/scale-up.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# Get the needed information about the current deployment\n- hosts: masters[0]\n  tasks:\n  - name: Get number of app nodes\n    shell: oc get nodes -l autoscaling=app --no-headers=true | wc -l\n    register: oc_old_num_nodes\n  - name: Get names of app nodes\n    shell: oc get nodes -l autoscaling=app --no-headers=true | cut -f1 -d \" \"\n    register: oc_old_app_nodes\n\n- hosts: localhost\n  tasks:\n  # Since both number and names of app nodes are to be removed\n  # localhost variables for these values need to be set\n  - name: Store old number and names of app nodes locally (if there is an existing deployment)\n    when: '\"masters\" in groups'\n    register: set_fact_result\n    set_fact:\n      oc_old_num_nodes: \"{{ hostvars[groups['masters'][0]]['oc_old_num_nodes'].stdout }}\"\n      oc_old_app_nodes: \"{{ hostvars[groups['masters'][0]]['oc_old_app_nodes'].stdout_lines }}\"\n\n  - name: Set default values for old app nodes (if there is no existing deployment)\n    when: 'set_fact_result | skipped'\n    set_fact:\n      oc_old_num_nodes: 0\n      oc_old_app_nodes: []\n\n  # Set how many nodes are to be added (1 by default)\n  - name: Set how many nodes are to be added\n    set_fact:\n      increment_by: 1\n  - name: Check that the number corresponds to scaling up (not down)\n    assert:\n      that: 'increment_by | int >= 1'\n      msg: >\n        FAIL: The value of increment_by must be at least 1\n        (but it is {{ increment_by | int }}).\n  - name: Update openstack_num_nodes variable\n    set_fact:\n      openstack_num_nodes: \"{{ oc_old_num_nodes | int + increment_by | int }}\"\n\n# Run provision.yaml with higher number of nodes to create a new app-node VM\n- include: provision.yaml\n\n# Run config.yml to perform openshift installation\n# Path to openshift-ansible can be customised:\n# - the value of openshift_ansible_dir has to be an absolute path\n# - the path cannot contain the '/' symbol at the end\n\n# Creating a new deployment by the full installation\n- include: \"{{ openshift_ansible_dir }}/playbooks/byo/config.yml\"\n  vars:\n    openshift_ansible_dir: ../../../../openshift-ansible\n  when: 'not groups[\"new_nodes\"] | list'\n\n# Scaling up existing deployment\n- include: \"{{ openshift_ansible_dir }}/playbooks/byo/openshift-node/scaleup.yml\"\n  vars:\n    openshift_ansible_dir: ../../../../openshift-ansible\n  when: 'groups[\"new_nodes\"] | list'\n\n# Post-verification: Verify new number of nodes\n- hosts: masters[0]\n  tasks:\n  - name: Get number of nodes\n    shell: oc get nodes -l autoscaling=app --no-headers=true | wc -l\n    register: oc_new_num_nodes\n  - name: Check that the actual result matches the defined value\n    assert:\n      that: 'oc_new_num_nodes.stdout | int == (hostvars[\"localhost\"][\"oc_old_num_nodes\"] | int + hostvars[\"localhost\"][\"increment_by\"] | int)'\n      msg: >\n        FAIL: Number of application nodes has not been increased accordingly\n        (it should be {{ hostvars[\"localhost\"][\"oc_old_num_nodes\"] | int + hostvars[\"localhost\"][\"increment_by\"] | int }}\n        but it is {{ oc_new_num_nodes.stdout | int }}).\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "ba24c93938dfd3fd481b66cf2336c5270ab616aa", "filename": "roles/client/tasks/systems/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- include_tasks: Debian.yml\n  when: ansible_distribution == 'Debian'\n\n- include_tasks: Ubuntu.yml\n  when: ansible_distribution == 'Ubuntu'\n\n- include_tasks: CentOS.yml\n  when: ansible_distribution == 'CentOS'\n\n- include_tasks: Fedora.yml\n  when: ansible_distribution == 'Fedora'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "189dc449d37bce2a30b386a6edcb800def3a79b3", "filename": "roles/dns/manage-dns-zones/tasks/route53/process-one-zone.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Remove Zone entries prior to remove the Zone\n  include_tasks: loop-zones.yml\n  when:\n    - zone.state == \"absent\"\n\n- name: Ensure the private zone is on the desired state\n  route53_zone:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    zone: \"{{ zone.dns_domain }}\"\n    vpc_id: \"{{ zone.route53.vpc_id }}\"\n    vpc_region: \"{{ zone.route53.vpc_region }}\"\n    state: \"{{ zone.state | default('present') }}\"\n  when:\n    - view.name == \"private\"\n\n- name: Ensure the public zone is on the desired state\n  route53_zone:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    zone: \"{{ zone.dns_domain }}\"\n    state: \"{{ zone.state | default('present') }}\"\n  when:\n    - view.name == \"public\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "fbc386ff441a9402f1ff2a8814d889417bebd222", "filename": "roles/openshift-management/handlers/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: cleanup openshift login\n  file: path=\"{{ kubeconfig | dirname }}\" state=absent"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ec2605490688aa4a95fa8e25be3aac3a55ebb7b9", "filename": "playbooks/notifications/email-notify-users.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Send HTML e-mail message to a user\"\n  hosts: mail-host\n  gather_facts: no\n  tasks:\n  - include_tasks: email-notify-single-user.yml\n    vars:\n      first_name: \"{{ item.first_name }}\"\n      user_name: \"{{ item.user_name }}\"\n      password: \"{{ item.password }}\"\n      email_to: \"{{ item.email }}\"\n    when:\n    - item.notify_user == True\n    with_items:\n    - \"{{ users }}\"\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "805f1c1737c68741643f0d5980e0de081fbdc074", "filename": "roles/config-clair/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Validate Quay Address Provided\n  fail:\n    msg: \"Quay Address Must Be Provided!\"\n  when: quay_enterprise_address is undefined or quay_enterprise_address|trim == \"\"\n\n- name: Set Clair Address\n  set_fact:\n    clair_address: \"http://{{ hostvars[inventory_hostname]['ansible_eth0']['ipv4']['address'] }}\"\n  when: clair_address is undefined or clair_address|trim == \"\"\n\n- name: Configure Configuration Directory\n  file:\n    state: directory\n    owner: root\n    group: root\n    mode: g+rw\n    path: \"{{ clair_config_dir }}\"\n  \n- name: Configure Trusted SSL\n  block:\n    - name: Check if Trusted SSL file exists\n      become: false\n      stat:\n        path: \"{{ clair_ssl_trust_src_file  }}\"\n      register: trusted_ssl_exists\n      changed_when: False\n      delegate_to: localhost\n    \n    - name: Fail if SSL source file does not exist\n      fail:\n        msg: \"Could not locate SSL trust certificate\"\n      when: trusted_ssl_exists.stat.exists == false\n  \n    - name: Copy SSL Certificate\n      copy:\n        src: \"{{ clair_ssl_trust_src_file }}\"\n        dest: \"{{ clair_ssl_trust_host_file }}\"\n        owner: root\n        group: root\n        mode: g+rw\n      notify: Restart Clair Service\n  when: clair_ssl_trust_configure|bool\n\n- name: Setup Clair configuration file\n  template:\n    src: config.yaml.j2\n    dest: \"{{ clair_config_dir }}/config.yaml\"\n    owner: root\n    group: root\n    mode: g+rw\n  notify: Restart Clair Service\n\n\n- name: Configure systemd environment files\n  template:\n    src: \"{{ clair_name }}.j2\"\n    dest: \"{{ systemd_environmentfile_dir}}/{{ clair_name }}\"\n  notify: \"Restart Clair Service\"\n\n- name: Configure systemd unit files\n  template:\n    src: \"{{ clair_service }}.j2\"\n    dest: \"{{ systemd_service_dir}}/{{ clair_service }}\"\n  notify: \"Restart Clair Service\"\n\n- name: Include firewall tasks\n  include_tasks: firewall.yml"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "909fca4b1e0f243afd78131f0fca63aed8602d14", "filename": "roles/nodogsplash/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install nodogsplash (Raspbian only)\n  include_tasks: rpi.yml\n  when: is_rpi\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "e55d80da5c0e27f1e4a7dbaeeefe1f134d77f21b", "filename": "roles/mesos/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for mesos\nmesos_zk_port: 2181\nmesos_zookeeper_group: zookeeper_servers\nmesos_master_port: 5050\nconsul_dir: /etc/consul.d\nmesos_executor_registration_timeout: 10mins\nmesos_cluster_name: \"Cluster01\"\nmesos_containerizers: \"docker,mesos\"\nmesos_resources: \"ports(*):[31000-32000]\"\nmesos_slave_work_dir: \"/tmp/mesos\"\nmesos_ip: \"{{ ansible_default_ipv4.address }}\"\nmesos_hostname: \"{{ ansible_ssh_host }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "86b71450d6f7239414f86413b352d4fa33b6d87a", "filename": "roles/idm-host-cert/tasks/generate-csr.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Generate the private Key and a CSR\"\n  generate_csr:\n    country: \"{{ csr_country }}\"\n    state: \"{{ csr_state }}\"\n    location: \"{{ csr_location }}\"\n    org_name: \"{{ csr_org_name }}\"\n    org_unit: \"{{ csr_org_unit }}\"\n    common_name: \"{{ host_name }}\"\n    email: \"{{ csr_email }}\"\n  register: csr_content\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "96946ba6d14882dceabe1aeb27e6fd81d03eba9a", "filename": "roles/ovirt-engine-install-packages/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# tasks file for ovirt-engine-install-packages\n- name: yum install engine\n  yum:\n    name: \"{{ovirt_engine_type}}\"\n    state: installed\n    update_cache: yes\n\n- name: yum install dwh\n  yum:\n    name: \"{{ovirt_engine_type}}-dwh\"\n    state: present\n  when: (ovirt_engine_dwh|bool == True) and (ovirt_engine_version|int < 4)\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "740c7640498f850e983ce11b9ad1f6644556bf48", "filename": "roles/zookeeper/vars/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# vars file for zookeeper\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "292d6c5ba4f901b14cc526a52ddff7421456d39b", "filename": "roles/ansible/tower/manage-credentials/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- block: # when ansible_tower.credentials is defined\n\n  - name: \"Set default values\"\n    set_fact:\n      processed_credentials: []\n      existing_organizations_output: []\n      existing_credentials_output: []\n      existing_credential_types_output: []\n\n  # Utilize the `rest_get` library routine to ensure REST pagination is handled\n  - name: \"Get the existing organizations\"\n    rest_get:\n      host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n      rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      rest_password: \"{{ ansible_tower.admin_password }}\"\n      api_uri: \"/api/v2/organizations/\"\n    register: existing_organizations_output\n\n  # Utilize the `rest_get` library routine to ensure REST pagination is handled\n  - name: \"Get the existing credentials\"\n    rest_get:\n      host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n      rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      rest_password: \"{{ ansible_tower.admin_password }}\"\n      api_uri: \"/api/v2/credentials/\"\n    register: existing_credentials_output\n\n  # Utilize the `rest_get` library routine to ensure REST pagination is handled\n  - name: \"Get the existing credential types\"\n    rest_get:\n      host_url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}\"\n      rest_user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      rest_password: \"{{ ansible_tower.admin_password }}\"\n      api_uri: \"/api/v2/credential_types/\"\n    register: existing_credential_types_output\n\n  - name: \"Process the inventory credentials\"\n    include_tasks: process-credential.yml\n    with_items:\n    - \"{{ ansible_tower.credentials }}\"\n    loop_control:\n      loop_var: credential\n\n  - name: \"Elminate the credentials that should not be present\"\n    uri:\n      url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/credentials/{{ item.id }}/\"\n      user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n      password: \"{{ ansible_tower.admin_password }}\"\n      force_basic_auth: yes\n      method: DELETE\n      validate_certs: no\n      status_code: 200,204\n    with_items:\n    - \"{{ existing_credentials_output.rest_output | get_remaining_items(processed_credentials, 'name', 'name')}}\"\n\n  when:\n  - ansible_tower.credentials is defined\n"}, {"commit_sha": "c91b6076e3a957fb0a165131d0ff3b3b208ed419", "sha": "e0c431a41226ce09f2764a738310e2d016903298", "filename": "tasks/ansible", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n\n\n  - name: 8.1.1.1 Configure Data Retention\n    lineinfile: dest='/etc/audit/auditd.conf' regexp='max_log_file' line='max_log_file = {{max_log_file}}' state=present\n    tags:\n      - section8\n      - section8.1\n      - section8.1.1\n      - section8.1.1.1\n\n  - name: 8.1.1.2 Configure Data Retention\n    lineinfile: dest='/etc/audit/auditd.conf' regexp='space_left_action'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.1\n      - section8.1.1.2\n\n  - name: 8.1.1.3 Configure Data Retention\n    lineinfile: dest='/etc/audit/auditd.conf' regexp='action_mail_acct'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.1\n      - section8.1.1.3\n\n  - name: 8.1.1.4 Configure Data Retention\n    lineinfile: dest='/etc/audit/auditd.conf' regexp='admin_space_left_action'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.1\n      - section8.1.1.4\n\n  - name: 8.1.1.5 Configure Data Retention\n    lineinfile: dest='/etc/audit/auditd.conf' regexp='max_log_file_action'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.1\n      - section8.1.1.5\n\n  - name: 8.1.2.1 Install and Enable auditd Service (Scored)\n    action: command >\n      ' dpkg -s auditd'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.2\n      - section8.1.2.1\n\n  - name: 8.1.2.2 Install and Enable auditd Service (Scored)\n    action: command >\n      ' ls /etc/rc*.d/S*auditd'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.2\n      - section8.1.2.2\n\n  - name: 8.1.3 Enable Auditing for Processes That Start Prior to auditd (Scored)\n    lineinfile: dest='/boot/grub/grub.cfg' regexp='\"linux\"'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.3\n\n  - name: 8.1.4.1 Record Events That Modify Date and Time Information (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='time-change'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.4\n      - section8.1.4.1\n\n  - name: 8.1.4.2 Record Events That Modify Date and Time Information (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='time-change'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.4\n      - section8.1.4.2\n\n  - name: 8.1.5 Record Events That Modify User/Group Information (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='identity'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.5\n\n  - name: 8.1.6.1 Record Events That Modify the System's Network Environment(Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='system-locale'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.6\n      - section8.1.6.1\n\n  - name: 8.1.6.2 Record Events That Modify the System's Network Environment(Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='system-locale'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.6\n      - section8.1.6.2\n\n  - name: 8.1.7 Record Events That Modify the System's Mandatory AccessControls (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='MAC-policy'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.7\n\n  - name: 8.1.8 Collect Login and Logout Events (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='logins'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.8\n\n  - name: 8.1.9 Collect Session Initiation Information (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='session'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.9\n\n  - name: 8.1.10.1 Collect Discretionary Access Control Permission ModificationEvents (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='perm_mod'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.10\n      - section8.1.10.1\n\n  - name: 8.1.10.2 Collect Discretionary Access Control Permission ModificationEvents (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='perm_mod'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.10\n      - section8.1.10.2\n\n  - name: 8.1.11.1 Collect Unsuccessful Unauthorized Access Attempts to Files(Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='access'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.11\n      - section8.1.11.1\n\n  - name: 8.1.11.2 Collect Unsuccessful Unauthorized Access Attempts to Files(Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='access'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.11\n      - section8.1.11.2\n\n  - name: 8.1.12.1 Collect Use of Privileged Commands (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='access'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.12\n      - section8.1.12.1\n\n  - name: 8.1.12.2 Collect Use of Privileged Commands (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='access'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.12\n      - section8.1.12.2\n\n  - name: 8.1.13.1 Collect Successful File System Mounts (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='mounts'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.13\n      - section8.1.13.1\n\n  - name: 8.1.13.2 Collect Successful File System Mounts (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='mounts'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.13\n      - section8.1.13.2\n\n  - name: 8.1.14.1 Collect File Deletion Events by User (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='delete'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.14\n      - section8.1.14.1\n\n  - name: 8.1.14.2 Collect File Deletion Events by User (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='delete'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.14\n      - section8.1.14.2\n\n  - name: 8.1.15 Collect Changes to System Administration Scope (sudoers)(Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='scope'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.15\n\n  - name: 8.1.16 Collect System Administrator Actions (sudolog) (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='actions'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.16\n\n  - name: 8.1.17 Collect Kernel Module Loading and Unloading (Scored)\n    lineinfile: dest='/etc/audit/audit.rules' regexp='modules'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.17\n\n  - name: 8.1.18 Make the Audit Configuration Immutable (Scored)\n    action: command >\n      ' tail -n 1 /etc/audit/audit.rules'\n    tags:\n      - section8\n      - section8.1\n      - section8.1.18\n\n  - name: 8.3.1 Install AIDE (Scored)\n    action: command >\n      ' dpkg -s aide'\n    tags:\n      - section8\n      - section8.3\n      - section8.3.1\n\n  - name: 8.3.2 Implement Periodic Execution of File Integrity (Scored)\n    action: command >\n      ' crontab -u root -l | grep aide'\n    tags:\n      - section8\n      - section8.3\n      - section8.3.2\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "ce8b224cf5027e64e92dfd4a59b4fbe72f78bc76", "filename": "roles/kiwix/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: \"Set Kiwix filename to d/l: {{ kiwix_src_file_armhf }} (armv6l or armv71)\"\n  set_fact:\n     kiwix_src_dir: \"{{ kiwix_version_armhf }}\"\n     kiwix_src_file: \"{{ kiwix_src_file_armhf }}\"\n  when: ansible_machine == \"armv7l\" or ansible_machine == \"armv6l\"\n  \n- name: \"Set Kiwix filename to d/l: {{ kiwix_src_file_linux64 }} (x86_64)\"\n  set_fact:\n     kiwix_src_dir: \"{{ kiwix_version_linux64 }}\"\n     kiwix_src_file: \"{{ kiwix_src_file_linux64 }}\"\n  when: ansible_machine == \"x86_64\"\n\n- name: \"Set Kiwix filename to d/l: {{ kiwix_src_file_i686 }} (i686)\"\n  set_fact:\n     kiwix_src_dir: \"{{ kiwix_version_i686 }}\"\n     kiwix_src_file: \"{{ kiwix_src_file_i686 }}\"\n  when: ansible_machine == \"i686\"\n# COMMENT OUT LINE ABOVE TO TEST i686 CODE PATH ON X86_64 (WORKS NOV 2017)\n\n- name: FAIL (force Ansible to exit) IF kiwix-tools appears unavailable for OS/architecture\n# debug:\n  fail:\n    msg: \"WARNING: kiwix-tools SOFTWARE APPEARS UNAVAILABLE FOR YOUR {{ ansible_machine }} OS/ARCHITECTURE.\"\n  when: not kiwix_src_file\n\n- name: Download Kiwix software to /opt/iiab/downloads\n  get_url:\n    url: \"{{ iiab_download_url }}/{{ kiwix_src_file }}\"\n    dest: \"{{ downloads_dir }}/{{ kiwix_src_file }}\"\n    timeout: \"{{ download_timeout }}\"\n  when: internet_available\n\n- name: Check for /opt/iiab/downloads/{{ kiwix_src_file }}\n  stat:\n    path: \"{{ downloads_dir }}/{{ kiwix_src_file }}\"\n  register: kiwix_src\n\n- name: FAIL (force Ansible to exit) IF /opt/iiab/downloads/{{ kiwix_src_file }} doesn't exist\n  fail:\n    msg: \"{ downloads_dir }}/{{ kiwix_src_file }} is REQUIRED in order to install Kiwix.\"\n  when: not kiwix_src.stat.exists\n\n- include_tasks: kiwix_install.yml\n  tags:\n    - kiwix\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "548a4ad8a3da58a7faf2862ced20f4b3e8ec1858", "filename": "tasks/Win32NT/install/adoptopenjdk_tarball.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Check that the java_folder exists\n  win_stat:\n    path: '{{ java_path }}\\{{ java_folder }}/bin'\n  register: java_folder_bin\n\n- name: Install java from tarball\n  block:\n  - name: Mkdir for java installation\n    win_file:\n      path: '{{ java_path }}\\{{ java_folder }}'\n      state: directory\n\n  - name: Create temporary directory\n    win_tempfile:\n      state: directory\n    register: temp_dir_path\n\n  - name: Unarchive to temporary directory\n    win_unzip:\n      src: '{{ java_artifact }}'\n      dest: '{{ temp_dir_path }}'\n\n  - name: Find java_folder in temp\n    win_find:\n      paths: '{{ temp_dir_path }}'\n      recurse: false\n      file_type: directory\n    register: java_temp_folder\n\n  - name: Copy from temporary directory\n    win_copy:\n      src: '{{ java_temp_folder.files | map(attribute=\"path\") | list | last }}\\'\n      dest: '{{ java_path }}\\{{ java_folder }}'\n      remote_src: true\n  when: not java_folder_bin.stat.exists\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "a3c91ab2fd30608a2a709300549419827fd5b153", "filename": "roles/elgg/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "elgg_xx: elgg\nelgg_version: \"2.3.8\"\n\n# elgg_mysql_password: defined in default_vars\nelgg_url: /elgg\nelgg_upload_path: /library/elgg\nelgg_install: True\nelgg_enabled: False\n\n# following variables used in elgg engine/settings.php template\ndbuser: Admin\ndbpassword: changeme\ndbname: elggdb\ndbhost: localhost\ndbprefix: elgg_\n\n# The following variables must be in sync with template/elggdb.sql.j2\n# If you change them, you will probably have to rebuild the database.\n# They can be changed from the administrative interface once elgg is installed.\n\nelgg_admin_user: Admin\nelgg_admin_password: changeme\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "81800958af84f562e49b02e1f3d3ccaf23bdfa26", "filename": "roles/marathon/vars/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# vars file for marathon\n"}, {"commit_sha": "c91b6076e3a957fb0a165131d0ff3b3b208ed419", "sha": "cd5607a47d0af0fa3491fcced679e5a67c2ada31", "filename": "tasks/section_01_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 1.1.1 Install Updates, Patches and Additional Security Software (NotScored)\n    apt: update_cache=yes\n    tags:\n      - section1\n      - section1.1\n      - section1.1.1\n\n  - name: 1.1.2 Install Updates, Patches and Additional Security Software (NotScored)\n    apt: upgrade=yes\n    when: apt_upgrade == True\n    tags:\n      - section1\n      - section1.1\n      - section1.1.2\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "7532a678bc1b5953a08c3ac3748e0d51abc3d0ff", "filename": "playbooks/gce/openshift-cluster/launch.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Launch instance(s)\n  hosts: localhost\n  connection: local\n  become: no\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  tasks:\n  - fail: msg=\"Deployment type not supported for gce provider yet\"\n    when: deployment_type == 'enterprise'\n\n  - include: ../../common/openshift-cluster/tasks/set_etcd_launch_facts.yml\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ etcd_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"default\"\n      gce_machine_type: \"{{ lookup('env', 'gce_machine_etcd_type') | default(lookup('env', 'gce_machine_type'), true) }}\"\n      gce_machine_image: \"{{ lookup('env', 'gce_machine_etcd_image') | default(lookup('env', 'gce_machine_image'), true) }}\"\n\n\n  - include: ../../common/openshift-cluster/tasks/set_master_launch_facts.yml\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ master_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"default\"\n      gce_machine_type: \"{{ lookup('env', 'gce_machine_master_type') | default(lookup('env', 'gce_machine_type'), true) }}\"\n      gce_machine_image: \"{{ lookup('env', 'gce_machine_master_image') | default(lookup('env', 'gce_machine_image'), true) }}\"\n\n  - include: ../../common/openshift-cluster/tasks/set_node_launch_facts.yml\n    vars:\n      type: \"compute\"\n      count: \"{{ num_nodes }}\"\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ node_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"{{ sub_host_type }}\"\n      gce_machine_type: \"{{ lookup('env', 'gce_machine_node_type') | default(lookup('env', 'gce_machine_type'), true) }}\"\n      gce_machine_image: \"{{ lookup('env', 'gce_machine_node_image') | default(lookup('env', 'gce_machine_image'), true) }}\"\n\n  - include: ../../common/openshift-cluster/tasks/set_node_launch_facts.yml\n    vars:\n      type: \"infra\"\n      count: \"{{ num_infra }}\"\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ node_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"{{ sub_host_type }}\"\n      gce_machine_type: \"{{ lookup('env', 'gce_machine_node_type') | default(lookup('env', 'gce_machine_type'), true) }}\"\n      gce_machine_image: \"{{ lookup('env', 'gce_machine_node_image') | default(lookup('env', 'gce_machine_image'), true) }}\"\n\n  - add_host:\n      name: \"{{ master_names.0 }}\"\n      groups: service_master\n    when: master_names is defined and master_names.0 is defined\n\n- include: update.yml\n\n- include: list.yml\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "c405c07cde00dc8ecdecee0aa2c238f812fa2b49", "filename": "tasks/delete_blobstore_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: delete_blobstore\n    args: \"{{ item }}\""}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "35c727eeca7606195c62106b864875b5cf2e9d37", "filename": "tasks/section_08_level2.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n# Change order or the default auditd.conf file will not be created\n  - name: 8.1.2 Install and Enable auditd Service (Scored)\n    apt: name=auditd state=present\n    tags:\n      - section8\n      - section8.1\n      - section8.1.2\n      - section8.1.2\n\n  - name: Check if the file auditd.conf exists (Not Scored)\n    stat: >\n        path=/etc/audit/auditd.conf\n    register: auditd_file\n    tags:\n      - section8\n      - section8.1\n      - section8.1.1\n      - section8.1.1.1\n\n  - name: Create the audit directory if it does not exists (Not Scored)\n    file: >\n        path=/etc/audit/\n        state=directory\n    when: not auditd_file.stat.exists\n    tags:\n      - section8\n      - section8.1\n      - section8.1.1\n      - section8.1.1.1\n\n  - name: 8.1.1-3 Configure Data Retention (Not Scored)\n    lineinfile: >\n        dest=/etc/audit/auditd.conf\n        regexp=\"{{ item.rxp }}\"\n        line=\"{{ item.line }}\"\n        state=present\n        create=yes\n    with_items:\n      - { rxp: '^max_log_file ', line: 'max_log_file = {{ max_log_file_auditd }}' }\n      - { rxp: '^space_left_action', line: 'space_left_action = email' }\n      - { rxp: '^action_mail_acct', line: 'action_mail_acct = root' }\n      - { rxp: '^admin_space_left_action', line: 'admin_space_left_action = halt' }\n      - { rxp: '^max_log_file_action', line: 'max_log_file_action = keep_logs' }\n    notify: restart auditd\n    tags:\n      - section8\n      - section8.1\n      - section8.1.1\n      - section8.1.1.1\n      - section8.1.1.2\n      - section8.1.1.3\n\n  - name: 8.1.3 Enable Auditing for Processes That Start Prior to auditd (Scored)\n    stat: path=/etc/default/grub\n    register: grubcfg_file\n    tags:\n      - section8\n      - section8.1\n      - section8.1.3\n\n  - name: 8.1.3 Enable Auditing for Processes That Start Prior to auditd (Scored)\n    file: >\n        path=/etc/default/grub\n        state=touch\n    when: not grubcfg_file.stat.exists\n    tags:\n      - section8\n      - section8.1\n      - section8.1.3\n\n  - name: 8.1.3 Enable Auditing for Processes That Start Prior to auditd (Scored)\n    lineinfile: >\n        dest=/etc/default/grub\n        line='GRUB_CMDLINE_LINUX=\"audit=1\"'\n    when: not grubcfg_file.stat.exists\n    tags:\n      - section8\n      - section8.1\n      - section8.1.3\n\n  - name: 8.1.4 Record Events That Modify Date and Time Information (Scored)\n    lineinfile: >\n      dest=/etc/audit/audit.rules\n      line='{{ item }}'\n      state=present\n      create=yes\n    with_items:\n      - '-a always,exit -F arch=b64 -S adjtimex -S settimeofday -k time-change'\n      - '-a always,exit -F arch=b32 -S adjtimex -S settimeofday -S stime -k time-change'\n      - '-a always,exit -F arch=b64 -S clock_settime -k time-change'\n      - '-a always,exit -F arch=b32 -S clock_settime -k time-change'\n      - '-w /etc/localtime -p wa -k time-change'\n    notify: restart auditd\n    when: ansible_userspace_bits == \"64\"\n    tags:\n      - section8\n      - section8.1\n      - section8.1.4\n\n  - name: 8.1.4 Record Events That Modify Date and Time Information (Scored)\n    lineinfile: >\n      dest=/etc/audit/audit.rules\n      line='{{ item }}'\n      state=present\n      create=yes\n    with_items:\n      - '-a always,exit -F arch=b32 -S adjtimex -S settimeofday -S stime -k time-change'\n      - '-a always,exit -F arch=b32 -S clock_settime -k time-change'\n      - '-w /etc/localtime -p wa -k time-change'\n    notify: restart auditd\n    when: ansible_userspace_bits == \"32\"\n    tags:\n      - section8\n      - section8.1\n      - section8.1.4\n\n  - name: 8.1.5,7,8,9,15,16,17 Record Events That Modify User/Group Information (Scored)\n    lineinfile: >\n      dest=/etc/audit/audit.rules\n      line='{{ item }}'\n      state=present\n      create=yes\n    with_items:\n      - '-w /etc/group -p wa -k identity'\n      - '-w /etc/passwd -p wa -k identity'\n      - '-w /etc/gshadow -p wa -k identity'\n      - '-w /etc/shadow -p wa -k identity'\n      - '-w /etc/security/opasswd -p wa -k identity'\n      - '-w /var/log/faillog -p wa -k logins'\n      - '-w /var/log/lastlog -p wa -k logins'\n      - '-w /var/log/tallylog -p wa -k logins'\n      - '-w /var/run/utmp -p wa -k session'\n      - '-w /var/log/wtmp -p wa -k session'\n      - '-w /var/log/btmp -p wa -k session'\n      - '-w /etc/selinux/ -p wa -k MAC-policy'\n      - '-w /etc/sudoers -p wa -k scope'\n      - '-w /var/log/sudo.log -p wa -k actions'\n      - '-w /sbin/insmod -p x -k modules'\n      - '-w /sbin/rmmod -px -k modules'\n      - '-w /sbin/modprobe -p x -k modules'\n    notify: restart auditd\n    tags:\n      - section8\n      - section8.1\n      - section8.1.5\n      - section8.1.7\n      - section8.1.8\n      - section8.1.9\n      - section8.1.15\n      - section8.1.16\n      - section8.1.17\n\n  - name: 8.1.6,10,11,13,14,17 Record Events That Modify the System's Network Environment (64b) (Scored)\n    lineinfile: >\n      dest=/etc/audit/audit.rules\n      line='{{ item }}'\n      state=present\n      create=yes\n    with_items:\n      - '-a exit,always -F arch=b64 -S sethostname -S setdomainname -k system-locale'\n      - '-a exit,always -F arch=b32 -S sethostname -S setdomainname -k system-locale'\n      - '-w /etc/issue -p wa -k system-locale'\n      - '-w /etc/issue.net -p wa -k system-locale'\n      - '-w /etc/hosts -p wa -k system-locale'\n      - '-w /etc/network -p wa -k system-locale'\n      - '-a always,exit -F arch=b64 -S chmod -S fchmod -S fchmodat -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S chmod -S fchmod -S fchmodat -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b64 -S chown -S fchown -S fchownat -S lchown -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S chown -S fchown -S fchownat -S lchown -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b64 -S setxattr -S lsetxattr -S fsetxattr -S removexattr -S lremovexattr -S fremovexattr -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S setxattr -S lsetxattr -S fsetxattr -S removexattr -S lremovexattr -S fremovexattr -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b64 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EACCES -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b32 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EACCES -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b64 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EPERM -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b32 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EPERM -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b64 -S mount -F auid>=500 -F auid!=4294967295 -k mounts'\n      - '-a always,exit -F arch=b32 -S mount -F auid>=500 -F auid!=4294967295 -k mounts'\n      - '-a always,exit -F arch=b64 -S unlink -S unlinkat -S rename -S renameat -F auid>=500 -F auid!=4294967295 -k delete'\n      - '-a always,exit -F arch=b32 -S unlink -S unlinkat -S rename -S renameat -F auid>=500 -F auid!=4294967295 -k delete'\n      - '-a always,exit -F arch=b64 -S init_module -S delete_module -k modules'\n    notify: restart auditd\n    when: ansible_userspace_bits == \"64\"\n    tags:\n      - section8\n      - section8.1\n      - section8.1.6\n      - section8.1.10\n      - section8.1.11\n      - section8.1.13\n      - section8.1.14\n      - section8.1.17\n\n  - name: 8.1.6,10,11,13,14,17 Record Events That Modify the System's Network Environment (32b) (Scored)\n    lineinfile: >\n      dest=/etc/audit/audit.rules\n      line='{{ item }}'\n      state=present\n      create=yes\n    with_items:\n      - '-a exit,always -F arch=b32 -S sethostname -S setdomainname -k system-locale'\n      - '-w /etc/issue -p wa -k system-locale'\n      - '-w /etc/issue.net -p wa -k system-locale'\n      - '-w /etc/hosts -p wa -k system-locale'\n      - '-w /etc/network -p wa -k system-locale'\n      - '-a always,exit -F arch=b32 -S chmod -S fchmod -S fchmodat -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S chown -S fchown -S fchownat -S lchown -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S setxattr -S lsetxattr -S fsetxattr -S removexattr -S lremovexattr -S fremovexattr -F auid>=500 -F auid!=4294967295 -k perm_mod'\n      - '-a always,exit -F arch=b32 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EACCES -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b32 -S creat -S open -S openat -S truncate -S ftruncate -F exit=-EPERM -F auid>=500 -F auid!=4294967295 -k access'\n      - '-a always,exit -F arch=b32 -S mount -F auid>=500 -F auid!=4294967295 -k mounts'\n      - '-a always,exit -F arch=b32 -S unlink -S unlinkat -S rename -S renameat -F auid>=500 -F auid!=4294967295 -k delete'\n      - '-a always,exit -F arch=b32 -S init_module -S delete_module -k modules'\n    notify: restart auditd\n    when: ansible_userspace_bits == \"32\"\n    tags:\n      - section8\n      - section8.1\n      - section8.1.6\n      - section8.1.10\n      - section8.1.11\n      - section8.1.13\n      - section8.1.14\n      - section8.1.17\n\n  - name: 8.3.1 Install AIDE (Scored)\n    apt: name=aide state=present\n    register: aide_installed\n    tags:\n      - section8\n      - section8.3\n      - section8.3.1\n\n  - name: 8.3.1 Install AIDE (init) (Scored)\n    command: aideinit\n    when: aide_installed.changed == True\n    tags:\n      - section8\n      - section8.3\n      - section8.3.1\n\n  - name: 8.3.1 Install AIDE (Scored)\n    stat: path=/var/lib/aide/aide.db.new\n    register: aide_db_path\n    when:\n      - aide_installed.changed == True\n    tags:\n      - section8\n      - section8.3\n      - section8.3.1\n\n  - name: 8.3.1 Install AIDE (copy db) (Scored)\n    command: mv /var/lib/aide/aide.db.new /var/lib/aide/aide.db\n    when:\n      - aide_installed.changed == True\n      - aide_db_path.stat.exists == True\n    tags:\n      - section8\n      - section8.3\n      - section8.3.1\n\n  - name: 8.3.2 Implement Periodic Execution of File Integrity (Scored)\n    cron: name=\"Check files integrity\" minute=\"0\" hour=\"5\" job=\"/usr/sbin/aide --check\"\n    tags:\n      - section8\n      - section8.3\n      - section8.3.2\n\n  # We have to run the check after AIDE installation as postfix create new matched binaries\n  - name: 8.1.12 Collect Use of Privileged Commands (Scored)\n    shell: find / -xdev \\( -perm -4000 -o -perm -2000 \\) -type f | awk '{print \"-a always,exit -F path=\" $1 \" -F perm=x -F auid>=500 -F auid!=4294967295 -k privileged\" }'\n    register: audit_lines_for_find\n    changed_when: False\n    tags:\n      - section8\n      - section8.1\n      - section8.1.12\n\n  - name: 8.1.12 Collect Use of Privileged Commands (infos) (Scored)\n    lineinfile: >\n        dest=/etc/audit/audit.rules\n        line='{{ item }}'\n        state=present\n        create=yes\n    with_items: audit_lines_for_find.stdout_lines\n    tags:\n      - section8\n      - section8.1\n      - section8.1.12\n\n  - name: 8.1.18 Make the Audit Configuration Immutable (Scored)\n    lineinfile: >\n        dest='/etc/audit/audit.rules'\n        line='-e 2'\n        insertafter=EOF\n        state=present\n        create=yes\n    tags:\n      - section8\n      - section8.1\n      - section8.1.18\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "29d5775f8055659548e2d59019ac2f87f98e5065", "filename": "roles/load-balancers/manage-haproxy/tasks/generate-config.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# Use a 'block' to ensure it all runs on 'localhost'\n- block:\n\n  - name: 'Create a temporary HAproxy config file'\n    tempfile:\n      state: file\n      suffix: haproxy\n    register: tempfile\n\n  - name: 'Store away the HAproxy temp config file name'\n    set_fact:\n      haproxy_temp_file: \"{{ tempfile.path }}\"\n\n  - name: 'Use a unique temporary directory to store the config files pre-assemble'\n    command: mktemp -d\n    register: tempdir\n    notify: 'cleanup temp dir'\n\n  - name: 'Store away the temp names'\n    set_fact:\n      haproxy_temp_dir: '{{ tempdir.stdout }}'\n\n  - name: 'Populate the common config portion for HAproxy'\n    template:\n      src: \"{{ lb_common_template | default('lb_common.j2') }}\"\n      dest: '{{ haproxy_temp_dir }}/0001_lb.cfg'\n\n  - name: 'Configure and Populate LB Frontends'\n    template:\n      src: \"{{ lb_frontend_template | default('lb_frontend.j2') }}\"\n      dest: '{{ haproxy_temp_dir }}/0002_lb_{{ fe.lb_name }}_{{ fe.lb_host_vip }}_{{ fe.lb_host_port }}.cfg'\n    loop_control:\n      loop_var: fe\n    loop: \"{{ lb_config.frontends|flatten(levels=1) }}\"\n\n  - name: 'Configure and Populate LB Backends'\n    template:\n      src: \"{{ lb_backend_template | default('lb_backend.j2') }}\"\n      dest: '{{ haproxy_temp_dir }}/0003_lb.cfg'\n\n  - name: 'Populate the stats page config'\n    vars:\n      page_config: \"{{ lb_config.stats_page }}\"\n    template:\n      src: lb_http_stats.j2\n      dest: '{{ haproxy_temp_dir }}/0004_lb.cfg'\n    when:\n    - lb_config.stats_page is defined\n    - lb_config.stats_page.enabled is defined\n    - lb_config.stats_page.enabled\n\n  - name: 'Assemble the final HAproxy config file'\n    assemble:\n      src: '{{ haproxy_temp_dir }}'\n      dest: '{{ haproxy_temp_file }}'\n\n  # Delegate the entire block to \"localhost\"\n  delegate_to: localhost\n  run_once: True\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "bff9f48559d06036e2ad5d5717820c3a734aa6df", "filename": "roles/manage-aws-infra/tasks/create-subnet.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "# Create the Subnets use by the OCP cluster\n---\n\n- name: \"Create Subnets for the VPC (HA Mode)\"\n  ec2_vpc_subnet:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    cidr: 172.31.{{ item }}.0/20\n    region: \"{{ aws_region }}\"\n    vpc_id: \"{{ aws_vpc_id }}\"\n    map_public: yes\n    az: \"{{ aws_region }}{{ aws_region_az | default('a') }}\"\n    state: \"present\"\n    tags:\n      env_id: \"{{ env_id }}\"\n  with_sequence: start=0 end=64 stride=16\n  when: ha_mode\n  register: new_subnets\n\n- name: \"Create Subnet for the VPC (Non HA Mode)\"\n  ec2_vpc_subnet:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    cidr: \"{{ vpc_subnet_cidr | default('172.31.0.0/20') }}\"\n    region: \"{{ aws_region }}\"\n    vpc_id: \"{{ aws_vpc_id }}\"\n    map_public: yes\n    az: \"{{ aws_region }}{{ aws_region_az | default('a') }}\"\n    state: \"present\"\n    tags:\n      env_id: \"{{ env_id }}\"\n  when: not ha_mode\n  register: new_subnets\n\n- name: \"Store away the (new) subnet id for use later\"\n  set_fact:\n    aws_vpc_subnet_id: \"{{ new_subnets.subnet.id }}\"\n\n- name: \"Create Gateway subnet route table for the VPC\"\n  ec2_vpc_route_table:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    vpc_id: \"{{ aws_vpc_id }}\"\n    region: \"{{ aws_region }}\"\n    tags:\n      Name: \"{{ env_id }}-route-table-gw\"\n      env_id: \"{{ env_id }}\"\n    subnets:\n      - \"{{ new_subnets.subnet.id }}\"\n    routes:\n      - dest: \"0.0.0.0/0\"\n        gateway_id: \"{{ aws_vpc_ig }}\"\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "871700f8c171bd03740e1aac3e92df5461b41533", "filename": "roles/static_inventory/defaults/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# Either to checkpoint the dynamic inventory into a static one\nrefresh_inventory: True\ninventory: static\ninventory_path: ~/openstack-inventory\n\n# Either to configure bastion\nuse_bastion: true\n\n# SSH user/key/options to access hosts via bastion\nssh_user: openshift\nssh_options: >-\n  -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no\n  -o ConnectTimeout=90 -o ControlMaster=auto -o ControlPersist=270s\n  -o ServerAliveInterval=30 -o GSSAPIAuthentication=no\n\n# SSH key to access nodes\nprivate_ssh_key: ~/.ssh/openshift\n\n# The patch to store the generated config to access bastion/hosts\nssh_config_path: /tmp/ssh.config.ansible\n\n# The IP:port to make an SSH tunnel to access UI on the 1st master\n# via bastion node (requires sudo on the ansible control node)\nui_ssh_tunnel: False\nui_port: \"{{ openshift_master_api_port | default(8443) }}\"\ntarget_ip: \"{{ hostvars[groups['masters.' + stack_name|quote][0]].private_v4 }}\"\n\nopenstack_private_network: private\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "6e9cf96cc9e99c25feb99533f5b13f1cb07adfe5", "filename": "roles/docker/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install docker\n  package:  name={{ item }} \n            state=present\n  with_items:\n    - docker\n    - python-docker-py\n  when: docker_install\n  tags: download\n\n- name: put the systemd startup file in place\n  template: src=docker.service\n            dest=/etc/systemd/system/\n            owner=root\n            group=root\n            mode=0644\n\n- name: create the socket for docker\n  template: src=docker.socket\n            dest=/etc/systemd/system/\n            owner=root\n            group=root\n            mode=0644\n\n- name: Create a folder for systemd unit files that are docker containers\n  file: path=/etc/systemd/system/docker.service.d\n        owner=root\n        group=root\n        mode=0644\n        state=directory\n\n- name: Enable docker\n  service: name=docker\n           state=started\n           enabled=true\n  when: docker_enabled\n\n- name: Disable docker\n  service: name=docker\n           state=stopped\n           enabled=false\n  when: not docker_enabled\n\n- name: add docker to service list\n  ini_file: dest='{{ service_filelist }}'\n            section=docker\n            option='{{ item.option }}'\n            value='{{ item.value }}'\n  with_items:\n    - option: name\n      value: Docker Container\n    - option: description\n      value: '\"Docker allows a person to package an application with all of its dependencies into a standardized unit for software development.\"'\n    - option: enabled\n      value: \"{{ docker_enabled }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "ba0fc09eb9ee8103644b736e909c16e697531662", "filename": "roles/config-quay-enterprise/tasks/complete_setup.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Configure Setup Process on 1st Quay Host\n  block:\n    - name: Set Setup Output File Location\n      set_fact:\n        tmp_setup_file_location: /tmp/quay\n\n    - name: Restart Quay\n      systemd:\n        name: \"{{ quay_service }}\"\n        enabled: yes\n        state: restarted\n        daemon_reload: yes\n\n    - name: Hit Quay Setup Endpoint\n      uri:\n        url: \"{{ quay_http_protocol }}://{{ quay_hostname }}/setup/\"\n        validate_certs: no\n        method: GET\n        return_content: yes\n      register: uri_setup\n      until: uri_setup.status == 200\n      retries: 30\n      delay: 10\n    - name: Write file\n      copy:\n        content: \"{{ uri_setup.content }}\"\n        dest: \"{{ tmp_setup_file_location }}\"\n    - name: Extract csrf token\n      shell: cat /tmp/quay | grep \"__token\" | awk -F\\' '{print $(NF-1)}'\n      register: token\n    - name: Create Superuser\n      uri:\n        url:  \"{{ quay_http_protocol }}://{{ quay_server_hostname }}/api/v1/superuser/config/createsuperuser?_csrf_token={{ token.stdout | urlencode }}\"\n        validate_certs: no\n        method: POST\n        body_format: json\n        body:\n          username: \"{{ quay_superuser_username }}\"\n          email: \"{{ quay_superuser_email }}\"\n          password: \"{{ quay_superuser_password }}\"\n        headers:\n          Cookie: \"{{ uri_setup.set_cookie }}\"\n\n    - name: Delete Temporary Setup File\n      file:\n        state: absent\n        path: \"{{ tmp_setup_file_location }}\"\n  when: inventory_hostname == groups['quay_enterprise'][0]\n\n- name: Set setup_complete Fact\n  set_fact:\n    quay_setup_complete: True\n\n- name: Setup initial quay configuration file\n  template:\n    src: config.yaml.j2\n    dest: \"{{ quay_config_dir }}/config.yaml\"\n    owner: root\n    group: root\n    mode: g+rw\n  notify: Restart quay service\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "fb26bce8237bace944e31c81d5485f73aa699d89", "filename": "roles/ansible/tower/manage-inventories/tests/inventory/group_vars/tower.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nansible_tower:\n  admin_password: \"admin01\"\n  inventories:\n  - name: \"Inventory1\"\n    description: \"My Hosts\"\n    organization: \"Default\"\n    variables: \"---\"\n    hosts:\n    - name: \"localhost\"\n      description: \"\"\n      variables: \"---\\\\nansible_connection: local\"\n    groups:\n    - name: \"seed_hosts\"\n      hosts:\n      - name: \"localhost\"\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "858f590ec06aa6b7ff74ec36f35ff593a6746740", "filename": "roles/zookeeper/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: Install zookeeper packages\n  yum:\n    name:\n      - java-11-openjdk-headless\n      - zookeeper\n    state: installed\n\n- name: Enable and Start zookeeper\n  systemd:\n    name: zookeeper\n    state: \"{{ 'started' if rock_services | selectattr('name', 'equalto', 'zookeeper') | map(attribute='enabled') | bool else 'stopped' }}\"\n    enabled: \"{{ rock_services | selectattr('name', 'equalto', 'docket') | map(attribute='enabled') | bool }}\"\n...\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "5a7b91ba47fb1faae0df50b6413c790d3139b226", "filename": "roles/suricata/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# handlers file for suricata\n\n- name: Configure monitor interfaces\n  shell: >\n    for intf in {{ rock_monifs | join(' ') }}; do\n      /sbin/ifup ${intf};\n    done\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "30aa2044d00374338b41c58fc0d25dbced37c53f", "filename": "dev/playbooks/install_nfs_clients.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: dtr\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Install NFS client\n      yum:\n        name: nfs-utils\n        state: latest\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "316a890454f3e520db2fbc244c9818ac47c74be9", "filename": "playbooks/manage-users/manage-atlassian-users.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Create Atlassian Users\n  hosts: localhost\n  roles:\n    - user-management/manage-atlassian-users\n  no_log: true\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "21e3d459e039446f1b659f3b5f20dd0001f7219e", "filename": "roles/cloud-lightsail/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- block:\n  - name: Build python virtual environment\n    import_tasks: venv.yml\n\n  - block:\n    - name: Include prompts\n      import_tasks: prompts.yml\n\n    - name: Create an instance\n      lightsail:\n        aws_access_key: \"{{ access_key }}\"\n        aws_secret_key: \"{{ secret_key }}\"\n        name: \"{{ algo_server_name }}\"\n        state: present\n        region: \"{{ algo_region }}\"\n        zone: \"{{ algo_region }}a\"\n        blueprint_id: \"{{ cloud_providers.lightsail.image }}\"\n        bundle_id: \"{{ cloud_providers.lightsail.size }}\"\n        wait_timeout: 300\n        open_ports:\n          - from_port: 4500\n            to_port: 4500\n            protocol: udp\n          - from_port: 500\n            to_port: 500\n            protocol: udp\n          - from_port: \"{{ wireguard_port }}\"\n            to_port: \"{{ wireguard_port }}\"\n            protocol: udp\n        user_data: |\n          #!/bin/bash\n          mkdir -p /home/ubuntu/.ssh/\n          echo \"{{ lookup('file', '{{ SSH_keys.public }}') }}\" >> /home/ubuntu/.ssh/authorized_keys\n          chown -R ubuntu: /home/ubuntu/.ssh/\n          chmod 0700 /home/ubuntu/.ssh/\n          chmod 0600 /home/ubuntu/.ssh/*\n          test\n      register: algo_instance\n\n    - set_fact:\n        cloud_instance_ip: \"{{ algo_instance['instance']['public_ip_address'] }}\"\n        ansible_ssh_user: ubuntu\n    environment:\n      PYTHONPATH: \"{{ lightsail_venv }}/lib/python2.7/site-packages/\"\n\n  rescue:\n  - debug: var=fail_hint\n    tags: always\n  - fail:\n    tags: always\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "271ee26fecfebba1f6f790883e7d87b779f3fcd2", "filename": "ops/files/splunk/linux/DOCKER_TAS/etc/apps/ta-dockerlogs_fileinput/metadata/default.meta", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "# Application-level permissions\n\n[]\naccess = read : [ admin ], write : [ admin ]\n\n### EVENT TYPES\n\n[eventtypes]\nexport = system\n\n### LOOKUPS\n\n[lookups]\nexport = system\n\n### PROPS\n\n[props]\nexport = system\n\n### TRANSFORMS\n\n[transforms]\nexport = system\n\n### SAVEDSEARCHES\n\n[savedsearches]\nexport = system\n\n### MACROS\n\n[macros]\nexport = system\n\n### VIEWSTATES: even normal users should be able to create shared viewstates\n\n[viewstates]\naccess = read : [ * ], write : [ * ]\n\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "57fb450e1407d1e424ccc1270712634885d834be", "filename": "roles/zookeeper/vars/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# vars file for zookeeper\nzookeeper_leader_port: \"2888\"\nzookeeper_election_port: \"3888\"\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "3168b5c05814d3b98bb6732d62e3641e4cc15bd3", "filename": "tasks/replication/slave/replication.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n- name: MYSQL_REPLICATION | Stop slave\n  mysql_replication:\n    mode: stopslave\n\n- name: MYSQL_REPLICATION | Configure master host\n  mysql_replication:\n    mode: changemaster\n    master_host: \"{{ mariadb_replication_host }}\"\n    master_port: \"{{ mariadb_replication_port }}\"\n    master_user: \"{{ mariadb_replication_user }}\"\n    master_password: \"{{ mariadb_replication_password }}\"\n\n- name: MYSQL_REPLICATION | Change master\n  mysql_replication:\n    mode: changemaster\n    master_log_file: \"{{ mariadb_master_log_file }}\"\n    master_log_pos: \"{{ mariadb_master_log_pos }}\"\n  when: mariadb_master_log_file is defined and mariadb_master_log_pos is defined\n\n- name: MYSQL_REPLICATION | Start slave\n  mysql_replication:\n    mode: startslave\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "608512b793ab075b6292874a53bfacf4e7f27641", "filename": "playbooks/aws/openshift-cluster/tasks/launch_instances.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- set_fact:\n    created_by: \"{{ lookup('env', 'LOGNAME')|default(cluster, true) }}\"\n    docker_vol_ephemeral: \"{{ lookup('env', 'os_docker_vol_ephemeral') | default(false, true) }}\"\n    cluster: \"{{ cluster_id }}\"\n    env: \"{{ cluster_env }}\"\n    host_type: \"{{ type }}\"\n    sub_host_type: \"{{ g_sub_host_type }}\"\n\n- set_fact:\n    ec2_instance_type: \"{{ lookup('env', 'ec2_master_instance_type') | default(deployment_vars[deployment_type].type, true) }}\"\n    ec2_security_groups: \"{{ lookup('env', 'ec2_master_security_groups') | default(deployment_vars[deployment_type].security_groups, true) }}\"\n  when: host_type == \"master\" and sub_host_type == \"default\"\n\n- set_fact:\n    ec2_instance_type: \"{{ lookup('env', 'ec2_etcd_instance_type') | default(deployment_vars[deployment_type].type, true) }}\"\n    ec2_security_groups: \"{{ lookup('env', 'ec2_etcd_security_groups') | default(deployment_vars[deployment_type].security_groups, true) }}\"\n  when: host_type == \"etcd\" and sub_host_type == \"default\"\n\n- set_fact:\n    ec2_instance_type: \"{{ lookup('env', 'ec2_infra_instance_type') | default(deployment_vars[deployment_type].type, true) }}\"\n    ec2_security_groups: \"{{ lookup('env', 'ec2_infra_security_groups') | default(deployment_vars[deployment_type].security_groups, true) }}\"\n  when: host_type == \"node\" and sub_host_type == \"infra\"\n\n- set_fact:\n    ec2_instance_type: \"{{ lookup('env', 'ec2_node_instance_type') | default(deployment_vars[deployment_type].type, true) }}\"\n    ec2_security_groups: \"{{ lookup('env', 'ec2_node_security_groups') | default(deployment_vars[deployment_type].security_groups, true) }}\"\n  when: host_type == \"node\" and sub_host_type == \"compute\"\n\n- set_fact:\n    ec2_instance_type: \"{{ deployment_vars[deployment_type].type }}\"\n  when: ec2_instance_type is not defined\n- set_fact:\n    ec2_security_groups: \"{{ deployment_vars[deployment_type].security_groups }}\"\n  when: ec2_security_groups is not defined\n\n- name: Find amis for deployment_type\n  ec2_ami_find:\n    region: \"{{ deployment_vars[deployment_type].region }}\"\n    ami_id: \"{{ deployment_vars[deployment_type].image }}\"\n    name: \"{{ deployment_vars[deployment_type].image_name }}\"\n  register: ami_result\n\n- fail: msg=\"Could not find requested ami\"\n  when: not ami_result.results\n\n- set_fact:\n    latest_ami: \"{{ ami_result.results | oo_ami_selector(deployment_vars[deployment_type].image_name) }}\"\n    volume_defs:\n      etcd:\n        root:\n          volume_size: \"{{ lookup('env', 'os_etcd_root_vol_size') | default(25, true) }}\"\n          device_type: \"{{ lookup('env', 'os_etcd_root_vol_type') | default('gp2', true) }}\"\n          iops: \"{{ lookup('env', 'os_etcd_root_vol_iops') | default(500, true) }}\"\n      master:\n        root:\n          volume_size: \"{{ lookup('env', 'os_master_root_vol_size') | default(25, true) }}\"\n          device_type: \"{{ lookup('env', 'os_master_root_vol_type') | default('gp2', true) }}\"\n          iops: \"{{ lookup('env', 'os_master_root_vol_iops') | default(500, true) }}\"\n        docker:\n          volume_size: \"{{ lookup('env', 'os_docker_vol_size') | default(10, true) }}\"\n          device_type: \"{{ lookup('env', 'os_docker_vol_type') | default('gp2', true) }}\"\n          iops: \"{{ lookup('env', 'os_docker_vol_iops') | default(500, true) }}\"\n      node:\n        root:\n          volume_size: \"{{ lookup('env', 'os_node_root_vol_size') | default(85, true) }}\"\n          device_type: \"{{ lookup('env', 'os_node_root_vol_type') | default('gp2', true) }}\"\n          iops: \"{{ lookup('env', 'os_node_root_vol_iops') | default(500, true) }}\"\n        docker:\n          volume_size: \"{{ lookup('env', 'os_docker_vol_size') | default(32, true) }}\"\n          device_type: \"{{ lookup('env', 'os_docker_vol_type') | default('gp2', true) }}\"\n          iops: \"{{ lookup('env', 'os_docker_vol_iops') | default(500, true) }}\"\n\n- set_fact:\n    volumes: \"{{ volume_defs | oo_ec2_volume_definition(host_type, docker_vol_ephemeral | bool) }}\"\n\n- name: Launch instance(s)\n  ec2:\n    state: present\n    region: \"{{ deployment_vars[deployment_type].region }}\"\n    keypair: \"{{ deployment_vars[deployment_type].keypair }}\"\n    group: \"{{ deployment_vars[deployment_type].security_groups }}\"\n    instance_type: \"{{ ec2_instance_type }}\"\n    image: \"{{ deployment_vars[deployment_type].image }}\"\n    count: \"{{ instances | length }}\"\n    vpc_subnet_id: \"{{ deployment_vars[deployment_type].vpc_subnet }}\"\n    assign_public_ip: \"{{ deployment_vars[deployment_type].assign_public_ip }}\"\n    user_data: \"{{ lookup('template', '../templates/user_data.j2') }}\"\n    wait: yes\n    instance_tags:\n      created-by: \"{{ created_by }}\"\n      clusterid: \"{{ cluster }}\"\n      environment: \"{{ cluster_env }}\"\n      host-type: \"{{ host_type }}\"\n      sub-host-type: \"{{ sub_host_type }}\"\n    volumes: \"{{ volumes }}\"\n  register: ec2\n\n- name: Add Name tag to instances\n  ec2_tag: resource={{ item.1.id }} region={{ deployment_vars[deployment_type].region }} state=present\n  with_together:\n  - \"{{ instances }}\"\n  - \"{{ ec2.instances }}\"\n  args:\n    tags:\n      Name: \"{{ item.0 }}\"\n\n- set_fact:\n    instance_groups: >\n      tag_created-by_{{ created_by }}, tag_clusterid_{{ cluster }},\n      tag_environment_{{ cluster_env }}, tag_host-type_{{ host_type }},\n      tag_sub-host-type_{{ sub_host_type }}\n\n- set_fact:\n    node_label:\n      region: \"{{ deployment_vars[deployment_type].region }}\"\n      type: \"{{sub_host_type}}\"\n  when: host_type == \"node\"\n\n- set_fact:\n    node_label:\n      region: \"{{ deployment_vars[deployment_type].region }}\"\n      type: \"{{host_type}}\"\n  when: host_type != \"node\"\n\n- set_fact:\n    logrotate:\n    - name: syslog\n      path: |\n        /var/log/cron\n        /var/log/maillog\n        /var/log/messages\n        /var/log/secure\n        /var/log/spooler\"\n      options:\n      - daily\n      - rotate 7\n      - compress\n      - sharedscripts\n      - missingok\n      scripts:\n        postrotate: \"/bin/kill -HUP `cat /var/run/syslogd.pid 2> /dev/null` 2> /dev/null || true\"\n\n- name: Add new instances groups and variables\n  add_host:\n    hostname: \"{{ item.0 }}\"\n    ansible_ssh_host: \"{{ item.1.dns_name }}\"\n    ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n    ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    groups: \"{{ instance_groups }}\"\n    ec2_private_ip_address: \"{{ item.1.private_ip }}\"\n    ec2_ip_address: \"{{ item.1.public_ip }}\"\n    ec2_tag_sub-host-type: \"{{ sub_host_type }}\"\n    openshift_node_labels: \"{{ node_label }}\"\n    logrotate_scripts: \"{{ logrotate }}\"\n  with_together:\n  - \"{{ instances }}\"\n  - \"{{ ec2.instances }}\"\n\n- name: Add new instances to nodes_to_add group if needed\n  add_host:\n    hostname: \"{{ item.0 }}\"\n    ansible_ssh_host: \"{{ item.1.dns_name }}\"\n    ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n    ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    groups: nodes_to_add\n    ec2_private_ip_address: \"{{ item.1.private_ip }}\"\n    ec2_ip_address: \"{{ item.1.public_ip }}\"\n    openshift_node_labels: \"{{ node_label }}\"\n    logrotate_scripts: \"{{ logrotate }}\"\n  with_together:\n  - \"{{ instances }}\"\n  - \"{{ ec2.instances }}\"\n  when: oo_extend_env is defined and oo_extend_env | bool\n\n- name: Wait for ssh\n  wait_for: \"port=22 host={{ item.dns_name }}\"\n  with_items: \"{{ ec2.instances }}\"\n\n- name: Wait for user setup\n  command: \"ssh -o StrictHostKeyChecking=no -o PasswordAuthentication=no -o ConnectTimeout=10 -o UserKnownHostsFile=/dev/null {{ hostvars[item.0].ansible_ssh_user }}@{{ item.1.dns_name }} echo {{ hostvars[item.0].ansible_ssh_user }} user is setup\"\n  register: result\n  until: result.rc == 0\n  retries: 20\n  delay: 10\n  with_together:\n  - \"{{ instances }}\"\n  - \"{{ ec2.instances }}\"\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "a0fc192d72cf9ef286c3b9f84db10b31eb152f3d", "filename": "tasks/section_07.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - include: section_07_level1.yml\n    tags:\n      - section07\n      - level1\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7417cadc39ba6d8538219c7c1efb0ddb02341075", "filename": "roles/ansible/tower/manage-job-templates/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: tower\n  roles:\n  - role: ansible/tower/manage-job-templates\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "99515c8ab15674760dbe15dc7a7d2c023bb5adc4", "filename": "roles/ovirt-iso-uploader-conf/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n- name: Make sure that ovirt-iso-uploader installed\n  yum:\n    name: \"ovirt-iso-uploader\"\n    state: present\n\n- name: Set ovirt-iso-uploader parameters in config file\n  lineinfile:\n    dest: \"{{ ovirt_iso_uploader_conf }}\"\n    line: \"{{ item.key }}={{ item.val }}\"\n    regexp: \"^{{ item.key }} *=.*$\"\n    insertafter: EOF\n  when:\n    item.val != \"\"\n  with_items:\n    - { key: \"user\", val: \"{{ ovirt_iso_uploader_user }}\" }\n    - { key: \"passwd\", val: \"{{ ovirt_iso_uploader_password }}\" }\n    - { key: \"engine\", val: \"{{ ovirt_iso_uploader_engine }}\" }\n    - { key: \"cert-file\", val: \"{{ ovirt_iso_uploader_cert_file }}\" }\n    - { key: \"iso-domain\", val: \"{{ ovirt_iso_uploader_iso_domain }}\" }\n    - { key: \"nfs-server\", val: \"{{ ovirt_iso_uploader_nfs_server }}\" }\n    - { key: \"ssh-user\", val: \"{{ ovirt_iso_uploader_ssh_user }}\" }\n    - { key: \"ssh-port\", val: \"{{ ovirt_iso_uploader_ssh_port }}\" }\n    - { key: \"key-file\", val: \"{{ ovirt_iso_uploader_key_file }}\" }\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "6615146b08a7bcaa8b5b06fe4633507911c58c4d", "filename": "roles/openshift-management/tasks/prune-deployments.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: Prune Deployments\n  shell: oc adm prune deployments --keep-complete={{ openshift_prune_deployments_complete }}  --keep-failed={{ openshift_prune_deployments_failed }} --keep-younger-than={{ openshift_prune_deployments_keep_younger }} --orphans --confirm\n  environment:\n    KUBECONFIG: \"{{ kubeconfig }}\""}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "be66086fd2f6c257e084a81dd8c33cd038cf0ce2", "filename": "tasks/bug-tweaks.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Configuration to avoid 'Device or resource busy' (CentOS/RedHat)\n  include_tasks: bug-tweaks/bug-centos7-resource-busy.yml\n  when:\n    - _docker_os_dist == \"CentOS\" or _docker_os_dist == \"RedHat\"\n    - ansible_kernel is version_compare('4', '<')\n\n- name: Best effort handling to directlvm for Debian 8 to get uniform behavior across distributions\n  include_tasks: bug-tweaks/tweak-debian8-directlvm.yml\n  when:\n    - _docker_os_dist == \"Debian\"\n    - _docker_os_dist_major_version | int == 8\n    - docker_daemon_config['storage-opts'] is defined\n    - docker_daemon_config['storage-opts'] | select('match', '^dm.directlvm_device.+')\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "5f3ebf02789de8a9b53ae3f52bb2609285b842f2", "filename": "roles/config-quay-enterprise/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Restart quay service\n  systemd:\n    name: \"{{ quay_name }}\"\n    enabled: yes\n    state: restarted\n    daemon_reload: yes\n\n- name: restart firewalld\n  service:\n    name: firewalld\n    state: restarted\n\n- name: restart iptables\n  service:\n    name: iptables\n    state: restarted"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f0f3578bcf1fd6f002cb1c1f411fbdac8e987658", "filename": "roles/config-quay-builder/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# Base Configurations\nquay_builder_name: quay-builder\nquay_builder_service: \"{{ quay_builder_name }}.service\"\n\n#Systemd\nsystemd_service_dir: /usr/lib/systemd/system\nsystemd_environmentfile_dir: /etc/sysconfig\n\n# Quay Builder\nquay_builder_image: quay.io/coreos/quay-builder:v2.9.3\nquay_builder_config_dir: /var/lib/quay-builder/config\nquay_builder_ssl_trust_configure: False\nquay_builder_ssl_trust_src_file: /tmp/quay-builder-ssl-trust.crt\nquay_builder_ssl_trust_host_file: \"{{ quay_builder_config_dir }}/ca.crt\"\nquay_builder_ssl_trust_container_file: /usr/local/share/ca-certificates/rootCA.pem\n\n# Container Credentials\ncontainer_credentials_file: /root/.docker/config.json\ncontainer_credentials_file_content: {}\nquay_registry_server: quay.io\nquay_registry_auth:\nquay_registry_email:\n\n# Quay\nquay_enterprise_hostname: \"\""}, {"commit_sha": "8d4956fcd97d78caa57ee3e5a36e9c44a23ab2a6", "sha": "29acfcd4e933ef9cf1f6be2f1a72eab211bde693", "filename": "tasks/install-docker.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Set version string\n  set_fact:\n    _docker_version_string: \"{{ docker_os_pkg_version_separator[_docker_os_dist] }}{{ docker_version }}\"\n  when: docker_version != ''\n\n- name: Set packages state to latest\n  set_fact:\n    _docker_pkg_state: 'latest'\n  when: docker_latest_version | bool and docker_version == ''\n\n- name: Filter out packages to match older Docker CE versions\n  set_fact:\n    _docker_packages:\n      - docker-ce\n  when:\n    - docker_version != ''\n    - docker_version is match('17.') or docker_version is match('18.03') or docker_version is match('18.06')\n\n- name: Ensure some kind of compatibility for no longer officially supported distributions since Docker CE 18.09\n  set_fact:\n    _docker_packages:\n      - docker-ce\n  when:\n    - _docker_packages is not defined\n    - (_docker_os_dist == \"Debian\" and _docker_os_dist_major_version | int < 9) or\n      (_docker_os_dist == \"Fedora\" and _docker_os_dist_major_version | int < 27) or\n      (_docker_os_dist == \"Ubuntu\" and _docker_os_dist_major_version | int < 16) or\n      (_docker_os_dist == \"Ubuntu\" and _docker_os_dist_major_version | int == 17) \n\n- name: Ensure Docker CE is installed\n  become: true\n  package:\n    name: \"{{ (item is search('docker')) | ternary((item + _docker_version_string | default('')), item) }}\"\n    state: \"{{ _docker_pkg_state | default('present') }}\"\n  loop: \"{{ _docker_packages | default(docker_packages) }}\"\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  notify: restart docker\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7ee05f1770af4e3b86b6ac0c3f8f4a62d6cf7065", "filename": "roles/dhcp/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- hosts: dhcp\n  become: yes\n\n  vars_files:\n  -  vars.yml\n\n#  build a DHCP server and configured it\n  pre_tasks:\n  - debug:\n      msg: \"Development Playbook to install a dhcp server and get it running\"\n\n  roles:\n  - dhcp\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "ad03902b984d9e0cbf6d12479da9006124779350", "filename": "roles/ovirt-collect-logs/tasks/dwh.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n- name: Link DWH logs\n  file:\n    src: \"{{ item.src }}\"\n    dest: \"{{ ovirt_collect_logs_tmp_dir }}/{{ item.dest }}\"\n    state: link\n  with_items:\n    - { src: \"/var/log/ovirt-engine-dwh\", dest: \"ovirt-engine-dwh-logs\" }\n    - { src: \"/var/lib/ovirt-engine-dwh\", dest: \"ovirt-engine-dwh-data\" }\n    - { src: \"/etc/ovirt-engine-dwh\", dest: \"ovirt-engine-dwh-etc\" }\n  ignore_errors: true\n\n- name: Dump DWH database\n  shell: \"su - postgres -c 'pg_dump ovirt_engine_history > {{ ovirt_collect_logs_tmp_dir }}/dwh_db.sql'\"\n  ignore_errors: true\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "498c2190813c229131da92947927df50bd92f53f", "filename": "tasks/replication/main.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n- name: INCLUDE | Replication Master\n  include: master.yml\n  when: mariadb_replication_master\n\n- name: INCLUDE | Replication slave\n  include: slave.yml\n  when: mariadb_replication_slave\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "2d7d53e19517bad7bb9c8f780dbae5cead3803c1", "filename": "roles/network/tasks/dnsmasq.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install dnsmasq\n  package:\n    name: dnsmasq\n    state: present\n\n#- name: Stop dnsmasq\n#  service:\n#    name: dnsmasq\n#    state: stopped\n\n#- name: Configure dnsmasq\n#  template:\n#    src: \"{{ item.src }}\"\n#    dest: \"{{ item.dest }}\"\n#    owner: root\n#    group: root\n#    mode: \"{{ item.mode }}\"\n#  with_items:\n##   - { src: 'network/dnsmasq.service.rh', dest: '/etc/systemd/system/dnsmasq.service', mode: '0644' }\n#   - { src: 'roles/network/templates/network/dnsmasq.conf.j2', dest: '/etc/dnsmasq.conf', mode: '644' }\n\n#- name: Start dnsmasq\n#  systemd:\n#    name: dnsmasq\n#    state: restarted\n#    enabled: yes\n#    daemon_reload: yes\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "70f0d581bf1d1352a858920bb6870c9d435d1753", "filename": "roles/config-postgresql/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nmode: containerized\n\npostgresql_name: postgresql\npostgresql_service: \"{{ postgresql_name }}.service\"\n\n#Systemd\nsystemd_service_dir: /usr/lib/systemd/system\nsystemd_environmentfile_dir: /etc/sysconfig\n\n# Postgresql\npostgresql_image: registry.access.redhat.com/rhscl/postgresql-96-rhel7:latest\npostgresql_storage_dir: /var/lib/{{ postgresql_name }}\npostgresql_container_storage_dir: /var/lib/pgsql/data\npostgresql_database: sampledb\npostgresql_container_port: 5432\npostgresql_host_port: 5432\n\n# These Values will be randomaly generated if not defined\n#postgresql_admin_user: postgresqladmin\n#postgresql_username: postgresql\n#postgresql_password: postgresql\n#postgresql_admin_password: postgresqladmin\n\npostgresql_db_uri: \"postgresql://{{ postgresql_admin_user }}:{{ postgresql_admin_password }}@{{ database_service }}/{{ postgresql_database }}\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "9362d1528df466fd5c958a2c2e304504aa02abbc", "filename": "archive/roles/cicd/tasks/jenkins.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n    \n- name: Install Jenkins Repository\n  get_url:\n    url: \"{{ jenkins_repo_url }}\"\n    dest: /etc/yum.repos.d/jenkins.repo\n  tags: jenkins\n\n- name: Add Jenkins GPG Key.\n  rpm_key:\n    state: present\n    key: \"{{ jenkins_repo_key_url }}\"\n  tags: jenkins\n\n- name: Install Jenkins.\n  yum:\n    pkg: jenkins\n    state: installed\n  tags: jenkins\n  \n- name: Copy Jenkins Configuration Files\n  copy:\n    src: jenkins/\n    dest: \"{{ jenkins_home_dir }}\"\n    group: \"{{ jenkins_group }}\"\n    owner: \"{{ jenkins_user }}\"\n  notify:\n  - restart jenkins\n  tags: jenkins\n  \n- name: Jenkins XML File\n  template:\n    src: jenkins-config.j2\n    dest: \"{{ jenkins_home_dir }}/config.xml\"\n    group: \"{{ jenkins_group }}\"\n    owner: \"{{ jenkins_user }}\"\n  notify:\n  - restart jenkins\n  tags: jenkins\n  \n  \n- name: Update Jenkins Service Configuration\n  lineinfile: \n    dest: /etc/sysconfig/jenkins\n    regexp: \"JENKINS_ARGS=\"\n    line: \"JENKINS_ARGS=\\\"--prefix=/jenkins\\\"\"\n  notify:\n  - restart jenkins\n  tags: jenkins\n  \n  \n- name: Install Jenkins Plugin\n  include: jenkins_install_plugins.yml \n  vars:\n    plugins:\n        - { name: \"github\", version: \"1.14.2\" }\n        - { name: \"github-api\", version: \"1.71\" }\n        - { name: \"git\", version: \"2.4.1\" }\n        - { name: \"git-client\", version: \"1.19.1\" }\n        - { name: \"git-server\", version: \"1.5\" }\n        - { name: \"credentials\", version: \"1.24\" }\n        - { name: \"scm-api\", version: \"1.0\" }\n        - { name: \"credentials-binding\", version: \"1.6\" }\n        - { name: \"plain-credentials\", version: \"1.1\" }\n        - { name: \"ace-editor\", version: \"1.0.1\" }\n        - { name: \"jquery-detached\", version: \"1.1.1\" }\n        - { name: \"workflow-basic-steps\", version: \"1.12\" }\n        - { name: \"workflow-scm-step\", version: \"1.12\" }\n        - { name: \"workflow-cps-global-lib\", version: \"1.12\" }\n        - { name: \"workflow-support\", version: \"1.12\" }\n        - { name: \"workflow-job\", version: \"1.12\" }\n        - { name: \"workflow-durable-task-step\", version: \"1.12\" }\n        - { name: \"workflow-cps\", version: \"1.12\" }\n        - { name: \"workflow-api\", version: \"1.12\" }\n        - { name: \"workflow-step-api\", version: \"1.12\" }\n        - { name: \"workflow-aggregator\", version: \"1.12\" }\n        - { name: \"parameterized-trigger\", version: \"2.30\" }\n        - { name: \"matrix-project\", version: \"1.6\" }\n        - { name: \"promoted-builds\", version: \"2.24.1\" }\n        - { name: \"ssh-credentials\", version: \"1.11\" }\n        - { name: \"mapdb-api\", version: \"1.0.6.0\" }\n        - { name: \"build-pipeline-plugin\", version: \"1.4.9\" }\n        - { name: \"jquery\", version: \"1.11.2-0\" }\n        - { name: \"delivery-pipeline-plugin\", version: \"0.9.8\" }\n        - { name: \"token-macro\", version: \"1.12.1\" }\n        - { name: \"ws-cleanup\", version: \"0.28\" }\n        - { name: \"job-dsl\", version: \"1.41\" }\n        - { name: \"cloudbees-folder\", version: \"5.1\" }\n        - { name: \"groovy\", version: \"1.27\" }\n        - { name: \"groovy-postbuild\", version: \"2.2.2\" }\n        - { name: \"script-security\", version: \"1.15\" }\n        - { name: \"durable-task\", version: \"1.7\" }\n        - { name: \"docker-plugin\", version: \"0.16.0\" }\n        - { name: \"openshift-pipeline\", version: \"1.0.4\" }\n        - { name: \"credentials\", version: \"1.24\", pinned: true }\n        - { name: \"cvs\", version: \"2.12\", pinned: true }\n        - { name: \"javadoc\", version: \"1.3\", pinned: true }\n        - { name: \"junit\", version: \"1.10\", pinned: true }\n        - { name: \"mailer\", version: \"1.16\", pinned: true }\n        - { name: \"matrix-auth\", version: \"1.2\", pinned: true }\n        - { name: \"matrix-project\", version: \"1.6\", pinned: true }\n        - { name: \"maven-plugin\", version: \"2.12.1\", pinned: true }\n        - { name: \"antisamy-markup-formatter\", version: \"1.3\", pinned: true }\n        - { name: \"script-security\", version: \"1.15\", pinned: true }\n        - { name: \"ssh-credentials\", version: \"1.11\", pinned: true }\n        - { name: \"ssh-slaves\", version: \"1.10\", pinned: true }\n        - { name: \"translation\", version: \"1.12\", pinned: true }\n        - { name: \"subversion\", version: \"2.5.5\", pinned: true }\n        - { name: \"windows-slaves\", version: \"1.1\", pinned: true }\n  notify:\n  - restart jenkins\n  tags: jenkins\n  \n- name: Create Docker Group\n  group:\n    name: docker\n    state: present\n  tags: jenkins\n    \n- name: Add Docker Group to Jenkins User\n  user:\n    name: \"{{ jenkins_user }}\"\n    groups: docker\n    append: yes\n  tags: jenkins\n  \n- name: Open Firewall for Jenkins\n  firewalld:\n    port: \"{{ item }}\"\n    zone: public\n    permanent: yes\n    immediate: yes\n    state: enabled\n  with_items:\n  - \"8080/tcp\"\n  - \"50000/tcp\"\n  tags: jenkins\n  \n- name: Enable Jenkins Service\n  service: \n    name: jenkins\n    enabled: true\n  tags: jenkins"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f86d55058107079340968c279efe2aed269e71d5", "filename": "roles/config-iscsi-client/tests/host_vars/node-1.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\niscsi_initiatorname: iqn.1994-05.com.example:node-1\n\ndisk_mapping:\n- lun: 0\n  vg: vg0\n  lv: lv0\n  mount_path: /mnt/vg0-lv0\n- lun: 1\n  vg: vg1\n  lv: lv0\n  mount_path: /var/vg1-lv0\n- lun: 2\n  vg: vg2\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "3554059adca4924e6a200a03c0e0183660116c91", "filename": "roles/config-nagios-target/tasks/enable-repos.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Enable the 'rhel-7-server-optional-rpms' repo\n  command: \"/usr/bin/subscription-manager repos --enable={{ item }}\"\n  with_items:\n  - rhel-7-server-optional-rpms \n"}, {"commit_sha": "3c8d04f3e0875a9baf1f1282f6665b2e7d6871a8", "sha": "ae644edfc69ef0c637a85ce59777ae10e8b1cd6c", "filename": "tasks/fail2ban-Debian.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\n- name: Install fail2ban.\n  package: name=fail2ban state=present\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "cfdeb9ca03b77c9c695ed0b432ea28b810e950f8", "filename": "roles/ansible/tower/manage-credentials/tasks/process-credential.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Get the org id based on the org name\"\n  set_fact:\n    org_id: \"{{ item.id }}\"\n  when:\n  - item.name|trim == credential.organization|trim\n  with_items:\n  - \"{{ existing_organizations_output.rest_output }}\"\n\n- name: \"Get the credential_type id based on the name\"\n  set_fact:\n    credential_type_id: \"{{ item.id }}\"\n  when:\n  - item.name|trim == credential.credential_type|trim\n  with_items:\n  - \"{{ existing_credential_types_output.rest_output }}\"\n\n- name: \"Load up the credential\"\n  uri:\n    url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/credentials/\"\n    user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n    password: \"{{ ansible_tower.admin_password }}\"\n    force_basic_auth: yes\n    method: POST\n    body: \"{{ lookup('template', 'credential.j2') }}\"\n    body_format: 'json'\n    headers:\n      Content-Type: \"application/json\"\n      Accept: \"application/json\"\n    validate_certs: no\n    status_code: 200,201,400\n\n- name: \"Clear/Update facts\"\n  set_fact:\n    org_id: ''\n    credential_type_id: ''\n    processed_credentials: \"{{ processed_credentials + [ { 'name': credential.name } ] }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "d274bcb645c3d6d9859e0b2883aa0437094355e6", "filename": "roles/config-satellite/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Configure Satellite'\n  hosts: satellite_servers\n  roles:\n  - role: config-satellite\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "4b4d28fe16337eeb2087be1edcf7eea30ddf5ef7", "filename": "roles/ajenti/tasks/ajenti-wondershaper.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install wondershaper ajenti plugin\n  pip: name=\"{{ iiab_download_url }}\"/ajenti-plugin-wondershaper-0.3.tar.gz\n  when: internet_available\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "6e8d7e7536b51f95e3638f05eb42353be3a003ed", "filename": "reference-architecture/vmware-ansible/playbooks/heketi-setup.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: crs\n  gather_facts: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - instance-groups\n\n- hosts: single_crs\n  gather_facts: yes\n  vars_files:\n  - vars/main.yaml\n  roles:\n  - instance-groups\n  - rhsm-subscription\n  - gluster-rhsm-repos\n  - heketi-install\n  - heketi-configure\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "86d5827a87ae81c48a9bb3ebbe11383470e7b1be", "filename": "roles/zookeeper/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "- name: Create zookeeper config file\n  template: src=zoo.cfg.j2 dest=/etc/zookeeper/conf/zoo.cfg\n  sudo: yes\n\n- name: Create zookeeper myid file\n  copy:\n    content: \"{{zookeeper_id}}\"\n    dest: /etc/zookeeper/conf/myid\n    mode: 0644\n  sudo: yes\n  notify:\n    - Start zookeeper\n\n- name: Set Zookeeper consul service definition\n  sudo: yes\n  template:\n    src: zookeeper-consul.j2\n    dest: \"{{ consul_dir }}/zookeeper.json\"\n  notify:\n    - Restart consul\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "87ec1d7fd48693c20b4c2932d22d037a54d7f4a8", "filename": "roles/cloud-scaleway/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- block:\n  - name: Include prompts\n    import_tasks: prompts.yml\n\n  - name: Set disk size\n    set_fact:\n      server_disk_size: 50000000000\n\n  - name: Check server size\n    set_fact:\n      server_disk_size: 25000000000\n    when: cloud_providers.scaleway.size == \"START1-XS\"\n\n  - name: Check if server exists\n    uri:\n      url: \"https://cp-{{ algo_region }}.scaleway.com/servers\"\n      method: GET\n      headers:\n        Content-Type: 'application/json'\n        X-Auth-Token: \"{{ algo_scaleway_token }}\"\n      status_code: 200\n    register: scaleway_servers\n\n  - name: Set server id as a fact\n    set_fact:\n      server_id: \"{{ item.id }}\"\n    no_log: true\n    when: algo_server_name == item.name\n    with_items: \"{{ scaleway_servers.json.servers }}\"\n\n  - name: Create a server if it doesn't exist\n    block:\n    - name: Get the organization id\n      uri:\n        url: https://account.cloud.online.net/organizations\n        method: GET\n        headers:\n          Content-Type: 'application/json'\n          X-Auth-Token: \"{{ algo_scaleway_token }}\"\n        status_code: 200\n      register: scaleway_organizations\n\n    - name: Set organization id as a fact\n      set_fact:\n        organization_id: \"{{ item.id }}\"\n      no_log: true\n      when: algo_scaleway_org == item.name\n      with_items: \"{{ scaleway_organizations.json.organizations }}\"\n\n    - name: Get total count of images\n      uri:\n        url: \"https://cp-{{ algo_region }}.scaleway.com/images\"\n        method: GET\n        headers:\n          Content-Type: 'application/json'\n          X-Auth-Token: \"{{ algo_scaleway_token }}\"\n        status_code: 200\n      register: scaleway_pages\n\n    - name: Get images\n      uri:\n        url: \"https://cp-{{ algo_region }}.scaleway.com/images?per_page=100&page={{ item }}\"\n        method: GET\n        headers:\n          Content-Type: 'application/json'\n          X-Auth-Token: \"{{ algo_scaleway_token }}\"\n        status_code: 200\n      register: scaleway_images\n      with_sequence: start=1 end={{ ((scaleway_pages.x_total_count|int / 100)| round )|int }}\n\n    - name: Set image id as a fact\n      include_tasks: image_facts.yml\n      with_items: \"{{ scaleway_images['results'] }}\"\n      loop_control:\n        loop_var: outer_item\n\n    - name: Create a server\n      uri:\n        url: \"https://cp-{{ algo_region }}.scaleway.com/servers/\"\n        method: POST\n        headers:\n          Content-Type: 'application/json'\n          X-Auth-Token: \"{{ algo_scaleway_token }}\"\n        body:\n          organization: \"{{ organization_id }}\"\n          name: \"{{ algo_server_name }}\"\n          image: \"{{ image_id }}\"\n          commercial_type: \"{{cloud_providers.scaleway.size }}\"\n          enable_ipv6: true\n          boot_type: local\n          tags:\n            - Environment:Algo\n            - AUTHORIZED_KEY={{ lookup('file', SSH_keys.public)|regex_replace(' ', '_') }}\n        status_code: 201\n        body_format: json\n      register: algo_instance\n\n    - name: Set server id as a fact\n      set_fact:\n        server_id: \"{{ algo_instance.json.server.id }}\"\n    when: server_id is not defined\n\n  - name: Power on the server\n    uri:\n      url: https://cp-{{ algo_region }}.scaleway.com/servers/{{ server_id }}/action\n      method: POST\n      headers:\n        Content-Type: application/json\n        X-Auth-Token: \"{{ algo_scaleway_token }}\"\n      body:\n        action: poweron\n      status_code: 202\n      body_format: json\n    ignore_errors: true\n    no_log: true\n\n  - name: Wait for the server to become running\n    uri:\n      url: \"https://cp-{{ algo_region }}.scaleway.com/servers/{{ server_id }}\"\n      method: GET\n      headers:\n        Content-Type: 'application/json'\n        X-Auth-Token: \"{{ algo_scaleway_token }}\"\n      status_code: 200\n    until:\n      - algo_instance.json.server.state is defined\n      - algo_instance.json.server.state == \"running\"\n    retries: 20\n    delay: 30\n    register: algo_instance\n\n  - set_fact:\n      cloud_instance_ip: \"{{ algo_instance['json']['server']['public_ip']['address'] }}\"\n      ansible_ssh_user: root\n\n  rescue:\n  - debug: var=fail_hint\n    tags: always\n  - fail:\n    tags: always\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "40825e9a0214068afaa70ee18933e3940feb913a", "filename": "roles/config-idm-server/tasks/configure_idm.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: 'Configure initial IdM setup'\n  command: > \n    ipa-server-install -U \n      --hostname=\"{{ idm_master_hostname | default(ansible_fqdn) }}\"\n      --domain=\"{{ idm_domain }}\"\n      --realm=\"{{ idm_realm }}\"\n      --ds-password=\"{{ idm_dm_password }}\"\n      --admin-password=\"{{ idm_admin_password }}\"\n  ignore_errors: true\n  when:\n  - idm_src is not defined \n\n- name: 'Add IdM client for replica'\n  command: > \n    ipa-client-install -U\n      --domain=\"{{ idm_domain }}\"\n      --server=\"{{ idm_src }}\"\n      --principal=\"{{ idm_principal }}\"\n      --password=\"{{ idm_admin_password }}\"\n      --force-join\n  when:\n  - idm_src is defined \n\n- name: 'Install and Configure IdM replica'\n  command: > \n    ipa-replica-install -U \n      --principal=\"{{ idm_principal }}\" \n      --admin-password=\"{{ idm_admin_password }}\"\n  ignore_errors: true\n  when:\n  - idm_src is defined \n\n- name: 'Ensure IdM is running at boot'\n  service:\n    name: ipa\n    enabled: yes\n\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "c630b20e31dc3b0b6daf661d940903e2a572c6df", "filename": "tasks/section_13_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 13.1 Ensure Password Fields are Not Empty (Scored)\n    command: awk -F':' '($2 == \"\" ) { print $1 }' /etc/shadow\n    register: awk_empty_shadow\n    changed_when: False\n    failed_when: awk_empty_shadow.stdout != '' and not lock_shadow_accounts\n    tags:\n      - section13\n      - section13.1\n\n  - name: 13.1 Ensure Password Fields are Not Empty (locking accounts) (Scored)\n    command: passwd -l '{{ item }}'\n    with_items:\n        awk_empty_shadow.stdout_lines\n    when: lock_shadow_accounts\n    tags:\n      - section13\n      - section13.1\n\n  - name: 13.2 Verify No Legacy \"+\" Entries Exist in /etc/passwd File (Scored)\n    command: grep '^+:' /etc/passwd\n    register: plus_pass\n    failed_when: plus_pass.rc == 0\n    changed_when: plus_pass.rc == 0\n    tags:\n      - section13\n      - section13.2\n\n  - name: 13.3 Verify No Legacy \"+\" Entries Exist in /etc/shadow File (Scored)\n    command: grep '^+:' /etc/shadow\n    register: plus_shadow\n    failed_when: plus_shadow.rc == 0\n    changed_when: plus_shadow.rc == 0\n    tags:\n      - section13\n      - section13.3\n\n  - name: 13.4 Verify No Legacy \"+\" Entries Exist in /etc/group File (Scored)\n    command: grep '^+:' /etc/group\n    register: plus_group\n    failed_when: plus_group.rc == 0\n    changed_when: plus_group.rc == 0\n    tags:\n      - section13\n      - section13.4\n\n  - name: 13.5 Verify No UID 0 Accounts Exist Other Than root (Scored)\n    command: awk -F':' '($3 == 0) { print $1 }' /etc/passwd\n    register: uid_zero_root\n    changed_when: False\n    failed_when: uid_zero_root.stdout != 'root'\n    tags:\n      - section13\n      - section13.5\n\n  - name: 13.6.1 Ensure root PATH Integrity (empty value) (Scored)\n    shell: 'echo $PATH | grep ::'\n    register: path_colon\n    changed_when: False\n    failed_when: path_colon.rc == 0\n    tags:\n      - section13\n      - section13.6\n\n  - name: 13.6.2 Ensure root PATH Integrity (colon end) (Scored)\n    shell: 'echo $PATH | grep :$'\n    register: path_colon_end\n    changed_when: False\n    failed_when: path_colon_end.rc == 0\n    tags:\n      - section13\n      - section13.6\n\n  - name: 13.6.3 Ensure root PATH Integrity (dot in path) (Scored)\n    shell: \"echo $PATH | sed -e 's/::/:/' -e 's/:$//' -e 's/:/\\\\n/g'\"\n    register: dot_in_path\n    changed_when: False\n    failed_when: '\".\" in dot_in_path.stdout_lines'\n    tags:\n      - section13\n      - section13.6\n\n  - name: 13.6.4 Ensure root PATH Integrity (Scored)\n    file: >\n        path='{{ item }}'\n        state=directory\n        owner=root\n        mode='o-w,g-w'\n    with_items:\n        dot_in_path.stdout_lines\n    tags:\n      - section13\n      - section13.6\n\n  - name: 13.7.1 Check Permissions on User Home Directories (gather users) (Scored)\n    shell: /bin/egrep -v '(root|halt|sync|shutdown|false)' /etc/passwd | /usr/bin/awk -F':' '($7 != \"/usr/sbin/nologin\") { print $6 }'\n    register: home_users\n    changed_when: False\n    failed_when: False\n    tags:\n      - section13\n      - section13.7\n\n  - name: 13.7.2 Check Permissions on User Home Directories (Scored)\n    file: >\n        path='{{ item }}'\n        mode='g-w,o-rwx'\n        state=directory\n    with_items:\n        home_users.stdout_lines\n    when: modify_user_homes == True\n    tags:\n      - section13\n      - section13.7\n\n  - name: 13.8.1 Check User Dot File Permissions (gather dotfiles) (Scored)\n    shell: for pth in `/bin/egrep -v '(root|halt|sync|shutdown)' /etc/passwd | /usr/bin/awk -F':' '($7 != \"/usr/sbin/nologin\") { print $6 }'`; do ls -d -A -1 $pth/.* | egrep -v '[..]$'; done\n    changed_when: False\n    failed_when: False\n    always_run: True\n    register: home_dot_files\n    tags:\n      - section13\n      - section13.8\n\n  - name: 13.8.2 Check User Dot File Permissions (Scored)\n    file: >\n        path='{{ item }}'\n        mode='o-w,g-w'\n    with_items:\n        home_dot_files.stdout_lines\n    tags:\n      - section13\n      - section13.8\n\n  - name: 13.9 Check Permissions on User .netrc Files (Scored)\n    file: >\n        path='{{ item }}/.netrc'\n        mode='g-rwx,o-rwx'\n        recurse=yes\n        state=directory\n    with_items:\n        home_users.stdout_lines\n    tags:\n      - section13\n      - section13.9\n\n  - name: 13.10 Check for Presence of User .rhosts Files (Scored)\n    file: >\n        state=absent\n        path='{{ item }}/.rhosts'\n    with_items:\n        home_users.stdout_lines\n    tags:\n      - section13\n      - section13.10\n\n  - name: 13.11 Check Groups in /etc/passwd (preparation) (Scored)\n    command: cut -s -d':' -f4 /etc/passwd\n    register: groups_id_cut\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.11\n\n  - name: 13.11 Check Groups in /etc/passwd (Scored)\n    command: grep -q -P \"^.*?:[^:]*:{{ item }}:\" /etc/group\n    with_items:\n        groups_id_cut.stdout_lines\n    register: groups_present\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.11\n\n  - name: 13.12 Check That Users Are Assigned Valid Home Directories (Scored)\n    stat: path='{{ item }}'\n    with_items:\n        home_users.stdout_lines\n    register: rstat\n    failed_when: rstat is defined and rstat.stat.isdir == False\n    always_run: True\n    tags:\n      - section13\n      - section13.12\n\n  - name: 13.13 Check User Home Directory Ownership (Scored)\n    debug: msg=\"*** Hardcore ***\"\n    tags:\n      - section13\n      - section13.13\n\n  - name: 13.14 Check for Duplicate UIDs (Scored)\n    shell: cut -f3 -d':' /etc/passwd | sort | uniq -d\n    register: uids_list\n    failed_when: uids_list.stdout != ''\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.14\n\n  - name: 13.15 Check for Duplicate GIDs (Scored)\n    shell: cut -f3 -d':' /etc/group | sort | uniq -d\n    register: gids_list\n    failed_when: gids_list.stdout != ''\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.15\n\n  - name: 13.16 Check for Duplicate User Names (Scored)\n    shell: cut -f1 -d':' /etc/passwd | sort | uniq -d\n    register: uids_list\n    failed_when: uids_list.stdout != ''\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.16\n\n  - name: 13.17 Check for Duplicate Group Names (Scored)\n    shell: cut -f1 -d':' /etc/group | sort | uniq -d\n    register: uids_list\n    failed_when: uids_list.stdout != ''\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.17\n\n  - name: 13.18 Check for Presence of User .netrc Files (stat) (Scored)\n    stat: path='{{ item }}/.netrc'\n    with_items: home_users.stdout_lines\n    register: netrc_files\n    tags:\n      - section13\n      - section13.18\n\n  - name: 13.18 Check for Presence of User .netrc Files (Scored)\n    file: >\n        path='{{ item }}'\n        state=absent\n    when: item is defined and item.stat.exists == True\n    with_items: netrc_files.results\n    tags:\n      - section13\n      - section13.18\n\n  - name: 13.19 Check for Presence of User .forward Files (Scored)\n    file: >\n        path='{{ item }}/.forward'\n        state=absent\n    with_items:\n        home_users.stdout_lines\n    tags:\n      - section13\n      - section13.19\n\n  - name: 13.20.1 Ensure shadow group is empty (Scored)\n    shell: grep '^shadow' /etc/group | cut -f4 -d':'\n    register: shadow_group_empty\n    failed_when: shadow_group_empty.stdout != ''\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.20\n      - section13.20.1\n\n  - name: 13.20.2 Ensure shadow group is empty (preparation) (Scored)\n    shell: grep '^shadow' /etc/group | cut -f1 -d':'\n    register: shadow_group_id\n    failed_when: False\n    changed_when: False\n    always_run: True\n    tags:\n      - section13\n      - section13.20\n      - section13.20.1\n\n  - name: 13.20.2 Ensure shadow group is empty (Scored)\n    shell: awk -F':' '($4 == \"{{ item }}\") { print }' /etc/passwd\n    register: awk_passwd_shadow\n    with_items:\n        shadow_group_id.stdout_lines\n    changed_when: False\n    failed_when: awk_passwd_shadow.stdout != ''\n    always_run: True\n    tags:\n      - section13\n      - section13.20\n      - section13.20.2\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "60a381501712819c3af73310067f1743eab23b80", "filename": "roles/haproxy/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for haproxy\n- name: assures {{ consul_template_dir }} dirs exists\n  file: path={{ consul_template_dir }}/{{ item.path }} state=directory\n  with_items:\n    - { path: 'config' }\n    - { path: 'templates' }\n\n- name: upload template config files\n  template: src=consul.cfg.j2 dest=\"{{ consul_template_dir }}/config/consul.cfg\" mode=0644\n  sudo: yes\n\n- name: upload static config files\n  copy: src={{ item.src }} dest=\"{{ consul_template_dir }}/{{ item.dst }}\" mode=0644\n  sudo: yes\n  with_items:\n    - { src: haproxy.cfg, dst: 'config/haproxy.cfg' }\n    - { src: haproxy.tmpl, dst: 'templates/haproxy.tmpl' }\n\n- name: run haproxy container\n  docker:\n    name: haproxy\n    image: \"{{ haproxy_image }}\"\n    state: started\n    net: host\n    restart_policy: always\n    ports:\n      - \"80:80\"\n    env:\n      HAPROXY_DOMAIN: \"{{ haproxy_domain }}\"\n      CONSUL_TEMPLATE_VERSION: \"{{ consul_template_version }}\"\n      CONSUL_LOGLEVEL: \"{{ consul_template_loglevel }}\"\n      CONSUL_CONNECT: \"{{ consul_backend }}\"\n      CONSUL_CONFIG: \"/config\"\n      SERVICE_NAME: haproxy\n    volumes:\n    - \"{{ consul_template_dir }}/config:/config\"\n    - \"{{ consul_template_dir }}/templates:/templates\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "3ed6aa5e1d514e01e68ad0a0eeab8ad3f19fdbef", "filename": "ops/playbooks/roles/windows_worker/tasks/main.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n\n    - name: Retrieve a token for the UCP API\n      uri:\n        url: \"https://{{ ARG_UCP_IP }}/auth/login\"\n        headers:\n          Content-Type: application/json\n        method: POST\n        status_code: 200\n        body_format: json\n        validate_certs: no\n        body: '{\"username\":\"{{ ARG_UCP_USER }}\",\"password\":\"{{ ARG_UCP_PASSWORD }}\"}'\n      delegate_to: localhost\n      register: resp\n      until: resp.status == 200\n      retries: 20\n      delay: 5\n\n    - name: Remember the API's token\n      set_fact:\n        auth_token:  \"{{resp.json.auth_token}}\"\n\n    - name: Is the node already in the swarm\n      uri:\n        url: 'https://{{ ARG_UCP_IP }}/nodes?filters={\"name\":{\"{{ inventory_hostname }}\":true}}'\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        validate_certs: no\n      delegate_to: localhost\n      register: resp\n\n    - set_fact:\n        swarm_member: \"{% if resp.json[0] is defined %}true{% else %}false{% endif %}\"  \n\n    - debug:\n        var: swarm_member\n      when: _debug is defined\n\n    - name: Retrieve a worker token\n      uri:\n        url: \"https://{{ ARG_UCP_IP }}/swarm\"\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      delegate_to: localhost\n      register: resp\n      when: swarm_member == false\n\n    - name: Memorize the swarm's token\n      set_fact:\n        token:  \"{{ resp.json.JoinTokens.Worker }}\"\n      when: swarm_member == false\n\n    - name: Add node to the swarm\n      win_command: \"docker swarm join --token {{ token }} {{ ARG_ADVERTIZE_IP }}\"\n      when: swarm_member == false\n\n    - name: Poll the status of the node\n      uri:\n        url: 'https://{{ ARG_UCP_IP }}/nodes/{{ inventory_hostname }}'\n        headers:\n          Content-Type: application/json\n          Authorization: Bearer {{ auth_token }}\n        method: GET\n        status_code: 200,404\n        body_format: json\n        validate_certs: no\n      delegate_to: localhost\n      register: resp\n      until: resp.status == 200 and resp.json.Spec.Role == \"worker\" and resp.json.Status.State == \"ready\"\n      delay: 10\n      retries:  \"{{ ( worker_join_delay  / 10 ) | int }}\"\n\n    - debug: msg=\"Availability={{resp.json.Spec.Availability}} Role={{resp.json.Spec.Role}} State={{resp.json.Status.State}}\"\n      when: _debug is defined\n#\n# set orchestrator type\n#\n    - name: Clear Orchestrator type \"swarm\"\n      command: \"docker node update --label-rm com.docker.ucp.orchestrator.swarm {{ inventory_hostname }}\"\n      when: orchestrator is defined and orchestrator == \"kubernetes\"\n      failed_when: false\n      delegate_to: \"{{ ucp_instance }}\"\n\n    - name: Set Ochestrator type Kubernetes\n      command: \"docker node update --label-add com.docker.ucp.orchestrator.kubernetes=true {{ inventory_hostname }}\"\n      when: orchestrator is defined and orchestrator == \"kubernetes\"\n      delegate_to: \"{{ ucp_instance }}\"\n\n    - name: Clear Orchestrator type \"Kubernetes\"\n      command: docker node update --label-rm com.docker.ucp.orchestrator.kubernetes {{ inventory_hostname }}\n      when: orchestrator is defined and orchestrator == \"swarm\"\n      failed_when: false\n      delegate_to: \"{{ ucp_instance }}\"\n\n    - name: Set Orchestrator type \"swarm\"\n      command: \"docker node update --label-add com.docker.ucp.orchestrator.swarm=true {{ inventory_hostname }}\"\n      when: orchestrator is defined and orchestrator == \"swarm\"\n      delegate_to: \"{{ ucp_instance }}\"\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "13953683f8f704f79cfd81a8f377a59f76128396", "filename": "playbooks/provision-satellite-server/generate-lvm-list.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Create a temporary list of device_name => storage_device mapping\"\n  set_fact:\n    tmp_device_mapping: \"{{ tmp_device_mapping | default([]) + [ {'name': item.1.name, 'device': item.1.device } ] }}\"\n  with_subelements:\n  - \"{{ hostvars['localhost'].os_servers.results }}\"\n  - server.volumes\n\n- name: \"Create the list of LVM devices / mounts\"\n  set_fact:\n    tmp_lvm_entries: \"{{ tmp_lvm_entries | default([]) + [ item.0 | combine({'storage_device': item.1.device}) ] }}\"\n  when:\n  - item.0.device_name == item.1.name\n  with_nested:\n  - \"{{ lvm_entries }}\"\n  - \"{{ tmp_device_mapping }}\"\n\n- name: \"Move tmp_lvm_entries back to lvm_entries\"\n  set_fact:\n    lvm_entries: \"{{ tmp_lvm_entries }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "d51403bd5cc6b8d53b0d8d068306e2148adb2327", "filename": "reference-architecture/gcp/ansible/playbooks/roles/deployment-delete/defaults/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ndeployment_name_with_prefix: '{{ prefix }}-{{ deployment_name }}'\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "222e57b3d6111f8884ac1cebcd0adc8246562962", "filename": "reference-architecture/gcp/ansible/playbooks/roles/registry-bucket-delete/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: delete registry bucket with all content\n  command: gsutil -m rm -r gs://{{ gcs_registry_bucket }}\n  ignore_errors: true\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "ab26083d8a3d9a1573f5fbab4b204c660100f36a", "filename": "playbooks/kubevirt.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "- hosts: localhost\n  connection: local\n  gather_facts: False\n  # unset http_proxy. required for running in the CI\n  environment:\n    http_proxy: \"\"\n  roles:\n    - role: kubevirt\n\n- import_playbook: \"{{ playbook_dir }}/storage.yml\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "12bcbdcc87b124d5039050aff75f2869e62b845e", "filename": "roles/owncloud/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# we need to install the rpm in order to get the dependencies\n# but we only need to do this the first time\n\n- name: add a repo def for ubuntu\n  template: dest=/etc/apt/sources.list.d/\n            src=owncloud.list\n  when: is_ubuntu\n\n- name: See if the owncloud startup page exists\n  stat: path={{ owncloud_prefix }}/owncloud/index.php\n  register: owncloud_page\n\n- name: Install owncloud package\n  package: name={{ item }}\n           state=present\n  with_items:\n      - curl\n      - owncloud\n  when: owncloud_page.stat.exists is defined and not owncloud_page.stat.exists\n\n- name: Remove owncloud package\n  package: name=owncloud\n           state=absent\n  when: owncloud_page.stat.exists is defined and not owncloud_page.stat.exists\n\n- name: remove config files for package\n  file: src=/etc/apache2/conf-available/owncloud.conf\n        dest=/etc/apache2/conf-enabled/owncloud.conf\n        state=absent\n\n- name: remove config files for package\n  file: path=/etc/apache2/conf-available/owncloud.conf\n        state=absent\n\n#- name: Remove /etc/owncloud to avoid confusion as we use the config in {{ owncloud_prefix }}/config/\n#  file: path=/etc/owncloud\n#        state=absent\n\n# but we use the tar file to get the latest version\n\n- name: Get the owncloud software\n  get_url: url={{ iiab_download_url }}/{{ owncloud_src_file }}  dest={{ downloads_dir }}/{{ owncloud_src_file }}\n  when: internet_available\n  async: 300\n  poll: 5\n\n- name: Copy it to permanent location /opt\n  unarchive: src={{ downloads_dir }}/{{ owncloud_src_file }}\n             dest={{ owncloud_prefix }}\n             creates={{ owncloud_prefix }}/owncloud/version.php\n  when: not is_F18\n\n# ansible 1.4.1 does not have \"creates\"\n- name: Copy it to permanent location /opt\n  unarchive: src={{ downloads_dir }}/{{ owncloud_src_file }}\n             dest={{ owncloud_prefix }}\n  when: is_F18\n\n- name: in Centos, the following config dir is symlink to /etc/owncloud\n  file: path=/etc/owncloud\n        state=directory\n\n- name: Add autoconfig file\n  template: src=autoconfig.php.j2\n            dest={{ owncloud_prefix }}/owncloud/config/autoconfig.php\n            owner={{ apache_user }}\n            group=apache\n            mode=0640\n\n- name: Make apache owner\n  file: path={{ owncloud_prefix }}/owncloud\n        owner={{ apache_user }}\n        group=apache\n        recurse=yes\n        state=directory\n\n- name: Create data directory library\n  file: path={{ item }}\n        mode=0750\n        owner={{ apache_user }}\n        group=apache\n        state=directory\n  with_items:\n    - \"{{ owncloud_data_dir }}\"\n\n- name: Create a mysql database for owncloud\n  mysql_db: name={{ owncloud_dbname }}\n  when: mysql_enabled and owncloud_enabled\n\n- name: Create a user to access the owncloud database\n  mysql_user: name={{ owncloud_dbuser }} host={{ item }} password={{ owncloud_dbpassword }} priv={{ owncloud_dbname }}.*:ALL,GRANT\n  with_items:\n        - \"{{ owncloud_dbhost }}\"\n        - 127.0.0.1\n        - ::1\n        - localhost\n  when: mysql_enabled and owncloud_enabled\n\n- name: Restart apache, so it picks up the new aliases\n  service: name={{ apache_service }} state=restarted\n  when: not owncloud_enabled\n\n# Enable owncloud by copying template to httpd config\n\n- include_tasks: owncloud_enabled.yml\n  when: owncloud_enabled\n\n- name: Add owncloud to service list\n  ini_file: dest='{{ service_filelist }}'\n            section=owncloud\n            option='{{ item.option }}'\n            value='{{ item.value }}'\n  with_items:\n    - option: name\n      value: owncloud\n    - option: description\n      value: '\"OwnCloud is a local server-based facility for sharing files, photos, contacts, calendars, etc.\"'\n    - option: path\n      value: \"{{ owncloud_prefix }}/owncloud\"\n    - option: source\n      value: \"{{ owncloud_src_file }}\"\n    - option: enabled\n      value: \"{{ owncloud_enabled }}\"\n"}, {"commit_sha": "92dabcd706e72a0dc15ce13086fb9d59f1a8760e", "sha": "c3824d42de5d5f3b36f67662c754792048b8b808", "filename": "meta/main.yml", "repository": "RocketChat/Rocket.Chat.Ansible", "decoded_content": "---\ngalaxy_info:\n  author:\n    - Calum MacRae\n    - Michael Goodwin\n  description: Deploy Rocket.Chat\n  license: MIT\n  min_ansible_version: 2.3.0\n  platforms:\n    - name: CentOS\n      versions:\n        - 7\n    - name: EL\n      versions:\n        - 7\n    - name: Fedora\n      versions:\n        - all\n    - name: Ubuntu\n      versions:\n        - trusty\n        - xenial\n    - name: Debian\n      versions:\n        - jessie\n        - stretch\n  galaxy_tags:\n    - cloud\n    - database\n    - database:nosql\n    - networking\n    - packaging\n    - system\n    - web\n    - chat\ndependencies: []\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "c75b8a7b9391bce1a87476db74715b186b59c10b", "filename": "roles/wireguard/tasks/ubuntu.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: WireGuard repository configured\n  apt_repository:\n    repo: ppa:wireguard/wireguard\n    state: present\n  register: result\n  until: result is succeeded\n  retries: 10\n  delay: 3\n\n- name: WireGuard installed\n  apt:\n    name: wireguard\n    state: present\n    update_cache: true\n\n- name: WireGuard reload-module-on-update\n  file:\n    dest: /etc/wireguard/.reload-module-on-update\n    state: touch\n\n- name: Configure unattended-upgrades\n  copy:\n    src: 50-wireguard-unattended-upgrades\n    dest: /etc/apt/apt.conf.d/50-wireguard-unattended-upgrades\n    owner: root\n    group: root\n    mode: 0644\n\n- set_fact:\n    service_name: \"wg-quick@{{ wireguard_interface }}\"\n  tags: always\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "3d8ff93e77d3c49e27f612992cb20bef163f36c6", "filename": "roles/config-iscsi-client/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: iscsi\n  roles:\n  - role: config-iscsi-client\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5d6e1de48e49353c25e636eea67ae26046c5827b", "filename": "roles/notifications/md-to-html/tasks/convert_md_to_html.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Convert markdown(md) to HTML\"\n  shell: >\n    echo \"{{ markdown_content }}\" | pandoc -f markdown -t html\n  register: result\n\n- name: \"Store away converted Markdown(md) in a dict\"\n  set_fact:\n    md_to_html: \n      html_body_message: \"<html><body>{{ result.stdout }}</body></html>\"\n      html_message: \"{{ result.stdout }}\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "725b9fa3d3be4ae721d79487554e7cf44590d344", "filename": "playbooks/roles/docket/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# handlers file for rocknsm.docket\n- name: docket | cleanup csr on docket host\n  file:\n    path: \"{{docket_x509_key}}.csr\"\n    state: absent\n  when: \"{{ inventory_hostname in groups['docket'] | bool }}\"\n\n- name: docket | cleanup csr on sensor hosts\n  file:\n    path: \"{{steno_certs_dir}}/{{hostvars[item].inventory_hostname}}.csr\"\n    state: absent\n  with_items: \"{{ groups['docket'] }}\"\n  when:\n    - \"{{ inventory_hostname in groups['stenographer'] | bool}}\"\n\n- name: docket | restart redis\n  service:\n    name: redis\n    state: restarted\n  when: docket_enable | bool\n\n- name: docket | seed random key\n  lineinfile:\n    path: /etc/docket/prod.yml\n    regexp: 'XX_NOT_A_SECRET_XX'\n    line: \"SECRET_KEY: {{ docket_secret }}\"\n    state: present\n\n- name: docket | restart docket celery services\n  service:\n    name: \"{{ item }}\"\n    state: restarted\n  with_items:\n    - docket-celery-io\n    - docket-celery-query\n  when: docket_enable | bool\n\n- name: docket | restart docket uwsgi\n  service:\n    name: docket\n    state: restarted\n  when: docket_enable | bool\n\n- name: docket | restart lighttpd\n  service:\n    name: lighttpd\n    state: restarted\n  when: docket_enable | bool\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "36416fb2cd6cc1678057243cf2054cef344179ff", "filename": "roles/osp/admin-network/tasks/manage-subnets.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Set up subnet(s)\"\n  os_subnet:\n    cloud: \"{{ item.cloud | default(osp_default_cloud) | default(omit) }}\"\n    state: \"{{ item.state | default(osp_resource_state) | default('present') }}\"\n    network_name: \"{{ item.network_name }}\"\n    name: \"{{ item.name | default(omit) }}\"\n    cidr: \"{{ item.cidr }}\"\n    gateway_ip: \"{{ item.gateway_ip | default(omit) }}\"\n    dns_nameservers: \"{{ item.dns_nameservers }}\"\n    allocation_pool_start: \"{{ item.allocation_pool_start | default(omit) }}\"\n    allocation_pool_end: \"{{ item.allocation_pool_end | default(omit) }}\"\n    enable_dhcp: \"{{ item.enable_dhcp | default(omit) }}\"\n    project: \"{{ item.project | default(omit) }}\"\n  with_items:\n  - \"{{ osp_subnets | default([]) }}\"\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "eb7428b3fd1e8cb7f35a235666831f488b73dee0", "filename": "tasks/create_repo_pypi_group_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_pypi_group\n    args: \"{{ _nexus_repos_pypi_defaults|combine(item) }}\""}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "e62f7cc8cccbfd3a7558959b226d7d431dc50d97", "filename": "tasks/security_policy_fetch/s3.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Download security policy artifact from s3\"\n  aws_s3:\n    bucket: \"{{ java_unlimited_policy_transport_s3_bucket }}\"\n    object: \"{{ java_unlimited_policy_transport_s3_path }}\"\n    dest: \"{{ download_path }}/\\\n      {{ java_unlimited_policy_transport_s3_path|basename }}\"\n    aws_access_key: \"{{ transport_s3_aws_access_key }}\"\n    aws_secret_key: \"{{ transport_s3_aws_secret_key }}\"\n    mode: get\n  retries: 5\n  delegate_to: \"localhost\"\n  connection: \"local\"\n\n- name: \"Downloaded security policy artifact\"\n  set_fact:\n    security_policy_oracle_artifact: \"{{ download_path }}/\\\n      {{ java_unlimited_policy_transport_s3_path|basename }}\"\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "8d53e59abf9b0ad7fdbf9931bda2202b49eca264", "filename": "playbooks/provider/lago/defaults/main.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "inventory_file: \"inventory\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "191872775fd5b1297f6470810e0726397844236a", "filename": "roles/httpd/tasks/php-stem.yml", "repository": "iiab/iiab", "decoded_content": "# Fixes search @ http://box/modules/es-wikihow (popular with Spanish youth)\n# Source code: http://download.iiab.io/packages/php-stem.src.tar\n# June 2018 debugging & compilation thanks to Tim Moody & George Hunt\n# Original bug: https://github.com/iiab/iiab/issues/829\n\n#- name: Download php-stem.rpi.tar\n#  command: cd /; wget http://download.iiab.io/packages/php-stem.rpi.tar\n#  when: is_rpi\n\n#- name: Download php-stem.x86.tar\n#  command: cd /; wget http://download.iiab.io/packages/php-stem.x64.tar\n#  when: not is_rpi\n\n- name: Download & unpack php-stem.rpi.tar to / (rpi)\n  unarchive:\n    src: http://download.iiab.io/packages/php-stem.rpi.tar\n    dest: /\n    owner: root\n    group: root\n    #mode: ????\n    remote_src: yes\n  when: is_rpi\n\n- name: Download & unpack php-stem.x86.tar to / (not rpi)\n  unarchive:\n    src: http://download.iiab.io/packages/php-stem.x64.tar\n    dest: /\n    owner: root\n    group: root\n    #mode: ????\n    remote_src: yes\n  when: not is_rpi\n\n# No need to do this twice?  Happens later @ https://github.com/iiab/iiab/blob/master/roles/3-base-server/tasks/main.yml#L24-L28\n#- name: Restart apache2 / httpd\n#  service:\n#    name: \"{{ apache_service }}\"\n#    state: restarted\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "2dd3355e1b09e239b460c3d23fd292039f1ef95a", "filename": "playbooks/templates/ifcfg-monif.j2", "repository": "rocknsm/rock", "decoded_content": "TYPE=Ethernet\nBOOTPROTO=none\nIPV4_FAILURE_FATAL=no\nIPV6INIT=no\nIPV6_FAILURE_FATAL=no\nNAME={{ item }}\nDEVICE={{ item }}\nONBOOT=yes\nNM_CONTROLLED=no\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "34b44d443cef376d745cf6fa3eb7b5fc9a9b221b", "filename": "roles/gluster-crs-prerequisites/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Clear yum cache\n  command: \"yum clean all\"\n  ignore_errors: true\n\n- name: Install the required rpms\n  package:\n    name: \"{{ item }}\"\n    state: latest\n  with_items: \"{{ gluster_crs_required_packages }}\"\n\n- name: Stop firewalld\n  service:\n    name: firewalld\n    state: stopped\n    enabled: no\n\n- name: Start Glusterd and iptables\n  service:\n    name: \"{{ item }}\"\n    state: started\n    enabled: true\n  with_items:\n  - iptables\n  - glusterd\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "0e39d036a00738df147cff9e42c79342733e9735", "filename": "roles/config-nagios-target/tasks/nrpe_disk.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Ensure the check_disk.cfg file exists\n  file:\n    path: /etc/nrpe.d/check_disk.cfg\n    state: touch\n    mode: 0644\n\n- name: Add the check for the root disk\n  lineinfile:\n    dest: /etc/nrpe.d/check_disk.cfg\n    regexp: '^command.check_root_disk.=.+check_disk .*'\n    line: \"command[check_root_disk]=/usr/lib64/nagios/plugins/check_disk -w 20% -c 10% -p {{ item.device }}\"\n    state: present\n  with_items:\n  - \"{{ ansible_mounts }}\"\n  when: item.mount == '/'\n\n\n"}, {"commit_sha": "584d564218d6e2e63d3ecca157daf817fe4d533c", "sha": "1dd640e5461a1736062757ae8e75bd7b609443b7", "filename": "tasks/main.yml", "repository": "mikolak-net/ansible-raspi-config", "decoded_content": "---\n- name: update all packages\n  apt: update_cache=yes upgrade=dist\n  when: raspi_config_update_packages\n- include: setup_replace_user.yml\n  when: raspi_config_replace_user[\"name\"] != ''\n- include: security_check.yml\n- name: ensure filesystem is resized\n  expand_fs:\n  when: raspi_config_expanded_filesystem\n- name: ensure mem split\n  pi_boot_config: config_vals=gpu_mem={{raspi_config_memory_split_gpu}}\n  notify:\n    - apply raspi-config\n    - reboot\n- name: ensure correct CPU parameters for Pi2\n  ensure_pi2_oc:\n  args:\n    cpu_types: \"{{raspi_config_pi_cpu}}\"\n  when: raspi_config_ensure_optimal_cpu_params\n  notify:\n    - apply raspi-config\n    - reboot\n- name: set camera state\n  include: camera.yml\n- name: set additional config vars\n  pi_boot_config:\n  args:\n    config_vals: \"{{raspi_config_other_options}}\"\n  notify:\n    - apply raspi-config\n    - reboot"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "399f1d04fd8c5f66a970954013a0f77721aeb73a", "filename": "reference-architecture/vmware-ansible/playbooks/library/vmware_folder.py", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n# (c) 2017, Davis Phillips davis.phillips@gmail.com\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\nANSIBLE_METADATA = {'status': ['preview'],\n                    'supported_by': 'community',\n                    'version': '1.0'}\n\nDOCUMENTATION = '''\n---\nmodule: vmware_folder\nshort_description: Add/remove folders to/from vCenter\ndescription:\n    - This module can be used to add/remove a folder to/from vCenter\nversion_added: 2.3\nauthor: \"Davis Phillips (@dav1x)\"\nnotes:\n    - Tested on vSphere 6.5\nrequirements:\n    - \"python >= 2.6\"\n    - PyVmomi\noptions:\n    datacenter:\n        description:\n            - Name of the datacenter to add the host\n        required: True\n    cluster:\n        description:\n            - Name of the cluster to add the host\n        required: True\n    folder:\n        description:\n            - Folder name to manage\n        required: True\n    hostname:\n        description:\n            - ESXi hostname to manage\n        required: True\n    username:\n        description:\n            - ESXi username\n        required: True\n    password:\n        description:\n            - ESXi password\n        required: True\n    state:\n        description:\n            - Add or remove the folder\n        default: 'present'\n        choices:\n            - 'present'\n            - 'absent'\nextends_documentation_fragment: vmware.documentation\n'''\n\nEXAMPLES = '''\n# Create a folder\n  - name: Add a folder to vCenter\n    vmware_folder:\n      hostname: vcsa_host\n      username: vcsa_user\n      password: vcsa_pass\n      datacenter: datacenter\n      cluster: cluster\n      folder: folder\n      state: present\n'''\n\nRETURN = \"\"\"\ninstance:\n    descripton: metadata about the new folder\n    returned: always\n    type: dict\n    sample: None\n\"\"\"\n\ntry:\n    from pyVmomi import vim, vmodl\n    HAS_PYVMOMI = True\nexcept ImportError:\n    HAS_PYVMOMI = False\n\nfrom ansible.module_utils.vmware import get_all_objs, connect_to_api, vmware_argument_spec, find_datacenter_by_name, \\\n    find_cluster_by_name_datacenter, wait_for_task\nfrom ansible.module_utils.basic import AnsibleModule\n\nclass VMwareFolder(object):\n    def __init__(self, module):\n        self.module = module\n        self.datacenter = module.params['datacenter']\n        self.cluster = module.params['cluster']\n        self.folder = module.params['folder']\n        self.hostname = module.params['hostname']\n        self.username = module.params['username']\n        self.password = module.params['password']\n        self.state = module.params['state']\n        self.dc_obj = None\n        self.cluster_obj = None\n        self.host_obj = None\n        self.folder_obj = None\n        self.folder_name = None\n        self.folder_expanded = None\n        self.folder_full_path = []\n        self.content = connect_to_api(module)\n\n    def find_host_by_cluster_datacenter(self):\n        self.dc_obj = find_datacenter_by_name(self.content, self.datacenter)\n        self.cluster_obj = find_cluster_by_name_datacenter(self.dc_obj, self.cluster)\n\n        for host in self.cluster_obj.host:\n            if host.name == self.hostname:\n                return host, self.cluster\n\n        return None, self.cluster\n\n    def select_folder(self, host):\n        fold_obj = None\n        self.folder_expanded = self.folder.split(\"/\")\n        last_e = self.folder_expanded.pop()\n        fold_obj = self.get_obj([vim.Folder],last_e)\n        if fold_obj:\n            return fold_obj\n        if fold_obj is None:\n            return fold_obj\n\n    def get_obj(self, vimtype, name, return_all = False):\n        obj = list()\n        container = self.content.viewManager.CreateContainerView(\n            self.content.rootFolder, vimtype, True)\n\n        for c in container.view:\n            if name in [c.name, c._GetMoId()]:\n                if return_all is False:\n                    return c\n                    break\n                else:\n                    obj.append(c)\n\n        if len(obj) > 0:\n            return obj\n        else:\n            # for backwards-compat\n            return None\n\n    def process_state(self):\n        try:\n            folder_states = {\n                'absent': {\n                    'present': self.state_remove_folder,\n                    'absent': self.state_exit_unchanged,\n                },\n                'present': {\n                    'present': self.state_exit_unchanged,\n                    'absent': self.state_add_folder,\n                }\n            }\n\n            folder_states[self.state][self.check_folder_state()]()\n\n        except vmodl.RuntimeFault as runtime_fault:\n            self.module.fail_json(msg = runtime_fault.msg)\n        except vmodl.MethodFault as method_fault:\n            self.module.fail_json(msg = method_fault.msg)\n        except Exception as e:\n            self.module.fail_json(msg = str(e))\n\n    def state_exit_unchanged(self):\n        self.module.exit_json(changed = False)\n\n    def state_remove_folder(self):\n        changed = True\n        result = None\n        self.folder_expanded = self.folder.split(\"/\")\n        f = self.folder_expanded.pop()\n        task = self.get_obj([vim.Folder],f).Destroy()\n\n        try:\n            success, result = wait_for_task(task)\n\n        except:\n            self.module.fail_json(msg = \"Failed to remove folder '%s' '%s'\" % (self.folder,folder))\n\n        self.module.exit_json(changed = changed, result = str(result))\n\n    def state_add_folder(self):\n        changed = True\n        result = None\n\n        self.dc_obj = find_datacenter_by_name(self.content, self.datacenter)\n        self.cluster_obj = find_cluster_by_name_datacenter(self.dc_obj, self.cluster)\n        self.folder_expanded = self.folder.split(\"/\")\n        index = 0\n        for f in self.folder_expanded:\n            if not self.get_obj([vim.Folder],f):\n                if index == 0:\n                #First object gets created on the datacenter\n                    task = self.dc_obj.vmFolder.CreateFolder(name=f)\n                else:\n                    parent_f = self.get_obj([vim.Folder],self.folder_expanded[index - 1])\n                    task = parent_f.CreateFolder(name=f)\n            index = index + 1\n\n        self.module.exit_json(changed = changed)\n\n    def check_folder_state(self):\n\n        self.host_obj, self.cluster_obj = self.find_host_by_cluster_datacenter()\n        self.folder_obj = self.select_folder(self.host_obj)\n\n        if self.folder_obj is None:\n            return 'absent'\n        else:\n            return 'present'\n\n\ndef main():\n    argument_spec = vmware_argument_spec()\n    argument_spec.update(dict(datacenter = dict(required = True, type = 'str'),\n                              cluster = dict(required = True, type = 'str'),\n                              folder = dict(required=True, type='str'),\n                              hostname = dict(required = True, type = 'str'),\n                              username = dict(required = True, type = 'str'),\n                              password = dict(required = True, type = 'str', no_log = True),\n                              state = dict(default = 'present', choices = ['present', 'absent'], type = 'str')))\n\n    module = AnsibleModule(argument_spec = argument_spec, supports_check_mode = True)\n\n    if not HAS_PYVMOMI:\n        module.fail_json(msg = 'pyvmomi is required for this module')\n\n    vmware_folder = VMwareFolder(module)\n    vmware_folder.process_state()\n\n\nif __name__ == '__main__':\n    main()\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "5d36991bb9e01a33bef2b469707578f516fa7ff1", "filename": "playbooks/openshift/stop-aws.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- hosts: localhost\n  roles:\n  - role: manage-aws-infra\n    operation: stopped\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "bc4261ce5091879cb24362154371d53c47b82d4f", "filename": "roles/cdi/templates/cdi-controller-deployment.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "apiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: cdi-deployment\nspec:\n  selector:\n    matchLabels:\n      app: cdi-controller\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: cdi-controller\n    spec:\n      containers:\n      - name: cdi-controller\n        image: {{ repo_tag }}/import-controller:{{ release_tag }}\n        imagePullPolicy: Always\n        env:\n          - name: OWN_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "6de2b666d21a276d3f9c3542b8096be694188887", "filename": "roles/idm-host-cert/tasks/print-certs.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Print Host Cert - if requested\"\n  debug:\n    msg: \"{{ host_cert.json.result.result.certificate }}\"\n    verbosity: 2\n\n- name: \"Print Host Key - if requested\"\n  debug:\n    msg: \"{{ csr_content.key }}\"\n    verbosity: 2\n\n- name: \"Print CA cert - if requested\"\n  debug:\n    msg: \"{{ ca_cert.json.result.result.certificate }}\"\n    verbosity: 2\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "f358d3e1951398fefaaf800529ca727c10cd2492", "filename": "roles/common/defaults/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\ninstall_headers: true\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "68afde415ac876dae36494cd1c72408984af35d8", "filename": "playbooks/provisioning/openstack/net_vars_check.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Check the provider network configuration\n  fail:\n    msg: \"Flannel SDN requires a dedicated containers data network and can not work over a provider network\"\n  when:\n    - openstack_provider_network_name is defined\n    - openstack_private_data_network_name is defined\n\n- name: Check the flannel network configuration\n  fail:\n    msg: \"A dedicated containers data network is only supported with Flannel SDN\"\n  when:\n    - openstack_private_data_network_name is defined\n    - not openshift_use_flannel|default(False)|bool\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "148b132062f4e9683a3326ff31d9db08a3170edd", "filename": "playbooks/manage-users/roles", "repository": "redhat-cop/infra-ansible", "decoded_content": "../../roles/"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "56e824ce4f12cc20d6f6439f1f0f51d2640ab459", "filename": "playbooks/enable-xfs-quotas.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- hosts: all\n  become: true\n  tasks:\n\n  - name: Discover facts about /data mount\n    set_fact:\n      rock_mounts:\n        mount: \"{{ item.mount }}\"\n        device: \"{{ item.device }}\"\n        size_total: \"{{ item.size_total }}\"\n    loop:\n      \"{{ ansible_mounts }}\"\n    when: (item.mount == default_mount and rock_mounts is not defined)\n\n  - debug:\n      msg: \"Unable to set quotas, ensure that default_mount exists and is defined\"\n    failed_when: rock_mounts.mount != default_mount\n    when: rock_mounts.mount != default_mount\n\n  - name: Enable project quotas on default_mount\n    mount:\n      path: \"{{ rock_mounts.mount }}\"\n      opts: defaults,prjquota\n      state: present\n      fstype: xfs\n      src: \"{{ rock_mounts.device }}\"\n    when: rock_mounts.mount == default_mount\n\n  - pause:\n      prompt: \"A reboot is required for the changes to take effect. Do you want to reboot now? (y/n)\"\n      echo: true\n    register: input\n\n  - name: Reboot\n    command: systemctl reboot\n    when: input.user_input  == 'y'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e567232a3d4aa0277a2d04b57e480f1384f7c2ba", "filename": "playbooks/services.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Configure DHCP'\n  hosts: dhcp_servers\n  roles:\n  - role: dhcp\n  tags:\n  - configure_dhcp\n\n- name: 'Configure DNS (internal)'\n  hosts: dns_servers_internal\n  roles:\n  - role: dns/config-dns-server\n  - role: dns/manage-dns-zones\n  tags:\n  - configure_dns_internal\n\n- name: 'Configure DNS (external)'\n  hosts: dns_servers_external\n  roles:\n  - role: dns/config-dns-server\n  - role: dns/manage-dns-zones\n  tags:\n  - configure_dns_external\n\n- name: 'Configure NTP / Chrony'\n  hosts: ntp_servers\n  roles:\n  - role: config-chrony\n  tags:\n  - configure_chrony\n\n- name: 'Configure IdM'\n  hosts: idm_servers\n  roles:\n  - role: idm\n  tags:\n  - configure_idm\n\n- name: 'Configure Satellite'\n  hosts: satellite_servers\n  roles:\n  - role: config-satellite\n  tags:\n  - configure_satellite\n\n- name: 'Configure www hosts'\n  hosts: www_servers\n  roles:\n  - role: config-httpd\n  tags:\n  - configure_www_hosts\n\n- name: 'Configure REPO hosts'\n  hosts: repo_servers\n  roles:\n  - role: config-software-src\n  - role: config-repo-server\n  tags:\n  - configure_repo_hosts\n\n- name: 'Configure PXE hosts'\n  hosts: pxe_servers\n  roles:\n  - role: config-pxe\n  tags:\n  - configure_pxe_hosts\n\n- name: 'Configure OpenVPN'\n  hosts: openvpn_servers\n  roles:\n  - role: config-openvpn\n  tags:\n  - configure_openvpn\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "4a61966eed84a4ce5c85fca4cc5c0a0b2e03cd41", "filename": "roles/git-server/handlers/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: restart httpd\n  service:\n    name: httpd\n    state: restarted\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "cd775814f31ad4261d9381515c3e69acfdecb76a", "filename": "roles/dns-server-detect/tasks/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- fail:\n    msg: 'Missing required private DNS server(s)'\n  when:\n    - external_nsupdate_keys['private'] is undefined\n    - hostvars[groups['dns'][0]] is undefined\n\n- fail:\n    msg: 'Missing required public DNS server(s)'\n  when:\n    - external_nsupdate_keys['public'] is undefined\n    - hostvars[groups['dns'][0]] is undefined\n\n- name: \"Set the private DNS server to use the external value (if provided)\"\n  set_fact:\n    private_dns_server: \"{{ external_nsupdate_keys['private']['server'] }}\"\n  when:\n    - external_nsupdate_keys['private'] is defined\n\n- name: \"Set the private DNS server to use the provisioned value\"\n  set_fact:\n    private_dns_server: \"{{ hostvars[groups['dns'][0]].private_v4 }}\"\n  when:\n    - private_dns_server is undefined\n\n- name: \"Set the public DNS server to use the external value (if provided)\"\n  set_fact:\n    public_dns_server: \"{{ external_nsupdate_keys['public']['server'] }}\"\n  when:\n    - external_nsupdate_keys['public'] is defined\n\n- name: \"Set the public DNS server to use the provisioned value\"\n  set_fact:\n    public_dns_server: \"{{ hostvars[groups['dns'][0]].public_v4 }}\"\n  when:\n    - public_dns_server is undefined\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "b2e388c26bd738fce8dd44b5effce45c4a08cc41", "filename": "playbooks/openshift/end-to-end.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n# Provision Openstack Instances\n- import_playbook: provision-instances.yml\n\n# Pre-Install Steps\n- import_playbook: pre-install.yml\n\n# Install\n- import_playbook: install.yml\n\n# Post Install\n- import_playbook: post-install.yml\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "7ea48de9318166e1a4bec931b567a5b229102e49", "filename": "roles/teamviewer/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Teamviewer exclude ARM and debian family\n  set_fact:\n    teamviewer_install: False\n    teamviewer_enabled: \"unavailable\"\n  when: ansible_architecture == \"armv7l\" or not is_redhat\n\n- name: Install Teamviewer if intel\n  include_tasks: install.yml\n  when: teamviewer_install\n\n- name: Add teamviewer to service list\n  ini_file: dest='{{ service_filelist }}'\n            section=teamviewer\n            option='{{ item.option }}'\n            value='{{ item.value }}'\n  with_items:\n    - option: name\n      value: teamviewer\n    - option: description\n      value: '\"TeamViewer - the All-In-One Software for Remote Support and Online Meetings\"'\n    - option: installed\n      value: \"{{ teamviewer_install }}\"\n    - option: enabled\n      value: \"{{ teamviewer_enabled }}\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "331163dc8cf0d0ed1754b80e443c81b51effc60c", "filename": "roles/kalite/tasks/install.yml", "repository": "iiab/iiab", "decoded_content": "# This is for an OS other than Fedora 18\n\n- name: Install missing packages required for KA Lite startup\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - python-virtualenv\n\n- name: Download KA Lite's requirements file\n  get_url:\n    url: \"{{ kalite_requirements }}\"\n    dest: \"{{ pip_packages_dir }}/kalite.txt\"\n    timeout: \"{{ download_timeout }}\"\n  when: internet_available\n\n#- name: Install KA Lite non-static + reqs file with pip - (debuntu)\n#  pip: requirements={{ pip_packages_dir }}/kalite.txt\n#       virtualenv={{ kalite_venv }}\n#       virtualenv_site_packages=no\n#       extra_args=\"--no-cache-dir\"\n#       extra_args=\"--disable-pip-version-check\"\n#  when: internet_available and is_debuntu\n\n- name: Install KA Lite static with pip (debuntu)\n  pip:\n    name: ka-lite-static\n    version: \"{{ kalite_version }}\"\n    virtualenv: \"{{ kalite_venv }}\"\n    virtualenv_site_packages: no\n    extra_args: \"--no-cache-dir\"\n#       extra_args=\"--disable-pip-version-check\"\n  when: internet_available and is_debuntu\n\n#- name: Install KA Lite non-static + reqs file with pip (OS's other than debuntu)\n#  pip: requirements={{ pip_packages_dir }}/kalite.txt\n#       virtualenv={{ kalite_venv }}\n#       virtualenv_site_packages=no\n#       extra_args=\"--no-cache-dir\"\n#       extra_args=\"--disable-pip-version-check\"\n#  when: internet_available and not is_debuntu\n\n- name: Install KA Lite static with pip (OS's other than debuntu)\n  pip:\n    name: ka-lite-static\n    version: \"{{ kalite_version }}\"\n    virtualenv: \"{{ kalite_venv }}\"\n    virtualenv_site_packages: no\n#       extra_args=\"--no-cache-dir\"\n#       extra_args=\"--disable-pip-version-check\"\n  when: internet_available and not is_debuntu\n\n- name: Default is to have cronserve started with KA Lite\n  set_fact:\n    job_scheduler_stanza: \"\"\n\n- name: Add --skip-job-scheduler to start if cronserve not enabled\n  set_fact:\n    job_scheduler_stanza: \"--skip-job-scheduler \"\n  when: not kalite_cron_enabled\n\n- name: Create 'kalite-serve' service, kalite.sh and kalite.conf\n  template:\n    backup: no\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: root\n    group: root\n    mode: \"{{ item.mode }}\"\n  with_items:\n    - { src: 'kalite-serve.service.j2', dest: '/etc/systemd/system/kalite-serve.service', mode: '0644'}\n    - { src: 'kalite.sh.j2', dest: '/usr/bin/kalite', mode: '0755'}\n    - { src: 'kalite.conf', dest: '/etc/{{ apache_config_dir }}', mode: '0644'}\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "2732fb26e03c950a482f3e207f4a3f77601150f6", "filename": "tasks/nexus_install.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n\n- name: Set detection method to fixed if we have a var\n  set_fact:\n    nexus_version_detected_from: fixed\n  when: nexus_version | length > 0\n\n- name: \"Check nexus-latest link stat in {{ nexus_installation_dir }}\"\n  stat:\n    path: \"{{ nexus_installation_dir }}/nexus-latest\"\n  register: nexus_latest_link\n  check_mode: no\n\n- name: Register current running version if any\n  set_fact:\n    nexus_version_running: >-\n      {{\n        nexus_latest_link.stat.lnk_target\n        | regex_replace('^.*/nexus-(\\d*\\.\\d*\\.\\d*-\\d*)', '\\1')\n      }}\n  when:\n    - nexus_latest_link.stat.exists | default(false)\n    - nexus_latest_link.stat.islnk | default(false)\n\n- name: No version given => Version detection\n  block:\n\n    - name: Register nexus_version from currently installed\n      # Note: setting nexus_version here skips the next block task.\n      set_fact:\n        nexus_version: \"{{ nexus_version_running }}\"\n        nexus_version_detected_from: installed\n      when:\n        - nexus_version_running is defined\n        - not (nexus_upgrade | default(false) | bool)\n\n    - name: Call latest nexus uri to get redirection\n      uri:\n        url: \"{{ nexus_download_url }}/latest-unix.tar.gz\"\n        method: CONNECT\n        status_code: 302\n        validate_certs: \"{{ nexus_download_ssl_verify | default(omit) }}\"\n      register: nexus_latest_uri_call\n      # No changes made, we only need the target uri. Safe for check mode and needed for next operations\n      check_mode: no\n\n    - name: Register nexus_version from latest nexus uri redirection\n      set_fact:\n        nexus_version: >-\n          {{\n            nexus_latest_uri_call.location\n            | regex_replace(\"^https://.*nexus-(\\d*\\.\\d*\\.\\d*-\\d*)-unix.tar.gz\", \"\\1\")\n          }}\n        nexus_version_detected_from: latest\n\n  when: nexus_version | length == 0\n\n- name: Print info about detected version to use\n  vars:\n    version_info: |-\n      Used version: {{ nexus_version }}\n      Version detected from: {{ nexus_version_detected_from }}\n      Upgrade allowed: {{ nexus_upgrade | default(false) | bool }}\n      Current running version: {{ nexus_version_running | default('none') }}\n  debug:\n    msg: \"{{ version_info.split('\\n') }}\"\n\n- name: Register nexus package name\n  set_fact:\n    nexus_package: \"nexus-{{ nexus_version }}-unix.tar.gz\"\n\n- name: Download nexus_package\n  get_url:\n    url: \"{{ nexus_download_url }}/{{ nexus_package }}\"\n    dest: \"{{ nexus_download_dir }}/{{ nexus_package }}\"\n    force: no\n    validate_certs: \"{{ nexus_download_ssl_verify | default(omit) }}\"\n  notify:\n    - nexus-service-stop\n\n- name: Ensure Nexus o/s group exists\n  group:\n    name: \"{{ nexus_os_group }}\"\n    state: present\n\n- name: Ensure Nexus o/s user exists\n  user:\n    name: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n    home: \"{{ nexus_os_user_home_dir }}\"\n    shell: \"/bin/bash\"\n    state: present\n\n- name: Ensure Nexus installation directory exists\n  file:\n    path: \"{{ nexus_installation_dir }}\"\n    state: \"directory\"\n\n- name: Unpack Nexus download\n  unarchive:\n    src: \"{{ nexus_download_dir }}/{{ nexus_package }}\"\n    dest: \"{{ nexus_installation_dir }}\"\n    creates: \"{{ nexus_installation_dir }}/nexus-{{ nexus_version }}\"\n    force: no\n    copy: false\n  notify:\n    - nexus-service-stop\n\n- name: Ensure proper ownership of nexus installation directory\n  file:\n    path: \"{{ nexus_installation_dir }}/nexus-{{ nexus_version }}\"\n    recurse: yes\n    mode: \"u=rwX,g=rX,o=rX\"\n\n- name: Update symlink nexus-latest\n  file:\n    path: \"{{ nexus_installation_dir }}/nexus-latest\"\n    src: \"{{ nexus_installation_dir }}/nexus-{{ nexus_version }}\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n    state: link\n    follow: false\n  register: nexus_latest_version\n  notify:\n    - nexus-service-stop\n\n- meta: flush_handlers\n\n- name: Delete unpacked data directory\n  file:\n    path: \"{{ nexus_installation_dir }}/nexus-latest/data\"\n    state: absent\n\n- name: Get path to default settings\n  set_fact:\n    nexus_default_settings_file: \"{{ nexus_installation_dir }}/nexus-latest/etc/org.sonatype.nexus.cfg\"\n  when: nexus_version is version_compare('3.1.0', '<')\n\n- name: Get path to default settings\n  set_fact:\n    nexus_default_settings_file: \"{{ nexus_installation_dir }}/nexus-latest/etc/nexus-default.properties\"\n  when: nexus_version is version_compare('3.1.0', '>=')\n\n- name: Get application settings directories\n  set_fact:\n    nexus_app_dir_settings_dirs:\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc\"\n  when: nexus_version is version_compare('3.1.0', '<')\n\n- name: Get application settings directories\n  set_fact:\n    nexus_app_dir_settings_dirs:\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/karaf\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/jetty\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/fabric\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/logback\"\n      - \"{{ nexus_installation_dir }}/nexus-latest/etc/scripts\"\n  when: nexus_version is version_compare('3.1.0', '>=')\n\n- name: Get rest API endpoint (v < 3.8.0)\n  set_fact:\n    nexus_rest_api_endpoint: \"service/siesta/rest/v1/script\"\n  when: nexus_version is version_compare('3.8.0', '<')\n\n- name: Get rest API endpoint (v >= 3.8.0)\n  set_fact:\n    nexus_rest_api_endpoint: \"service/rest/v1/script\"\n  when: nexus_version is version_compare('3.8.0', '>=')\n\n- name: Get path to database restore dir (v < 3.11.0)\n  set_fact:\n    nexus_db_restore_dir: \"{{ nexus_data_dir }}/backup\"\n  when: nexus_version is version_compare('3.11.0', '<')\n\n- name: Get path to database restore dir (v >= 3.11.0)\n  set_fact:\n    nexus_db_restore_dir: \"{{ nexus_data_dir }}/restore-from-backup\"\n  when: nexus_version is version_compare('3.11.0', '>=')\n\n- name: Allow nexus to create first-time install configuration files in  {{ nexus_installation_dir }}/nexus-latest/etc\n  file:\n    path: \"{{ item }}\"\n    state: \"directory\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n    mode: \"0755\"\n    recurse: false\n  with_items: \"{{ nexus_app_dir_settings_dirs }}\"\n  when: nexus_latest_version.changed\n  register: chown_config_first_time\n  tags:\n    # hard to run as a handler for time being\n    - skip_ansible_lint\n\n- name: Create Nexus data directory\n  file:\n    path: \"{{ nexus_data_dir }}\"\n    state: \"directory\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n\n- name: Setup Nexus data directory\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Dkaraf.data=.*\"\n    line: \"-Dkaraf.data={{ nexus_data_dir }}\"\n  notify:\n    - nexus-service-stop\n\n- name: Setup JVM logfile directory\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-XX:LogFile=.*\"\n    line: \"-XX:LogFile={{ nexus_data_dir }}/log/jvm.log\"\n  notify:\n    - nexus-service-stop\n\n- name: Setup Nexus default timezone\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Duser.timezone=.*\"\n    line: \"-Duser.timezone={{ nexus_timezone }}\"\n  notify:\n    - nexus-service-stop\n\n- name: Setup Nexus JVM min heap size\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Xms.*\"\n    line: \"-Xms{{ nexus_min_heap_size }}\"\n  notify: nexus-service-stop\n\n- name: Setup Nexus JVM max heap size\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Xmx.*\"\n    line: \"-Xmx{{ nexus_max_heap_size }}\"\n  notify: nexus-service-stop\n\n- name: Setup Nexus JVM max direct memory\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-XX:MaxDirectMemorySize=.*\"\n    line: \"-XX:MaxDirectMemorySize={{ nexus_max_direct_memory }}\"\n  notify: nexus-service-stop\n\n- name: Stop the admin wizard from running\n  lineinfile:\n    path: \"{{ nexus_default_settings_file }}\"\n    line: \"nexus.onboarding.enabled={{ nexus_onboarding_wizard }}\"\n    create: true\n  when: nexus_version is version_compare('3.17.0', '>=')\n\n- name: Create Nexus tmp directory\n  file:\n    path: \"{{ nexus_tmp_dir }}\"\n    state: \"directory\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n\n- name: Create Nexus backup directory\n  file:\n    path: \"{{ nexus_backup_dir }}\"\n    state: \"directory\"\n    owner: \"{{ nexus_os_user }}\"\n    group: \"{{ nexus_os_group }}\"\n  when: nexus_backup_dir_create | bool\n\n- name: Setup Nexus tmp directory\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.vmoptions\"\n    regexp: \"^-Djava.io.tmpdir=.*\"\n    line: \"-Djava.io.tmpdir={{ nexus_tmp_dir }}\"\n  notify:\n    - nexus-service-stop\n\n- name: Set NEXUS_HOME for the service user\n  lineinfile:\n    dest: \"{{ nexus_os_user_home_dir }}/.bashrc\"\n    regexp: \"^export NEXUS_HOME=.*\"\n    line: \"export NEXUS_HOME={{ nexus_installation_dir }}/nexus-latest\"\n  notify:\n    - nexus-service-stop\n\n- name: Set nexus user\n  lineinfile:\n    dest: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus.rc\"\n    regexp: \".*run_as_user=.*\"\n    line: \"run_as_user=\\\"{{ nexus_os_user }}\\\"\"\n  notify:\n    - nexus-service-stop\n\n- name: Set nexus port\n  lineinfile:\n    dest: \"{{ nexus_default_settings_file }}\"\n    regexp: \"^application-port=.*\"\n    line: \"application-port={{ nexus_default_port }}\"\n  notify:\n    - nexus-service-stop\n\n- name: Set nexus context path\n  lineinfile:\n    dest: \"{{ nexus_default_settings_file }}\"\n    regexp: \"^nexus-context-path=.*\"\n    line: \"nexus-context-path={{ nexus_default_context_path }}\"\n  notify:\n    - nexus-service-stop\n\n- name: \"Set nexus service listening ip to {{ nexus_application_host }}\"\n  lineinfile:\n    dest: \"{{ nexus_default_settings_file }}\"\n    regexp: \"^application-host=.*\"\n    line: \"application-host={{ nexus_application_host }}\"\n  notify:\n    - nexus-service-stop\n\n- name: Create systemd service configuration\n  template:\n    src: \"nexus.service\"\n    dest: \"/etc/systemd/system\"\n  notify:\n    - systemd-reload\n  when: \"ansible_service_mgr == 'systemd'\"\n\n- name: Create sysv service configuration\n  file:\n    path: \"/etc/init.d/nexus\"\n    src: \"{{ nexus_installation_dir }}/nexus-latest/bin/nexus\"\n    state: link\n  when: \"ansible_service_mgr != 'systemd'\"\n\n- block:\n    - name: \"Deploy backup restore script\"\n      template:\n        src: \"nexus-blob-restore.sh.j2\"\n        dest: \"{{ nexus_script_dir }}/nexus-blob-restore.sh\"\n        mode: 0755\n    - name: \"Symlink backup restore script to /sbin\"\n      file:\n        src: \"{{ nexus_script_dir }}/nexus-blob-restore.sh\"\n        dest: \"/sbin/nexus-blob-restore.sh\"\n        state: link\n  when: nexus_backup_configure | bool\n\n- name: 'Check if data directory is empty (first-time install)'\n  command: \"ls {{ nexus_data_dir }}\"\n  register: nexus_data_dir_contents\n  check_mode: no\n  changed_when: false\n\n- name: Clean cache for upgrade process\n  file:\n    path: \"{{ nexus_data_dir }}/clean_cache\"\n    state: touch\n  when: nexus_latest_version.changed and nexus_data_dir_contents.stdout | length > 0\n  tags:\n    # hard to run as a handler for time being\n    - skip_ansible_lint\n\n- meta: flush_handlers\n\n- name: Enable nexus systemd service and make sure it is started\n  systemd:\n    name: nexus.service\n    enabled: yes\n    state: started\n    no_block: yes\n  notify:\n    - wait-for-nexus\n    - wait-for-nexus-port\n  when: \"ansible_service_mgr == 'systemd'\"\n\n- name: Enable nexus sysv service and make sure it is started\n  service:\n    name: nexus\n    enabled: yes\n    state: started\n  notify:\n    - wait-for-nexus\n    - wait-for-nexus-port\n  when: \"ansible_service_mgr != 'systemd'\"\n\n- meta: flush_handlers\n\n- name: Chown configuration files from {{ nexus_installation_dir }}/nexus-latest/etc back to root\n  file:\n    path: \"{{ nexus_installation_dir }}/nexus-latest/etc\"\n    owner: \"root\"\n    group: \"root\"\n    mode: a=rX,u+w\n    recurse: true\n  when: chown_config_first_time.changed\n  tags:\n    # hard to run as a handler for time being\n    - skip_ansible_lint\n\n- name: Prevent nexus to create any new configuration files in  {{ nexus_installation_dir }}/nexus-latest/etc\n  file:\n    path: \"{{ item }}\"\n    state: \"directory\"\n    owner: \"root\"\n    group: \"root\"\n    mode: \"0755\"\n    recurse: false\n  with_items: \"{{ nexus_app_dir_settings_dirs }}\"\n\n- name: Install plugins from remote source\n  get_url:\n    url: \"{{ item }}\"\n    dest: \"{{ nexus_installation_dir }}/nexus-{{ nexus_version }}/deploy/\"\n  with_items: \"{{ nexus_plugin_urls }}\"\n\n- name: Access scripts API endpoint with defined admin password\n  uri:\n    url: \"{{ nexus_api_scheme }}://{{ nexus_api_hostname }}:{{ nexus_api_port }}\\\n      {{ nexus_api_context_path }}{{ nexus_rest_api_endpoint }}\"\n    method: 'HEAD'\n    user: 'admin'\n    password: \"{{ nexus_admin_password }}\"\n    force_basic_auth: yes\n    status_code: 200, 401\n    validate_certs: \"{{ nexus_api_validate_certs }}\"\n  register: nexus_api_head_with_defined_password\n  check_mode: no\n\n- name: Register defined admin password for next operations\n  set_fact:\n    current_nexus_admin_password: \"{{ nexus_admin_password }}\"\n  when: nexus_api_head_with_defined_password.status == 200\n  no_log: true\n\n- name: Check if admin.password file exists\n  stat:\n    path: \"{{ nexus_data_dir }}/admin.password\"\n  register: admin_password_file\n\n- name: Get generated admin password from file (nexus >= 3.17)\n  when:\n    - admin_password_file.stat.exists\n    - nexus_api_head_with_defined_password.status == 401\n    - nexus_version is version_compare('3.17.0', '>=')\n  block:\n    - name: Slurp content of remote generated password file\n      slurp:\n        src: \"{{ nexus_data_dir }}/admin.password\"\n      register: _slurpedpass\n\n    - name: Set default password from slurped content\n      set_fact:\n        nexus_default_admin_password: \"{{ _slurpedpass.content | b64decode }}\"\n\n- name: Access scripts API endpoint with default admin password\n  uri:\n    url: \"{{ nexus_api_scheme }}://{{ nexus_api_hostname }}:{{ nexus_api_port }}\\\n      {{ nexus_api_context_path }}{{ nexus_rest_api_endpoint }}\"\n    method: 'HEAD'\n    user: 'admin'\n    password: \"{{ nexus_default_admin_password }}\"\n    force_basic_auth: yes\n    status_code: 200, 401\n    validate_certs: \"{{ nexus_api_validate_certs }}\"\n  register: nexus_api_head_with_default_password\n  when: nexus_api_head_with_defined_password.status == 401\n\n- name: Register default admin password for next operations\n  set_fact:\n    current_nexus_admin_password: \"{{ nexus_default_admin_password }}\"\n  when: (nexus_api_head_with_default_password.status | default(false)) == 200\n\n- name: Ensure current Nexus password is known\n  fail:\n    msg: >-\n      Failed to determine current Nexus password\n      (it is neither the default/generated nor the defined password).\n      If you are trying to change nexus_admin_password after first\n      install, please set `-e nexus_default_admin_password=oldPassword`\n      on the ansible-playbook command line.\n      See https://github.com/ansible-ThoTeam/nexus3-oss/blob/master/README.md#change-admin-password-after-first-install\n  when: current_nexus_admin_password is not defined\n\n- name: Force (re-)registration of groovy scripts (purge reference dir)\n  file:\n    path: \"{{ nexus_data_dir }}/groovy-raw-scripts\"\n    state: absent\n  when: nexus_force_groovy_scripts_registration | default(false)\n\n- name: Create directories to hold current groovy scripts for reference\n  file:\n    path: \"{{ item }}\"\n    state: directory\n    owner: root\n    group: root\n  with_items:\n    - \"{{ nexus_data_dir }}/groovy-raw-scripts/current\"\n    - \"{{ nexus_data_dir }}/groovy-raw-scripts/new\"\n\n- name: Archive scripts\n  become: no\n  archive:\n    path: \"{{ role_path }}/files/groovy/*\"\n    dest: \"/tmp/nexus-upload-groovy-scripts.tar.gz\"\n  run_once: true\n  delegate_to: localhost\n\n- name: Upload new scripts\n  unarchive:\n    src: \"/tmp/nexus-upload-groovy-scripts.tar.gz\"\n    dest: \"{{ nexus_data_dir }}/groovy-raw-scripts/new/\"\n\n- block:\n    - name: Sync new scripts to old and get differences\n      shell: >\n        set -o pipefail &&\n        rsync -ric {{ nexus_data_dir }}/groovy-raw-scripts/new/ {{ nexus_data_dir }}/groovy-raw-scripts/current/\n        | cut -d\" \" -f 2 | sed \"s/\\.groovy//g\"\n      register: nexus_groovy_files_changed\n      check_mode: no\n      changed_when: false\n      # simple check on changed files kept on host\n      # skip ansible lint (we don't want to use synchronize module for this)\n      args:\n        warn: false\n        executable: /bin/bash\n  rescue:\n    - name: Fail with information on rsync error\n      fail:\n        msg: >-\n          A task involving running rsync on the host just failed, most probably because rsync is not installed.\n          Please make sure rsync is installed on your host or double check the above error and try again.\n\n- name: Declare new or changed groovy scripts in nexus\n  include_tasks: declare_script_each.yml\n  with_items: \"{{ nexus_groovy_files_changed.stdout_lines }}\"\n\n- name: Change admin password if we are still using default\n  block:\n    - include_tasks: call_script.yml\n      vars:\n        script_name: update_admin_password\n        args:\n          new_password: \"{{ nexus_admin_password }}\"\n\n    - name: Admin password changed\n      set_fact:\n        current_nexus_admin_password: \"{{ nexus_admin_password }}\"\n      no_log: true\n\n    - name: Clear generated password file from install (nexus > 3.17)\n      file:\n        path: \"{{ nexus_data_dir }}/admin.password\"\n        state: absent\n      when: nexus_version is version_compare('3.17.0', '>=')\n\n  when: (nexus_api_head_with_default_password.status | default(false)) == 200\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "8c0312fae4405e44ab29a46d663093d820a6f160", "filename": "roles/awstats/tasks/install.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install AWStats package\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - awstats\n    - pwauth\n    - openssl\n  tags:\n    - download\n\n- name: Install AWStats package (debuntu)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - libapache2-mod-authnz-external\n    - apache2-utils\n  when: is_debuntu\n  tags:\n    - download\n\n- name: Enable cgi execution (debuntu)\n  command: a2enmod cgi\n  when: is_debuntu\n\n- name: Create directory for AWStats to use as intermediate summary storage\n  file:\n    path: \"{{ item }}\"\n    mode: 0750\n    owner: \"{{ apache_user }}\"\n    group: \"{{ apache_user }}\"\n    state: directory\n    recurse: true\n    force: true\n  with_items:\n    - \"{{ awstats_data_dir }}\"\n    - \"{{ apache_log_dir }}\"\n\n- name: Install the Apache config for AWStats (debuntu)\n  template:\n    src: apache.conf\n    dest: \"/etc/{{ apache_config_dir }}/awstats.conf\"\n    owner: root\n    group: root\n    mode: 0644\n  when: awstats_enabled and is_debuntu\n\n- name: Install the Apache config for AWStats (OS's other than debuntu)\n  template:\n    src: apache-awstats.conf\n    dest: \"/etc/{{ apache_config_dir }}/awstats.conf\"\n    owner: root\n    group: root\n    mode: 0644\n  when: awstats_enabled and not is_debuntu\n\n- name: Make sure logrotate does not make logs unreadable (debuntu)\n  template:\n    src: logrotate.d.apache2\n    dest: /etc/logrotate.d/apache2\n  when: is_debuntu\n\n- name: See if AWStats package installed a config file\n  stat:\n    path: /etc/awstats/awstats.conf\n  register: awstats\n\n- name: If there was a config file installed by package, move it aside\n  command: mv /etc/awstats/awstats.conf /etc/awstats/awstats.conf.dist\n  when: awstats.stat.islnk is defined and not awstats.stat.islnk\n\n- name: Enable AWStats (debuntu)\n  file:\n    src: /etc/apache2/sites-available/awstats.conf\n    path: /etc/apache2/sites-enabled/awstats.conf\n    state: link\n  when: awstats_enabled and is_debuntu\n\n- name: Disable AWStats (debuntu)\n  file:\n    path: /etc/apache2/sites-enabled/awstats.conf\n    state: absent\n  when: not awstats_enabled and is_debuntu\n\n- name: Install the AWStats config\n  template:\n    src: awstats.schoolserver.conf.j2\n    dest: /etc/awstats/awstats.schoolserver.conf\n    owner: root\n    group: root\n    mode: 0644\n  when: awstats_enabled\n\n- name: Create a symbolic link to use when access is by IP address\n  file:\n    src: /etc/awstats/awstats.schoolserver.conf\n    dest: /etc/awstats/awstats.conf\n    state: link\n  when: awstats_enabled\n\n- name: On first enabling of AWStats, summarize httpd logs up to now (OS's other than debuntu)\n  shell: /bin/perl /usr/share/awstats/wwwroot/cgi-bin/awstats.pl -config=schoolserver -update\n  when: awstats_enabled and not is_debuntu\n\n- name: On first enabling of AWStats, summarize httpd logs up to now (debuntu)\n  shell: /usr/bin/perl /usr/lib/cgi-bin/awstats.pl -config=schoolserver -update\n  when: awstats_enabled and is_debuntu\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "0529bee6307520b74df035ac1fe784fc5615c043", "filename": "playbooks/aws-prerequisite.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: cluster_hosts\n  gather_facts: yes\n  become: yes\n  serial: 1\n  roles:\n  - role: aws-rhsm-subscription\n    when: deployment_type in [\"enterprise\", \"atomic-enterprise\", \"openshift-enterprise\"] and\n          ansible_distribution == \"RedHat\" and rhel_subscription_user is not defined\n\n- hosts: cluster_hosts\n  gather_facts: no\n  become: yes\n  roles:\n  - role: rhsm-repos\n    when: deployment_type in [\"enterprise\", \"atomic-enterprise\", \"openshift-enterprise\"] and\n          ansible_distribution == \"RedHat\" and rhel_subscription_user is not defined\n  - prerequisites\n\n- hosts: master\n  gather_facts: no\n  become: yes\n  roles:\n  - master-prerequisites\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "d740802a0b835fbca0a063b0d9ca11f7f43fc1c6", "filename": "reference-architecture/aws-ansible/playbooks/roles/cloudformation-infra/files/add-infra-node.json", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "{\n  \"AWSTemplateFormatVersion\": \"2010-09-09\",\n  \"Parameters\": {\n    \"KeyName\": {\n      \"Type\": \"AWS::EC2::KeyPair::KeyName\"\n    },\n    \"Route53HostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"AmiId\": {\n      \"Type\": \"AWS::EC2::Image::Id\"\n    },\n    \"InstanceType\": {\n      \"Type\": \"String\",\n      \"Default\": \"t2.medium\"\n    },\n    \"NodeRootVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"30\"\n    },\n    \"NodeDockerVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"NodeDockerVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"NodeUserData\": {\n      \"Type\": \"String\"\n    },\n    \"NodeEmptyVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"NodeEmptyVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"NodeRootVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"Subnet\": {\n      \"Type\": \"String\"\n    },\n    \"NodeName\": {\n      \"Type\": \"String\"\n    },\n    \"NodeInstanceProfile\": {\n      \"Type\": \"String\"\n    },\n    \"NodeType\": {\n      \"Type\": \"String\"\n    },\n    \"InfraSg\": {\n      \"Type\": \"String\"\n    },\n    \"NodeSg\": {\n      \"Type\": \"String\"\n    }\n  },\n  \"Resources\": {\n    \"Route53Records\": {\n      \"Type\": \"AWS::Route53::RecordSetGroup\",\n      \"DependsOn\": [\n        \"NewNode\"\n      ],\n      \"Properties\": {\n        \"HostedZoneName\": { \"Ref\": \"Route53HostedZone\" },\n        \"RecordSets\": [\n          {\n            \"Name\":  { \"Ref\": \"NodeName\" },\n            \"Type\": \"A\",\n\t            \"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"NewNode\", \"PrivateIp\"] }]\n          }\n        ]\n      }\n    },\n    \"NewNode\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InstanceType\"},\n\t  \"SecurityGroupIds\": [{\"Ref\": \"NodeSg\"}, {\"Ref\": \"InfraSg\"}],\n          \"SubnetId\" : {\"Ref\": \"Subnet\"},\n          \"IamInstanceProfile\": { \"Ref\": \"NodeInstanceProfile\" },\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Ref\": \"NodeName\"}\n            },\n            { \"Key\": \"provision\",\n              \"Value\": \"node\"\n            },\n            { \"Key\": \"openshift-role\",\n              \"Value\": {\"Ref\": \"NodeType\"}\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeDockerVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeDockerVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdc\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeEmptyVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeEmptyVolType\"}\n            }\n          }\n         ]\n     }\n   }\n }\n}\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "5d7a21613e9469e81e06d1e8bdce11a040d862ad", "filename": "roles/schooltool/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "schooltool_install: True\nschooltool_enabled: False\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "248426ea209789157979671437be1a6ae9e556f0", "filename": "reference-architecture/rhv-ansible/playbooks/roles/gdeployer/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Install gdeploy\n  package:\n    name: gdeploy\n    state: latest\n- name: Ensure root has an .ssh directory\n  file:\n    path: /root/.ssh\n    owner: root\n    group: root\n    mode: 0700\n    state: directory\n- name: Wipe existing known_hosts file\n  file:\n    path: /root/.ssh/known_hosts\n    state: absent\n- name: Gather ssh keys\n  shell: /usr/bin/ssh-keyscan -t ecdsa  {{ hostvars[ item ]['ansible_default_ipv4']['address'] }} >> /root/.ssh/known_hosts\n  with_items:\n    - \"{{ groups['hypervisors'] }}\"\n- name: Create gdeploy.conf from template\n  template:\n    src: gdeploy.conf\n    dest: /root/gdeploy.conf\n  notify: run gdeploy\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "489df12ab51f930355e12c31ed547660d2a441ef", "filename": "ops/playbooks/reconfigure_dtr.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- name: Reconfigure DTR\n  hosts: local\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - includes/internal_vars.yml\n#    - ../group_vars/backups\n\n  vars:\n    http_proxy_switch:  \"{% if  env.http_proxy is defined %} --http-proxy {{ env.http_proxy }} {% endif %}\"\n    https_proxy_switch:  \"{% if  env.https_proxy is defined %} --https-proxy {{ env.https_proxy }} {% endif %}\"\n    no_proxy_switch:  \"{% if  env.no_proxy is defined %} --no-proxy '{{ env.no_proxy }}' {% endif %}\"\n\n  environment:\n    - UCP_USERNAME: \"{{ ucp_username }}\"\n    - UCP_PASSWORD: \"{{ ucp_password }}\"\n    - UCP_CA: \"{{ ucp_ca_cert | default('') }}\"\n    - DTR_CA: \"{{ dtr_ca_cert | default('') }}\"\n    - DTR_CERT: \"{{ dtr_server_cert | default('') }}\"\n    - DTR_KEY: \"{{ dtr_server_key | default('') }}\"\n\n  pre_tasks:\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n\n  roles:\n    - role: get-dtr-replica\n      ARG_UCP_IP:        \"{{ ucp_instance }}.{{domain_name}}\"\n      ARG_UCP_USER:      \"{{ ucp_username }}\"\n      ARG_UCP_PASSWORD:  \"{{ ucp_password }}\"\n\n  tasks:\n\n\n#\n# note: existing_dtr_replica_id is returned by the role get-dtr-replica\n#\n\n#\n# Load Certificates\n#\n    - include_tasks: includes/load_certificates.yml\n\n#\n# We reconfigure the UCP endpoint, for some reason, using the LB fqdn does not work when we install/join DTR\n#\n\n    - name: Do we have UCP Load Balancer defined\n      set_fact:\n        ucp_url: \"{% if ucp_lb_fqdn | length > 0 %}https://{{ ucp_lb_fqdn }}{% else %}https://{{ ucp_instance }}.{{domain_name}}{% endif %}\"\n    - name: Do we have DTR Load Balancer defined\n      set_fact:\n        dtr_url: \"{% if dtr_lb_fqdn | length > 0 %}https://{{ dtr_lb_fqdn }}{% else %}https://{{ groups.dtr[0] }}.{{domain_name}}{% endif %}\"\n\n    - name: Reconfigure DTR\n      command: docker run \n        --env UCP_USERNAME \n        --env UCP_PASSWORD \n        {{ switch_env_ucp_ca }} {{  switch_env_dtr_certificates }}\n        docker/dtr:{{ dtr_version }} reconfigure\n          --ucp-url {{ ucp_url }}\n          --dtr-external-url {{ dtr_url }}\n          {{ switch_ucp_insecure }} --existing-replica-id {{ existing_dtr_replica_id }}\n      vars:\n        switch_ucp_insecure:         \"{% if ucp_ca_cert is defined %}{% else %}--ucp-insecure-tls{% endif %}\"\n        switch_env_ucp_ca:           \"{% if ucp_ca_cert is defined %}--env UCP_CA{% endif %}\"\n        switch_env_dtr_certificates: \"{% if dtr_ca_cert is defined %}--env DTR_CA --env DTR_CERT --env DTR_KEY{% endif %}\"\n      register: task_result\n      until: task_result.rc == 0\n      retries: 3\n      delay: 60\n      when: existing_dtr_replica_id != 0\n      delegate_to: \"{{ ucp_instance }}\" \n\n    - debug: var=task_result\n      when: _debug is defined\n#\n# find a working DTR instance\n#\n    - name: Find a DTR instance\n      include_tasks: includes/find_dtr.yml\n      vars:\n        ping_servers: \"{{ groups.dtr }}\"\n    - debug: var=dtr_instance\n#\n# Enable Image Scanning\n#\n    - name: Enable image scanning\n      uri:\n        url: \"https://{{ dtr_instance }}.{{ domain_name }}/api/v0/meta/settings\"\n        method: POST\n        user: \"{{ ucp_username }}\"\n        password: \"{{ ucp_password }}\"\n        body: {\"scanningEnabled\":true}\n        status_code: 202\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      when: dtr_instance != \".none.\"\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "a24e684cc895500bf7f4678ab30fe18227a2cbe3", "filename": "roles/openstack-stack/defaults/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n\nstack_state: 'present'\n\nssh_ingress_cidr: 0.0.0.0/0\nnode_ingress_cidr: 0.0.0.0/0\nmaster_ingress_cidr: 0.0.0.0/0\nlb_ingress_cidr: 0.0.0.0/0\nbastion_ingress_cidr: 0.0.0.0/0\nnum_etcd: 0\nnum_masters: 1\nnum_nodes: 1\nnum_dns: 1\nnum_infra: 1\nnodes_to_remove: []\netcd_volume_size: 2\ndns_volume_size: 1\nlb_volume_size: 5\nuse_bastion: False\nui_ssh_tunnel: False\nprovider_network: False\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b8020cb04cac0537d728999b26393ae7070bfcc2", "filename": "reference-architecture/vmware-ansible/playbooks/roles/vmware-guest-setup/templates/chrony.conf.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "# This file is managed by Ansible\n\nserver 0.rhel.pool.ntp.org\nserver 1.rhel.pool.ntp.org\nserver 2.rhel.pool.ntp.org\nserver 3.rhel.pool.ntp.org\n\ndriftfile /var/lib/chrony/drift\nmakestep 10 3\n\nkeyfile /etc/chrony.keys\ncommandkey 1\ngeneratecommandkey\n\nnoclientlog\nlogchange 0.5\n\nlogdir /var/log/chrony\nlog measurements statistics tracking\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "b1d1c57e38f60429d775ee951b92054ecf7945ee", "filename": "playbooks/cluster/kubernetes/config.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- hosts: all\n  remote_user: root\n  pre_tasks:\n    - name: override roles variables\n      include_vars: \"{{ item }}\"\n      with_items:\n        - \"{{ playbook_dir }}/vars/default_vars.yml\"\n  roles:\n    - kubernetes-prerequisites\n\n- hosts: masters\n  remote_user: root\n  roles:\n    - kubernetes-master\n\n- hosts: nodes\n  remote_user: root\n  roles:\n    - kubernetes-node\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9dad6927eb8c2d558ec0af3bf3f0c3b272795c1b", "filename": "roles/osp/admin-sec-group/test/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: all\n  connection: local\n  roles:\n  - role: osp/admin-sec-group\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "08ae57763d47882d3b40102a270f5c152efc77f4", "filename": "reference-architecture/vmware-ansible/playbooks/vars/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# OpenShift variables\nopenshift_master_cluster_hostname: \"{{ lb_host }}\"\nopenshift_master_cluster_public_hostname: \"{{ lb_host }}\"\nconsole_port: 8443\nmaster_tag: master\ninfra_tag: infra\napp_tag: app\nmaster_group_tag: \"tag_openshift-role_{{ master_tag }}\"\napp_group_tag: \"tag_openshift-role_{{ app_tag }}\"\ninfra_group_tag: \"tag_openshift-role_{{ infra_tag }}\"\nopenshift_major_version: 6\nopenshift_vers: v3_{{ openshift_major_version }}\nopenshift_ansible_branch: release-3.{{ openshift_major_version }}\nopenshift_required_repos:\n- rhel-7-server-rpms\n- rhel-7-server-extras-rpms\n- rhel-7-server-ose-3.{{ openshift_major_version }}-rpms\n- rhel-7-fast-datapath-rpms\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "0fb99ba00c4ac895f05d634ad50f836e3028c117", "filename": "roles/openshift-applier/tasks/copy-inventory-file.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: \"Create target directory to ensure it exists\"\n  file:\n    path: \"{{ tmp_inv_dir }}{{ file|dirname }}\"\n    state: directory\n  when:\n  - file|dirname != ''\n\n- name: \"Copy file to target directory\"\n  copy: \n    src: \"{{ file }}\"\n    dest: \"{{ tmp_inv_dir }}{{ file }}\"\n  failed_when: false\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "0d025fc07fc90e9fdd0e919f6351b96db450794f", "filename": "playbooks/facts/FreeBSD.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- set_fact:\n    config_prefix: \"/usr/local/\"\n    root_group: wheel\n    ssh_service_name: sshd\n    apparmor_enabled: false\n    strongswan_additional_plugins:\n      - kernel-pfroute\n      - kernel-pfkey\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "4735b21de16b144e5ae3ba153179b5b4052cf72f", "filename": "reference-architecture/gcp/ansible/playbooks/roles/rhel-image/defaults/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\nrhel_image_dir: '{{ rhel_image_path | dirname }}'\nrhel_image_raw: '{{ rhel_image_dir }}/disk.raw'\nrhel_image_archive: '{{ rhel_image_dir }}/{{ rhel_image }}.tar.gz'\nrhel_image_bucket: gs://{{ gcloud_project }}-rhel-guest-raw-image\nrhel_image_in_bucket: '{{ rhel_image_bucket }}/{{ rhel_image_archive | basename }}'\nrhel_image_gce_family: 'rhel-guest-clean'\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "1dbd812bd440e39315e012cc9396181ce3953bd9", "filename": "roles/ovirt-collect-logs/tasks/engine.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n\n- name: Prepare directory structure\n  file:\n    src: \"{{ item }}\"\n    dest: \"{{ ovirt_collect_logs_tmp_dir }}/{{ item }}\"\n    state: directory\n  with_items:\n    - \"etc\"\n    - \"httpd\"\n\n- name: ovirt-requests-logs files\n  shell: ls /var/log/httpd/ovirt-requests-log*\n  register: ovirt_requests_logs\n  ignore_errors: yes\n\n- name: Link ovirt-engine logs\n  file:\n    src: \"{{ item.src }}\"\n    dest: \"{{ ovirt_collect_logs_tmp_dir }}/{{ item.dest }}\"\n    state: link\n  with_items:\n    -\n      src: \"/var/log/ovirt-engine\"\n      dest: \"ovirt-engine-logs\"\n    -\n      src: \"/var/lib/ovirt-engine\"\n      dest: \"ovirt-engine-data\"\n    -\n      src: \"/etc/ovirt-engine\"\n      dest: \"etc/ovirt-engine\"\n    -\n      src: \"/etc/ovirt-engine-setup.conf.d\"\n      dest: \"etc/ovirt-engine-setup.conf.d\"\n    -\n      src: \"/etc/ovirt-host-deploy.conf.d\"\n      dest: \"etc/ovirt-host-deploy.conf.d\"\n    -\n      src: \"/etc/ovirt-vmconsole\"\n      dest: \"etc/ovirt-vmconsole\"\n    -\n      src: \"/var/log/ovirt-provider-ovn.log\"\n      dest: \"ovirt-provider-ovn.log\"\n  ignore_errors: true\n\n- name: Link ovirt-requests logs\n  file:\n    src: \"{{ item }}\"\n    dest: \"{{ ovirt_collect_logs_tmp_dir }}/httpd/{{ item|replace('/var/log/httpd/', '')}}\"\n    state: link\n  with_items: \"{{ ovirt_requests_logs.stdout_lines }}\"\n  ignore_errors: true\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "5443c25233b58b3315ea17515bcd6a1e668e1e0b", "filename": "roles/3-base-server/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# Base Server\n\n- name: ...IS BEGINNING =====================================\n  command: echo\n\n- name: HTTPD\n  include_role:\n    name: httpd\n  # has no \"when: XXXXX_install\" flag\n  tags: base, httpd\n\n- name: IIAB-ADMIN\n  include_role:\n    name: iiab-admin\n  # has no \"when: XXXXX_install\" flag\n  tags: base, iiab-admin\n\n- name: MYSQL\n  include_role:\n    name: mysql\n  # has no \"when: XXXXX_install\" flag\n  tags: base, mysql\n\n- name: Restart httpd\n  service:\n    name: \"{{ apache_service }}\"\n    state: restarted\n  when: not installing\n\n- name: Recording STAGE 3 HAS COMPLETED =====================\n  lineinfile:\n    dest: /etc/iiab/iiab.env\n    regexp: '^STAGE=*'\n    line: 'STAGE=3'\n    state: present\n"}, {"commit_sha": "e9fb46dc84b9c815a69f6de1347c9ece5db01cc8", "sha": "10b97401c5f5e5a65bd7b4bb5a46cf154b9ca23b", "filename": "handlers/main.yml", "repository": "fubarhouse/ansible-role-nodejs", "decoded_content": "---\n# handlers file for fubarhouse.nodejs\n"}, {"commit_sha": "b11c4477d973b0cc87a296f6b028eaf9abab4686", "sha": "2c307c60a332af536d304b4059175c10570610dc", "filename": "tasks/main-Generic.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# tasks file for ansible-role-docker-ce\n\n- name: Determine Docker version\n  command: bash -c \"docker version | grep Version | awk '{print $2}'\"\n  ignore_errors: yes\n  changed_when: false\n  register: cmd_docker_version\n\n- name: Set fact if old Docker installation shall be removed\n  set_fact:\n    remove_old_docker: \"{{docker_remove_pre_ce | bool }} == true and {{ cmd_docker_version.stdout_lines[0] | search('-ce') }} == false\"\n  when: cmd_docker_version.stdout_lines is defined and cmd_docker_version.stdout_lines[0] is defined\n\n- name: Check if Docker is running\n  command: systemctl status docker\n  ignore_errors: yes\n  changed_when: false\n  register: service_docker_status\n  when: remove_old_docker | default(false) | bool == true\n  become: true\n\n- name: Stop Docker service\n  service:\n    name: docker\n    state: stopped\n  when: \"service_docker_status.rc | default(1) == 0\"\n\n- name: Remove old Docker installation before Docker CE\n  package:\n    name: \"{{ item }}\"\n    state: absent\n  become: true\n  when: remove_old_docker|default(false) | bool == true\n  with_items:\n    - docker\n    - docker-common\n    - container-selinux\n    - docker-selinux\n    - docker-engine\n\n- name: Ensure docker-ce is the latest version\n  package:\n    name: docker-ce\n    state: latest\n  become: true\n  notify: restart docker\n\n- name: Ensure /etc/docker directory exists\n  file:\n    path: /etc/docker\n    state: directory\n    mode: 0755\n  become: true\n\n- name: Configure Docker daemon (file)\n  copy:\n    src: \"{{ docker_daemon_config_file }}\"\n    dest: /etc/docker/daemon.json\n  become: true\n  notify: restart docker\n  when: docker_daemon_config_file is defined\n\n- name: Configure Docker daemon (variables)\n  copy:\n    content: \"{{ docker_daemon_config | to_nice_json }}\"\n    dest: /etc/docker/daemon.json\n  become: true\n  notify: restart docker\n  when: docker_daemon_config_file is not defined and \n        docker_daemon_config is defined\n\n- name: Enable and start Docker service\n  service:\n    name: docker\n    state: started\n    enabled: true\n  become: true\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "f237c1a60a398ed04c687be65f124dcd6229604d", "filename": "playbooks/libvirt/openshift-cluster/tasks/configure_libvirt.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- include: configure_libvirt_storage_pool.yml\n  when: libvirt_storage_pool is defined and libvirt_storage_pool_path is defined\n\n- include: configure_libvirt_network.yml\n  when: libvirt_network is defined\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "96b6626843ae0e9a0025ab8f094da15cb35c0d05", "filename": "reference-architecture/vmware-ansible/playbooks/roles/haproxy-server/defaults/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\nhaproxy_socket: /var/lib/haproxy/stats\nhaproxy_chroot: /var/lib/haproxy\nhaproxy_user: haproxy\nhaproxy_group: haproxy\n\n# Frontend settings.\nhaproxy_frontend_name: 'hafrontend'\nhaproxy_frontend_bind_address: '*'\nhaproxy_frontend_port: 80\nhaproxy_frontend_mode: 'http'\n\n# Backend settings.\nhaproxy_backend_name: 'habackend'\nhaproxy_backend_mode: 'http'\nhaproxy_backend_balance_method: 'roundrobin'\nhaproxy_backend_httpchk: 'HEAD / HTTP/1.1\\r\\nHost:localhost'\n\n# List of backend servers.\nhaproxy_backend_servers: []\n# - name: app1\n#   address: 192.168.0.1:80\n# - name: app2\n#   address: 192.168.0.2:80\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "5740703c843c93e994026e4252b2ed7233af550b", "filename": "roles/dns_encryption/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Include tasks for Ubuntu\n  include_tasks: ubuntu.yml\n  when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'\n\n- name: Include tasks for FreeBSD\n  include_tasks: freebsd.yml\n  when: ansible_distribution == 'FreeBSD'\n\n- name: dnscrypt-proxy ip-blacklist configured\n  template:\n    src: ip-blacklist.txt.j2\n    dest: \"{{ config_prefix|default('/') }}etc/dnscrypt-proxy/ip-blacklist.txt\"\n  notify:\n    - restart dnscrypt-proxy\n\n- name: dnscrypt-proxy configured\n  template:\n    src: dnscrypt-proxy.toml.j2\n    dest: \"{{ config_prefix|default('/') }}etc/dnscrypt-proxy/dnscrypt-proxy.toml\"\n  notify:\n    - restart dnscrypt-proxy\n\n- name: dnscrypt-proxy enabled and started\n  service:\n    name: dnscrypt-proxy\n    state: started\n    enabled: true\n\n- meta: flush_handlers\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "a11e21298419d17130c528ae125cb410706b91de", "filename": "roles/vpn/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: Gather Facts\n  setup:\n\n- name: Enable IPv6\n  set_fact:\n    ipv6_support: true\n  when: ansible_default_ipv6.gateway is defined\n\n- name: Generate password for the CA key\n  shell: >\n    openssl rand -hex 6\n  register: CA_password\n\n- set_fact:\n    easyrsa_p12_export_password: \"{{ p12_export_password|default((ansible_date_time.iso8601_basic|sha1|to_uuid).split('-')[0]) }}\"\n    easyrsa_CA_password: \"{{ CA_password.stdout }}\"\n    IP_subject_alt_name: \"{{ IP_subject_alt_name }}\"\n\n- name: Change the algorithm to RSA\n  set_fact:\n    algo_params: \"rsa:2048\"\n  when: Win10_Enabled is defined and Win10_Enabled == \"Y\"\n\n- name: Ensure that the strongswan group exist\n  group: name=strongswan state=present\n\n- name: Ensure that the strongswan user exist\n  user: name=strongswan group=strongswan state=present\n\n- include: ubuntu.yml\n  when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'\n\n- include: freebsd.yml\n  when: ansible_distribution == 'FreeBSD'\n\n- name: Install StrongSwan\n  package: name=strongswan state=present\n\n- include: ipec_configuration.yml\n- include: openssl.yml\n- include: distribute_keys.yml\n- include: client_configs.yml\n\n- meta: flush_handlers\n\n- name: StrongSwan started\n  service: name=strongswan state=started\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "ba836a5a6b5e9a0b6b9ec1f2258c5d214eb31f33", "filename": "playbooks/roles/kafka/defaults/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# defaults file for kafka"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "c3f5855c55d30239c61ff2bf25a1598767dd30a5", "filename": "roles/network/tasks/detected_network.yml", "repository": "iiab/iiab", "decoded_content": "- name: iiab_wan_device\n  shell: grep IIAB_WAN_DEVICE /etc/iiab/iiab.env | awk -F \"=\" '{print $2}'\n  when: iiab_stage|int > 4\n  register: prior_gw\n\n- name: Checking for old device gateway interface for device test\n  set_fact:\n      device_gw: \"{{ prior_gw.stdout }}\"\n      device_gw2: \"{{ prior_gw.stdout }}\"\n  when: iiab_stage|int > 4 and prior_gw is defined and prior_gw.stdout != \"\"\n\n- name: Setting WAN if detected\n  set_fact:\n      iiab_wan_iface: \"{{ discovered_wan_iface }}\"\n      device_gw: \"{{ discovered_wan_iface }}\"\n  when: ansible_default_ipv4.gateway is defined\n\n- name: Red Hat network detection (redhat)\n  include_tasks: detected_redhat.yml\n  when: is_redhat\n\n- name: Setting dhcpcd_test results\n  set_fact:\n    dhcpcd_result: \"{{ ansible_local.local_facts.dhcpcd }}\"\n\n- name: Setting systemd_networkd results\n  set_fact:\n    systemd_networkd_active: True\n  when: 'ansible_local.local_facts.systemd_networkd == \"enabled\"'\n\n- name: Setting systemd_networkd-2 results\n  set_fact:\n    systemd_networkd_active: True\n  when: 'ansible_local.local_facts.systemd_networkd == \"enabled-runtime\"'\n\n- name: Setting network_manager results\n  set_fact:\n    network_manager_active: True\n  when: 'ansible_local.local_facts.network_manager == \"enabled\"'\n\n- name: Check /etc/network/interfaces for gateway\n  shell: grep {{ device_gw }} /etc/network/interfaces | wc -l\n  when: is_debuntu\n  register: wan_file\n\n- name: Setting wan_in_interfaces\n  set_fact:\n    wan_in_interfaces: True\n  when: is_debuntu and wan_file.stdout|int >= \"0\"\n\n# WIRELESS -- if any wireless is detected as gateway, it becomes WAN\n- name: Look for any wireless interfaces\n  shell: \"cat /proc/net/wireless | grep -v -e Inter -e face | awk -F: '{print $1}' \"\n  register: wireless_list1\n  ignore_errors: True\n  changed_when: False\n\n- name: Set the discovered wireless, if found\n  set_fact:\n     wifi1: \"{{ item|trim }}\"\n     discovered_wireless_iface: \"{{ item|trim }}\"\n  when: item|trim != \"\" and item|trim != discovered_wan_iface\n  with_items:\n      - \"{{ wireless_list1.stdout_lines }}\"\n\n# WIRELESS -- Sigh... Not all drivers update /proc/net/wireless correctly\n- name: Look for any wireless interfaces (take 2)\n  shell: \"ls -la /sys/class/net/*/phy80211 | awk -F / '{print $5}'\"\n  register: wireless_list2\n  ignore_errors: True\n  changed_when: False\n\n# Last device is used\n- name: Set the discovered wireless, if found (take 2)\n  set_fact:\n     wifi2: \"{{ item|trim }}\"\n     discovered_wireless_iface: \"{{ item|trim }}\"\n  when: wireless_list2.stdout is defined\n  with_items:\n      - \"{{ wireless_list2.stdout_lines }}\"\n#item|trim != discovered_wan_iface\n\n- name: Count WiFi ifaces\n  shell: \"ls -la /sys/class/net/*/phy80211 | awk -F / '{print $5}' | wc -l\"\n  register: count_wifi_interfaces\n\n# facts are apparently all stored as text, so do text comparisons from here on\n- name: Remember number of WiFi devices\n  set_fact:\n      num_wifi_interfaces: \"{{ count_wifi_interfaces.stdout|int }}\"\n\n# XO hack here ap_device would not be active therefore not set with\n# wired as gw use ap_device to exclude eth0 from network calulations\n\n- name: XO laptop override 2 WiFi on LAN\n  set_fact:\n      ap_device: \"eth0\"\n  when: iiab_wan_iface != \"eth0\" and discovered_wireless_iface != \"none\" and xo_model == \"XO-1.5\"\n\n- name: Exclude reserved WiFi adapter if defined - takes adapter name\n  set_fact:\n      ap_device: \"{{ reserved_wifi }}\"\n  when: reserved_wifi is defined and discovered_wireless_iface != iiab_wan_iface and num_wifi_interfaces >= \"2\"\n\n- name: Count LAN ifaces\n  shell: ls /sys/class/net | grep -v  -e wwlan -e ppp -e lo -e br0 -e tun -e {{ device_gw }} -e {{ ap_device }} | wc -l\n  register: num_lan_interfaces_result\n\n- name: Calculate number of LAN interfaces including WiFi\n  set_fact:\n      num_lan_interfaces: \"{{ num_lan_interfaces_result.stdout|int }}\"\n\n# LAN - pick non WAN's\n- name: Create list of LAN (non WAN) ifaces\n  shell: ls /sys/class/net | grep -v -e wwlan -e ppp -e lo -e br0 -e tun -e {{ device_gw }} -e {{ ap_device }}\n  when: num_lan_interfaces != \"0\"\n  register: lan_list_result\n\n# If 2 interfaces found in gateway mode, with one wifi, declare other to be wan\n#- name: In gateway mode with one wifi adapter, the other is WAN\n#  set_fact:\n#      iiab_wan_iface: \"{{ discovered_lan_iface }}\"\n#      iiab_lan_iface: \"{{ discovered_wireless_iface }}\"\n#      num_lan_interfaces: \"1\"\n#  when: iiab_lan_enabled and iiab_wan_enabled and num_lan_interfaces == \"2\" and discovered_wireless_iface != \"none\" and iiab_wan_iface == \"none\"\n\n# Select an adapter that is not WAN and not wireless\n# if there is more than one the last one wins\n- name: Set discovered_wired_iface if present\n  set_fact:\n      discovered_wired_iface: \"{{ item|trim }}\"\n  when: lan_list_result.stdout_lines is defined and item|trim != discovered_wireless_iface\n  with_items:\n      - \"{{ lan_list_result.stdout_lines }}\"\n\n- name: Set iiab_wireless_lan_iface if present\n  set_fact:\n       iiab_wireless_lan_iface: \"{{ discovered_wireless_iface }}\"\n  when: discovered_wireless_iface is defined and discovered_wireless_iface != \"none\" and discovered_wireless_iface != iiab_wan_iface\n\n- name: Set iiab_wired_lan_iface if present\n  set_fact:\n       iiab_wired_lan_iface: \"{{ discovered_wired_iface }}\"\n  when: discovered_wired_iface is defined and discovered_wired_iface != \"none\" and discovered_wired_iface != iiab_wan_iface\n\n#unused\n#- name: Get a list of ifcfg files to delete\n# moved to detected_redhat\n\n# use value only if present\n- name: 2 or more devices on the LAN - use bridging\n  set_fact:\n      iiab_lan_iface: br0\n  when: num_lan_interfaces|int >= 2 and not is_rpi\n\n- name: For Debian, always use bridging - except RPi\n  set_fact:\n      iiab_lan_iface: br0\n  when: num_lan_interfaces|int >= 1 and is_debuntu and not is_rpi\n\n- name: WiFi is on the LAN - use bridging - except RPi\n  set_fact:\n      iiab_lan_iface: br0\n  when: iiab_wireless_lan_iface is defined and not nobridge is defined\n\n- name: Setting wired LAN as only interface - RPi\n  set_fact:\n      iiab_lan_iface: \"{{ iiab_wired_lan_iface }}\"\n  when: iiab_wired_lan_iface is defined and nobridge is defined\n\n- name: Setting wireless LAN as only interface - RPi\n  set_fact:\n      iiab_lan_iface: \"{{ iiab_wireless_lan_iface }}\"\n  when: iiab_wireless_lan_iface is defined and nobridge is defined\n\n- name: In VM disable LAN - needs local_vars entry to activate\n  set_fact:\n      iiab_lan_iface: none\n      no_net_restart: True\n  when: is_VM is defined\n\n# OK try old gw this is a best guess based on what's in\n# /etc/sysconfig/iiab_wan_device's last state intended to\n# provide a seed value to display in the GUI when no\n# gateway is present but we had one.\n- name: Has old gateway and no discovered gateway setting WAN\n  set_fact:\n    gui_wan_iface: \"{{ device_gw }}\"\n  when: user_wan_iface == \"auto\" and device_gw != \"none\" and discovered_wan_iface == \"none\"\n\n- name: Add location section to config file\n  ini_file: dest='{{ iiab_config_file }}'\n            section=detected_network\n            option='{{ item.option }}'\n            value='{{ item.value }}'\n  with_items:\n  - option: 'has_ifcfg_gw'\n    value: '{{ has_ifcfg_gw }}'\n  - option: 'prior_gateway_(device_gw2)'\n    value: '{{ device_gw2 }}'\n  - option: 'dhcpcd_result'\n    value: '{{ dhcpcd_result }}'\n  - option: 'network_manager_active'\n    value: '{{ network_manager_active }}'\n  - option: 'systemd_networkd_active'\n    value: '{{ systemd_networkd_active }}'\n  - option: 'wan_in_interfaces'\n    value: '{{ wan_in_interfaces }}'\n  - option: 'wireless_list_1(wifi1)'\n    value: '{{ wifi1 }}'\n  - option: 'wireless_list_2(wifi2)'\n    value: '{{ wifi2 }}'\n  - option: 'num_wifi_interfaces'\n    value: '{{ num_wifi_interfaces }}'\n  - option: 'discovered_wireless_iface'\n    value: '{{ discovered_wireless_iface }}'\n  - option: 'discovered_wired_iface'\n    value: '{{ discovered_wired_iface }}'\n#  - option: 'iiab_wireless_lan_iface'\n#    value: '{{ iiab_wireless_lan_iface }}'\n  - option: 'num_lan_interfaces'\n    value: '{{ num_lan_interfaces }}'\n  - option: 'gui_static_wan'\n    value: '{{ gui_static_wan }}'\n  - option: 'iiab_lan_iface'\n    value: '{{ iiab_lan_iface }}'\n  - option: 'iiab_wan_iface'\n    value: '{{ iiab_wan_iface }}'\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "05c6ed9d66f6e6e5765c5d1a3f436d9bef7a7c8f", "filename": "roles/config-software-src/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: all\n  roles:\n    - config-software-src\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "41dad890a6bc3e0f56f1a7f63035362449e779b6", "filename": "tasks/section_13.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - include: section_13_level1.yml\n    tags:\n      - section13\n      - level1\n\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "057f4a0f8aed5a36d902e7f8d4887d4fef8a9833", "filename": "tasks/unknown-transport.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Warn on unsupported transport\n  debug:\n    msg: |\n      This role does not support '{{ transport }}' transport.\n      Please contact support@lean-delivery.com\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "212a57e1edb23ba29c33a964c25eea86acda5675", "filename": "tasks/configure-docker/configure-systemd.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Combine all systemd service configuration options\n  set_fact:\n    _systemd_service_config: \"{{ docker_systemd_service_config_tweaks + docker_systemd_service_config }}\"\n\n- name: Ensure /etc/systemd/system/docker.service.d directory exists\n  become: true\n  file:\n    path: /etc/systemd/system/docker.service.d\n    state: directory\n    mode: 0755\n\n- name: Setup default Docker drop-in to enable use of environment file\n  become: true\n  template:\n    src: drop-ins/default.conf.j2\n    dest: /etc/systemd/system/docker.service.d/default.conf\n  register: _systemd_docker_dropin\n  notify: restart docker\n  vars:\n    systemd_envs_dir: \"{{ docker_envs_dir[_docker_os_dist] }}\"\n    systemd_service_conf: \"{{ _systemd_service_config }}\"\n\n- name: Combine Docker daemon environment variable configuration\n  set_fact:\n    docker_service_envs: \"{{ docker_service_envs | combine(_docker_service_opts) | combine(docker_daemon_envs) }}\"\n  vars:\n    _docker_service_opts:\n      DOCKER_OPTS: \"{{ docker_daemon_opts }}\"\n\n- name: Setup Docker environment file {{ docker_envs_dir[_docker_os_dist] }}/docker-envs\n  become: true\n  template:\n    src: docker-envs.j2\n    dest: \"{{ docker_envs_dir[_docker_os_dist] }}/docker-envs\"\n  notify: restart docker\n  vars:\n    docker_envs: \"{{ docker_service_envs }}\"\n\n- name: Force daemon reload of systemd\n  become: true\n  systemd:\n    daemon_reload: yes\n  notify: restart docker\n  when: _systemd_docker_dropin is changed\n  tags:\n    - skip_ansible_lint"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "53e4cf92bc1a3603e266561ef941367cf5505240", "filename": "roles/idm-host-cert/tasks/write-certs-to-file.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Set certificate fact\"\n  set_fact:\n    certificate: \"{{ host_cert.json.result.result.certificate }}\"\n\n- name: \"Write the Host Specific Certificate to a file\"\n  template:\n    src: cert.j2\n    dest: \"{{ target_host_cert_file }}\"\n  when:\n  - target_host_cert_file is defined\n  - target_host_cert_file|trim != ''\n\n- name: \"Write the Certificate key to a file\"\n  copy:\n    content: \"{{ csr_content.key }}\"\n    dest: \"{{ target_host_key_file }}\"\n  when:\n  - target_host_key_file is defined\n  - target_host_key_file|trim != ''\n\n- name: \"Set certificate fact\"\n  set_fact:\n    certificate: \"{{ ca_cert.json.result.result.certificate }}\"\n\n- name: \"Write the CA Certificate to a file\"\n  template:\n    src: cert.j2\n    dest: \"{{ target_ca_cert_file }}\"\n  when:\n  - target_ca_cert_file is defined\n  - target_ca_cert_file|trim != ''\n\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "ea3a67a4955f36a1ed63b4dc074fcc9e99e96590", "filename": "roles/cloud-ec2/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- block:\n    - name: Build python virtual environment\n      import_tasks: venv.yml\n\n    - block:\n      - name: Include prompts\n        import_tasks: prompts.yml\n\n      - set_fact:\n          algo_region: >-\n            {% if region is defined %}{{ region }}\n            {%- elif _algo_region.user_input is defined and _algo_region.user_input != \"\" %}{{ aws_regions[_algo_region.user_input | int -1 ]['region_name'] }}\n            {%- else %}{{ aws_regions[default_region | int - 1]['region_name'] }}{% endif %}\n          stack_name: \"{{ algo_server_name | replace('.', '-') }}\"\n\n      - name: Locate official AMI for region\n        ec2_ami_facts:\n          aws_access_key: \"{{ access_key }}\"\n          aws_secret_key: \"{{ secret_key }}\"\n          owners: \"{{ cloud_providers.ec2.image.owner }}\"\n          region: \"{{ algo_region }}\"\n          filters:\n            name: \"ubuntu/images/hvm-ssd/{{ cloud_providers.ec2.image.name }}-amd64-server-*\"\n        register: ami_search\n\n      - import_tasks: encrypt_image.yml\n        when: encrypted\n\n      - name: Set the ami id as a fact\n        set_fact:\n          ami_image: >-\n            {% if ami_search_encrypted.image_id is defined %}{{ ami_search_encrypted.image_id }}\n            {%- elif search_crypt.images is defined and search_crypt.images|length >= 1 %}{{ (search_crypt.images | sort(attribute='creation_date') | last)['image_id'] }}\n            {%- else %}{{ (ami_search.images | sort(attribute='creation_date') | last)['image_id'] }}{% endif %}\n\n      - name: Deploy the stack\n        import_tasks: cloudformation.yml\n\n      - set_fact:\n          cloud_instance_ip: \"{{ stack.stack_outputs.ElasticIP }}\"\n          ansible_ssh_user: ubuntu\n      environment:\n        PYTHONPATH: \"{{ ec2_venv }}/lib/python2.7/site-packages/\"\n  rescue:\n    - debug: var=fail_hint\n      tags: always\n    - fail:\n      tags: always\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "c44e6d0160246cea70457b9b7a2c9e777eee1d0e", "filename": "roles/elasticsearch/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: Reload systemd\n  systemd:\n    daemon_reload: true\n\n- name: Restart elasticsearch\n  debug: msg=\"Restarting Elasticsearch Sequence\"\n  notify:\n    - Disable cluster shard allocation\n\n- name: Disable cluster shard allocation\n  uri:\n    url: \"http://{{ ansible_hostname }}:9200/_cluster/settings\"\n    body: '{\"transient\": {\"cluster.routing.allocation.enable\":\"none\" }}'\n    body_format: json\n    timeout: 2\n    method: PUT\n  register: result\n  until: result.json.acknowledged is defined\n  retries: 300\n  delay: 3\n  changed_when: result.json.acknowledged | bool\n  notify:\n    - do restart elasticsearch\n\n- name: do restart elasticsearch\n  service:\n    name: elasticsearch\n    state: restarted\n  notify:\n    - wait node online\n\n- name: wait node online\n  uri:\n    url: \"http://{{ ansible_hostname }}:9200/_nodes/{{ ansible_hostname }}/name\"\n    timeout: 2\n  register: result\n  until: result.json._nodes.total == 1\n  retries: 200\n  delay: 3\n  notify:\n    - Enable cluster shard allocation\n\n- name: Enable cluster shard allocation\n  uri:\n    url: \"http://{{ ansible_hostname }}:9200/_cluster/settings\"\n    body: '{\"transient\": {\"cluster.routing.allocation.enable\":\"all\" }}'\n    body_format: json\n    timeout: 2\n    method: PUT\n  register: result\n  until: result.json.acknowledged is defined\n  retries: 300\n  delay: 3\n  changed_when: result.json.acknowledged | bool\n  notify:\n    - Wait until cluster green\n\n- name: Wait until cluster green\n  uri:\n    url: \"http://{{ ansible_hostname }}:9200/_cluster/health\"\n    timeout: 2\n  register: result\n  until: result.json.status == \"green\"\n  retries: 300\n  delay: 3\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "bf7458ea504d529eabf16ab028e692c86a90c825", "filename": "reference-architecture/aws-ansible/playbooks/node-setup.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- include: /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-node/scaleup.yml\n  vars:\n    debug_level: 2\n    openshift_debug_level: \"{{ debug_level }}\"\n    openshift_node_debug_level: \"{{ node_debug_level | default(debug_level, true) }}\"\n    osm_controller_args:\n      cloud-provider:\n      - \"aws\"\n    osm_api_server_args:\n      cloud-provider:\n      - \"aws\"\n    openshift_node_kubelet_args:\n      cloud-provider:\n      - \"aws\"\n      node-labels:\n      - \"role={{ openshift_node_labels.role }}\"\n    openshift_master_debug_level: \"{{ master_debug_level | default(debug_level, true) }}\"\n    openshift_master_access_token_max_seconds: 2419200\n    openshift_master_api_port: \"{{ console_port }}\"\n    openshift_master_console_port: \"{{ console_port }}\"\n    osm_cluster_network_cidr: 172.16.0.0/16\n    openshift_registry_selector: \"role=infra\"\n    openshift_router_selector: \"role=infra\"\n    openshift_hosted_router_replicas: 3\n    openshift_hosted_registry_replicas: 3\n    openshift_node_local_quota_per_fsgroup: 512Mi\n    openshift_master_cluster_method: native\n    openshift_cloudprovider_kind: aws\n    openshift_master_cluster_hostname: \"internal-openshift-master.{{ public_hosted_zone }}\"\n    openshift_master_cluster_public_hostname: \"openshift-master.{{ public_hosted_zone }}\"\n    osm_default_subdomain: \"{{ wildcard_zone }}\"\n    osm_default_node_selector: \"role=app\"\n    deployment_type: openshift-enterprise\n    osm_use_cockpit: false\n    containerized: false\n    os_sdn_network_plugin_name: \"{{ openshift_sdn }}\"\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "c4f908698720faaa97357d3034d8aaba58d180fd", "filename": "roles/stenographer/defaults/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# defaults file for stenographer\nstenographer_packagename: stenographer\nstenographer_user: stenographer\nstenographer_group: stenographer\nstenographer_monitor_interfaces: [eth0]\nenable_stenographer: true\nmethod: deploy\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "1231458a456e1353374fbe16f47cfd7d6c4c7620", "filename": "reference-architecture/azure-ansible/3.6/ansibledeployocp/playbooks/roles/azure-deploy/defaults/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ntemplatelink: \"https://raw.githubusercontent.com/openshift/openshift-ansible-contrib/master/reference-architecture/azure-ansible/3.6/azuredeploy.json\"\nnumberofnodes: 3\nimage: \"rhel\"\nmastervmsize: \"Standard_DS3_v2\"\ninfranodesize: \"Standard_DS3_v2\"\nnodevmsize: \"Standard_DS12_v2\"\nlocation: \"westus\"\nopenshiftsdn: \"redhat/openshift-ovs-multitenant\"\nmetrics: true\nlogging: true\nopslogging: false\n"}, {"commit_sha": "bf6e08dcb2440421477b6536ff6a8d11adc2be17", "sha": "a23c38c387fb316d6ff7f7873a36f28a8a10ef0b", "filename": "roles/registrator/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n\n# Install (docker-py) python package as is a docker module dependency.\n- pip:\n    name: docker-py\n    version: 1.1.0\n  tags:\n    - registrator\n\n# tasks file for docker registrator\n- name: run registrator container\n  docker:\n    name: registrator\n    image: \"{{ registrator_image }}\"\n    state: started\n    restart_policy: always\n    net: host\n    command: \"-internal {{ registrator_uri }}\"\n    volumes:\n    - \"/var/run/docker.sock:/tmp/docker.sock\"\n  tags:\n    - registrator\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "2828f2d58f7296885763b76cb40bb0d59d276e67", "filename": "roles/config-satellite/tasks/prereq.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - satellite\n  - firewalld\n  - python-firewall\n\n- name: 'Ensure firewalld is running'\n  service:\n    name: firewalld\n    state: started\n    enabled: yes\n\n- name: 'Open Firewall for Satellite use'\n  firewalld:\n    service: \"{{ item }}\"\n    permanent: yes\n    state: enabled\n    immediate: yes\n  with_items:\n  - http\n  - https \n  - ssh\n\n"}, {"commit_sha": "218cdc58f9fe9d7ece7d43e5f100fe9631fde5cc", "sha": "26354e010e6f8af4d495d78c9519c51c8822195b", "filename": "tasks/go-get.yml", "repository": "fubarhouse/ansible-role-golang", "decoded_content": "---\n\n- name: \"Go-Lang | Define list of package directories\"\n  set_fact:\n    go_package_locations:\n    - \"{{ GOPATH }}/bin\"\n    - \"{{ GOPATH }}/pkg\"\n    - \"{{ GOPATH }}/src\"\n  when: go_reget == true\n\n- name: \"Go-Lang | Remove installed workspace packages\"\n  become: yes\n  become_user: \"root\"\n  file:\n    path: \"{{ item }}\"\n    state: absent\n  with_items: \"{{ go_package_locations }}\"\n  ignore_errors: yes\n  when:\n    - go_reget == true\n    - go_package_locations is defined\n\n- name: \"Go-Lang | Run get commands\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell: \"{{ GOPATH }}/go get -u {{ item.url }}\"\n  environment:\n    GOPATH: \"{{ GOPATH }}\"\n    GOROOT: \"{{ GOROOT }}\"\n  with_items: \"{{ go_get }}\"\n  changed_when: false\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "9d23c81f90261ee560826f01e16259fcdccdcd7a", "filename": "reference-architecture/vmware-ansible/playbooks/infrastructure.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- include: setup.yaml\n  tags: ['setup']\n\n- include: nfs.yaml\n  tags: ['nfs']\n\n- include: prod.yaml\n  tags: ['prod']\n\n- include: haproxy.yaml\n  tags: ['haproxy']\n\n- include: ocp-install.yaml\n  tags: ['ocp-install']\n\n- include: ocp-configure.yaml\n  tags: ['ocp-configure']\n\n- include: ocp-demo.yaml\n  tags: ['ocp-demo']\n\n- include: ocp-upgrade.yaml\n  tags: ['ocp-upgrade']\n\n- include: clean.yaml\n  tags: ['clean']\n"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "8e978fb951fd3f14177dcc8055671fc21bfbe0cb", "filename": "tasks/security_policy_fetch/local.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Security policy artifact stored localy\"\n  set_fact:\n    security_policy_oracle_artifact: \"{{ java_unlimited_policy_transport_local }}\"\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "2af14ed3a2fd8459aa9f70c586fe278714cd5501", "filename": "roles/bro/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# tasks file for bro\n\n- name: Install packages\n  yum:\n    name:\n      - bro\n      - bro-plugin-af_packet\n      - bro-plugin-kafka\n      - GeoIP\n      - GeoIP-update\n      - postfix\n    state: installed\n\n- name: Set monitor interface config\n  template:\n    src: templates/ifcfg-monif.j2\n    dest: /etc/sysconfig/network-scripts/ifcfg-{{ item }}\n    mode: 0644\n    owner: root\n    group: root\n    force: true\n  loop: \"{{ rock_monifs }}\"\n\n- name: Configure local ifup script\n  template:\n    src: templates/ifup-local.j2\n    dest: /sbin/ifup-local\n    mode: 0755\n    owner: root\n    group: root\n    force: true\n  notify: Configure monitor interfaces\n\n- name: Configure GeoIP Update\n  copy:\n    src: GeoIP.conf\n    dest: /etc/GeoIP.conf\n\n# There's an issue w/ geoipupdate when env is empty\n- name: Update GeoIP\n  shell: >\n    if [ \"x$HTTP_PROXY\" == \"x\" ]; then\n        unset HTTP_PROXY;\n    fi\n    if [ \"x$http_proxy\" == \"x\" ]; then\n        unset http_proxy;\n    fi\n    if [ \"x$HTTPS_PROXY\" == \"x\" ]; then\n        unset HTTPS_PROXY;\n    fi\n    if [ \"x$https_proxy\" == \"x\" ]; then\n        unset https_proxy;\n    fi\n    /usr/bin/geoipupdate\n  args:\n    creates: /usr/share/GeoIP/GeoLiteASNum.dat\n  register: result\n  failed_when: (result.rc != 0) and (result.rc != 1)\n\n- name: Create GeoIP symlinks\n  file:\n    src: \"/usr/share/GeoIP/{{ item.src }}\"\n    dest: \"/usr/share/GeoIP/{{ item.dest }}\"\n    force: true\n    state: link\n  loop:\n    - { src: 'GeoLiteCity.dat', dest: 'GeoIPCity.dat' }\n    - { src: 'GeoLiteCountry.dat', dest: 'GeoIPCountry.dat' }\n    - { src: 'GeoLiteASNum.dat', dest: 'GeoIPASNum.dat' }\n    - { src: 'GeoLiteCityv6.dat', dest: 'GeoIPCityv6.dat' }\n\n- name: Create bro group\n  group:\n    name: \"{{ bro_group }}\"\n    state: present\n    system: true\n\n- name: Create bro user\n  user:\n    name: \"{{ bro_user }}\"\n    comment: \"bro service account\"\n    createhome: false\n    group: \"{{ bro_group }}\"\n    home: /var/spool/bro\n    shell: /sbin/nologin\n    system: true\n    state: present\n\n- name: Create bro directories\n  file:\n    path: \"{{ item }}\"\n    mode: 0755\n    owner: \"{{ bro_user }}\"\n    group: \"{{ bro_group }}\"\n    state: directory\n    setype: var_log_t\n  loop:\n    - \"{{ bro_data_dir }}\"\n    - \"{{ bro_data_dir }}/logs\"\n    - \"{{ bro_data_dir }}/spool\"\n\n- name: Create /opt/bro for wandering users\n  file:\n    dest: \"/opt/bro\"\n    state: directory\n\n- name: Create note to wandering users\n  copy:\n    dest: \"/opt/bro/README.md\"\n    content: |\n      Hey! Where's my Bro?\n      =========================\n      RockNSM has aligned the Bro package to be inline with Fedora packaging\n      guidelines in an effort to push the package upstream for maintenance.\n      Fedora and EPEL have a great community and we believe others can benefit\n      from our hard work.\n      Here's where you can find your stuff:\n      Bro configuration files\n      -----------------------\n      /opt/bro/etc -> /etc/bro\n      Bro site scripts\n      -----------------------\n      /opt/bro/share/bro/site -> /usr/share/bro/site\n      Bro logs and spool dirs (same as previous ROCK iterations)\n      -----------------------\n      /opt/bro/logs -> /data/bro/logs\n      /opt/bro/spool -> /data/bro/spool\n\n- name: Create bro configs\n  template:\n    src: \"{{ item }}.j2\"\n    dest: \"{{ bro_sysconfig_dir }}/{{ item }}\"\n    mode: 0644\n    owner: root\n    group: root\n  notify: Reload bro\n  loop:\n    - node.cfg\n    - broctl.cfg\n    - networks.cfg\n\n- name: Add bro custom scripts directory\n  file:\n    path: \"{{ bro_site_dir }}/scripts\"\n    owner: root\n    group: root\n    mode: 0755\n    state: directory\n\n- name: Set permissions on broctl scripts\n  file:\n    path: \"{{ bro_prefix }}/share/broctl/scripts\"\n    owner: \"{{ bro_user }}\"\n    group: \"{{ bro_user }}\"\n    mode: 0755\n    state: directory\n\n- name: Add README to scripts\n  copy:\n    src: bro-scripts-readme.txt\n    dest: \"{{ bro_site_dir }}/scripts/README.txt\"\n    mode: 0644\n    owner: root\n    group: root\n\n- name: Checkout ROCK bro scripts\n  git:\n    repo: \"{{ bro_rockscripts_repo }}\"\n    dest: \"{{ bro_site_dir }}/scripts/rock\"\n    version: \"{{ bro_rockscripts_branch }}\"\n  when: rock_online_install\n\n- name: Deploy offline ROCK bro scripts\n  unarchive:\n    src: \"{{ rock_cache_dir }}/{{ bro_rockscripts_filename }}\"\n    dest: \"{{ bro_site_dir }}/scripts/\"\n    owner: root\n    group: root\n    creates: \"{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}\"\n    remote_src: true\n  when: not rock_online_install | bool\n\n- name: Symlink offline ROCK bro scripts\n  file:\n    src: \"{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}\"\n    dest: \"{{ bro_site_dir }}/scripts/rock\"\n    state: link\n    force: true\n  when: not rock_online_install | bool\n\n- name: Update owner for ROCK bro scripts\n  file:\n    path: \"{{ bro_site_dir }}/scripts/rock\"\n    owner: \"{{ bro_user }}\"\n    group: \"{{ bro_group }}\"\n    state: directory\n    recurse: true\n    follow: true\n  tags:\n    - bro_scripts\n\n- name: Add ROCK scripts to local.bro\n  lineinfile:\n    dest: \"{{ bro_site_dir }}/local.bro\"\n    line: \"@load scripts/rock # ROCK NSM customizations\"\n    state: present\n\n- name: Enable bro kafka output to local.bro\n  lineinfile:\n    dest: \"{{ bro_site_dir }}/local.bro\"\n    line: \"@load scripts/rock/plugins/kafka\"\n    state: present\n  when: \"rock_services | selectattr('name', 'equalto', 'kafka') | map(attribute='enabled')\"\n\n- name: Add bro aliases\n  copy:\n    src: profile.d-bro.sh\n    dest: /etc/profile.d/bro.sh\n    mode: 0644\n    owner: root\n    group: root\n\n- name: Add broctl wrapper for admin use\n  copy:\n    src: broctl.sh\n    dest: /usr/sbin/broctl\n    mode: 0754\n    owner: root\n    group: root\n\n- name: Set bro capabilities\n  capabilities:\n    path: /usr/bin/bro\n    capability: \"{{ item }}\"\n    state: present\n  loop:\n    - \"cap_net_raw+eip\"\n    - \"cap_net_admin+eip\"\n\n- name: Set capstats capabilities\n  capabilities:\n    path: /usr/bin/capstats\n    capability: \"{{ item }}\"\n    state: present\n  loop:\n    - \"cap_net_raw+eip\"\n    - \"cap_net_admin+eip\"\n\n- name: Set broctl cron\n  cron:\n    name: \"broctl maintenance\"\n    minute: \"*/5\"\n    cron_file: rocknsm_broctl\n    user: \"{{ bro_user }}\"\n    job: \"/usr/bin/broctl cron >/dev/null 2>&1\"\n\n- name: Initialize bro scripts for workers\n  command: /usr/bin/broctl install\n  args:\n    creates: \"{{ bro_data_dir }}/spool/broctl-config.sh\"\n  become: true\n  become_user: \"{{ bro_user }}\"\n\n- name: Check status of interfaces\n  command: >\n    /usr/sbin/ip -oneline link show dev {{ item }}\n  register: iplink\n  changed_when: false\n  loop: \"{{ rock_monifs }}\"\n\n- name: Bring up interfaces\n  command: >\n    /usr/sbin/ip -oneline link set dev {{ item.stdout.split(':')[1] | trim }} up\n  when: item.stdout is search(\"state DOWN\")\n  loop: \"{{ iplink.results }}\"\n\n- name: Discover facts about data mount\n  set_fact:\n    rock_mounts:\n      mount: \"{{ item.mount }}\"\n      device: \"{{ item.device }}\"\n      size_total: \"{{ item.size_total }}\"\n  loop:\n    \"{{ ansible_mounts }}\"\n  when: (default_mount is defined and item.mount == default_mount and rock_mounts is not defined)\n\n- name: Determining if quotas are enabled\n  shell: grep \"{{ default_mount }}\" /etc/fstab | awk /prjquota/\n  register: prjquota\n  changed_when: false\n\n- name: Create bro quota project id\n  getent:\n    database: group\n    split: ':'\n    key: bro\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Map bro quota project id to name\n  lineinfile:\n    create: true\n    state: present\n    insertafter: EOF\n    path: /etc/projid\n    line: \"bro:{{ getent_group.bro[1] }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Define bro quota project directories\n  lineinfile:\n    create: true\n    state: present\n    insertafter: EOF\n    path: /etc/projects\n    line: \"{{ getent_group.bro[1] }}:{{ bro_data_dir }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: set bro weight\n  set_fact:\n    bro_weight: \"{{ rock_services | selectattr('name', 'equalto', 'bro') | map(attribute='quota_weight') | first }}\"\n  when: bro_quota is not defined and (prjquota.stdout|length>0)\n\n- name: set bro quota if not user defined\n  set_fact:\n    bro_quota: \"{{ rock_mounts.size_total | int / xfs_quota_weight | int * bro_weight | int }}\"\n  when: bro_quota is not defined and (prjquota.stdout|length>0)\n\n- name: set bro project quota\n  xfs_quota:\n    type: project\n    name: bro\n    bhard: \"{{ bro_quota }}\"\n    state: present\n    mountpoint: \"{{ rock_mounts.mount }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Enable and start bro\n  service:\n    name: bro\n    state: \"{{ 'started' if rock_services | selectattr('name', 'equalto', 'bro') | map(attribute='enabled') | bool else 'stopped' }}\"\n    enabled: \"{{ rock_services | selectattr('name', 'equalto', 'bro') | map(attribute='enabled') | bool }}\"\n  notify: Reload bro\n\n- name: Apply Logstash role\n  include_role:\n    name: logstash\n    apply:\n      delegate_to: \"{{ host }}\"\n      vars:\n        logstash_configs:\n          - { src: 'ls-input-bro.j2', dest: 'logstash-100-input-kafka-zeek.conf' }\n  loop:\n    \"{{ groups['logstash'] }}\"\n  loop_control:\n    loop_var: host\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "148d503cceb07040c255394df57ab3d67bc9bf2d", "filename": "roles/config-bonding/tasks/interfaces.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Check if bonding has been initiated\" \n  stat:\n    path: /proc/net/bonding\n  register: proc_bonding\n\n- name: \"Detect which bonds already exist\"\n  command: ls -1 /proc/net/bonding\n  register: existing_bonds\n  when:\n  - proc_bonding.stat.isdir is defined\n  - proc_bonding.stat.isdir\n\n- name: \"Set fact for existing_bonds\"\n  set_fact: \n    existing_bonds: \"{{ existing_bonds.stdout_lines | default([]) }}\"\n\n- name: \"Configure bonding master interfaces\"\n  template:\n    src: bonding_master.j2 \n    dest: /etc/sysconfig/network-scripts/ifcfg-{{ ifcfg.device }}\n  with_items:\n  - '{{ bonds }}'\n  loop_control:\n    loop_var: ifcfg\n  when:\n  - ifcfg.device not in existing_bonds\n\n- name: \"Configure bonding slave interfaces\"\n  template:\n    src: bonding_slave.j2 \n    dest: /etc/sysconfig/network-scripts/ifcfg-{{ ifcfg.1.device }}\n  with_subelements:\n  - '{{ bonds }}'\n  - slaves\n  loop_control:\n    loop_var: ifcfg\n  when:\n  - ifcfg.0.device not in existing_bonds\n\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "70cf3aa8effc872e8cdf61c54cb0056fbd7644f3", "filename": "tasks/section_09.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - include: section_09_level1.yml\n    tags:\n      - section09\n      - level1\n\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "4597692ce439f8f658dab8d0b23e8611088e1ef9", "filename": "roles/openshift-applier/tasks/copy-inventory-dir.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: \"Copy dir to target directory\"\n  copy: \n    src: \"{{ dir }}/\"\n    dest: \"{{ tmp_inv_dir }}{{ dir }}\"\n  failed_when: false\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "c8b2009a6fa90b790cbabd41943a8fa3e2416dd9", "filename": "roles/ipaserver/tasks/python_2_3_test.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "---\n- block:\n  - name: Verify Python3 import\n    script: py3test.py\n    register: result_py3test\n    failed_when: False\n    changed_when: False\n    check_mode: no\n\n  - name: Set python interpreter to 3\n    set_fact:\n      ansible_python_interpreter: \"/usr/bin/python3\"\n    when: result_py3test.rc == 0\n\n  - name: Fail for IPA 4.5.90\n    fail: msg=\"You need to install python2 bindings for ipa server usage\"\n    when: result_py3test.rc != 0 and \"not usable with python3\"\n          in result_py3test.stdout\n\n  - name: Set python interpreter to 2\n    set_fact:\n      ansible_python_interpreter: \"/usr/bin/python2\"\n    when: result_py3test.failed or result_py3test.rc != 0\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "967693af2b1fbcad312af34f2e69519e9f4f8247", "filename": "roles/config-iscsi-client/tasks/iscsi-config.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Ensure the iSCSI initiatorname is set correctly\"\n  lineinfile:\n    path: /etc/iscsi/initiatorname.iscsi\n    regexp: '^InitiatorName='\n    line: 'InitiatorName={{ iscsi_initiatorname }}'\n    create: yes\n    owner: root\n    group: root\n    mode: 0644\n    state: present\n\n- name: \"Ensure the iSCSI service is running (and restarted)\"\n  service:\n    name: iscsid\n    state: restarted\n\n- name: \"Discover and Login to the available iSCSI targets\"\n  open_iscsi:\n    portal: '{{ iscsi_target }}'\n    login: yes\n    discover: yes\n    auto_node_startup: yes\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "db566d321d5f4b18c1bc5f0e0ed995cf433a620c", "filename": "roles/validate-etcd/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Check to see if etcd client is installed\n  stat:\n    path: /usr/bin/etcdctl\n  register: etcd_result\n\n- name: Validate etcd\n  command: \"etcdctl -C https://{{ ansible_hostname if validate_etcd_short_hostname else ansible_fqdn }}:2379 --ca-file=/etc/origin/master/master.etcd-ca.crt --cert-file=/etc/origin/master/master.etcd-client.crt --key-file=/etc/origin/master/master.etcd-client.key cluster-health | grep 'cluster is'\"\n  register: etcd_health\n  when: etcd_result.stat.exists\n\n- name: ETCD Cluster is healthy\n  debug:\n    msg: \"Cluster is healthy\"\n  when: etcd_result.stat.exists and etcd_health.stdout.find('cluster is healthy') != -1\n\n- name: ETCD Cluster is NOT healthy\n  debug:\n    msg: \"Cluster is NOT healthy\"\n  when: etcd_result.stat.exists and etcd_health.stdout.find('cluster is healthy') == -1\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "28d425219470a66d4e5733446edb505b39e980fa", "filename": "roles/cloud-azure/tasks/prompts.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- pause:\n    prompt: |\n      Enter your azure secret id (https://github.com/trailofbits/algo/blob/master/docs/cloud-azure.md)\n      You can skip this step if you want to use your defaults credentials from ~/.azure/credentials\n    echo: false\n  register: _azure_secret\n  when:\n    - azure_secret is undefined\n    - lookup('env','AZURE_SECRET')|length <= 0\n\n- pause:\n    prompt: |\n      Enter your azure tenant id (https://github.com/trailofbits/algo/blob/master/docs/cloud-azure.md)\n      You can skip this step if you want to use your defaults credentials from ~/.azure/credentials\n    echo: false\n  register: _azure_tenant\n  when:\n    - azure_tenant is undefined\n    - lookup('env','AZURE_TENANT')|length <= 0\n\n- pause:\n    prompt: |\n      Enter your azure client id (application id) (https://github.com/trailofbits/algo/blob/master/docs/cloud-azure.md)\n      You can skip this step if you want to use your defaults credentials from ~/.azure/credentials\n    echo: false\n  register: _azure_client_id\n  when:\n    - azure_client_id is undefined\n    - lookup('env','AZURE_CLIENT_ID')|length <= 0\n\n- pause:\n    prompt: |\n      Enter your azure subscription id (https://github.com/trailofbits/algo/blob/master/docs/cloud-azure.md)\n      You can skip this step if you want to use your defaults credentials from ~/.azure/credentials\n    echo: false\n  register: _azure_subscription_id\n  when:\n    - azure_subscription_id is undefined\n    - lookup('env','AZURE_SUBSCRIPTION_ID')|length <= 0\n\n- set_fact:\n    secret: \"{{ azure_secret | default(_azure_secret.user_input|default(None)) | default(lookup('env','AZURE_SECRET'), true) }}\"\n    tenant: \"{{ azure_tenant | default(_azure_tenant.user_input|default(None)) | default(lookup('env','AZURE_TENANT'), true) }}\"\n    client_id: \"{{ azure_client_id | default(_azure_client_id.user_input|default(None)) | default(lookup('env','AZURE_CLIENT_ID'), true) }}\"\n    subscription_id: \"{{ azure_subscription_id | default(_azure_subscription_id.user_input|default(None)) | default(lookup('env','AZURE_SUBSCRIPTION_ID'), true) }}\"\n\n- block:\n  - name: Set facts about the regions\n    set_fact:\n      azure_regions: \"{{ _azure_regions|from_json | sort(attribute='name') }}\"\n\n  - name: Set the default region\n    set_fact:\n      default_region: >-\n        {% for r in azure_regions %}\n        {%- if r['name'] == \"eastus\" %}{{ loop.index }}{% endif %}\n        {%- endfor %}\n\n  - pause:\n      prompt: |\n        What region should the server be located in?\n          {% for r in azure_regions %}\n          {{ loop.index }}. {{ r['displayName'] }}\n          {% endfor %}\n\n        Enter the number of your desired region\n        [{{ default_region }}]\n    register: _algo_region\n  when: region is undefined\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "4c022125d45d2deb952e000635a18d65b58fe7fc", "filename": "roles/stenographer/tasks/config.yml", "repository": "rocknsm/rock", "decoded_content": "---\n#######################################################\n################# Config Stenographer #################\n#######################################################\n\n- name: Set stenographer config\n  template:\n    src: stenographer-config.j2\n    dest: \"/etc/stenographer/config.{{ item.1 }}\"\n  with_indexed_items: \"{{ stenographer_monitor_interfaces }}\"\n  notify: Restart stenographer per interface\n\n- name: Create stenographer directories\n  file:\n    path: \"{{ stenographer_data_dir }}/{{ item[0] }}/{{ item[1] }}\"\n    mode: 0755\n    owner: \"{{ stenographer_user }}\"\n    group: \"{{ stenographer_group }}\"\n    state: directory\n  with_nested:\n    - \"{{ stenographer_monitor_interfaces }}\"\n    - [ 'index', 'packets' ]\n\n- name: Install stenographer service files\n  copy:\n    src: \"{{ item }}\"\n    dest: \"/etc/systemd/system/{{ item }}\"\n    mode: 0644\n    owner: root\n    group: root\n  loop:\n    - stenographer.service\n    - stenographer@.service\n\n- name: Generate stenographer keys\n  command: >\n    /usr/bin/stenokeys.sh {{ stenographer_user }} {{ stenographer_group }}\n  environment:\n    STENOGRAPHER_CONFIG: \"/etc/stenographer/config.{{ stenographer_monitor_interfaces[0] }}\"\n  args:\n    creates: /etc/stenographer/certs/client_key.pem\n\n- name: Discover facts about data mount\n  set_fact:\n    rock_mounts:\n      mount: \"{{ item.mount }}\"\n      device: \"{{ item.device }}\"\n      size_total: \"{{ item.size_total }}\"\n  loop:\n    \"{{ ansible_mounts }}\"\n  when: (default_mount is defined and item.mount == default_mount and rock_mounts is not defined)\n\n- name: Determining if quotas are enabled\n  shell: grep \"{{ default_mount }}\" /etc/fstab | awk /prjquota/\n  register: prjquota\n  changed_when: false\n\n- name: Create stenographer quota project id\n  getent:\n    database: group\n    split: ':'\n    key: stenographer\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Map stenographer quota project id to name\n  lineinfile:\n    create: true\n    state: present\n    insertafter: EOF\n    path: /etc/projid\n    line: \"stenographer:{{ getent_group.stenographer[1] }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Define stenographer quota project directories\n  lineinfile:\n    create: true\n    state: present\n    insertafter: EOF\n    path: /etc/projects\n    line: \"{{ getent_group.stenographer[1] }}:{{ stenographer_data_dir }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: set stenographer weight\n  set_fact:\n    stenographer_weight: \"{{ rock_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='quota_weight') | first }}\"\n  when: stenographer_quota is not defined and (prjquota.stdout|length>0)\n\n- name: set stenographer quota if not user defined\n  set_fact:\n    stenographer_quota: \"{{ rock_mounts.size_total | int / xfs_quota_weight | int * stenographer_weight | int }}\"\n  when: stenographer_quota is not defined and (prjquota.stdout|length>0)\n\n- name: set stenographer project quota\n  xfs_quota:\n    type: project\n    name: stenographer\n    bhard: \"{{ stenographer_quota }}\"\n    state: present\n    mountpoint: \"{{ rock_mounts.mount }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Configure stenographer service\n  service:\n    name: stenographer\n    enabled: \"{{ rock_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | bool }}\"\n  notify: Start stenographer service\n\n- name: Configure stenographer per interface\n  service:\n    name: \"stenographer@{{ item }}\"\n    enabled: \"{{ rock_services | selectattr('name', 'equalto', 'stenographer') | map(attribute='enabled') | bool }}\"\n  loop: \"{{ stenographer_monitor_interfaces }}\"\n  notify: Start stenographer per interface\n\n- name: Configure firewall ports\n  firewalld:\n    port: \"{{ 1234 + index }}/tcp\"\n    permanent: true\n    state: enabled\n    immediate: true\n  loop: \"{{ stenographer_monitor_interfaces }}\"\n  loop_control:\n    index_var: index\n  when: groups['stenographer'] | difference(groups['docket']) | count > 0\n...\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "444f3241f77eecd032862625b6a1d9d348fd9faf", "filename": "roles/network/tasks/ifcfg_mods.yml", "repository": "iiab/iiab", "decoded_content": "- name: Stop the Access Point hostapd program\n  systemd:\n    name: hostapd\n    state: stopped\n  when: iiab_wireless_lan_iface != \"none\"\n\n# might need an exclude for F18 here\n- name: Now disconnect bridge slaves\n  shell: nmcli c delete id \"System {{ item|trim }}\"\n  ignore_errors: True\n  when: item|trim != iiab_wireless_lan_iface\n  with_items:\n    - \"{{ ifcfg_slaves.stdout_lines }}\"\n\n# clear all bridge ifcfg files\n- name: Now delete slave bridge ifcfg files\n  shell: rm -f /etc/sysconfig/network-scripts/ifcfg-\"{{ item }}\"\n  ignore_errors: True\n  when: num_lan_interfaces != 0 or iiab_wireless_lan_iface != \"none\"\n  with_items:\n    - \"{{ ifcfg_slaves.stdout_lines }}\"\n\n- name: Now delete original ifcfg files\n  shell: rm -f /etc/sysconfig/network-scripts/ifcfg-\"{{ item }}\"\n  when: num_lan_interfaces == 1 and iiab_lan_iface != \"br0\"\n  with_items:\n    - \"{{ discovered_lan_iface }}\"\n\n- name: BIND may be affected\n  service:\n    name: \"{{ dns_service }}\"\n    state: stopped\n  when: named_install and dnsmasq_enabled\n\n- name: dhcpd_server may be affected - stopping dhcpd\n  service:\n    name: dhcpd\n    state: stopped\n  when: dhcpd_install\n\n- name: dhcpd_server may be affected - stopping dnsmasq\n  service:\n    name: dnsmasq\n    state: stopped\n  when: dnsmasq_install\n\n- name: Stop the LAN/Bridge deleting iiab-LAN\n  shell: nmcli con delete id iiab-LAN\n  ignore_errors: True\n  changed_when: False\n  when: (num_lan_interfaces != 0 or iiab_wireless_lan_iface != \"none\")\n\n## vars/ users should set user_wan_iface to avoid messy redetect\n- include_tasks: redetect.yml\n  when: discovered_wan_iface == \"none\" and user_wan_iface == \"auto\"\n\n# move gateway if not WAN\n# might have wifi info if wireless is used as uplink.\n- include_tasks: edit_ifcfg.yml\n  when: has_wifi_gw == \"none\" and has_ifcfg_gw != \"none\" and has_ifcfg_gw != \"/etc/sysconfig/network-scripts/ifcfg-WAN\"\n\n# create ifcfg-WAN if missing\n# if we get here we have gateway but no ifcfg file\n#- include_tasks: create_ifcfg.yml\n#  when: iiab_wan_iface != \"none\" and not has_WAN and has_ifcfg_gw == \"none\" and xo_model == \"none\" and not iiab_demo_mode\n\n- name: Configuring LAN interface as iiab_lan_iface\n  template:\n    src: network/ifcfg.j2\n    dest: /etc/sysconfig/network-scripts/ifcfg-LAN\n  when: iiab_lan_iface != \"none\"\n\n# can be more than one wired interface\n- name: Wired enslaving ## lan_list_result ## to Bridge\n  template:\n    src: network/ifcfg-slave.j2\n    dest: \"/etc/sysconfig/network-scripts/ifcfg-{{ item|trim }}\"\n  when: iiab_lan_iface == \"br0\" and item|trim != iiab_wireless_lan_iface and item|trim != iiab_wan_iface\n  with_items:\n    - \"{{ lan_list_result.stdout_lines }}\"\n\n- name: WiFi enslaving {{ iiab_wireless_lan_iface }} to Bridge\n  template:\n    src: network/wifi-slave.j2\n    dest: \"/etc/sysconfig/network-scripts/ifcfg-{{ iiab_wireless_lan_iface }}\"\n  when: iiab_lan_iface == \"br0\" and iiab_wireless_lan_iface != \"none\"\n  tags:\n    - network\n\n- include_tasks: enable_wan.yml\n  when: not installing and not iiab_demo_mode\n\n- name: Ask systemd to reread the unit files, picks up changes done\n  systemd:\n    daemon_reload: yes\n  when: not installing\n\n# monitor-connection-files defaults to no with F21, F18-F20 defaults to yes\n- name: Re-read network config files\n  shell: nmcli con reload\n  when: not installing and not no_NM_reload\n\n# test point, we should always have one with any kind of starting point\n# no ifcfg = supply\n# had but not WAN = rename and edit if wired, skip wifi gateway.\n# test point, confirm onboot=no is OK everywhere\n\n- name: Enabling pre-existing ifcfg-WAN file\n  shell: nmcli conn up id iiab-WAN\n  when: has_WAN and iiab_wan_iface != \"none\" and not installing and not iiab_demo_mode\n\n- name: Enabling ifcfg-LAN file\n  shell: nmcli conn up id iiab-LAN\n  ignore_errors: True\n  when: iiab_lan_iface != \"none\" and not installing and not iiab_demo_mode\n\n# we could do the DEVICE name stuff for a cleaner looking nmcli\n- name: Enabling ifcfg slaves\n  shell: nmcli conn up id \"System {{ item|trim }}\"\n  ignore_errors: True\n  when: iiab_lan_iface == \"br0\" and item|trim != iiab_wireless_lan_iface and item|trim != iiab_wan_iface and not iiab_demo_mode\n  with_items:\n    - \"{{ lan_list_result.stdout_lines }}\"\n\n#- name: restart hostapd when wifi is present\n#  service: name=hostapd state=started\n#  when: iiab_wireless_lan_iface != \"none\" and hostapd_enabled and iiab_network_mode != \"Appliance\"\n\n#- name: dhcp_server may be affected - starting - user choice\n#  service: name={{ dhcp_service2 }} state=started\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8edae182b09ab2e07687db5b7ef6c85d5aba0882", "filename": "roles/config-versionlock/tests/inventory/group_vars/all.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nversionlock_packages:\n  - 'bash-*'\n  - 'NetworkManager-*'\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "0cb5193b549f3b10b81367905033f06b3a6802ea", "filename": "roles/kiwix/tasks/kiwix_install.yml", "repository": "iiab/iiab", "decoded_content": "# 1. CREATE/VERIFY CRITICAL DIRECTORIES & FILES ARE IN PLACE\n\n- name: Create various directories for Kiwix ZIM files\n  file:\n    path: \"{{ item }}\"\n    owner: root\n    group: root\n    mode: 0755\n    state: directory\n  with_items:\n  - \"{{ iiab_zim_path }}\"\n  - \"{{ iiab_zim_path }}/content\"\n  - \"{{ iiab_zim_path }}/index\"\n\n- name: Check for /library/zims/library.xml\n  stat:\n    path: \"{{ kiwix_library_xml }}\"\n  register: kiwix_xml\n\n- name: Place a stub /library/zims/library.xml if file does not exist\n  template:\n    src: \"{{ item }}\"\n    dest: \"{{ kiwix_library_xml }}\"\n    mode: 0644\n    owner: root\n    group: root\n    force: no\n  with_items:\n  - library.xml\n  when: not kiwix_xml.stat.exists\n\n- name: Check for /opt/iiab/kiwix/bin/kiwix-serve binary\n  stat:\n    path: \"{{ kiwix_path }}/bin/kiwix-serve\"\n  register: kiwix_bin\n\n- name: Set kiwix_force_install if kiwix-serve not found\n  set_fact:\n    kiwix_force_install: True\n  when: not kiwix_bin.stat.exists\n\n- name: Copy test.zim file if kiwix_force_install\n  copy:\n    src: test.zim\n    dest: \"{{ iiab_zim_path }}/content/test.zim\"\n    mode: 0644\n    owner: root\n    group: root\n    force: no\n  when: kiwix_force_install\n\n- name: Create /opt/iiab/kiwix/bin directory\n  file:\n    path: \"{{ kiwix_path }}/bin\"\n    owner: root\n    group: root\n    mode: 0755\n    state: directory\n\n# 2. INSTALL KIWIX-TOOLS EXECUTABLES IF kiwix_force_install\n\n- name: Unarchive kiwix-tools .tar.gz to /tmp\n  unarchive:\n    src: \"{{ downloads_dir }}/{{ kiwix_src_file }}\"\n    dest: /tmp\n    owner: root\n    group: root\n  when: kiwix_force_install\n\n- name: Move /tmp/{{ kiwix_src_dir }}/* to permanent location /opt/iiab/kiwix/bin (armhf & linux64 & i686)\n  shell: \"mv /tmp/{{ kiwix_src_dir }}/* {{ kiwix_path }}/bin/\"\n  when: kiwix_force_install\n\n# 3. ENABLE MODS FOR APACHE PROXY IF DEBUNTU\n\n- name: Enable the mods which permit Apache to proxy (debuntu)\n  apache2_module:\n    name: \"{{ item }}\"\n  with_items:\n  - proxy\n  - proxy_html\n  - proxy_http\n  - rewrite\n  when: is_debuntu\n\n# 4. CREATE/ENABLE/DISABLE KIWIX SERVICE & ITS CRON JOB\n\n- name: Create 'kiwix-serve' service and related files\n  template:\n    backup: no\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n    owner: root\n    group: root\n    mode: \"{{ item.mode }}\"\n  with_items:\n  - { src: 'kiwix-serve.service.j2', dest: '/etc/systemd/system/kiwix-serve.service', mode: '0644'}\n#   - { src: 'kiwix-serve-init.j2', dest: '/usr/libexec/kiwix-serve-init', mode: '0755'}\n  - { src: 'iiab-make-kiwix-lib', dest: '/usr/bin/iiab-make-kiwix-lib', mode: '0755'}\n  - { src: 'iiab-make-kiwix-lib.py', dest: '/usr/bin/iiab-make-kiwix-lib.py', mode: '0755'}\n#  - { src: 'iiab-make-apache-config.py', dest: '/usr/bin/iiab-make-apache-config.py', mode: '0755'}\n  - { src: 'kiwix.conf.j2', dest: '/etc/{{ apache_config_dir }}/kiwix.conf', mode: '0644'}\n\n- name: Enable Kiwix Proxy in Apache - is disabled by turning off kiwix service\n  file: path=/etc/apache2/sites-enabled/kiwix.conf\n        src=/etc/apache2/sites-available/kiwix.conf\n        state=link\n  when: is_debuntu\n\n- name: Enable 'kiwix-serve' service\n  service:\n    name: kiwix-serve\n    enabled: yes\n    state: restarted\n  when: kiwix_enabled\n\n- name: Disable 'kiwix-serve' service\n  service:\n    name: kiwix-serve\n    enabled: no\n    state: stopped\n  when: not kiwix_enabled\n# IN THEORY: BOTH CRON ENTRIES BELOW *SHOULD* BE DELETED \"when: not kiwix_enabled\"\n\n# In the past kiwix-serve did not stay running, so we'd been doing this hourly.\n# @mgautierfr & others suggest kiwix-serve might be auto-restarted w/o cron in\n# future, whenever service fails, if this really catches all cases??\n# https://github.com/iiab/iiab/issues/484#issuecomment-342151726\n- name: Make a crontab entry to restart kiwix-serve at 4AM (debuntu)\n  lineinfile:\n         # mn hr dy mo day-of-week[Sunday=0] username command-to-be-executed\n    line: \"0  4  *  *  * root /bin/systemctl restart kiwix-serve.service\"\n    dest: /etc/crontab\n  when: kiwix_enabled and is_debuntu\n\n- name: Make a crontab entry to restart kiwix-serve at 4AM (redhat)\n# *  *  *  *  * user-name  command to be executed\n  lineinfile:\n         # mn hr dy mo day-of-week[Sunday=0] username command-to-be-executed\n    line: \"0  4  *  *  * root /usr/bin/systemctl restart kiwix-serve.service\"\n    dest: /etc/crontab\n  when: kiwix_enabled and is_redhat\n\n- name: Restart apache, so it picks up kiwix.conf\n  service: name={{ apache_service }} state=restarted\n\n# 5. FINALIZE\n\n- name: Add 'kiwix' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: kiwix\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n  - option: name\n    value: Kiwix\n  - option: description\n    value: '\"Part of https://github.com/kiwix/kiwix-tools/ - kiwix-serve is the most used web server for ZIM files.\"'\n  - option: kiwix_url\n    value: \"{{ kiwix_url }}\"\n  - option: kiwix_path\n    value: \"{{ kiwix_path }}\"\n  - option: kiwix_port\n    value: \"{{ kiwix_port }}\"\n  - option: iiab_zim_path\n    value: \"{{ iiab_zim_path }}\"\n  - option: kiwix_library_xml\n    value: \"{{ kiwix_library_xml }}\"\n  - option: enabled\n    value: \"{{ kiwix_enabled }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8d1779e5979e71a112a9362eab1649dc8ba84b36", "filename": "roles/config-redis/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Install Containerized Redis\n  include_tasks: install_containerized.yml\n  when: mode == \"containerized\"\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "7d4955bdd434dcdadf109138df14311fb0477ee1", "filename": "tasks/selinux-RedHat.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n\n- name: Make sure we have the necessary yum packages available for selinux\n  yum:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - libselinux-python\n    - libsemanage-python\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "0f31b094471612acc6392c18dcdcb9b68a8387da", "filename": "reference-architecture/gcp/ansible/playbooks/roles/empty-image-delete/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: delete empty gce image\n  command: gcloud --project {{ gcloud_project }} compute images delete {{ empty_image_gce }}\n  ignore_errors: true\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "2aed5c713740415d3b58347e65fc140ccc25d3c9", "filename": "roles/config-vnc-server/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Prep for VNC server install\"\n  import_tasks: prereq.yml\n  when: \n  - vnc_server_install|default(False)\n\n- name: \"Install, configure and enable VNC server\"\n  import_tasks: vnc-server.yml\n  when: \n  - vnc_server_install|default(False)\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "95b3950855ab4f5d8cd6ba5dd90e295d6efbeda0", "filename": "reference-architecture/gcp/ansible/playbooks/roles/empty-image/defaults/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\nempty_image: empty-1g\nempty_image_raw: disk.raw\nempty_image_archive: '{{ empty_image }}.tar.gz'\nempty_image_bucket: gs://{{ gcloud_project }}-empty-raw-image\nempty_image_in_bucket: '{{ empty_image_bucket }}/{{ empty_image_archive }}'\nempty_image_gce: '{{ empty_image }}'\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "fbe551818c050df3dfde20321b5e5404c5a57f90", "filename": "roles/4-server-options/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# Server Options\n\n- name: ...IS BEGINNING ==================================\n  command: echo\n\n# MANDATORY SO PERHAPS THIS BELONGS IN 3-BASE-SERVER ?\n- name: SSHD\n  include_role:\n    name: sshd\n  # has no \"when: XXXXX_install\" flag\n  tags: base, sshd\n\n- name: OPENVPN\n  include_role:\n    name: openvpn\n  when: openvpn_install\n  tags: openvpn\n\n- name: Installing dnsmasq\n  include_tasks: roles/network/tasks/dnsmasq.yml\n  when: dnsmasq_install\n  tags: base, domain, dnsmasq, network\n\n- name: Installing named\n  include_tasks: roles/network/tasks/named.yml\n  when: named_install\n  tags: base, named, network, domain\n\n- name: Installing dhcpd\n  include_tasks: roles/network/tasks/dhcpd.yml\n  when: dhcpd_install\n  tags: base, dhcpd, network, domain\n\n- name: Installing Squid\n  include_tasks: roles/network/tasks/squid.yml\n  when: squid_install\n  tags: base, squid, network, domain\n\n#- name: NETWORK\n#  include_role:\n#    name: network\n#  # has no \"when: XXXXX_install\" flag\n#  tags: base, network\n\n# MANDATORY SO PERHAPS THIS BELONGS IN 3-BASE-SERVER ?\n- name: HOMEPAGE\n  include_role:\n    name: homepage\n  # has no \"when: XXXXX_install\" flag\n  tags: base, homepage\n\n- name: POSTGRESQL\n  include_role:\n    name: postgresql\n  when: postgresql_install\n  tags: postgresql, pathagar, moodle\n\n- name: AUTHSERVER\n  include_role:\n    name: authserver\n  when: authserver_install\n  tags: olpc, authserver\n\n- name: CUPS\n  include_role:\n    name: cups\n  when: cups_install\n  tags: cups\n\n- name: SAMBA\n  include_role:\n    name: samba\n  when: samba_install\n  tags: samba\n\n- name: USB-LIB\n  include_role:\n    name: usb-lib\n  when: usb_lib_install\n  tags: usb-lib\n\n# MANDATORY SO PERHAPS THIS BELONGS IN 3-BASE-SERVER ?\n- name: Create a Python interface to iiab.env\n  template:\n    src: roles/1-prep/templates/iiab_env.py.j2\n    dest: /etc/iiab/iiab_env.py\n\n- name: Run /usr/bin/iiab-refresh-wiki-docs (scraper script) to create http://box/info offline documentation (script was installed at the beginning of Stage 3 = roles/3-base-server/tasks/main.yml, which runs the HTTPD playbook = roles/httpd/tasks/main.yml)\n  command: /usr/bin/iiab-refresh-wiki-docs\n  when: not nodocs\n\n- name: Recording STAGE 4 HAS COMPLETED ==================\n  lineinfile:\n    dest: /etc/iiab/iiab.env\n    regexp: '^STAGE=*'\n    line: 'STAGE=4'\n    state: present\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "2af660284841bf389715f654682f4f78069c993d", "filename": "roles/weave/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for weave\n- name: Up weave interface\n  shell: ifup weave\n  sudo: yes\n\n- name: Down weave interface\n  shell: ifdown weave\n  sudo: yes\n\n- name: Restart networking\n  service: name=networking state=restarted\n  sudo: yes\n\n- name: Weave launch\n  command: /usr/local/bin/weave launch {{ weave_launch_peers }}\n  sudo: yes\n  tags:\n    - weave\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "af8fca924a8a84dceb7aff858e4bcdf6ea952f36", "filename": "roles/manage-jira/tests/playbook.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Test Jira Role\n  hosts: jira\n  vars_files:\n  - vars/vars_atlassian\n  roles:\n  - manage-jira\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "090f34bbe5425b115079beb9f1740284f2851c9b", "filename": "roles/cfme-ocp-provider/tests/test.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- hosts: localhost\n  roles:\n    - cfme-ocp-provider\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "09a9471be6d5714f28e6f9479ff4439ddd6cb4df", "filename": "roles/ovirt-collect-logs/vars/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_collect_logs_tmp_dir: \"/var/tmp/ovirt-logs\"\novirt_collect_logs_archive: \"/var/tmp/ovirt-logs.tar.gz\"\novirt_collect_logs_shell_commands:\n  rpm-list: \"rpm -qa | sort -f\"\n  yum-list: \"yum list installed\"\n  services: \"systemctl -t service --failed --no-legend | awk '{print $1}'\n            | xargs -r -n1 journalctl -u\"\n  iptables: \"iptables -L\"\n  lsof: \"lsof -P\"\n  pstree: \"pstree -p\"\n  sysctl: \"sysctl -a\"\n  netstat: \"netstat -lnp\"\n  lsmod: \"lsmod\"\n  lspci: \"lspci\"\n  memory_usage: \"ps -e -orss=,args= | sort  -b -k1,1n | tac\"\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "55c2f3318f316db0cc3b9fa1de1b7c30e4891a0a", "filename": "roles/manage-jira/tasks/create_project.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Create Jira Project\n  uri:\n    url: \"{{ atlassian.jira.url }}/rest/api/2/project\"\n    method: POST\n    user: \"{{ atlassian.jira.username }}\"\n    password: \"{{ atlassian.jira.password }}\"\n    return_content: yes\n    force_basic_auth: yes\n    body_format: json\n    header:\n      - Accept: 'application/json'\n      - Content-Type: 'application/json'\n    body: \"{ 'key': '{{ atlassian.jira.project.key }}',\n             'name': '{{ atlassian.jira.project.name }}',\n             'projectTypeKey': '{{ atlassian.jira.project.type_key | default('software')}}',\n             'projectTemplateKey': '{{ atlassian.jira.project.template_key | default('com.pyxis.greenhopper.jira:gh-simplified-scrum')}}',\n             'description': '{{ atlassian.jira.project.description }}',\n             'lead': '{{ atlassian.jira.lead }}',\n             'assigneeType': 'PROJECT_LEAD',\n             'avatarId': 10200,\n             'permissionScheme': '{{ atlassian.jira.permission_scheme.id | default(PermissionScheme) }}',\n             'notificationScheme': 10000, \n             'categoryId': '{{ CategoryID }}' \n           }\"\n    status_code: 201         \n  register: result\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "f0327f6115ebbd2a79606384cad436334ab82914", "filename": "roles/docker/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for docker\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "677d1f9caa2f6a3ea4f0694224993b7674c23fc8", "filename": "tasks/section_11.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - include: section_11_level1.yml\n    tags:\n      - section11\n      - level1\n\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "cc0215e8f2c569fba38226f3c284693d46862803", "filename": "roles/elasticsearch/defaults/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\nes_user: elasticsearch\nes_group: elasticsearch\nes_data_dir: \"{{ rock_data_dir }}/elasticsearch\"\nes_cluster_name: rocknsm\nes_node_name: \"{{ inventory_hostname }}\"\nes_network_host: >-\n  {%- if (groups['elasticsearch']|union(groups['logstash'])|union(groups['kibana']))| count > 1 -%}\n  _site:ipv4_\n  {%- else -%}\n  _local:ipv4_\n  {%- endif -%}\nes_interface: >-\n    {%- if (groups['elasticsearch']|union(groups['logstash'])|union(groups['kibana']))| count > 1 -%}\n    {{ ansible_hostname }}\n    {%- else -%}\n    localhost\n    {%- endif -%}\nes_action_auto_create_index: true\nes_min_master_nodes: \"{{ 2 if ( groups['es_masters'] | length ) == 3 else 1 }}\"\nes_mem: \"{{ (ansible_memtotal_mb // 1024 // 2) if (ansible_memtotal_mb // 1024) < 64 else 31 }}\"\nes_url: \"http://127.0.0.1:9200\"\nes_log_dir: /var/log/elasticsearch\nes_memlock_override: |\n  [Service]\n  LimitMEMLOCK=infinity\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "af28fc98f35672bd1ab2dbc921a1b5df3c00bbff", "filename": "roles/openstack-stack/tasks/subnet_update_dns_servers.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Live update the subnet's DNS servers\n  os_subnet:\n    name: openshift-ansible-{{ stack_name }}-subnet\n    network_name: openshift-ansible-{{ stack_name }}-net\n    state: present\n    use_default_subnetpool: yes\n    dns_nameservers: \"{{ [private_dns_server|default(public_dns_nameservers[0])]|union(public_dns_nameservers)|unique }}\"\n  when: not provider_network\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "1024f329ab13a1fe7409e6c18fb357183a3eb2fd", "filename": "tasks/section_10_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 10.1.1 Set Password Expiration Days (Scored)\n    lineinfile: >\n        dest='/etc/login.defs'\n        line='PASS_MAX_DAYS 90'\n        state=present\n        regexp='^PASS_MAX_DAYS'\n    tags:\n      - section10\n      - section10.1\n      - section10.1.1\n\n  - name: 10.1.2 Set Password Change Minimum Number of Days (Scored)\n    lineinfile: >\n        dest='/etc/login.defs'\n        line='PASS_MIN_DAYS 7'\n        regexp='^PASS_MIN_DAYS'\n    tags:\n      - section10\n      - section10.1\n      - section10.1.2\n\n  - name: 10.1.3 Set Password Expiring Warning Days (Scored)\n    lineinfile: >\n        dest='/etc/login.defs'\n        line='PASS_WARN_AGE'\n        regexp='^PASS_WARN_AGE'\n    tags:\n      - section10\n      - section10.1\n      - section10.1.3\n\n  - name: 10.2 Disable System Accounts (check) (Scored)\n    shell: awk -F':' '($1!=\"root\" && $1!=\"sync\" && $1!=\"shutdown\" &&$1!=\"halt\" && $3<500 && $7!=\"/usr/sbin/nologin\" && $7!=\"/bin/false\") {print $1}' /etc/passwd\n    register: awk_etc_passwd\n    changed_when: False\n    tags:\n      - section10\n      - section10.2\n\n  - name: 10.2 Disable System Accounts (Scored)\n    command: /usr/sbin/usermod -s /usr/sbin/nologin {{ item }}\n    with_items: awk_etc_passwd.stdout_lines\n    tags:\n      - section10\n      - section10.2\n\n  - name: 10.3 Set Default Group for root Account (Scored)\n    user: >\n        name=root\n        group=root\n    tags:\n      - section10\n      - section10.3\n\n  - name: 10.4 Set Default umask for Users (Scored)\n    lineinfile: >\n        dest=/etc/login.defs\n        line='UMASK\\t077'\n        regexp='^UMASK'\n        state=present\n    tags:\n      - section10\n      - section10.4\n\n  - name: 10.5 Lock Inactive User Accounts (check) (Scored)\n    command: grep INACTIVE /etc/login.defs\n    changed_when: False\n    failed_when: False\n    always_run: True\n    register: lock_inactive_rc\n    tags:\n      - section10\n      - section10.5\n        \n  - name: 10.5 Lock Inactive User Accounts (Scored)\n    lineinfile: >\n        dest=/etc/login.defs\n        line='INACTIVE=35'\n        state=present\n    when: lock_inactive_rc.rc == 1\n    tags:\n      - section10\n      - section10.5\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "5b8028ec8ff48213229e9bf303e142fecaa5aca3", "filename": "roles/9-local-addons/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# Local Add-ons\n\n- name: ...IS BEGINNING ====================================\n  command: echo\n  \n- name: CALIBRE\n  include_role:\n    name: calibre\n  when: calibre_install\n  tags: calibre\n\n- name: Recording STAGE 9 HAS COMPLETED ====================\n  lineinfile:\n    dest: /etc/iiab/iiab.env\n    regexp: '^STAGE=*'\n    line: 'STAGE=9'\n    state: present\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "4c480d0f5d3890413d41ede6891bacf32d07fe4f", "filename": "tasks/create_repo_pypi_proxy_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_pypi_proxy\n    args: \"{{ _nexus_repos_pypi_defaults|combine(item) }}\""}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "86e265e95027fd4472e4dda64e5eece96276bfa7", "filename": "roles/manage-confluence-space/tasks/download_attachment.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Download attachment from source\n  get_url:\n    url: '{{ confluence_source_url }}/wiki/{{ attachment_data._links.download }}'\n    dest: '{{ attachment_tempdir.path }}/{{ attachment_data.title }}'\n    force: yes\n    force_basic_auth: yes\n    url_username: '{{ confluence_source_username }}'\n    url_password: '{{ confluence_source_password }}'\n  no_log: true\n  delegate_to: 127.0.0.1\n\n- name: Upload attachment to destination\n  command: 'curl -u {{ confluence_destination_username }}:{{ confluence_destination_password }} -X POST -H \"X-Atlassian-Token: no-check\" -F \"file=@{{ attachment_tempdir.path }}/{{ attachment_data.title }}\" {{ confluence_destination_url }}/wiki/rest/api/content/{{ confluence_content_ids.value.id }}/child/attachment'\n  delegate_to: 127.0.0.1\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "5edad19f91b0c906c75c15d1985ad409f1d08471", "filename": "dev/playbooks/install_nfs_server.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: nfs\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Open required ports\n      command: firewall-cmd --permanent --zone=public --add-port=2049/tcp --add-port=2049/udp --add-port=111/tcp --add-port=111/udp --add-port=54302/tcp --add-port=54302/udp --add-port=20048/tcp --add-port=20048/udp --add-port=46666/tcp --add-port=46666/udp --add-port=42955/tcp --add-port=42955/udp --add-port=875/tcp --add-port=875/udp\n\n    - name: Reload firewalld configuration\n      command: firewall-cmd --reload\n\n    - name: Check for partitions on disk\n      shell: parted -s {{ disk2 }} print 1\n      register: partPresent\n      failed_when: partPresent.rc is not defined\n\n    - name: Create partition on second disk\n      parted:\n        device: \"{{ disk2 }}\"\n        number: 1\n        state: present    \n      when: partPresent.rc != 0\n\n    - name: Create filesystem\n      filesystem:\n        fstype: xfs\n        dev: \"{{ disk2_part }}\"\n      when: partPresent.rc != 0\n\n    - name: Create images folder\n      file:\n        path: \"{{ images_folder }}\"\n        state: directory\n        mode: 0777\n      when: partPresent.rc != 0\n\n    - name: Mount filesystem\n      mount:\n        path: \"{{ images_folder }}\"\n        src: \"{{ disk2_part }}\"\n        fstype: xfs\n        state: mounted\n\n    - name: Install NFS server\n      yum:\n        name: \"{{ item }}\"\n        state: latest\n      with_items:\n        - rpcbind\n        - nfs-utils\n\n    - name: Enable and start NFS services on server\n      systemd:\n        name: \"{{ item }}\"\n        enabled: yes\n        state: started\n      with_items:\n        - rpcbind\n        - nfs-server\n        - nfs-lock\n        - nfs-idmap\n\n    - name: Modify exports file on NFS server\n      template: src=../templates/exports.j2 dest=/etc/exports\n\n    - name: Refresh exportfs\n      command: exportfs -a\n"}, {"commit_sha": "c91b6076e3a957fb0a165131d0ff3b3b208ed419", "sha": "4bbaf74e0dfca0864005485ed5fa66027276a966", "filename": "tasks/section_09_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 9.1.1.1 Check that cron conf file exists (check) (Scored)\n    stat: path=/etc/init/cron.conf\n    register: cron_conf_stat\n    tags:\n      - section9\n      - section9.1\n      - section9.1.1\n      - section9.1.1.1\n\n  - name: 9.1.1.2 Enable cron Daemon (Scored)\n    service: >\n        name=cron\n        state=started\n        enabled=yes\n    when: not cron_conf_stat.stat.exists\n    tags:\n      - section9\n      - section9.1\n      - section9.1.1\n      - section9.1.1.2\n\n  - name: 9.1.2 Set User/Group Owner and Permission on /etc/crontab (Scored)\n    file: path='/etc/crontab' owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.2\n\n  - name: 9.1.3 Set User/Group Owner and Permission on /etc/cron.hourly (Scored)\n    file: path=/etc/cron.hourly owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.3\n\n  - name: 9.1.4 Set User/Group Owner and Permission on /etc/cron.daily (Scored)\n    file: path=/etc/cron.daily owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.4\n\n  - name: 9.1.5 Set User/Group Owner and Permission on /etc/cron.weekly(Scored)\n    file: path=/etc/cron.weekly owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.5\n\n  - name: 9.1.6 Set User/Group Owner and Permission on /etc/cron.monthly(Scored)\n    file: path=/etc/cron.monthly owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.6\n\n  - name: 9.1.7 Set User/Group Owner and Permission on /etc/cron.d (Scored)\n    file: path=/etc/cron.d owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.7\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (Scored)\n    file: path={{ item }} state=absent\n    with_items:\n        - /etc/cron.deny\n        - /etc/at.deny\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n  \n  - name: 9.1.8 Restrict at/cron to Authorized Users (preparation) (Scored)\n    stat: path=/etc/cron.allow\n    register: cron_allow_path\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (preparation) (Scored)\n    shell: touch /etc/cron.allow\n    when: cron_allow_path.stat.exists == False\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (Scored)\n    file: path=/etc/cron.allow owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (preparation) (Scored)\n    stat: path=/etc/at.allow\n    register: at_allow_path\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (preparation) (Scored)\n    shell: touch /etc/at.allow\n    when: at_allow_path.stat.exists == False\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n\n  - name: 9.1.8 Restrict at/cron to Authorized Users (Scored)\n    file: path=/etc/at.allow owner=root group=root mode=\"og-rwx\"\n    tags:\n      - section9\n      - section9.1\n      - section9.1.8\n \n  - name: 9.2.1 Set Password Creation Requirement Parameters Using pam_cracklib (install) (Scored)\n    apt: name=libpam-cracklib state=present\n    when: use_pam_cracklib == True\n    tags:\n      - section9\n      - section9.2\n      - section9.2.1\n      \n  - name: 9.2.1 Set Password Creation Requirement Parameters Using pam_cracklib (Scored)\n    lineinfile: >\n        dest='/etc/pam.d/common-password'\n        regexp=\"pam_cracklib.so\"\n        line=\"password required pam_cracklib.so retry=3 minlen=14 dcredit=-1 ucredit=-1 ocredit=-1 lcredit=-1\"\n        state=present\n    when: use_pam_cracklib == True\n    tags:\n      - section9\n      - section9.2\n      - section9.2.1\n\n#Note for section 9.2.2:\n#If a user has been locked out because they have reached the maximum consecutive failure count \n#defined by denied= in the pam_tally2.so module, the user can be unlocked by issuing the command\n#/sbin/pam_tally2 -u username --reset\n#This command sets the failed count to 0, effectively unlocking the user\n  - name: 9.2.2 Set Lockout for Failed Password Attempts (Not Scored)\n    lineinfile: >\n        dest='/etc/pam.d/login' \n        regexp='pam_tally2'\n        line=\"auth required pam_tally2.so onerr=fail audit silent deny=5 unlock_time=900\"\n        state=present\n    tags:\n      - section9\n      - section9.2\n      - section9.2.2\n\n  - name: 9.2.3 Limit Password Reuse (Scored)\n    lineinfile: >\n        dest='/etc/pam.d/common-password' \n        regexp='remember=5'\n        line=\"password sufficient pam_unix.so remember=5\"\n        state=present\n    tags:\n      - section9\n      - section9.2\n      - section9.2.3\n\n  - name: 9.3 Check if ssh is installed (check)\n    stat: path='/etc/ssh/sshd_config'\n    register: ssh_config_file\n    tags:\n      - section9\n      - section9.3\n      - section9.3.1\n\n\n  - name: 9.3.1 Set SSH Protocol to 2 (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config' \n        regexp='^Protocol'\n        state=present\n        line='Protocol 2'\n    when: ssh_config_file.stat.exists == True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.1\n\n  - name: 9.3.2 Set LogLevel to INFO (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config' \n        regexp='^LogLevel'\n        state=present\n        line='LogLevel INFO'\n    when: ssh_config_file.stat.exists == True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.2\n\n  - name: 9.3.3 Set Permissions on /etc/ssh/sshd_config (Scored)\n    file: path='/etc/ssh/sshd_config' owner=root group=root mode=600\n    when: ssh_config_file.stat.exists == True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.3\n\n#Regroups sections 9.3.4 9.3.7 9.3.8 9.3.9 9.3.10\n  - name: 9.3.{4,7,8,9,10} Disable some SSH options (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^{{ item }}'\n        line='{{ item }} no'\n        state=present\n    with_items: \n      - X11Forwarding\n      - HostbasedAuthentication\n      - PermitRootLogin\n      - PermitEmptyPasswords\n      - PermitUserEnvironment\n    tags:\n      - section9\n      - section9.3\n      - section9.3.4\n      - section9.3.7\n      - section9.3.8\n      - section9.3.9\n      - section9.3.10\n\n  - name: 9.3.5 Set SSH MaxAuthTries to 4 or Less (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^MaxAuthTries'\n        line='MaxAuthTries 4'\n        state=present\n    tags:\n      - section9\n      - section9.3\n      - section9.3.5\n\n  - name: 9.3.6 Set SSH IgnoreRhosts to Yes (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^IgnoreRhosts'\n        line='IgnoreRhosts yes'\n        state=present\n    tags:\n      - section9\n      - section9.3\n      - section9.3.6\n\n  - name: 9.3.11 Use Only Approved Cipher in Counter Mode (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^Ciphers'\n        line='Ciphers aes128-ctr,aes192-ctr,aes256-ctr'\n        state=present\n    tags:\n      - section9\n      - section9.3\n      - section9.3.11\n\n  - name: 9.3.12.1 Set Idle Timeout Interval for User Login (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^ClientAliveInterval'\n        line='ClientAliveInterval 300'\n        state=present\n    tags:\n      - section9\n      - section9.3\n      - section9.3.12\n      - section9.3.12.1\n\n  - name: 9.3.12.2 Set Idle Timeout Interval for User Login (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^ClientAliveCountMax'\n        line='ClientAliveCountMax 0'\n        state=present\n    tags:\n      - section9\n      - section9.3\n      - section9.3.12\n      - section9.3.12.2\n\n  - name: 9.3.13.1 Limit Access via SSH (Scored)\n    shell: grep AllowUsers /etc/ssh/sshd_config\n    register: allow_rc\n    failed_when: allow_rc.rc == 1\n    changed_when: False\n    ignore_errors: True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.13\n      - section9.3.13.1\n\n  - name: 9.3.13.2 Limit Access via SSH (Scored)\n    shell: grep AllowGroups /etc/ssh/sshd_config\n    register: allow_rc\n    failed_when: allow_rc.rc == 1\n    changed_when: False\n    ignore_errors: True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.13\n      - section9.3.13.2\n\n  - name: 9.3.13.3 Limit Access via SSH (Scored)\n    shell: grep DenyUsers /etc/ssh/sshd_config\n    register: allow_rc\n    failed_when: allow_rc.rc == 1\n    changed_when: False\n    ignore_errors: True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.13\n      - section9.3.13.3\n\n  - name: 9.3.13.4 Limit Access via SSH (Scored)\n    shell: grep DenyGroups /etc/ssh/sshd_config\n    register: allow_rc\n    failed_when: allow_rc.rc == 1\n    changed_when: False\n    ignore_errors: True\n    tags:\n      - section9\n      - section9.3\n      - section9.3.13\n      - section9.3.13.4\n\n  - name: 9.3.14 Set SSH Banner (Scored)\n    lineinfile: >\n        dest='/etc/ssh/sshd_config'\n        regexp='^Banner'\n        line='Banner /etc/issue.net'\n        state=present\n    tags:\n      - section9\n      - section9.3\n      - section9.3.14\n\n  - name: 9.4 Restrict root Login to System Console (Not Scored)\n    stat: path=/etc/securetty\n    register: securetty_file\n    tags:\n      - section9\n      - section9.4\n\n  - name: 9.4 Restrict root Login to System Console (Not Scored)\n    debug: msg='*** Check /etc/securetty for console allowed for root access ***'\n    when: securetty_file.stat.exists == True\n    tags:\n      - section9\n      - section9.4\n\n  - name: 9.5.1 Restrict Access to the su Command (Scored)\n    lineinfile: >\n        dest='/etc/pam.d/su'\n        line='auth            required        pam_wheel.so use_uid'\n        state=present\n    tags:\n      - section9\n      - section9.5\n      - section9.5.1\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "1a3c41540b504f7595502b78e7117194c51701e2", "filename": "roles/gluster-ports/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: open gluster ports\n  iptables:\n    chain: INPUT\n    destination_port: \"{{ item }}\"\n    jump: ACCEPT\n    ctstate: NEW\n    protocol: tcp\n    action: insert\n    match: tcp\n  with_items: \"{{ gluster_ports }}\"\n  when: groups['storage'] is defined and groups['storage'] != []\n  register: rule\n\n- name: save iptables\n  shell: iptables-save > /etc/sysconfig/iptables\n  when: rule|changed\n  notify:\n    - restart iptables\n\n- name: open gluster ports\n  iptables:\n    chain: INPUT\n    destination_port: \"{{ item }}\"\n    ctstate: NEW\n    jump: ACCEPT\n    protocol: tcp\n    action: insert\n    match: tcp\n  with_items: \"{{ crs_ports }}\"\n  when: groups['crs'] is defined and groups['crs'] != []\n  register: heketi\n\n- name: save iptables\n  shell: iptables-save > /etc/sysconfig/iptables\n  when: heketi|changed\n  notify:\n    - restart iptables\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e2d969eb3bd70c5c993666acf645dbc1a8f49800", "filename": "roles/manage-confluence-space/tests/playbook.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Test Confluence Role\n  hosts: confluence\n  vars_files:\n  - vars/vars_atlassian\n  roles:\n  - manage-confluence-space\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "da241437c80c7e9aacff98f9865480214ac18101", "filename": "reference-architecture/gcp/ansible/playbooks/roles/dns-records-delete/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: delete ns records\n  gcdns_record:\n    record: '{{ item.record }}'\n    zone: '{{ public_hosted_zone }}'\n    type: '{{ item.type }}'\n    overwrite: true\n    service_account_email: '{{ service_account_id }}'\n    credentials_file: '{{ credentials_file }}'\n    project_id: '{{ gcloud_project }}'\n    state: absent\n  with_items:\n  - record: '{{ openshift_master_cluster_public_hostname }}'\n    type: A\n  - record: '{{ openshift_master_cluster_hostname }}'\n    type: A\n  - record: '{{ wildcard_zone }}'\n    type: A\n  - record: '*.{{ wildcard_zone }}'\n    type: CNAME\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "ed19dc5e4e3db8dd64d8c27c289c0dd4ef88c30f", "filename": "tasks/setup_ldap_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include_tasks: call_script.yml\n  vars:\n    script_name: setup_ldap\n    args:\n      name: \"{{ item.ldap_name }}\"\n      protocol: \"{{ item.ldap_protocol }}\"\n      hostname: \"{{ item.ldap_hostname }}\"\n      port: \"{{ item.ldap_port }}\"\n      use_trust_store: \"{{ item.ldap_use_trust_store | default(false) | bool }}\"\n      auth: \"{{ item.ldap_auth | default('none') }}\"\n      username: \"{{ item.ldap_auth_username | default('') }}\"\n      password: \"{{ item.ldap_auth_password | default('') }}\"\n      search_base: \"{{ item.ldap_search_base }}\"\n      user_base_dn: \"{{ item.ldap_user_base_dn | default('ou=users') }}\"\n      user_ldap_filter: \"{{ item.ldap_user_filter | default('') }}\"\n      user_object_class: \"{{ item.ldap_user_object_class }}\"\n      user_id_attribute: \"{{ item.ldap_user_id_attribute }}\"\n      user_real_name_attribute: \"{{ item.ldap_user_real_name_attribute }}\"\n      user_email_attribute: \"{{ item.ldap_user_email_attribute }}\"\n      map_groups_as_roles: \"{{ item.ldap_map_groups_as_roles | default(false) }}\"\n      map_groups_as_roles_type: \"{{ item.ldap_map_groups_as_roles_type | default('static') }}\"\n      user_memberof_attribute: \"{{ item.ldap_user_memberof_attribute | default('memberOf') }}\"\n      group_base_dn: \"{{ item.ldap_group_base_dn | default('ou=groups') }}\"\n      group_object_class: \"{{ item.ldap_group_object_class | default('groupOfNames') }}\"\n      group_id_attribute: \"{{ item.ldap_group_id_attribute | default('cn') }}\"\n      group_member_attribute: \"{{ item.ldap_group_member_attribute | default('member') }}\"\n      group_member_format: \"{{ item.ldap_group_member_format | default('uid=${username},ou=users,dc=yourcompany') }}\"\n      user_subtree: \"{{ item.ldap_user_subtree | default(false) }}\"\n      group_subtree: \"{{ item.ldap_group_subtree | default(false) }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "77b3bc0ff5c97d8e2d23149146887b648ebc89b8", "filename": "reference-architecture/azure-ansible/3.6/ansibledeployocp/playbooks/destroy.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  connection: local\n  gather_facts: no\n  become: false\n  roles:\n    - { role: azure-delete, tags: ['delete'] }\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "7a61ce7ffd3249d3b9d0a2d5766ecbf4b380f5bf", "filename": "reference-architecture/vmware-ansible/playbooks/roles/instance-groups/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# create rhsm_user, rhsm_password, rhsm_subscription_pool and rhsm_server for functionality with older rhsm_user\n- name: Set deprecated fact for rhel_subscription_user\n  set_fact:\n    rhsm_user: \"{{ rhel_subscription_user }}\"\n  when: rhel_subscription_user is defined\n\n- name: Set deprecated fact for rhel_subscription_pass\n  set_fact:\n    rhsm_password: \"{{ rhel_subscription_pass }}\"\n  when: rhel_subscription_pass is defined\n\n- name: Set deprecated fact for rhel_subscription_pool\n  set_fact:\n    rhsm_pool: \"{{ rhel_subscription_pool }}\"\n  when: rhel_subscription_pool is defined\n\n- name: Add masters to requisite groups\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    openshift_hostname: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: allnodes, masters, etcd, nodes, cluster_hosts, master\n    openshift_node_labels:\n      role: master\n  with_items: \"{{ groups[cluster_id + '-master'] }}\"\n\n- name: Add a master to the single master group\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: single_master\n    openshift_node_labels:\n      role: master\n  with_items: \"{{ groups[cluster_id + '-master'][0] }}\"\n\n- name: Add infra instances to host group\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: allnodes, nodes, cluster_hosts, schedulable_nodes, infra\n    openshift_hostname: \"{{ hostvars[item].inventory_hostname }}\"\n    openshift_node_labels:\n      role: infra\n  with_items: \"{{ groups[cluster_id + '-infra'] }}\"\n\n- name: Add app instances to host group\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    openshift_hostname: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: allnodes, nodes, cluster_hosts, schedulable_nodes, app\n    openshift_node_labels:\n      role: app\n  with_items: \"{{ groups[cluster_id + '-app'] }}\"\n\n- name: Add new node instances to host group\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    openshift_hostname: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: allnodes, new_nodes\n    openshift_node_labels:\n      role: \"{{ node_type }}\"\n  with_items: \"{{ groups.tag_provision_node | default([]) }}\"\n  when:\n    - add_node is defined\n\n- name: Add NFS instances to allnodes\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: allnodes\n  with_items: \"{{ groups[cluster_id + '-networkfs'] }}\"\n\n- name: Add loadbalancer instances to allnodes\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: allnodes\n  with_items: \"{{ groups[cluster_id + '-loadbalancer'] }}\"\n\n- name: Add cns instances to allnodes\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: allnodes\n  with_items: \"{{ groups[cluster_id + '-storage'] | default([]) }}\"\n\n- name: Add crs instances to allnodes\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: allnodes\n  with_items: \"{{ groups[cluster_id + '-crs'] | default([]) }}\"\n\n- name: Add cns instances to host group\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: nodes, cluster_hosts, schedulable_nodes\n    openshift_node_labels:\n      role: storage\n  with_items: \"{{ groups[cluster_id + '-storage'] }}\"\n  when:\n    - \"'cns' in container_storage and add_node is defined and 'storage' in node_type\"\n\n- name: Add crs nodes to the storage group\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: storage, crs\n  with_items: \"{{ groups[cluster_id + '-crs'] }}\"\n  when:\n    - \"'crs' in container_storage and add_node is defined and 'storage' in node_type\"\n\n- name: Add a crs node to the single crs group\n  add_host:\n    name: \"{{ hostvars[item].inventory_hostname }}\"\n    groups: single_crs\n  with_items: \"{{ groups[cluster_id + '-crs'][0] }}\"\n  when:\n    - \"'crs' in container_storage and add_node is defined and 'storage' in node_type\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7b9ade87ed0e2abf2cd4ef66a42bce2b06ca0995", "filename": "playbooks/manage-confluence-space/roles", "repository": "redhat-cop/infra-ansible", "decoded_content": "../roles/"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "d801d5ab81109abe0807da8f19e936d83eaf64b8", "filename": "tasks/section_04.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - include: section_04_level1.yml\n    tags:\n      - section04\n      - level1\n\n  - include: section_04_level2.yml\n    tags:\n      - section04\n      - level2\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "557fa489e07aba0d612092823e976b42fe97bfef", "filename": "tasks/Win32NT/fetch/chocolatey.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: Set java_binary_type = chocolatey\n  set_fact:\n    java_binary_type: chocolatey\n\n- name: Chocolatey will download artifact itself\n  debug:\n    msg: 'Chocolatey will download artifact itself'\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "17430ea807dddbb6d2290b9cd99abc279972c1c4", "filename": "ops/playbooks/scale_ucp.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- name: Scale UCP\n  hosts: ucp\n  serial: 1\n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - includes/internal_vars.yml\n\n  pre_tasks:\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n\n  roles:\n    - role: ucp\n      ARG_UCP_IP:        \"{{ ucp_instance }}.{{ domain_name }}\"\n      ARG_UCP_USER:      \"{{ ucp_username }}\"\n      ARG_UCP_PASSWORD:  \"{{ ucp_password }}\"\n      ARG_ADVERTIZE_IP:  \"{{ ucp_instance }}.{{ domain_name }}:2377\"\n      ucp_role_ports:    \"{{ internal_ucp_ports }}\"\n      ucp_role_join_delay: 180\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "1551150dc2a302e3e6abbd96f030a84592b4b25a", "filename": "roles/config-repo-server/tasks/mount-iso.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Ensure the ISO mount dir exist\"\n  file:\n    path: \"{{ item.iso_file_target }}\"\n    state: directory\n  with_items:\n  - \"{{ hosted_isos }}\"\n\n- name: \"Mount the ISOs\"\n  mount:\n    path: \"{{ item.iso_file_target }}\"\n    src: \"{{ item.iso_file_path }}\"\n    fstype: iso9660\n    state: mounted\n  with_items:\n  - \"{{ hosted_isos }}\"\n\n\n\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "29e975349b7b3434bbc8fced1490725bd53e6db9", "filename": "roles/user-management/manage-atlassian-users/tasks/create_atlassian_users.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Create User\n  uri:\n    url: '{{ atlassian_url }}/rest/api/2/user'\n    method: POST\n    user: '{{ atlassian_username }}'\n    password: '{{ atlassian_password }}'\n    force_basic_auth: yes\n    status_code: 201\n    body_format: json\n    body: \"{'name': '{{ atlassian_user.email.split(\\\"@\\\") | first }}', \n            'password': '{{ atlassian_user.password }}', \n            'emailAddress': '{{ atlassian_user.email }}', \n            'displayName': '{{ atlassian_user.firstname }} {{ atlassian_user.lastname }}' }\"\n    return_content: yes\n  register: user_data\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "8f427cd221c0cfacbff14db93927fea12cc1ed3e", "filename": "playbooks/openshift-management.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- hosts: openshift\n  roles:\n    - openshift-management\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "45570d910498e56bcf16fcd8cd5c3d5dd07b9e51", "filename": "roles/ansible/tower/manage-job-templates/tasks/set-permissions.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Obtain team id based on the team name\"\n  set_fact:\n    object_id: \"{{ item.id }}\"\n  when:\n  - permissions_object == \"teams\"\n  - item.name|trim == permissions_value.name|trim\n  with_items:\n  - \"{{ existing_teams_output.rest_output }}\"\n\n- name: \"Obtain user id based on the username\"\n  set_fact:\n    object_id: \"{{ item.id }}\"\n  when:\n  - permissions_object == \"users\"\n  - item.username|trim == permissions_value.name|trim\n  with_items:\n  - \"{{ existing_users_output.rest_output }}\"\n\n- name: \"Obtain role id based on the job_template name + role name\"\n  set_fact:\n    role_id: \"{{ item.id }}\"\n  when:\n  - item.summary_fields is defined\n  - item.summary_fields.resource_name is defined\n  - item.summary_fields.resource_name|trim == job_template.name|trim\n  - item.name|trim == permissions_value.role|trim\n  with_items:\n  - \"{{ existing_roles_output.rest_output }}\"\n\n- name: \"Set Permission\"\n  uri:\n    url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/{{ permissions_object }}/{{ object_id }}/roles/\"\n    user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n    password: \"{{ ansible_tower.admin_password }}\"\n    force_basic_auth: yes\n    method: POST\n    body: \"{{ { 'id': role_id|int } }}\"\n    body_format: 'json'\n    headers:\n      Content-Type: \"application/json\"\n      Accept: \"application/json\"\n    validate_certs: no\n    status_code: 200,204\n  when:\n  - object_id is defined\n  - object_id|trim != ''\n  - role_id is defined\n  - role_id|trim != ''\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b9b8ecb4827e7fa2ace6f9ac4c472cbe8270a94c", "filename": "reference-architecture/vmware-ansible/playbooks/roles/create-vm-prod-ose/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Assign app memory for container_storage\n  set_fact:\n    app_memory: 32768\n  when: \"'cns' in container_storage\"\n\n- name: Assign app memory for container_storage\n  set_fact:\n    app_memory: 8192\n  when: \"'cns' not in container_storage\"\n\n- name: Create production master node VMs on vCenter\n  vmware_guest:\n    hostname: \"{{ vcenter_host }}\"\n    username: \"{{ vcenter_username }}\"\n    password: \"{{ vcenter_password }}\"\n    validate_certs: False\n    name: \"{{ item.value.guestname }}\"\n    cluster: \"{{ vcenter_cluster}}\"\n    datacenter: \"{{ vcenter_datacenter }}\"\n    resource_pool: \"{{ vcenter_resource_pool }}\"\n    template: \"{{vcenter_template_name}}\"\n    state: poweredon\n    wait_for_ip_address: true\n    folder: \"/{{ vcenter_folder }}\"\n    annotation: \"{{ item.value.tag }}\"\n    disk:\n    - size_gb: 60\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 40\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 40\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 40\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    hardware:\n      memory_mb: 16384\n    networks:\n    - name: \"{{ vm_network }}\"\n      ip: \"{{ item.value.ip4addr }}\"\n      netmask: \"{{ vm_netmask }}\"\n      gateway: \"{{ vm_gw }}\"\n    customization:\n      domain: \"{{dns_zone}}\"\n      dns_servers:\n      - \"{{ vm_dns }}\"\n      dns_suffix: \"{{dns_zone}}\"\n      hostname: \"{{ item.value.guestname}}\"\n  with_dict: \"{{host_inventory}}\"\n  when: \"'master' in item.value.guestname\"\n\n- name: Create production infra node VMs on vCenter\n  vmware_guest:\n    hostname: \"{{ vcenter_host }}\"\n    username: \"{{ vcenter_username }}\"\n    password: \"{{ vcenter_password }}\"\n    validate_certs: False\n    name: \"{{ item.value.guestname }}\"\n    cluster: \"{{ vcenter_cluster}}\"\n    datacenter: \"{{ vcenter_datacenter }}\"\n    resource_pool: \"{{ vcenter_resource_pool }}\"\n    template: \"{{vcenter_template_name}}\"\n    state: poweredon\n    wait_for_ip_address: true\n    disk:\n    - size_gb: 60\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 40\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 40\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    folder: \"/{{ vcenter_folder }}\"\n    annotation: \"{{ item.value.tag }}\"\n    hardware:\n      memory_mb: 8192\n    networks:\n    - name: \"{{ vm_network }}\"\n      ip: \"{{ item.value.ip4addr }}\"\n      netmask: \"{{ vm_netmask }}\"\n      gateway: \"{{ vm_gw }}\"\n    customization:\n      domain: \"{{dns_zone}}\"\n      dns_servers:\n      - \"{{ vm_dns }}\"\n      dns_suffix: \"{{dns_zone}}\"\n      hostname: \"{{ item.value.guestname}}\"\n  with_dict: \"{{host_inventory}}\"\n  when: \"'infra' in item.value.guestname\"\n\n- name: Create production app node VMs on vCenter\n  vmware_guest:\n    hostname: \"{{ vcenter_host }}\"\n    username: \"{{ vcenter_username }}\"\n    password: \"{{ vcenter_password }}\"\n    validate_certs: False\n    name: \"{{ item.value.guestname }}\"\n    cluster: \"{{ vcenter_cluster}}\"\n    datacenter: \"{{ vcenter_datacenter }}\"\n    resource_pool: \"{{ vcenter_resource_pool }}\"\n    template: \"{{vcenter_template_name}}\"\n    state: poweredon\n    wait_for_ip_address: true\n    disk:\n    - size_gb: 60\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 40\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 40\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    folder: \"/{{ vcenter_folder }}\"\n    annotation: \"{{ item.value.tag }}\"\n    hardware:\n      memory_mb: \"{{ app_memory }}\"\n    networks:\n    - name: \"{{ vm_network }}\"\n      ip: \"{{ item.value.ip4addr }}\"\n      netmask: \"{{ vm_netmask }}\"\n      gateway: \"{{ vm_gw }}\"\n    customization:\n      domain: \"{{dns_zone}}\"\n      dns_servers:\n      - \"{{ vm_dns }}\"\n      dns_suffix: \"{{dns_zone}}\"\n      hostname: \"{{ item.value.guestname}}\"\n  with_dict: \"{{host_inventory}}\"\n  when: \"'app' in item.value.guestname\"\n\n- name: Add production VMs to inventory\n  add_host: hostname=\"{{ item.value.guestname }}\" ansible_fqdn=\"{{ item.value.guestname }}.{{ dns_zone }}\" ansible_ssh_host=\"{{ item.value.ip4addr }}\" groups=\"{{ item.value.tag }}, production_group\"\n  with_dict: \"{{host_inventory}}\"\n  when: \"'master' in item.value.guestname or 'app' in item.value.guestname or 'infra' in item.value.guestname\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "efe7f25f61a5e4cb7db4d7358fc5a158e1276b26", "filename": "playbooks/roles/sensor-common/defaults/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\nmethod: deploy\n...\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "56f07cd0614228e2cbe622fa092dbff66a425308", "filename": "ops/playbooks/install_playwithdocker.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "---\n- hosts: playwithdocker\n  gather_facts: true\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n# Install missing PIP package and deps\n\n\n  tasks:\n    - name: Install easy_install\n      yum:\n        name: python-setuptools\n        state: latest\n      changed_when: false\n\n    - name: Install PIP\n      easy_install:\n        name: pip\n        state: latest\n\n    - name: Ensure latest docker.py is installed\n      pip:\n        name: docker.py\n        state: latest\n      changed_when: false\n\n    #\n    # If any old versions of docker-compose, docker-py or docker PIP\n    # packages are present, remove them.\n    #\n    - name: Remove Docker PIP components\n      pip: \n        name: \"{{ item }}\"\n        state: absent\n      with_items:\n        - docker-compose\n        - docker-py\n        - docker\n\n    #\n    # Install the latest docker-compose PIP package.\n    #\n    - name: Install latest docker-compose PIP package\n      pip: \n        name: \"{{ item }}\"\n        state: latest\n      with_items:\n        - docker-compose\n\n    #\n    # Install the latest docker-compose binary.\n    #\n    - name: Install docker-compose binary\n      get_url:\n        url: \"{{ compose_url }}\"\n        dest: /usr/local/bin/docker-compose\n        mode: 0755\n\n    #\n    # Load the xt_ipvs Kernel Module needed by Docker-in-Docker.\n    #\n    - name: Configure xt_ipvs Kernel Module\n      modprobe:\n        name: xt_ipvs\n        state: present\n\n    #\n    # Load the overlay Kernel Module needed by Docker-in-Docker.\n    #\n    - name: Configure overlay Kernel Module\n      modprobe:\n        name: overlay\n        state: present\n\n    #\n    # Download the latest Play-with-Docker pwd image.\n    #\n    - name: Pull Play-with-Docker pwd Image\n      docker_image:\n        name: franela/play-with-docker\n\n    #\n    # Download the latest Play-with-Docker l2 image.\n    #\n    - name: Pull Play-with-Docker l2 Image\n      docker_image:\n        name: franela/play-with-docker:l2\n\n    #\n    # Download the latest Docker-in-Docker image.\n    #\n    - name: Pull Docker-in-Docker Image\n      docker_image:\n        name: franela/dind\n\n    #\n    # Download the latest HAProxy image.\n    #\n    - name: Pull HAProxy Image\n      docker_image:\n        name: haproxy\n\n    #\n    # Tune sysctl Settings\n    #\n    - name: Tune net.ipv4.neigh.default.gc_thresh1 = 4096\n      sysctl:\n        name: net.ipv4.neigh.default.gc_thresh1\n        value: 4096 \n        state: present\n        reload: yes\n\n    - name: Tune net.ipv4.neigh.default.gc_thresh2 = 8192\n      sysctl:\n        name: net.ipv4.neigh.default.gc_thresh2\n        value: 8192 \n        state: present\n        reload: yes\n\n    - name: Tune net.ipv4.neigh.default.gc_thresh3 = 8192\n      sysctl:\n        name: net.ipv4.neigh.default.gc_thresh3\n        value: 8192 \n        state: present\n        reload: yes\n\n    - name: Tune net.ipv4.tcp_tw_recycle = 1\n      sysctl:\n        name: net.ipv4.tcp_tw_recycle\n        value: 1\n        state: present\n        reload: yes\n\n    - name: Tune net.ipv4.ip_local_port_range = 1024 65000\n      sysctl:\n        name: net.ipv4.ip_local_port_range\n        value: 1024 65000\n        state: present\n        reload: yes\n\n    - name: Tune fs.inotify.max_user_instances = 10000\n      sysctl:\n        name: fs.inotify.max_user_instances\n        value: 10000\n        state: present\n        reload: yes\n\n    - name: Tune net.netfilter.nf_conntrack_tcp_timeout_established = 600\n      sysctl:\n        name: net.netfilter.nf_conntrack_tcp_timeout_established\n        value: 600\n        state: present\n        reload: yes\n\n    - name: Tune net.netfilter.nf_conntrack_tcp_timeout_time_wait = 1\n      sysctl:\n        name: net.netfilter.nf_conntrack_tcp_timeout_time_wait\n        value: 1\n        state: present\n        reload: yes\n\n    #\n    # Check if Play-with-Docker directory exists.\n    #\n    - name: Check if play-with-docker directory exists\n      stat:\n        path: \"{{ pwd_path }}\"\n      register: pwd_dir\n\n    #\n    # Shutdown any previous instances of play-with-docker.\n    #\n    - name: Stop any running play-with-docker Instance\n      docker_service:\n        project_name: play-with-docker\n        project_src: \"{{ pwd_path }}\"\n        state: absent\n      when: pwd_dir.stat.exists == true\n\n    #\n    # Remove any existing play-with-docker directory.\n    #\n    - name: Delete existing play-with-docker Directory\n      file:\n        path: \"{{ pwd_path }}\"\n        state: absent\n      when: pwd_dir.stat.exists == true\n\n    #\n    # Create Play-with-Docker directory\n    #\n    - name: Create Play-with-Docker directory\n      file:\n        path: \"{{ pwd_path }}\"\n        state: directory\n        mode: 0755\n\n    #\n    # Create HAProxy directory\n    #\n    - name: Create HAProxy Directory\n      file:\n        path: \"{{ pwd_path }}/haproxy\"\n        state: directory\n        mode: 0755\n\n    #\n    # Copy pwd haproxy.cfg File\n    #\n    - name: Copy haproxy.cfg File\n      copy:\n        src: ../files/haproxy.cfg\n        dest: \"{{ pwd_path }}/haproxy/haproxy.cfg\"\n        owner: root\n        group: root\n        mode: 0644\n\n    #\n    # Deploy Play-with-Docker docker-compose.yml File\n    #\n    - name: Deploy Play-with-Docker docker-compose.yml File\n      template:\n        src: ../templates/pwd-docker-compose.yml.j2\n        dest: \"{{ pwd_path }}/docker-compose.yml\"\n        owner: root\n        group: root\n        mode: 0644\n\n    #\n    # Start the new play-with-docker instance.\n    #\n    - name: Start Play-with-Docker\n      docker_service: \n        project_name: play-with-docker\n        project_src: \"{{ pwd_path }}\"\n        build: no\n        state: present\n\n    #\n    # Pause for 10 seconds to let play-with-docker instance start.\n    #\n    - name: Pause 10 seconds while play-with-docker initializes\n      pause:\n        seconds: 10\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "f6c375460ff046aa43b73152313bd3f2c4c6abfa", "filename": "roles/teamviewer/tasks/install.yml", "repository": "iiab/iiab", "decoded_content": "# we need to install X11 and the Xfce display manager\n- name: Install Xfce group of packages\n  shell: \"yum groupinstall -y xfce\"\n  when: xo_model == \"none\" and internet_available and ansible_distribution_version <= \"20\"\n  tags:\n    - download\n\n- name: Install X11 group of packages\n  shell: \"yum groupinstall -y 'X Window system'\"\n  when: xo_model == \"none\" and internet_available and ansible_distribution_version <= \"20\"\n  tags:\n    - download\n\n- name: Install Xfce group of packages\n  shell: yum groupinstall -y \"Xfce Desktop\" --exclude fedora-release\\*\n  when: xo_model == \"none\" and internet_available and ansible_distribution_version >= \"21\"\n  tags:\n    - download\n\n- name: Install X Windows on CentOS\n  shell: yum groupinstall -y \"Server with GUI\"\n  when: internet_available and ansible_distribution == \"CentOS\"\n  tags:\n    - download\n\n- name: Download TeamViewer software\n  get_url:\n    url: \"{{ teamviewer_url }}/{{ teamviewer_rpm_file }}\"\n    dest: \"{{ yum_packages_dir }}/{{ teamviewer_rpm_file }}\"\n    timeout: \"{{ download_timeout }}\"\n  when: internet_available\n  tags:\n    - download\n\n# F22 has issues with yum localinstall exclude for now\n- name: Do the install of TeamViewer, pulling in any required dependencies\n  shell: \"yum localinstall -y {{ yum_packages_dir }}/{{ teamviewer_rpm_file }}\"\n  when: teamviewer_install and internet_available\n           and xo_model == \"none\" and ansible_distribution_version <= \"21\"\n\n- name: Making local copy available\n  shell: createrepo {{ yum_packages_dir }}\n  when: teamviewer_install and xo_model == \"none\" and ansible_distribution_version >= \"22\"\n\n- name: Using local copy\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - teamviewer*\n  when: teamviewer_install and xo_model == \"none\" and ansible_distribution_version >= \"22\"\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "d3b3c825b445249a785e67b877e327a2fa71f8e9", "filename": "playbooks/roles/docket/tasks/crypto.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# tasks file for rocknsm.docket\n\n# Install packages\n\n# Ensure `stenographer` and `nginx` groups exists\n\n# Configure docket app settings\n- name: docket | ensure ansible_cache dir exists\n  local_action: file path={{ ansible_cache }} state=directory mode=0700\n  changed_when: false\n\n# TODO: this should probably\n- name: docket | ensure rock nsm conf dir exists\n  file:\n    path: \"{{ rocknsm_conf_dir }}\"\n    state: directory\n    owner: \"{{ rocknsm_conf_user }}\"\n    group: \"{{ rocknsm_conf_group }}\"\n\n- name: docket | ensure docket x509 user exists\n  user:\n    name: \"{{ docket_x509_user }}\"\n    state: present\n  when: inventory_hostname in groups['docket']\n\n- name: docket | ensure docket x509 group exists\n  group:\n    name: \"{{ docket_x509_group }}\"\n    system: yes\n    state: present\n  when: inventory_hostname in groups['docket']\n\n- name: docket | ensure docket x509 dir exists\n  file:\n    path: \"{{ docket_x509_dir }}\"\n    state: directory\n    mode:  0750\n    owner:  \"{{ docket_x509_user }}\"\n    group: \"{{ docket_x509_group }}\"\n  when: inventory_hostname in groups['docket']\n\n\n# Generate/copy x509 client cert/keys and CA certs\n# Use new openssl module in ansible 2.3\n- name: docket | create docket private key\n  openssl_privatekey:\n    path: \"{{docket_x509_key}}\"\n    size: 4096\n  when: inventory_hostname in groups['docket']\n\n- name: docket | set perms on private key\n  file:\n    path: \"{{docket_x509_key}}\"\n    owner: \"{{ docket_x509_user }}\"\n    group: \"{{ docket_x509_group }}\"\n    mode: \"0644\"\n  when: inventory_hostname in groups['docket']\n\n- name: docket | check for certificate\n  stat:\n    path: \"{{docket_x509_dir}}/docket-{{inventory_hostname}}_sensor-{{item}}_cert.pem\"\n  register: docket_cert\n  changed_when: false\n  with_items: \"{{ groups['stenographer'] }}\"\n  when: inventory_hostname in groups['docket']\n\n- debug: var=docket_cert.results\n  when: inventory_hostname in groups['docket']\n\n- name: docket | create docket csr\n  openssl_csr:\n    path: \"{{ docket_x509_key }}.csr\"\n    privatekey_path: \"{{ docket_x509_key }}\"\n    commonName: \"{{ docket_x509_cn }}\"\n    organizationName: \"{{ docket_x509_o }}\"\n    countryName: \"{{ docket_x509_c }}\"\n    keyUsage: digitalSignature\n    extendedKeyUsage: clientAuth\n  when:\n    - inventory_hostname in groups['docket']\n    - docket_cert|json_query('results[?stat.exists==`false`]')|length\n  register: new_csr\n\n- name: docket | fetch csr\n  fetch:\n    src: \"{{docket_x509_key}}.csr\"\n    dest: \"{{ ansible_cache }}/{{inventory_hostname}}.csr\"\n    flat: yes\n  when:\n    - inventory_hostname in groups['docket']\n    - not new_csr|skipped\n\n- debug:\n    msg: \"{{ hostvars[item].docket_cert|json_query('results[?stat.exists==`false`]')|length }}\"\n  when:\n    - inventory_hostname in groups['stenographer']\n  with_items:\n    - \"{{ groups['docket'] }}\"\n\n- debug:\n    var: hostvars[item].docket_cert|json_query('results[?item==inventory_hostname]')\n    #selectattr(\"item\", \"equalto\", inventory_hostname)|map(attribute=\"stat.exists\")|select(\"equalto\", false)|list|length\n  with_items:\n    - \"{{ groups['docket'] }}\"\n  when:\n    - inventory_hostname in groups['stenographer']\n\n- name: docket | push csr to stenographer hosts\n  copy:\n    src: \"{{ansible_cache}}/{{hostvars[item].inventory_hostname}}.csr\"\n    dest: \"{{steno_certs_dir}}/{{hostvars[item].inventory_hostname}}.csr\"\n  with_items:\n    - \"{{ groups['docket'] }}\"\n  when:\n    - inventory_hostname in groups['stenographer']\n    - hostvars[item].docket_cert.results|selectattr(\"item\", \"equalto\", inventory_hostname)|map(attribute=\"stat.exists\")|select(\"equalto\",false)|list|length\n\n- name: docket | sign certificate signing requests\n  openssl_certificate:\n    path: \"{{steno_certs_dir}}/docket-{{hostvars[item].inventory_hostname}}_sensor-{{inventory_hostname}}_cert.pem\"\n    privatekey_path: \"{{steno_ca_key}}\"\n    csr_path: \"{{steno_certs_dir}}/{{hostvars[item].inventory_hostname}}.csr\"\n    provider: localsigned\n    cacert_path: \"{{steno_ca_cert}}\"\n  with_items: \"{{ groups['docket'] }}\"\n  when:\n    - inventory_hostname in groups['stenographer']\n    - hostvars[item].docket_cert.results|map(attribute=\"stat.exists\")|select(\"equalto\",false)|list|length\n\n- name: docket | pull certificates back\n  fetch:\n    src: \"{{steno_certs_dir}}/docket-{{hostvars[item].inventory_hostname}}_sensor-{{inventory_hostname}}_cert.pem\"\n    dest: \"{{ansible_cache}}/docket-{{hostvars[item].inventory_hostname}}_sensor-{{inventory_hostname}}_cert.pem\"\n    flat: yes\n  with_items: \"{{ groups['docket'] }}\"\n  when:\n    - inventory_hostname in groups['stenographer']\n    - hostvars[item].docket_cert.results|map(attribute=\"stat.exists\")|select(\"equalto\",false)|list|length\n\n- name: docket | pull back ca certificates\n  fetch:\n    src: \"{{steno_ca_cert}}\"\n    dest: \"{{ansible_cache}}/{{inventory_hostname}}_ca_cert.pem\"\n    flat: yes\n  when:\n    - inventory_hostname in groups['stenographer']\n  changed_when: false\n\n- name: docket | push certificates to docket hosts\n  copy:\n    src: \"{{ansible_cache}}/docket-{{inventory_hostname}}_sensor-{{hostvars[item].inventory_hostname}}_cert.pem\"\n    dest: \"{{docket_x509_dir}}/docket-{{inventory_hostname}}_sensor-{{hostvars[item].inventory_hostname}}_cert.pem\"\n    owner: \"{{ docket_x509_user }}\"\n    group: \"{{ docket_x509_group }}\"\n    mode: \"0644\"\n  with_items: \"{{ groups['stenographer'] }}\"\n  when:\n    - inventory_hostname in groups['docket']\n    - docket_cert.results|map(attribute=\"stat.exists\")|select(\"equalto\",false)|list|length\n\n- name: docket | push stenographer ca certs\n  copy:\n    src: \"{{ansible_cache}}/{{hostvars[item].inventory_hostname}}_ca_cert.pem\"\n    dest: \"{{docket_x509_dir}}/{{hostvars[item].inventory_hostname}}_ca_cert.pem\"\n    owner: \"{{ docket_x509_user }}\"\n    group: \"{{ docket_x509_group }}\"\n    mode: \"0644\"\n  with_items: \"{{ groups['stenographer'] }}\"\n  when:\n    - inventory_hostname in groups['docket']\n\n- name: docket | cleanup {{ ansible_cache }} dir\n  file:\n    name: \"{{ ansible_cache }}/\"\n    state: absent\n  changed_when: false\n"}, {"commit_sha": "584d564218d6e2e63d3ecca157daf817fe4d533c", "sha": "7bc8caca4b1c764cd7509e52bb9b8cfc36f65fad", "filename": "tasks/security_check.yml", "repository": "mikolak-net/ansible-raspi-config", "decoded_content": "---\n- name: ensure utility present\n  apt: name=sshpass state=present\n- name: check for login\n  command: sshpass -p {{raspi_config_auth_test_password}} ssh {{raspi_config_auth_test_username}}@localhost -o NoHostAuthenticationForLocalhost=yes \"echo {{raspi_config_auth_test_string}}\"\n  register: auth_test\n  changed_when: False\n  failed_when: False\n- name: optional warning\n  debug: msg=\"{{raspi_config_auth_test_fail_msg}}\"\n  when: \"raspi_config_auth_test_string == auth_test.stdout\"\n  changed_when: \"raspi_config_auth_test_string == auth_test.stdout\" # for highlighting purposes\n  failed_when: \"raspi_config_fail_on_auth_test and raspi_config_replace_user['name'] == ''\"\n- name: additional info\n  debug: msg=\"{{raspi_config_auth_test_replace_info}}\"\n  when: \"raspi_config_auth_test_string == auth_test.stdout and raspi_config_replace_user['name'] != ''\""}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "239911063d68a50b3475746e08c4216c8f007090", "filename": "roles/config-linux-desktop/config-lxde/tasks/lxde-Fedora.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: \"Install additional packages for LXDE\"\n  dnf:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n  - '@lxde-desktop'\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "fa794dd3c74bc7a4e06bf5b847908dcdc19074da", "filename": "roles/wordpress/tasks/install.yml", "repository": "iiab/iiab", "decoded_content": "# \"Emergency\" reinstalls (from /opt/iiab/downloads/wordpress.tar.gz\n# to /library/wordpress) should also work offline...\n#\n# ONLINE OR OFFLINE, IF YOU NEED A CLEAN REINSTALL OF WORDPRESS DURING YOUR\n# NEXT RUN OF \"./runrole wordpress\" OR \"./iiab-install\" PLEASE FIRST DO:\n#\n# - \"mv /library/wordpress /library/wordpress.old\"\n# - back up WordPress's database then drop it\n#\n# REASON: \"keep_newer: yes\" below tries to preserves WordPress's self-upgrades\n# and security enhancements using timestamps under /library/wordpress, as these\n# can arise without warning when WordPress is online, since WordPress ~4.8.\n\n- name: Download the latest WordPress software\n  get_url:\n    url: \"{{ wordpress_download_base_url }}/{{ wordpress_src }}\"\n    dest: \"{{ downloads_dir }}\"\n    timeout: \"{{ download_timeout }}\"\n#   force: yes\n#   backup: yes\n  register: wp_download_output\n  when: internet_available\n\n- name: Create link /opt/iiab/downloads/wordpress.tar.gz pointing to {{ wp_download_output.dest }}\n  file:\n    src: \"{{ wp_download_output.dest }}\"\n    dest: \"{{ downloads_dir }}/wordpress.tar.gz\"\n    state: link\n  when: wp_download_output.dest is defined\n\n- name: Check if /opt/iiab/downloads/wordpress.tar.gz link exists\n  stat:\n    path: \"{{ downloads_dir }}/wordpress.tar.gz\"\n  register: wp_link\n\n- name: FAIL (force Ansible to exit) IF /opt/iiab/downloads/wordpress.tar.gz doesn't exist\n  fail:\n    msg: \"{{ downloads_dir }}/wordpress.tar.gz is REQUIRED in order to install WordPress.\"\n  when: not wp_link.stat.exists\n\n- name: \"Unpack /opt/iiab/downloads/wordpress.tar.gz to permanent location /library/wordpress - owner: root, group: {{ apache_user }}, mode: 0664, keep_newer: yes\"\n  unarchive:\n    src: \"{{ downloads_dir }}/wordpress.tar.gz\"\n    dest: \"{{ wp_install_path }}\"\n    owner: root\n    group: \"{{ apache_user }}\"\n    mode: 0664\n    keep_newer: yes\n\n# - name: Rename /library/wordpress* to /library/wordpress\n#   shell: if [ ! -d {{ wp_abs_path }} ]; then mv {{ wp_abs_path }}* {{ wp_abs_path }}; fi\n\n#- name: Make Apache owner and group, 1st pass permissions set to 0664\n#  file: path={{ wp_abs_path }}\n#        recurse=yes\n#        owner=root\n#        group={{ apache_user }}\n#        mode=0664\n#        state=directory\n\n- name: Make /library/wordpress directories 775 so Apache can traverse and write (most files remain 0664)\n  command: \"/usr/bin/find {{ wp_abs_path }} -type d -exec chmod 775 {} +\"\n\n- name: Copy wp salt values\n  copy:\n    src: wp-keys.php.BAK\n    dest: \"{{ wp_abs_path }}/wp-keys.php.BAK\"\n    owner: root\n    group: \"{{ apache_user }}\"\n    mode: 0640\n\n# Fetch random salts for WordPress config into wp-keys.php file by generating script and running\n\n- name: Create wp salt script\n  template:\n    src: get-iiab-wp-salts.j2\n    dest: /tmp/get-iiab-wp-salts\n    owner: root\n    group: root\n    mode: 0700\n\n- name: Run wp salt script to create /library/wordpress/wp-keys.php\n  command: /tmp/get-iiab-wp-salts\n\n- name: Cleanup - remove wp salt script\n  file:\n    path: /tmp/get-iiab-wp-salts\n    state: absent\n\n- name: MySQL database needs to be running if we are trying to create a new db\n  service:\n    state: started\n    name: \"{{ mysql_service }}\"\n\n- name: Create MySQL wordpress database\n  mysql_db:\n    name: \"{{ wp_db_name }}\"\n    state: present\n\n- name: Create MySQL wordpress database user\n  mysql_user:\n    name: \"{{ wp_db_user }}\"\n    password: \"{{ wp_db_user_password }}\"\n    priv: \"{{ wp_db_name }}.*:ALL,GRANT\"\n    state: present\n\n- name: Copy WordPress config file\n  template:\n    src: wp-config.php.j2\n    dest: \"{{ wp_abs_path }}/wp-config.php\"\n    owner: root\n    group: \"{{ apache_user }}\"\n    mode: 0660\n\n- name: Copy WordPress httpd conf file\n  template:\n    src: wordpress.conf.j2\n    dest: \"/etc/{{ apache_config_dir }}/wordpress.conf\"\n\n- name: Enable httpd conf file if we are disabled (debuntu)\n  file:\n    src: /etc/apache2/sites-available/wordpress.conf\n    dest: /etc/apache2/sites-enabled/wordpress.conf\n    state: link\n  when: wordpress_enabled and is_debuntu\n\n- name: Remove httpd conf file if we are disabled (OS's other than debuntu)\n  file:\n    path: /etc/apache2/sites-enabled/wordpress.conf\n    state: absent\n  when: not wordpress_enabled and is_debuntu\n\n- name: Restart Apache, so it picks up the new aliases\n  service:\n    name: \"{{ apache_service }}\"\n    state: restarted\n\n- name: Add 'wordpress' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: wordpress\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: WordPress\n    - option: description\n      value: '\"WordPress is a blog and web site management application.\"'\n    - option: wordpress_src\n      value: \"{{ wordpress_src }}\"\n    - option: wp_abs_path\n      value: \"{{ wp_abs_path }}\"\n    - option: wp_db_name\n      value: \"{{ wp_db_name }}\"\n    - option: wp_db_user\n      value: \"{{ wp_db_user }}\"\n    - option: wp_url\n      value: \"{{ wp_url }}\"\n    - option: wp_full_url\n      value: \"{{ wp_full_url }}\"\n    - option: wordpress_enabled\n      value: \"{{ wordpress_enabled }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "01d2f1c23a0eff14da80c7009903a5d1f62ba319", "filename": "roles/config-lvm/tasks/prep.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Install required packages'\n  package:\n    name: '{{ item }}'\n    state: installed\n  with_items:\n  - lvm2\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "6f9f708dc4ee4d12525712464d2899a133fcce77", "filename": "roles/sugar-stats/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: Install sugar-stats required packages\n  package: name={{ item }}\n           state=present\n  with_items:\n    - sugar-stats-server\n    - active-document\n    - restful-document\n    - python-xappy\n  tags:\n    - download\n\n- name: Create sugar-stats directory tree\n  file: path={{ item }}\n        owner=sugar-stats\n        group=sugar-stats\n        mode=0755\n        state=directory\n  with_items:\n    - /library/sugar-stats/\n    - /library/sugar-stats/rrd\n    - /library/sugar-stats/users\n\n- name: Copy sugar-stats config file\n  template: backup=yes\n            src=sugar-stats.conf.j2\n            dest=/etc/sugar-stats.conf\n            owner=sugar-stats\n            group=sugar-stats\n            mode=0644\n\n- name: Enable sugar-stats service\n  service: name=sugar-stats-server\n           enabled=yes\n  when: sugar_stats_enabled\n\n- name: Disable sugar-stats service\n  service: name=sugar-stats-server\n           enabled=no\n  when: not sugar_stats_enabled\n\n- include_tasks: statistics-consolidation.yml\n\n- name: Add sugar-stats to service list\n  ini_file: dest='{{ service_filelist }}'\n            section=sugar_stats\n            option='{{ item.option }}'\n            value='{{ item.value }}'\n  with_items:\n    - option: name\n      value: sugar_stats\n    - option: description\n      value: '\"Collect Sugar statistics, originally written by Alexy Lim, see: http://wiki.sugarlabs.org/go/Platform_Team/Usage_Statistics\"'\n    - option: installed\n      value: \"{{ sugar_stats_install }}\"\n    - option: enabled\n      value: \"{{ sugar_stats_enabled }}\"\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "9b34d7c60b81ee2e0ec6b4b7c8171c9a969acea8", "filename": "roles/local/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: Add the instance to an inventory group\n  add_host:\n    name: \"{{ server_ip }}\"\n    groups: vpn-host\n    ansible_ssh_user: \"{{ server_user }}\"\n    ansible_python_interpreter: \"/usr/bin/python2.7\"\n    cloud_provider: local\n  when: server_ip != \"localhost\"\n\n- name: Add the instance to an inventory group\n  add_host:\n    name: \"{{ server_ip }}\"\n    groups: vpn-host\n    ansible_ssh_user: \"{{ server_user }}\"\n    ansible_python_interpreter: \"/usr/bin/python2.7\"\n    ansible_connection: local\n    cloud_provider: local\n  when: server_ip == \"localhost\"\n\n- set_fact:\n    cloud_instance_ip: \"{{ server_ip }}\"\n\n- name: Ensure the group local exists in the dynamic inventory file\n  lineinfile:\n    state: present\n    dest: configs/inventory.dynamic\n    line: '[local]'\n\n- name: Populate the dynamic inventory\n  lineinfile:\n    state: present\n    dest: configs/inventory.dynamic\n    insertafter: '\\[local\\]'\n    regexp: \"^{{ server_ip }}.*\"\n    line: \"{{ server_ip }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ed8aac3982b8760d795afe2e92e24d5de52c5e6a", "filename": "playbooks/aws/openshift-cluster/list.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Generate oo_list_hosts group\n  hosts: localhost\n  gather_facts: no\n  connection: local\n  become: no\n  vars_files:\n  - vars.yml\n  tasks:\n  - set_fact: scratch_group=tag_clusterid_{{ cluster_id }}\n    when: cluster_id != ''\n  - set_fact: scratch_group=all\n    when: cluster_id == ''\n  - add_host:\n      name: \"{{ item }}\"\n      groups: oo_list_hosts\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n      oo_public_ipv4: \"{{ hostvars[item].ec2_ip_address }}\"\n      oo_private_ipv4: \"{{ hostvars[item].ec2_private_ip_address }}\"\n    with_items: \"{{ groups[scratch_group] | default([]) | difference(['localhost']) }}\"\n  - debug:\n      msg: \"{{ hostvars | oo_select_keys(groups[scratch_group] | default([])) | oo_pretty_print_cluster }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "074f315a34f5b43ca34b45082ab7b81805abba2b", "filename": "reference-architecture/rhv-ansible/playbooks/roles/gdeployer/templates/gdeploy.conf", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "[hosts]\n{% for host in groups['hypervisors'] %}\n{{hostvars[host]['ansible_default_ipv4']['address'] }}\n{% endfor %}\n\n[backend-setup]\ndevices=/dev/sda5\nvgs=gluster_vg1\nmountpoints=/rhgs\n\n[peer]\nmanage=probe\n\n[volume1]\naction=create\nvolname=engine\nreplica=yes\nreplica_count=3\nkey=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm\nvalue=virt,36,36,on,512MB,32,full\nbrick_dirs=/rhgs/brick1\n\n[volume2]\naction=create\nvolname=data\nreplica=yes\nreplica_count=3\nkey=group,storage.owner-uid,storage.owner-gid,features.shard,features.shard-block-size,performance.low-prio-threads,cluster.data-self-heal-algorithm\nvalue=virt,36,36,on,512MB,32,full\nbrick_dirs=/rhgs/brick2\n\n[volume3]\naction=create\nvolname=registry\nreplica=yes\nreplica_count=3\nbrick_dirs=/rhgs/brick3\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "660daed635df5ced76173db3bd856ac63fdf7361", "filename": "reference-architecture/aws-ansible/playbooks/roles/openshift-versions/defaults/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\norigin_release: \"3.6.0\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "d3d607439d006da3fa3dd1c67a683aeabcd18d47", "filename": "reference-architecture/aws-ansible/playbooks/validation.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- hosts: localhost\n  gather_facts: yes\n  become: no\n  vars_files:\n  - vars/main.yaml\n  pre_tasks:\n  - name: set fact\n    set_fact:\n      openshift_master_cluster_public_hostname: \"{{ openshift_master_cluster_public_hostname }}\"\n  - name: set fact\n    set_fact:\n      openshift_master_cluster_hostname: \"{{ openshift_master_cluster_hostname }}\"\n  roles:\n  # Group systems\n  - instance-groups\n\n- include: ../../../playbooks/post-validation.yaml\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "21750cc2430a1150a4ee9dd6695e9c60276e0546", "filename": "roles/dnsmasq/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for dnsmasq\n- name: Restart dnsmasq\n  service: name=dnsmasq state=restarted\n  sudo: yes\n\n- name: Reload dnsmasq\n  service: name=dnsmasq state=reloaded\n  sudo: yes\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "9d21e16160222f2ffb37b5cad25756cf940076f8", "filename": "playbooks/roles/zookeeper/tasks/install.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: Include OS specific tasks\n  include_tasks: \"{{ item }}\"\n  with_first_found:\n    - \"{{ ansible_distribution }}-{{ ansible_distribution_major_version }}.yml\"\n    - \"{{ ansible_distribution }}.yml\"\n    - \"{{ ansible_os_family }}.yml\"\n  vars:\n    method: install\n\n...\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "29822b92a18211ae736f0cad7d529eb3f077ccab", "filename": "roles/dnsmasq/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for dnsmasq\n- name: remove dnsmasq override\n  command: /bin/rm -f /etc/init/dnsmasq.override\n\n- name: ensure dnsmasq is running (and enable it at boot)\n  service: name=dnsmasq state=started enabled=yes\n\n- name: configure dnsmasq\n  sudo: yes\n  template: src=10-consul.j2 dest=/etc/dnsmasq.d/10-consul owner=root group=root mode=0644\n  notify:\n    - Restart dnsmasq\n  tags:\n    - dnsmasq\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "982f2a58363df6ef37031fb77a91dcb8dafde946", "filename": "roles/mysql/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "mysql_install: True\nmysql_enabled: False\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "4e65ae79550da5f22921ce35dbf58f73e0dd92a2", "filename": "playbooks/automation/deprovision.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "- import_playbook: \"{{ playbook_dir }}/../kubevirt.yml\"\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "e5add5bdb21e91d52738455c0f51eac26b4d7244", "filename": "roles/elasticsearch/tasks/before.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n- name: Install packages\n  yum:\n    name:\n      - java-11-openjdk-headless\n      - elasticsearch\n    state: installed\n  register: es_install\n\n- name: Create elasticsearch directory\n  file:\n    path: \"{{ es_data_dir }}\"\n    mode: 0755\n    owner: \"{{ es_user }}\"\n    group: \"{{ es_group }}\"\n    state: directory\n\n- name: Setup elasticsearch config\n  template:\n    src: elasticsearch.yml.j2\n    dest: /etc/elasticsearch/elasticsearch.yml\n    owner: root\n    group: \"{{ es_group }}\"\n    mode: 0640\n  register: es_config\n\n- name: Create elasticsearch systemd override dir\n  file:\n    path: /etc/systemd/system/elasticsearch.service.d\n    owner: root\n    group: root\n    mode: 0755\n    state: directory\n\n- name: Enable elasticsearch memlock in service override\n  copy:\n    content: \"{{ es_memlock_override }}\"\n    dest: /etc/systemd/system/elasticsearch.service.d/override.conf\n    mode: 0644\n    owner: root\n    group: root\n  register: es_memlock\n\n- name: Setup elasticsearch JVM options\n  template:\n    src: templates/es-jvm.options.j2\n    dest: /etc/elasticsearch/jvm.options\n    mode: 0640\n    owner: root\n    group: \"{{ es_group }}\"\n  register: es_jvm\n\n- name: Reload systemd\n  systemd:\n    daemon_reload: true\n  when: es_memlock.changed\n  tags:\n    - skip_ansible_lint  # [503] Tasks that run when changed should be handlers\n\n- name: Discover facts about data mount\n  set_fact:\n    rock_mounts:\n      mount: \"{{ item.mount }}\"\n      device: \"{{ item.device }}\"\n      size_total: \"{{ item.size_total }}\"\n  loop:\n    \"{{ ansible_mounts }}\"\n  when: (default_mount is defined and item.mount == default_mount and rock_mounts is not defined)\n\n- name: Determining if quotas are enabled\n  shell: grep \"{{ default_mount }}\" /etc/fstab | awk /prjquota/\n  register: prjquota\n  changed_when: false\n\n# - debug:\n#    msg: \"{{prjquota}}\"\n\n- name: Create elasticsearch quota project id\n  getent:\n    database: group\n    split: ':'\n    key: elasticsearch\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Map elasticsearch quota project id to name\n  lineinfile:\n    create: true\n    state: present\n    insertafter: EOF\n    path: /etc/projid\n    line: \"elasticsearch:{{ getent_group.elasticsearch[1] }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Define elasticsearch quota project directories\n  lineinfile:\n    create: true\n    state: present\n    insertafter: EOF\n    path: /etc/projects\n    line: \"{{ getent_group.elasticsearch[1] }}:{{ es_data_dir }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: set elasticsearch weight\n  set_fact:\n    elastic_weight: \"{{ rock_services | selectattr('name', 'equalto', 'elasticsearch') | map(attribute='quota_weight') | first }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: set elasticsearch quota if not user defined\n  set_fact:\n    elasticsearch_quota: \"{{ rock_mounts.size_total | int / xfs_quota_weight | int * elastic_weight | int }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: set elasticsearch project quota\n  xfs_quota:\n    type: project\n    name: elasticsearch\n    bhard: \"{{ elasticsearch_quota }}\"\n    state: present\n    mountpoint: \"{{ rock_mounts.mount }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Enable and start elasticsearch\n  service:\n    name: elasticsearch\n    state: \"started\"\n    enabled: \"{{ 'true' if rock_services | selectattr('name', 'equalto', 'elasticsearch') | map(attribute='enabled') | bool else 'false' }}\"\n\n- name: Create internal firewall zone\n  firewalld:\n    state: present\n    zone: internal\n    permanent: true\n  register: result\n\n- name: Reload firewalld to load zone\n  service:\n    name: firewalld\n    state: restarted\n  when: result.changed\n  tags:\n    - skip_ansible_lint  # [503] Tasks that run when changed should be handlers\n\n- name: Configure firewall ports for internal elastic communication\n  firewalld:\n    port: \"{{ item.1 }}/tcp\"\n    permanent: true\n    state: enabled\n    immediate: true\n    source: \"{{ hostvars[item.0]['ansible_default_ipv4']['address'] }}\"\n    zone: work  # This should be a different zone, but leaving this here for now\n  when: (groups['elasticsearch']|union(groups['logstash'])|union(groups['kibana']))| count > 1\n  loop: \"{{ (groups['elasticsearch']|union(groups['logstash'])|union(groups['kibana'])) | product([9200, 9300]) | list }}\"\n\n- name: Determine if Elasticsearch needs to be restarted\n  set_fact:\n    es_restart: true\n  when: \"(es_config.changed or es_memlock.changed or es_jvm.changed) and not es_install.changed\"\n  tags:\n    - skip_ansible_lint  # [503] Tasks that run when changed should be handlers\n\n...\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a1062cfd476d200ae5d8396572e7e736273c81af", "filename": "roles/scm/github.com/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n# defaults file for github\ngithub_api_base: https://api.github.com/orgs/{{ github_org_name }}\ngithub_api_proj: \"{{ github_api_base }}/projects\"\ngithub_api_teams: \"{{ github_api_base }}/teams\"\ngithub_api_repos: \"{{ github_api_base }}/repos\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "6b6ebfa7b120b6b2e545e62fc33ed8edef09df7f", "filename": "playbooks/rhsm.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n# If you want to use username/password for subscription, please ensure\n# to run the \"prep.yml\" playbook before running this playbook\n# - alternatively use the \"subscribe-host.yml\" playbook instead\n\n- hosts: rhsm_hosts\n  vars:\n    rhsm_username: \"{{ hostvars['localhost'].rhsm_username|default(omit) }}\"\n    rhsm_password: \"{{ hostvars['localhost'].rhsm_password|default(omit) }}\"\n  roles:\n  - role: rhsm\n    when:\n    - rhsm_register|default(False)\n  tags:\n  - configure_rhsm\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "31d8659936b762717e87765de38c475fa45ed451", "filename": "roles/notifications/md-to-html/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Test converting MD to HTML\n  hosts: localhost\n  roles:\n    - notifications/md-to-html\n  tasks:\n    - debug:\n        msg: \"{{ md_to_html.html_body_message }}\"\n    - debug:\n        msg: \"{{ md_to_html.html_message }}\"\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "664bd28d0d46a2bf153bfe9deae3194e3f396aef", "filename": "roles/wordpress/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "# SEE \"emergency\" REINSTALL INSTRUCTIONS IN roles/wordpress/tasks/install.yml\n\n- name: Include the install playbook\n  include_tasks: install.yml\n  when: wordpress_install\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "882db5a286cc2b403a95b046824a36b8def4310e", "filename": "examples/playbooks/backup_engine.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# Example playbook for backup of engine\n# use with backup_engine.inv\n- hosts: engine\n  remote_user: root\n  roles:\n    - {role: ovirt-engine-backup}\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "41269845da26c86253bea1b35b865af2c7c159ea", "filename": "roles/cloud-scaleway/tasks/image_facts.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: Set image id as a fact\n  set_fact:\n    image_id: \"{{ item.id }}\"\n  no_log: true\n  when:\n    - cloud_providers.scaleway.image == item.name\n    - cloud_providers.scaleway.arch == item.arch\n    - server_disk_size == item.root_volume.size\n  with_items: \"{{ outer_item['json']['images'] }}\"\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "8300adcb5fcb716e225169707f9cb00c2fc75b99", "filename": "tasks/create_content_selector_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_content_selector\n    args: \"{{ item }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "13e7d4f5c8a2d598b0eac088ece3b316008e22b5", "filename": "roles/config-selinux/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Configure SELinux\"\n  selinux:\n    state: \"{{ target_state | default('enforcing') }}\"\n    policy: \"{{ (target_state == 'disabled') | ternary(omit, target_policy) }}\"\n\n- name: \"Relabel SElinux contexts\"\n  command: \"restorecon -r {{ selinux_relabel_dir | default('/') }}\"\n  when: selinux_relabel | default('no') | bool\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8e314abcc68eee1f847c254dea115c63b9db872c", "filename": "roles/user-management/manage-atlassian-users/tasks/delete_atlassian_users.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Delete User \n  uri:\n    url: '{{ atlassian.url }}/rest/api/2/user?username={{ atlassian_user.firstname }}'\n    method: DELETE\n    user: '{{ atlassian.username }}'\n    password: '{{ atlassian.password }}'\n    force_basic_auth: yes\n    status_code: 204\n    body_format: json\n  register: result\n  when:\n    - atlassian_user.state is defined\n    - atlassian_user.state == \"absent\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "a69861b50c86b8693f465f6510d62ac159a845e6", "filename": "ops/playbooks/backup_ucp.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- name: Backup UCP metadata\n  hosts: ucp_main \n  gather_facts: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - ../group_vars/backups\n\n  environment:\n    - \"{{ env }}\"\n    - UCP_USERNAME: \"{{ ucp_username }}\"\n    - UCP_PASSWORD: \"{{ ucp_password }}\"\n    - UCP_CA: \"{{ ucp_ca_cert | default('') }}\"\n\n  tasks:\n#\n# Find a UCP endpoint and the actual version of UCP\n#\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n    - debug: var=ucp_instance\n      when: _debug is defined\n    - include_tasks: includes/get_ucp_version.yml\n    - debug: var=detected_ucp_version\n      when: _debug is defined\n   \n    - debug: msg=\"ucp_version {{ ucp_version }}, detected version {{ detected_ucp_version }}\"\n      when: _debug is defined\n\n#\n# configure passwordless ssh from ucp_main to our ansible box\n#\n    - name: Register key\n      stat: path=/root/.ssh/id_rsa\n      register: key\n    - name: Create keypairs\n      shell: ssh-keygen -t rsa -f /root/.ssh/id_rsa -N ''\n      when: key.stat.exists == False\n    - name: Fetch all public ssh keys\n      shell: cat ~/.ssh/id_rsa.pub\n      register: ssh_keys\n    - name: Deploy keys on localhost\n      authorized_key: user=root key=\"{{ item }}\"\n      delegate_to: localhost\n      with_items:\n        - \"{{ ssh_keys.stdout }}\"\n\n#\n# Get a timestamp, will be used to name the backup\n#\n    - name: Get TimeStamp\n      command: date -u '+%Y_%m_%d_%H%M%S'\n      register: timestamp\n    - name: Store the timestamp\n      set_fact:\n        timestamp: \"{{ timestamp.stdout }}\"\n#\n# Copy the backup script to the target node\n#\n    - name: Creates directory on the VM \n      file:\n        path: /root/scripts\n        state: directory\n    - name: Copy backup scripts to backup VM\n      template: src=../templates/backup_ucp.sh.j2 dest=/root/scripts/backup_ucp.sh\n    - file:\n        path: /root/scripts/backup_ucp.sh\n        mode: 0744\n\n#\n# Get the UCP ID\n#\n    - name: Copy utils to backup VM\n      template: src=../templates/ucp_get_id.sh.j2 dest=/root/scripts/ucp_get_id.sh\n    - file:\n        path: /root/scripts/ucp_get_id.sh\n        mode: 0744\n    - name: Get the UCP ID\n      shell: /root/scripts/ucp_get_id.sh {{ ucp_version }}\n      register: ucp_id\n    - set_fact:\n        ucp_id: \"{{ ucp_id.stdout }}\" \n\n    - set_fact:\n        backup_name:  \"backup_ucp_{{ ucp_id }}_{{ inventory_hostname }}_{{ timestamp }}\"\n      when: backup_name is undefined\n\n#\n# Backup now\n#\n    - name: Backup UCP now\n      shell: /root/scripts/backup_ucp.sh {{ ucp_id }} {{ backup_name }}\n      register: res\n\n    - debug: var=res\n      when: _debug is defined\n\n    - name: Create a temporary directory to receive files that contains metadata\n      tempfile:\n        state: directory\n        suffix: temp\n      register: res\n      delegate_to: localhost\n\n    - template:\n        src: ../templates/backup_meta.yml.j2\n        dest: \"{{ res.path }}/meta.yml\"\n      delegate_to: localhost\n\n    - copy:\n         src: \"{{ item }}\"\n         dest: \"{{ res.path }}/\"\n      with_items:\n        - ../vm_hosts\n        - ../group_vars/vars\n        - ../group_vars/backups\n      delegate_to: localhost\n\n    - name: Backup the Metadata as well\n      archive:\n        path:\n          - \"{{ res.path }}/\"\n        dest: \"{{ backup_dest }}/{{ backup_name }}.vars.tgz\"\n      delegate_to: localhost\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "673fd06f4cb5ba9becb6964b8b6c0d1c73f59e76", "filename": "reference-architecture/azure-ansible/3.6/ansibledeployocp/playbooks/roles/prepare/defaults/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\npackages:\n  - ansible\n  - python-devel\n  - openssl-devel\n  - gcc\n\nepelpackages:\n  - python2-pip\n  - python2-jmespath\n\npippackages:\n  - \"packaging\"\n  - \"msrestazure\"\n  - \"azure==2.0.0rc5\"\n# https://docs.ansible.com/ansible/guide_azure.html#requirements\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "c25dae0aaca64fd84bc382b0fba1f2b7fff20b7f", "filename": "playbooks/update-dns-records.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Update DNS records'\n  hosts: dns-records-manage-host\n  roles:\n  - role: dns/manage-dns-records\n  tags:\n  - update_dns_records\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "233a44965d5acdb7d39003f2cfa2a86f99f96f89", "filename": "reference-architecture/osp-cli/ch5.8.4_enable_lvmetad_nodes_ansible.sh", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "ansible nodes -i inventory -m yum -a \"name=lvm2 state=present\"\nansible nodes -i inventory -m service -a \\\n        \"name=lvm2-lvmetad enabled=yes state=started\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "649e97e5b7973323fec712ad98cce05ad9618e84", "filename": "roles/config-bonding/tests/infrahosts.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Configure bonding on the infrastructure hosts'\n  hosts: infra_hosts\n  roles:\n  - role: config_bonding\n  tags: \n  - configure_infra_hosts\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f767271010d3aa85de82f52309640341f2453e21", "filename": "roles/config-iscsi-client/tests/host_vars/node-2.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\niscsi_initiatorname: iqn.1994-05.com.example:node-2\n\ndisk_mapping:\n- lun: 0\n  vg: vg0\n  lv: lv0\n  mount_path: /mnt/vg0-lv0\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "c5b69971105752f1f114ccded912d750e6074c0f", "filename": "roles/client/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: Gather Facts\n  setup:\n\n- name: Include system based facts and tasks\n  include: systems/main.yml\n\n- name: Cheking the signature algorithm\n  local_action: >\n      shell openssl x509 -text -in certs/{{ IP_subject_alt_name }}.crt  | grep 'Signature Algorithm' | head -n1\n  become: no\n  register: sig_algo\n  args:\n    chdir: \"configs/{{ IP_subject_alt_name }}/pki/\"\n\n- name: Change the algorithm to RSA\n  set_fact:\n    Win10_Enabled: \"Y\"\n  when: '\"ecdsa\" not in sig_algo.stdout'\n\n- name: Install prerequisites\n  package: name=\"{{ item }}\" state=present\n  with_items:\n    - \"{{ prerequisites }}\"\n\n- name: Install StrongSwan\n  package: name=strongswan state=present\n\n- name: Setup the ipsec config\n  template:\n    src: \"roles/vpn/templates/client_ipsec.conf.j2\"\n    dest: \"{{ configs_prefix }}/ipsec.{{ IP_subject_alt_name }}.conf\"\n    mode: '0644'\n  with_items:\n    - \"{{ vpn_user }}\"\n  notify:\n    - restart strongswan\n\n- name: Setup the ipsec secrets\n  template:\n    src: \"roles/vpn/templates/client_ipsec.secrets.j2\"\n    dest: \"{{ configs_prefix }}/ipsec.{{ IP_subject_alt_name }}.secrets\"\n    mode: '0600'\n  with_items:\n    - \"{{ vpn_user }}\"\n  notify:\n    - restart strongswan\n\n- name: Include additional ipsec config\n  lineinfile:\n    dest: \"{{ item.dest }}\"\n    line: \"{{ item.line }}\"\n    create: yes\n  with_items:\n    - dest: \"{{ configs_prefix }}/ipsec.conf\"\n      line: \"include ipsec.*.conf\"\n    - dest: \"{{ configs_prefix }}/ipsec.secrets\"\n      line: \"include ipsec.*.secrets\"\n  notify:\n    - restart strongswan\n\n- name: Setup the certificates and keys\n  template:\n    src: \"{{ item.src }}\"\n    dest: \"{{ item.dest }}\"\n  with_items:\n    - src: \"configs/{{ IP_subject_alt_name }}/pki/certs/{{ vpn_user }}.crt\"\n      dest: \"{{ configs_prefix }}/ipsec.d/certs/{{ IP_subject_alt_name }}_{{ vpn_user }}.crt\"\n    - src: \"configs/{{ IP_subject_alt_name }}/pki/cacert.pem\"\n      dest: \"{{ configs_prefix }}/ipsec.d/cacerts/{{ IP_subject_alt_name }}.pem\"\n    - src: \"configs/{{ IP_subject_alt_name }}/pki/private/{{ vpn_user }}.key\"\n      dest: \"{{ configs_prefix }}/ipsec.d/private/{{ IP_subject_alt_name }}_{{ vpn_user }}.key\"\n  notify:\n    - restart strongswan\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "f2b91216e1df27975704ec7c6a40758b7f690390", "filename": "roles/config-routes/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Notify about Network reload\"\n  debug:\n    msg: \"Networking Static Routes altered - Network reload needed.\" \n\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "156a1c135cbe53205ef2a8b9bf9c23b77257382c", "filename": "archive/roles/registry/tasks/main.yaml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: Setting Registry Facts\n  set_fact:\n    template_registry_domain_name: \"{{ registry_domain_name | default(default_domain_name) }}\"\n    certificate_path: \"{{ default_certificate_path }}\"\n    certificate_subject: \"{{ registry_certificate_subject | default(default_certificate_subject) }}\"\n    nginx_repo_url: \"{{ registry_nginx_repo_url | default(default_nginx_repo_url) }}\"\n    auth_layer: \"{{ registry_auth_layer | default(default_auth_layer) }}\"\n\n- name: Install Registry\n  action: \"{{ ansible_pkg_mgr }} name=docker-distribution state=latest\"\n\n- name: Enable & Start Registry\n  service: name=docker-distribution enabled=yes state=started\n\n- name: Install firewalld\n  action: \"{{ ansible_pkg_mgr }} name=firewalld state=latest\"\n\n#- name: Install firewall-cmd\n#  yum: name=firewall-cmd state=latest\n\n- name: Enable Firewalld\n  service: name=firewalld enabled=yes state=started\n\n- name: Open Firewall for Registry\n  firewalld: port=5000/tcp permanent=yes state=enabled immediate=yes zone=public\n  when: not auth_layer | bool\n\n- name: Lay Down Registry Config\n  template: src=\"{{role_path}}/templates/nginx.j2\" dest=\"/etc/docker-distribution/registry/config.xml\"\n  notify: restart docker registry\n\n- name: Add Auth Layer\n  include: \"{{role_path}}/tasks/auth_layer_nginx.yaml\"\n  when: registry_auth_layer | default(false) | bool\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "dcc3c942f1f8c11c34d87618fade7faa70644201", "filename": "roles/docker/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "docker_install: True\ndocker_enabled: False\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "d2548eb500ff0fb50a6f7ecb57ce132a706a1935", "filename": "roles/mesos/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for mesos\n- name: start mesos master\n  sudo: yes\n  service:\n    name: mesos-master\n    state: started\n\n- name: start mesos slave\n  sudo: yes\n  service:\n    name: mesos-slave\n    state: started\n\n- name: restart mesos master\n  sudo: yes\n  service:\n    name: mesos-master\n    state: restarted\n\n- name: restart mesos slave\n  sudo: yes\n  service:\n    name: mesos-slave\n    state: restarted\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "9dd45950d4e50345f9aa5e5b6469dab602669359", "filename": "dev/playbooks/install_haproxy.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: lbs\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Install haproxy\n      yum:\n        name: haproxy\n        state: latest\n\n    - name: Updating haproxy.cfg on Worker Load Balancer\n      template:\n        src: ../templates/haproxy.cfg.worker.j2\n        dest: /etc/haproxy/haproxy.cfg\n        owner: root\n        group: root\n        mode: 0644        \n      when: inventory_hostname in groups.worker_lb\n\n    - name: Updating haproxy.cfg on UCP Load Balancer\n      template:\n        src: ../templates/haproxy.cfg.ucp.j2\n        dest: /etc/haproxy/haproxy.cfg\n        owner: root\n        group: root\n        mode: 0644\n      when: inventory_hostname in groups.ucp_lb\n\n    - name: Updating haproxy.cfg on DTR Load Balancer\n      template:\n        src: ../templates/haproxy.cfg.dtr.j2\n        dest: /etc/haproxy/haproxy.cfg\n        owner: root\n        group: root\n        mode: 0644\n      when: inventory_hostname in groups.dtr_lb\n\n    - name: Enable and start haproxy service\n      systemd:\n        name: haproxy\n        enabled: yes\n        state: started\n\n    - name: Open http and https ports\n      command: firewall-cmd --permanent --zone=public --add-port=80/tcp --add-port=443/tcp\n\n    - name: Reload firewalld\n      command: firewall-cmd --reload\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "bb74cc840b224ae0779bb61605e2f1aa58a43e5b", "filename": "roles/config-idm-server/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n# handlers file for idm"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "a1898867a04a1a5d1bda93ddd15cf8b594e52b21", "filename": "roles/ipareplica/vars/default.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# defaults file for ipareplica\n# vars/default.yml\nipareplica_packages: [ \"freeipa-server\", \"python3-libselinux\" ]\nipareplica_packages_dns: [ \"freeipa-server-dns\" ]\nipareplica_packages_adtrust: [ \"freeipa-server-trust-ad\" ]\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b15a4ec08ca9acf43dcaa363afb1e8d4107f06b8", "filename": "roles/dns/config-dns-server/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\npackage_state: present\n\ndns_server_type: 'master'\n\nnamed_config_recursion: yes\n\nnamed_config_views: []\nnamed_config_allow_query: []\nnamed_config_allow_transfer: []\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "101bd4fcfe83e6988650b0e6bf09c3ec1ce32473", "filename": "ops/playbooks/includes/internal_vars.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\nk8s: \"{{ k8s_enabled | default('true') }}\"\n\nucp_certificates_volume_name: 'ucp-controller-server-certs'\n\ninternal_ucpk8s_ports:\n  - 179/tcp\n  - 443/tcp\n  - 2376/tcp\n  - 2377/tcp\n  - 4789/udp\n  - 6443/tcp\n  - 6444/tcp\n  - 7946/tcp\n  - 7946/udp\n  - 10250/tcp\n  - 12376/tcp\n  - 12378/tcp\n  - 12379-12386/tcp\n  - 12388/tcp\n\ninternal_ucp200_ports:\n  - 443/tcp\n  - 2376/tcp\n  - 2377/tcp\n  - 4789/udp\n  - 7946/tcp\n  - 7946/udp\n  - 12376/tcp\n  - 12379-12387/tcp\n\ninternal_dtrk8s_ports:\n  - 80/tcp\n  - 443/tcp\n  - 179/tcp\n  - 4789/udp\n  - 6444/tcp\n  - 7946/tcp\n  - 7946/udp\n  - 10250/tcp\n  - 12376/tcp\n  - 12378/tcp\n\ninternal_dtr200_ports:\n  - 80/tcp\n  - 443/tcp\n  - 2377/tcp\n  - 4789/udp\n  - 7946/tcp\n  - 7946/udp\n  - 12376/tcp\n\ninternal_wrk200_ports:\n  - 443/tcp\n  - 2377/tcp\n  - 4789/udp\n  - 7946/tcp\n  - 7946/udp\n  - 12376/tcp\n\ninternal_wrkk8s_ports:\n  - 179/tcp\n  - 4789/udp\n  - 6444/tcp\n  - 7946/tcp\n  - 7946/udp\n  - 10250/tcp\n  - 12376/tcp\n  - 12378/tcp\n \n\ninternal_ucp_ports:\n  \"{% if k8s == 'true' %}{{ internal_ucpk8s_ports }}{% else %}{{  internal_ucp200_ports }}{% endif %}\"\n\ninternal_worker_ports:\n  \"{% if k8s == 'true' %}{{ internal_wrkk8s_ports }}{% else %}{{  internal_wrk200_ports }}{% endif %}\"\n\ninternal_dtr_ports:\n  \"{% if k8s == 'true' %}{{ internal_dtrk8s_ports }}{% else %}{{  internal_dtr200_ports }}{% endif %}\"\n\ninternal_syslog_ports:\n  - 514/tcp\n  - 514/udp\n\n\ninternal_ucp200_lb_ports:\n  - 80/tcp\n  - 443/tcp\n\ninternal_ucpk8s_lb_ports:\n  - 80/tcp\n  - 443/tcp\n  - 6443/tcp\n\ninternal_ucp_lb_ports: \n  \"{% if k8s == 'true' %}{{ internal_ucpk8s_lb_ports }}{% else %}{{ internal_ucp200_lb_ports }}{% endif %}\"\n\ninternal_dtr_lb_ports:\n  - 80/tcp\n  - 443/tcp\n\ninternal_worker_lb_ports:\n  - 80/tcp\n  - 443/tcp\n\n#\n# if new-style load balancers are wanted, they take precedence over the old-style singleton load balancers once specified with ucp_lb, dtr_lb and worker_lb\n#\nucp_lb_fqdn_legacy: \"{% if groups.ucp_lb is defined and groups.ucp_lb | length > 0 %}{{ groups.ucp_lb[0] }}.{{ domain_name }}{% endif %}\"\nucp_lb_fqdn_vrrp: \"{% if loadbalancers is defined and loadbalancers.ucp is defined %}{{ loadbalancers.ucp.public_fqdn }}{% endif %}\"\nucp_lb_fqdn: \"{% if ucp_lb_fqdn_vrrp | length > 0 %}{{ ucp_lb_fqdn_vrrp }}{% else %}{{ ucp_lb_fqdn_legacy }}{% endif %}\"\n\nucp_lb_ipv4_legacy: \"{% if groups.ucp_lb is defined and groups.ucp_lb | length > 0 %}{{ hostvars[groups.ucp_lb[0]].ip_addr | ipaddr('address') }}{% endif %}\"\nucp_lb_ipv4_vrrp: \"{% if loadbalancers is defined and loadbalancers.ucp is defined %}{{ loadbalancers.ucp.public_vip }}{% endif %}\"\nucp_lb_ipv4: \"{% if ucp_lb_ipv4_vrrp | length > 0 %}{{ ucp_lb_ipv4_vrrp }}{% else %}{{ ucp_lb_ipv4_legacy }}{% endif %}\"\n\ndtr_lb_fqdn_legacy: \"{% if groups.dtr_lb is defined and groups.dtr_lb | length > 0 %}{{ groups.dtr_lb[0] }}.{{ domain_name }}{% endif %}\"\ndtr_lb_fqdn_vrrp: \"{% if loadbalancers is defined and loadbalancers.dtr is defined %}{{ loadbalancers.dtr.public_fqdn }}{% endif %}\"\ndtr_lb_fqdn: \"{% if dtr_lb_fqdn_vrrp | length > 0 %}{{ dtr_lb_fqdn_vrrp }}{% else %}{{ dtr_lb_fqdn_legacy }}{% endif %}\"\n\n"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "72efab2d0987e1a196809d1120cd1b509a8a5953", "filename": "tasks/security_policy_fetch/unknown-transport.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Warn on unsupported transport\"\n  debug:\n    msg: \"This role does not support '{{ transport }}' transport.\\\n          Please contact support@lean-delivery.com\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "55be4841408efc38cb6b35e4612ab25c7c5e02a1", "filename": "roles/config-satellite/tasks/satellite.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install Satellite with default parameters\"\n  command: > \n    satellite-installer \n      --scenario satellite\n      --foreman-initial-organization \"{{ satellite_organization }}\"\n      --foreman-initial-location \"{{ satellite_location }}\"\n      --foreman-admin-username \"{{ satellite_username }}\"\n      --foreman-admin-password \"{{ satellite_password }}\"\n      --foreman-proxy-dns-managed=false\n      --foreman-proxy-dhcp-managed=false\n\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "c0ae0d0322c25fc96c22f7369def58a8a84f5334", "filename": "tasks/create_repo_yum_proxy_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_yum_proxy\n    args: \"{{ _nexus_repos_yum_defaults|combine(item) }}\"\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "042ca6ffbcadcb3a10a94f60cde8a42125ee400d", "filename": "dev/playbooks/power_on_guests.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: vms\n  gather_facts: false\n  connection: local\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  environment: \"{{ env }}\"\n \n  tasks:\n    - name: Power on guests\n      vsphere_guest:\n        vcenter_hostname: \"{{ vcenter_hostname }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        validate_certs: \"{{ vcenter_validate_certs }}\"\n        guest: \"{{ inventory_hostname }}\"\n        state: powered_on\n\n    - name: Wait for VMs to boot up\n      pause:\n        minutes: 3\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "240fbed84f38b844a958d64f8cb0e78e69f9b91e", "filename": "reference-architecture/vmware-ansible/playbooks/roles/create-vm-haproxy/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Create haproxy VM on vCenter\n  vmware_guest:\n    hostname: \"{{ vcenter_host }}\"\n    username: \"{{ vcenter_username }}\"\n    password: \"{{ vcenter_password }}\"\n    validate_certs: False\n    name: \"{{ item.value.guestname }}\"\n    cluster: \"{{ vcenter_cluster}}\"\n    datacenter: \"{{ vcenter_datacenter }}\"\n    resource_pool: \"{{ vcenter_resource_pool }}\"\n    template: \"{{vcenter_template_name}}\"\n    folder: \"/{{ vcenter_folder }}\"\n    annotation: \"{{ item.value.tag }}\"\n    state: poweredon\n    wait_for_ip_address: true\n    disk:\n    - size_gb: 60\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 40\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    - size_gb: 40\n      datastore: \"{{ vcenter_datastore }}\"\n      type: thin\n    networks:\n    - name: \"{{ vm_network }}\"\n      ip: \"{{ item.value.ip4addr }}\"\n      netmask: \"{{ vm_netmask }}\"\n      gateway: \"{{ vm_gw }}\"\n    customization:\n      domain: \"{{dns_zone}}\"\n      dns_servers:\n      - \"{{ vm_dns }}\"\n      dns_suffix: \"{{dns_zone}}\"\n      hostname: \"{{ item.value.guestname}}\"\n  with_dict: \"{{host_inventory}}\"\n  when: \"'haproxy' in item.value.guestname\"\n\n- name: Add haproxy server to inventory\n  add_host: hostname=\"{{ item.value.guestname}}\" ansible_ssh_host=\"{{ item.value.ip4addr }}\" groups=\"{{ item.value.tag }}, haproxy_group\"\n  with_dict: \"{{ host_inventory }}\"\n  when: \"'haproxy' in item.value.guestname\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "fe1aae2149d081b6c174249d34923d792a975e07", "filename": "roles/scm/add-webhooks-github/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n# tasks file for add-webhooks\n\n- name: Add Webhooks\n  include_tasks: add-webhook.yml\n  vars:\n    url: \"{{ item.url }}\"\n    events: \"{{ item.events }}\"\n    is_active: \"{{ item.is_active }}\"\n  with_items: \"{{ webhooks }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "da134fc75b6535abc3aedb8d496b6fc1cb5d4ecd", "filename": "roles/osp/packstack-post/tasks/keystone.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Configure the keystone parameters to allow for multiple domains\"\n  ini_file:\n    path: \"/etc/keystone/keystone.conf\"\n    section: \"identity\"\n    option: \"{{ item.key }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n  - { key: 'domain_specific_drivers_enabled', value: 'true' }\n  - { key: 'domain_config_dir', value: '/etc/keystone/domains' }\n  notify: 'restart keystone'\n\n- name: \"Configure the openstack-dashboard parameters to allow for multiple domains\"\n  lineinfile:\n    path: \"/etc/openstack-dashboard/local_settings\"\n    regexp: '^{{ item.key }} ='\n    line: '{{ item.key }} = {{ item.value }}'\n  with_items:\n  - { key: 'OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT', value: 'True' }\n  - { key: 'OPENSTACK_KEYSTONE_DEFAULT_DOMAIN', value: '\"default\"' }\n  notify: 'restart keystone'\n\n- name: \"Ensure target domain directory exists\"\n  file:\n    path: \"/etc/keystone/domains/\"\n    state: directory\n\n- name: \"Set SELinux boolean\"\n  seboolean:\n    name: authlogin_nsswitch_use_ldap\n    state: yes\n    persistent: yes\n\n- name: \"Setup LDAP domain configuration\"\n  template:\n    src: keystone_ldap.j2\n    dest: \"/etc/keystone/domains/keystone.{{ item.domain }}.conf\"\n  notify: 'restart keystone'\n  with_items:\n  - \"{{ keystone_ldap }}\"\n\n- name: \"Create temporary file for keystone rc variables\"\n  tempfile:\n    state: file\n    prefix: keystonerc\n  register: keystonerc_file\n  notify:\n  - \"remove temporary keystone rc file\"\n\n- name: \"Source the admin keystonerc file\"\n  shell: \"source {{ admin_keystonerc_file }}; env | grep '^OS_' > {{ keystonerc_file.path }}\"\n\n- name: \"Retrive the sourced rc content file\"\n  fetch:\n    src: \"{{ keystonerc_file.path }}\"\n    dest: \"{{ keystonerc_file.path }}\"\n    flat: yes\n  notify:\n  - \"remove local temporary keystone rc file\"\n\n- name: \"Ensure target directory for 'clouds.cfg' exists\"\n  file:\n    path: \"{{ clouds_yaml_file | dirname }}\"\n    state: directory\n\n- name: \"Configure the 'clouds.cfg' file for admin\"\n  template:\n    src: clouds_yaml.j2\n    dest: \"{{ clouds_yaml_file }}\"\n\n- name: \"Restart keystone (httpd) to apply config\"\n  service:\n    name: httpd\n    state: restarted\n\n- name: \"Wait a bit for keystone to catch up\"\n  pause:\n    seconds: 10\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "af2b557781a384fa1ab987519eb8f33a415df852", "filename": "ops/playbooks/roles/hpe.haproxy/templates/00-intro", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "global\n  log         127.0.0.1 local2\n  chroot      /var/lib/haproxy\n  pidfile     /var/run/haproxy.pid\n  maxconn     256\n  user        haproxy\n  group       haproxy\n  daemon\n  stats socket /var/lib/haproxy/stats\n\ndefaults\n  mode    tcp\n  timeout connect 5000\n  timeout client 50000\n  timeout server 50000\n#\n# If you want stats, you ' ll have to enable the corresponding port manually in the LB's firewall\n#\nlisten stats :1936\n  mode http\n  stats enable\n  stats hide-version\n  stats realm Haproxy\\ Statistics\n  stats uri /\n  stats auth admin:admin\n#\n# LB rules following\n#\n"}, {"commit_sha": "bf6e08dcb2440421477b6536ff6a8d11adc2be17", "sha": "f640fa076debe966d5d24e5246a05fbdf5dc8ddc", "filename": "roles/mesos/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "# used for leader election amongst masters\n- name: Set ZooKeeper URL\n  template:\n    src: zk.j2\n    dest: /etc/mesos/zk\n    mode: 0644\n  sudo: yes\n\n# Tasks for Master nodes\n- name: Set Mesos Master ip\n  copy:\n    content: \"{{ mesos_ip }}\"\n    dest: /etc/mesos-master/ip\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart mesos master\n  when: mesos_install_mode == \"master\"\n\n- name: Set Mesos Master hostname\n  copy:\n    content: \"{{ mesos_hostname }}\"\n    dest: /etc/mesos-master/hostname\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart mesos master\n  when: mesos_install_mode == \"master\"\n\n  # The Mesos quorum value is based on the number of Mesos Masters. Take the\n  # number of masters, divide by 2, and round-up to nearest integer. For example,\n  # if there are 1 or 2 masters the quorum count is 1. If there are 3 or 4\n  # masters then the quorum count is 2. For 5 or 6 masters it's 3 and so on.\n- name: Set Mesos Master quorum count\n  template:\n    src: quorum.j2\n    dest: /etc/mesos-master/quorum\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart mesos master\n  when: mesos_install_mode == \"master\"\n\n- name: Set Mesos Master Cluster name\n  copy:\n    content: \"{{mesos_cluster_name}}\"\n    dest: /etc/mesos-master/cluster\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart mesos master\n  when: mesos_install_mode == \"master\"\n\n- name: Set Mesos Master consul service definition\n  sudo: yes\n  template:\n    src: mesos-master-consul.j2\n    dest: \"{{ consul_dir }}/mesos-master.json\"\n  notify:\n    - restart consul\n  when: mesos_install_mode == \"master\"\n\n- name: remove mesos-master override\n  file:\n    path: /etc/init/mesos-master.override\n    state: absent\n  notify:\n    - restart mesos master\n  when: mesos_install_mode == \"master\"\n\n- name: start mesos-master (and enable it at boot)\n  service:\n    name: mesos-master\n    state: started\n    enabled: yes\n  when: mesos_install_mode == \"master\"\n\n# Tasks for Slave nodes\n- name: remove mesos-slave override\n  file:\n    path: /etc/init/mesos-slave.override\n    state: absent\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: Set Mesos Slave hostname\n  copy:\n    content: \"{{ mesos_hostname }}\"\n    dest: /etc/mesos-slave/hostname\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: set executor registration timeout\n  sudo: yes\n  copy:\n    dest: /etc/mesos-slave/executor_registration_timeout\n    content: \"{{ mesos_executor_registration_timeout }}\"\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: set containerizers\n  sudo: yes\n  copy:\n    dest: /etc/mesos-slave/containerizers\n    content: \"{{ mesos_containerizers }}\"\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: set slave resources\n  sudo: yes\n  copy:\n    dest: /etc/mesos-slave/resources\n    content: \"{{ mesos_resources }}\"\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: Set Mesos Slave ip\n  copy:\n    content: \"{{ mesos_ip }}\"\n    dest: /etc/mesos-slave/ip\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: Create Mesos Slave work area\n  file:\n    dest: \"{{mesos_slave_work_dir}}\"\n    mode: 0755\n    state: directory\n  sudo: yes\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: Set Mesos Slave work area\n  copy:\n    content: \"{{mesos_slave_work_dir}}\"\n    dest: /etc/mesos-slave/work_dir\n    mode: 0644\n  sudo: yes\n  notify:\n    - restart mesos slave\n  when: mesos_install_mode == \"slave\"\n\n- name: start mesos-slave (and enable it at boot)\n  service:\n    name: mesos-slave\n    state: started\n    enabled: yes\n  when: mesos_install_mode == \"slave\"\n\n- meta: flush_handlers\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7bacb6b117a246d57ab731d3f5b2de01bcb6a673", "filename": "roles/osp/packstack-post/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- hosts: infra_osp_hosts\n  roles:\n    - packstack-post\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "b18eb552b0eedea3eccce8ea9481f3d6f58149a3", "filename": "playbooks/roles/docket/templates/docket_prod.yaml.j2", "repository": "rocknsm/rock", "decoded_content": "---\r\n# Configure your stenographer instances here. sensor is a\r\n# just an arbitrary unique name\r\nSTENOGRAPHER_INSTANCES:\r\n  {{ docket_steno_instances | to_json }}\r\n\r\n# ==== Flask ====\r\nDEBUG: {{ docket_debug }}\r\nTESTING: {{ docket_testing }}\r\nSECRET_KEY: {{ docket_secret }}\r\nSESSION_COOKIE_NAME: {{ docket_session_cookie }}\r\n\r\n\r\n# ==== Celery ====\r\n# CELERY_URL: the broker for sharing celery queues across processes\r\nCELERY_URL: {{ docket_celery_url }}\r\n\r\n# ==== Basic Docket Configuration ====\r\n# SPOOL_DIR where captures are saved and served\r\n#             configure the webserver to serve SPOOL_DIR/[a-z0-9-]{32,}/MERGED_NAME.pcap\r\nSPOOL_DIR: {{ docket_spool_dir }}\r\n\r\n# WEB_ROOT  the base url for all docket requests: default is /api\r\n#  examples:\r\n#     http://HOST:PORT/api/stats/\r\n#     http://HOST:PORT/api/uri/host/1.2.3.4/\r\nWEB_ROOT: /api\r\n# WEB_ROOT  the base url for PCAP requests: default is /results\r\n#  example:\r\n#     http://HOST:PORT/results/<JOB_ID>/merged.pcap\r\nPCAP_WEB_ROOT: /results\r\n\r\n# ==== Logging ====\r\nLOGGER_NAME: {{ docket_logger }}\r\n# LOG_LEVEL  (critical, error, warning, info, debug) any other value will be ignored\r\nLOG_LEVEL: info\r\n\r\n# Python log format strings:\r\nLOG_MSG_FORMAT: \"[%(processName)-8s:%(thread)d] %(message)s\"\r\nLOG_DATE_FORMAT: '%FT%T.%fZ'\r\nLOG_DATE_UTC: True\r\n# LOG_FILE   a secondary logging handler - this file will be rotated at midnight\r\nLOG_FILE: /var/log/docket/docket.log\r\nLOG_FILE_LEVEL: info\r\nLOG_FILE_MSG_FORMAT: \"%(asctime)s[%(processName)-8s:%(thread)d] %(message)s\"\r\nLOG_FILE_DATE_FORMAT: '%FT%T'\r\n\r\n# ==== Query ====\r\n# TIME_WINDOW  Integer: specifies the size of a time 'chunk' in seconds for request de-duplication\r\n#              Requested times are widened to a multiple of this value.\r\nTIME_WINDOW: 60\r\n# EMPTY_RESULT capacity: the size of a pcap that contains packets (detects 'no matching packets')\r\nEMPTY_RESULT: 25B\r\n\r\n# MERGED_NAME   - base name (no extension) of the result capture file. Just a string.\r\nMERGED_NAME: merged\r\n\r\n# QUERY_FILE    - base name of the query meta-data save file. Just a string.\r\nQUERY_FILE: query\r\n\r\n#DOCKET_NO_REDIS - if True, docket will use files (instead of redis) to maintain query meta-data\r\nDOCKET_NO_REDIS: {{ docket_no_redis }}\r\n\r\n# LONG_AGO      - The default 'start-time' aka 'after' clause isn't 1970, but this long before now()\r\nLONG_AGO: {{ docket_long_ago }}\r\n\r\n# ==== Timeout ====\r\n# TIMEOUTs      - Docket queries will fail if stenoboxes are unresponsive.\r\n#                 For each stenobox instance:\r\n#                   Docket remembers the last time it was idle. IDLE_TIME\r\n#                   If it is not Docket waits IDLE_SLEEP, then requests stats, repeat if not idle\r\n#                   If a stenobox had not become idle after QUERY_TIMEOUT, this instance FAILs.\r\n#\r\n#                   Once idle: a stenobox is queried and results are written to disk.\r\n#                   If the 'Requests' module times-out (QUERY_TIMEOUT), this instance FAILs.\r\n#\r\n#                   Results are shared with other Docket processes (Web, celery queues...)\r\n#                   This is subject to LOCK_TIMEOUT, which is never expected to happen\r\n#\r\n# IDLE_TIME     - 5.0, assume stenoboxes remain IDLE for 5 seconds and check again after that.\r\n# IDLE_SLEEP    - 2.0, wait time between IDLE queries. will occur at least once every IDLE_TIME.\r\n# STAT_TIMEOUT  - 3.0, assume a stenobox is broken if we can't get stats within this many seconds\r\n# QUERY_TIMEOUT - 720, seconds. A query request will fail if stenobox doesn't complete this quickly\r\n# LOCK_TIMEOUT  - 2.0, seconds to wait on a IPC shared data (file) lock before giving up.\r\nIDLE_TIME: 5.0\r\nIDLE_SLEEP: 2.0\r\nSTAT_TIMEOUT: 3.0\r\nQUERY_TIMEOUT: 720.0\r\nLOCK_TIMEOUT: 2.0\r\n\r\n# WEIGHTS       'Fat Finger' protection\r\n#               - request 'weight' can be estimated to prevent clogging up the system.\r\n#               - Estimate the amount of data in the system (total buffered packet data)\r\n#               - Pick a threshold for a single request 1% * (TOTAL / 8 hours / 60 minutes / 5 requests per minute)\r\n#               - IPs  - the number of (active) IPs captured\r\n#               - NETS - the number of (active) subnets captured\r\n#               - PORTs- the number of (active) Ports per IP\r\n#               - HOURS- the number of hours in the buffer (I know variance is likely above 50%)\r\n#               - 'weight' = (TOTAL / (HOURS * 3600) * QUERY_SECONDS) / ( SUM(IPs + NETs) * SUM( PORTs )\r\n#  ex: a 5TB/8hour system, query: 1 IP and 1 port for 10 seconds == 381774 == 380KB\r\n#  5TB / (8 * 3600) * 10seconds / ( (50.0 * 1ip + 2 * 0net) * 100.0 * 1port )\r\n#               NOTE: Raw queries are not weighed or widened so only identical raw queries are deduplicated.\r\n#WEIGHT_THRESHOLD: 22MB     # valid quantifiers( B KB MB GB TB PB )\r\n#WEIGHT_TOTAL: 5TB      # valid quantifiers( B KB MB GB TB PB )\r\n#WEIGHT_IPS: 50.0\r\n#WEIGHT_NETS: 2.0\r\n#WEIGHT_PORTS: 100.0\r\n#WEIGHT_HOURS: 8.0\r\n\r\n# ==== EXPIRATION ====\r\n# EXPIRE_SPACE  - if set, the oldest items will be removed until this much space becomes available.\r\n# EXPIRE_TIME   - how long after last 'touch' does a request expire ?\r\n# CLEANUP_PERIOD - delete expired queries will run every CLEANUP_PERIOD seconds\r\nEXPIRE_SPACE: 500MB\r\nEXPIRE_TIME: 48h\r\nCLEANUP_PERIOD: 1h\r\n\r\n# ==== Formatting ====\r\n# DATE_FORMAT strftime format for showing a datetime to a user\r\nDATE_FORMAT: '%FT%T'\r\n\r\n# ID_FORMAT: if True, IDs will contain dashes: https://en.wikipedia.org/wiki/Universally_unique_identifier\r\nUUID_FORMAT: false\r\n\r\n# Stop accepting requests if SPOOL_DIR's free space or nodes go below these values:\r\nFREE_BYTES: 200MB\r\nFREE_NODES: 10111\r\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "0d7254fab0f6115185234972a22e8e718be3dca9", "filename": "dev/playbooks/create_vms.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- hosts: vms\n  gather_facts: false\n  connection: local\n  user: remote\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n  \n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Create all VMs\n      vmware_guest:\n        hostname: \"{{ vcenter_hostname }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        validate_certs: no\n        esxi_hostname: \"{{ esxi_host }}\"\n        datacenter: \"{{ datacenter }}\"\n        folder: \"{{ folder_name }}\"\n        name: \"{{ inventory_hostname }}\"\n        template: \"{{ vm_template }}\"\n        state: poweredon\n        hardware:\n          memory_mb: \"{{ ram }}\"\n          num_cpus: \"{{ cpus }}\"\n\n    - name: Add secondary disks\n      vsphere_guest:\n        vcenter_hostname: \"{{ vcenter_hostname }}\"\n        username: \"{{ vcenter_username }}\"\n        password: \"{{ vcenter_password }}\"\n        validate_certs: \"{{ vcenter_validate_certs }}\"\n        guest: \"{{ inventory_hostname }}\"\n        state: reconfigured\n        vm_disk:\n          disk2:\n            size_gb: \"{{ disk2_size }}\"\n            type: thin\n            datastore: \"{{ datastores | random }}\"\n      when: disk2_size is defined\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "c450edb39ba5ce20544c1e21152e01be171fab57", "filename": "tasks/secure.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n- name: MYSQL_USER | Update mysql root password for all root accounts\n  mysql_user:\n    name: root\n    host: \"{{ item }}\"\n    password: \"{{ mariadb_root_password }}\"\n  with_items:\n    - \"{{ ansible_hostname }}\"\n    - 127.0.0.1\n    - ::1\n    - localhost\n\n- name: MYSQL_USER | Remove all anonymous users\n  mysql_user:\n    name: ''\n    host: \"{{ item }}\"\n    state: absent\n  with_items:\n    - \"{{ ansible_hostname }}\"\n    - 127.0.0.1\n    - ::1\n    - localhost\n\n- name: MYSQL_DB | Remove the test database\n  mysql_db:\n    name: test\n    state: absent\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "eefa002ce8418f96eea795341198d39fe84bdd72", "filename": "ops/playbooks/roles/hpe.openports/tests/test.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "---\n- hosts: localhost\n  remote_user: root\n  roles:\n    - hpe.openports\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "eb12c1ef8bed5e7f0785f266b357ea424f5f114a", "filename": "roles/user-management/manage-user-passwd/test/changepasswd.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n# These tests covers testing password change of an individual user\n\n- name: \"TEST 1: Change user's IdM Passwd when both set, provded changed\"\n  hosts: ipa\n\n  vars_files:\n    - vars/passwdfile1.json\n\n  vars:\n    ipa_admin_user: admin\n    ipa_admin_password: test123\n    ipa_host: idm.example.com \n  \n  roles:\n    - manage-user-passwd\n\n- name: \"TEST 2: Change user's IdM Passwd is not set and generate set, random, changed\"\n  hosts: ipa\n\n  vars_files:\n    - vars/passwdfile2.json\n\n  vars:\n    ipa_admin_user: admin\n    ipa_admin_password: test123\n    ipa_host: idm.example.com \n\n  roles:\n    - manage-user-passwd\n\n- name: \"TEST 3: Change user's IdM password: when neither is set: skipped\"\n  hosts: ipa\n\n  vars_files:\n    - vars/passwdfile3.json\n\n  vars:\n    ipa_admin_user: admin\n    ipa_admin_password: test123\n    ipa_host: idm.example.com \n\n  roles:\n    - manage-user-passwd\n\n- name: \"TEST 4:Change user's IdM password: missing generate field, no passwd: skipped \"\n  hosts: ipa\n\n  vars_files:\n    - vars/passwdfile4.json\n\n  vars:\n    ipa_admin_user: admin\n    ipa_admin_password: test123\n    ipa_host: idm.example.com \n\n  roles:\n    - manage-user-passwd\n\n- name: \"TEST 5:Change user's IdM password: missing generate=False, valid passwd: changed\"\n  hosts: ipa\n\n  vars_files:\n    - vars/passwdfile5.json\n\n  vars:\n    ipa_admin_user: admin\n    ipa_admin_password: test123\n    ipa_host: idm.example.com \n\n  roles:\n    - manage-user-passwd\n\n- name: \"TEST 6: Change user's IdM password: with list\"\n  hosts: ipa\n\n  vars_files:\n    - vars/passwordall.json\n\n  vars:\n    ipa_admin_user: admin\n    ipa_admin_password: test123\n    ipa_host: idm.example.com \n\n  roles:\n    - manage-user-passwd\n\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "78c487ec8514120759c55e24da8f8ae28ac3749e", "filename": "roles/weave/vars/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# vars file for weave\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "eaa33f07aa100a9a3fc94381aa360031b9849d99", "filename": "ops/playbooks/splunk_demo.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- name: install demo Splunk Enterprise\n  hosts: ucp\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - ./includes/internal_vars.yml\n\n  environment:\n    - \"{{ env }}\"\n\n  tasks:\n\n  - block:\n\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n\n    - name: source stack specific variables\n      include_vars:\n        file: ../templates/splunk/splunk_demo/vars.yml\n\n    #\n    # Deploy a stack for Docker hosts\n    #\n    - block:\n\n      - name: Create script directory\n        file:\n          path: /root/scripts/monitoring\n          state: directory\n\n      - name: Copy script file for Docker hosts\n        template:\n          src: ../templates/splunk/splunk_demo/splunk.yml.j2\n          dest: /root/scripts/monitoring/splunk.yml\n\n      - name: Deploy Splunk Stack\n        command: docker stack deploy --compose-file splunk.yml splunk_demo\n        args:\n          chdir: /root/scripts/monitoring\n \n      when: inventory_hostname == ucp_instance\n\n    when: monitoring_stack is defined and monitoring_stack == \"splunk_demo\"\n\n\n  - debug: msg=\"No splunk integration wanted\"\n    when: monitoring_stack is not defined\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "c2723d1d8f457ac0945d07a323362e6979669bdc", "filename": "roles/calibre/tasks/create-db.yml", "repository": "iiab/iiab", "decoded_content": "- name: Check if sample book exists in /opt/iiab/downloads\n  stat:\n    path: \"{{ content_base }}/downloads/{{ calibre_sample_book }}\"\n  register: sample_bk\n\n- name: Download sample book (mandatory since Calibre 3.x)\n  get_url:\n    url: \"{{ iiab_download_url }}/{{ calibre_sample_book }}\"\n    dest: \"{{ content_base }}/downloads\"\n    timeout: \"{{ download_timeout }}\"\n  when: internet_available and not sample_bk.stat.exists\n\n- name: Check if sample book exists in /opt/iiab/downloads\n  stat:\n    path: \"{{ content_base }}/downloads/{{ calibre_sample_book }}\"\n  register: sample_bk \n\n- name: Incorporate sample book into Calibre DB (mandatory since Calibre 3.x)\n  shell: \"calibredb add {{ content_base }}/downloads/{{ calibre_sample_book }} --with-library {{ calibre_dbpath }}\"\n  when: sample_bk.stat.exists\n\n- name: Make /library/calibre/metadata.db writable for Calibre client SW\n  file:\n    path: \"{{ calibre_dbpath }}/metadata.db\"\n    mode: \"ugo+w\"\n    #mode: 0666\n    #owner: pi\n    #group: pi\n    #owner: iiab-admin\n    #group: iiab-admin\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "6f585441f96da2d2e8496b0974fa099f14782b18", "filename": "roles/vpn/tasks/ubuntu.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\n- set_fact:\n    strongswan_additional_plugins: []\n\n- name: Ubuntu | Install strongSwan\n  apt:\n    name: strongswan\n    state: latest\n    update_cache: yes\n    install_recommends: yes\n\n- name: Ubuntu | Enforcing ipsec with apparmor\n  shell: aa-enforce \"{{ item }}\"\n  when: apparmor_enabled|default(false)|bool == true\n  with_items:\n    - /usr/lib/ipsec/charon\n    - /usr/lib/ipsec/lookip\n    - /usr/lib/ipsec/stroke\n  notify:\n    - restart apparmor\n  tags: ['apparmor']\n\n- name: Ubuntu | Enable services\n  service: name={{ item }} enabled=yes\n  with_items:\n    - apparmor\n    - strongswan\n    - netfilter-persistent\n\n- name: Ubuntu | Ensure that the strongswan service directory exist\n  file:\n    path: /etc/systemd/system/strongswan.service.d/\n    state: directory\n    mode: 0755\n    owner: root\n    group: root\n\n- name: Ubuntu | Setup the cgroup limitations for the ipsec daemon\n  template:\n    src: 100-CustomLimitations.conf.j2\n    dest: /etc/systemd/system/strongswan.service.d/100-CustomLimitations.conf\n  notify:\n    - daemon-reload\n    - restart strongswan\n\n- include_tasks: iptables.yml\n  tags: iptables\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "0fb92113ab159222d56eff850c644573e7542df0", "filename": "roles/awstats/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "awstats_install: True\nawstats_enabled: False\nawstats_data_dir: /library/awstats/\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "e053a772d735b7b99d7132edd8f4eeb6cafc4c4f", "filename": "reference-architecture/aws-ansible/playbooks/library/ec2_vol_facts.py", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#!/usr/bin/python\n#\n# This is a free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This Ansible library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this library.  If not, see <http://www.gnu.org/licenses/>.\n\nDOCUMENTATION = '''\n---\nmodule: ec2_vol_facts\nshort_description: Gather facts about ec2 volumes in AWS\ndescription:\n    - Gather facts about ec2 volumes in AWS\nversion_added: \"2.1\"\nauthor: \"Rob White (@wimnat)\"\noptions:\n  filters:\n    description:\n      - A dict of filters to apply. Each dict item consists of a filter key and a filter value. See U(http://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeVolumes.html) for possible filters.\n    required: false\n    default: null\nextends_documentation_fragment:\n    - aws\n    - ec2\n'''\n\nEXAMPLES = '''\n# Note: These examples do not set authentication details, see the AWS Guide for details.\n\n# Gather facts about all volumes\n- ec2_vol_facts:\n\n# Gather facts about a particular volume using volume ID\n- ec2_vol_facts:\n    filters:\n      volume-id: vol-00112233\n\n# Gather facts about any volume with a tag key Name and value Example\n- ec2_vol_facts:\n    filters:\n      \"tag:Name\": Example\n\n# Gather facts about any volume that is attached\n- ec2_vol_facts:\n    filters:\n      attachment.status: attached\n\n'''\n\n# TODO: Disabled the RETURN as it was breaking docs building. Someone needs to\n# fix this\nRETURN = '''# '''\n\ntry:\n    import boto.ec2\n    from boto.exception import BotoServerError\n    HAS_BOTO = True\nexcept ImportError:\n    HAS_BOTO = False\n\ndef get_volume_info(volume):\n\n    attachment = volume.attach_data\n\n    volume_info = {\n                    'create_time': volume.create_time,\n                    'id': volume.id,\n                    'iops': volume.iops,\n                    'size': volume.size,\n                    'snapshot_id': volume.snapshot_id,\n                    'status': volume.status,\n                    'type': volume.type,\n                    'zone': volume.zone,\n                    'region': volume.region.name,\n                    'attachment_set': {\n                        'attach_time': attachment.attach_time,\n                        'device': attachment.device,\n                        'instance_id': attachment.instance_id,\n                        'status': attachment.status\n                    },\n                    'tags': volume.tags\n                }\n    \n    return volume_info\n\ndef list_ec2_volumes(connection, module):\n\n    filters = module.params.get(\"filters\")\n    volume_dict_array = []\n\n    try:\n        all_volumes = connection.get_all_volumes(filters=filters)\n    except BotoServerError as e:\n        module.fail_json(msg=e.message)\n\n    for volume in all_volumes:\n        volume_dict_array.append(get_volume_info(volume))\n\n    module.exit_json(volumes=volume_dict_array)\n\n\ndef main():\n    argument_spec = ec2_argument_spec()\n    argument_spec.update(\n        dict(\n            filters = dict(default=None, type='dict')\n        )\n    )\n\n    module = AnsibleModule(argument_spec=argument_spec)\n\n    if not HAS_BOTO:\n        module.fail_json(msg='boto required for this module')\n\n    region, ec2_url, aws_connect_params = get_aws_connection_info(module)\n\n    if region:\n        try:\n            connection = connect_to_aws(boto.ec2, region, **aws_connect_params)\n        except (boto.exception.NoAuthHandlerFound, StandardError), e:\n            module.fail_json(msg=str(e))\n    else:\n        module.fail_json(msg=\"region must be specified\")\n\n    list_ec2_volumes(connection, module)\n\nfrom ansible.module_utils.basic import *\nfrom ansible.module_utils.ec2 import *\n\nif __name__ == '__main__':\n    main()\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "fa21e34cabcb7fb2585e281d43d8b5ea17d8cf29", "filename": "roles/ipaserver/vars/Fedora-27.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "ipaserver_packages: [ \"ipa-server\", \"libselinux-python\" ]\nipaserver_packages_dns: [ \"ipa-server-dns\" ]\nipaserver_packages_adtrust: [ \"ipa-server-trust-ad\" ]\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "046c6c2f4dd22606c04da8929dca3983e80fb1a6", "filename": "roles/user-management/manage-atlassian-users/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- include_tasks: create_atlassian_groups.yml\n  with_items: \"{{ atlassian_groups }}\"\n  loop_control:\n    loop_var: group\n  when: atlassian_groups|length > 0\n\n- include_tasks: create_atlassian_users.yml\n  with_items: \"{{ atlassian_users }}\"\n  loop_control:\n    loop_var: atlassian_user\n  when: atlassian_users|length > 0\n\n- include_tasks: add_user_to_groups.yml\n  with_items: \"{{ atlassian_users }}\"\n  loop_control:\n    loop_var: atlassian_user\n  when: atlassian_users|length > 0\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "8a63d11a550db95f6d7f07230bc84371516fd11e", "filename": "playbooks/libvirt/openshift-cluster/terminate.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n# TODO: does not handle a non-existent cluster gracefully\n\n- name: Terminate instance(s)\n  hosts: localhost\n  become: no\n  connection: local\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  tasks:\n  - set_fact: cluster_group=tag_clusterid-{{ cluster_id }}\n  - add_host:\n      name: \"{{ item }}\"\n      groups: oo_hosts_to_terminate\n      ansible_ssh_user: \"{{ deployment_vars[deployment_type].ssh_user }}\"\n      ansible_become: \"{{ deployment_vars[deployment_type].become }}\"\n    with_items: '{{ groups[cluster_group] | default([]) }}'\n\n- name: Unsubscribe VMs\n  hosts: oo_hosts_to_terminate\n  vars_files:\n  - vars.yml\n  roles:\n  - role: rhel_unsubscribe\n    when: deployment_type in ['atomic-enterprise', 'enterprise', 'openshift-enterprise'] and\n          ansible_distribution == \"RedHat\" and\n          lookup('oo_option', 'rhel_skip_subscription') | default(rhsub_skip, True) |\n            default('no', True) | lower in ['no', 'false']\n\n- name: Terminate instance(s)\n  hosts: localhost\n  become: no\n  connection: local\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  tasks:\n  - name: Destroy VMs\n    virt:\n      name: '{{ item[0] }}'\n      command: '{{ item[1] }}'\n      uri: '{{ libvirt_uri }}'\n    with_nested:\n    - \"{{ groups['oo_hosts_to_terminate'] }}\"\n    - [ destroy, undefine ]\n\n  - name: Delete VM drives\n    command: 'virsh -c {{ libvirt_uri }} vol-delete --pool {{ libvirt_storage_pool }} {{ item }}.qcow2'\n    args:\n      removes: '{{ libvirt_storage_pool_path }}/{{ item }}.qcow2'\n    with_items: \"{{ groups['oo_hosts_to_terminate'] }}\"\n\n  - name: Delete VM docker drives\n    command: 'virsh -c {{ libvirt_uri }} vol-delete --pool {{ libvirt_storage_pool }} {{ item }}-docker.qcow2'\n    args:\n      removes: '{{ libvirt_storage_pool_path }}/{{ item }}-docker.qcow2'\n    with_items: \"{{ groups['oo_hosts_to_terminate'] }}\"\n\n  - name: Delete the VM cloud-init image\n    file:\n      path: '{{ libvirt_storage_pool_path }}/{{ item }}_cloud-init.iso'\n      state: absent\n    with_items: \"{{ groups['oo_hosts_to_terminate'] }}\"\n\n  - name: Remove the cloud-init config directory\n    file:\n      path: '{{ libvirt_storage_pool_path }}/{{ item }}_configdrive/'\n      state: absent\n    with_items: \"{{ groups['oo_hosts_to_terminate'] }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "fceec03c2b5fc023c0c244992ce2a3cfaae3b473", "filename": "playbooks/container-registry/quay-enterprise.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Preflight Checks\n  hosts: localhost\n  tasks:\n    - name: Validate Required Group Configuration\n      fail:\n        msg: \"'database', 'redis' and 'quay_enterprise' groups must be specified\"\n      when:\n        - \"'redis' not in groups or groups['redis']| length == 0 or 'database' not in groups or groups['database']| length == 0 or 'quay_enterprise' not in groups or groups['quay_enterprise']| length == 0\"\n\n- name: Install Docker\n  hosts: docker_hosts\n  tasks:\n    - name: Configure Docker\n      include_role:\n        name: config-container-storage-setup\n      when: docker_install|default(false)\n\n    - name: Install Docker\n      include_role:\n        name: config-docker\n      when: docker_install|default(false)\n\n- name: Install HAProxy\n  hosts: lb\n  pre_tasks:\n    - name: Setup\n      setup:\n      delegate_to: \"{{ item }}\"\n      delegate_facts: true\n      with_items:\n        - \"{{ groups['quay_enterprise'] }}\"\n  roles:\n    - role: load-balancers/manage-haproxy\n      lb_config:\n        stats_page:\n          enabled: True\n          host_vip: \"{{ haproxy_host_vip | default('*') }}\"\n          host_port: 8080\n          username: \"{{ haproxy_stats_username | default('admin') }}\"\n          password: \"{{ haproxy_stats_password | default('admin') }}\"\n        frontends:\n        - lb_name: quay_http\n          lb_host_vip: \"{{ haproxy_host_vip | default('*') }}\"\n          lb_host_port: 80\n        - lb_name: quay_https\n          lb_host_vip: \"{{ haproxy_host_vip | default('*') }}\"\n          lb_host_port: 443\n        - lb_name: redis\n          lb_host_vip: \"{{ haproxy_host_vip | default('*') }}\"\n          lb_host_port: 6379\n          lb_ssl_enabled: True\n      lb_backend_template: \"{{ playbook_dir }}/templates/haproxy_backend.cfg.j2\"\n\n- name: Install and Configure Database for Quay\n  hosts: database\n  become: True\n  tasks:\n    - name: Install MySQL\n      include_role:\n        name: config-mysql\n      vars:\n        mode: containerized\n        mysql_name: \"{{ quay_database_service_name | default('mysql-quay') }}\"\n        mysql_username: \"{{ quay_database_username }}\"\n        mysql_password: \"{{ quay_database_password }}\"\n        mysql_root_username: \"{{ quay_database_admin_username }}\"\n        mysql_root_password: \"{{ quay_database_admin_password }}\"\n        mysql_database: \"{{ quay_database_name }}\"\n      when: quay_database_type == \"mysql\"\n\n    - name: Install and Configure PostgreSQL for Quay\n      block:\n        - name: Install PostgreSQL for Quay\n          include_role:\n            name: config-postgresql\n          vars:\n            mode: containerized\n            postgresql_name: \"{{ quay_database_service_name | default('postgresql-quay') }}\"\n            postgresql_username: \"{{ quay_database_username }}\"\n            postgresql_password: \"{{ quay_database_password }}\"\n            postgresql_admin_user: \"{{ quay_database_admin_username }}\"\n            postgresql_admin_password: \"{{ quay_database_admin_password }}\"\n            postgresql_port: \"{{ quay_database_port | default('5432') }}\"\n            postgresql_database: \"{{ quay_database_name }}\"\n\n        - name: Flush Handlers\n          meta: flush_handlers\n\n        - name: Sleep to give PostgreSQL a chance to finish starting up\n          pause:\n            seconds: 10\n\n        - name: Locate PostgreSQL Container\n          command: docker ps --filter=name=\"{{ quay_database_service_name | default('postgresql-quay') }}\" -q\n          register: postgresql_container\n\n        - name: Configure PostgreSQL\n          shell: docker exec -i {{ postgresql_container.stdout }} /bin/bash -c 'PGPASSWORD={{ quay_database_admin_password }} psql {{ quay_database_name }} -c \"CREATE EXTENSION pg_trgm;\"'\n          register: shell_result\n          failed_when:\n            - shell_result.rc != 0\n            - \"'already exists' not in shell_result.stderr\"\n      when: quay_database_type == \"postgresql\"\n\n- name: Install Redis\n  hosts: redis\n  become: True\n  tasks:\n    - name: Install Redis\n      include_role:\n        name: config-redis\n      vars:\n        mode: containerized\n\n- name: Install Quay Enterprise\n  hosts: quay_enterprise\n  become: True\n  tasks:\n    - name: Set Quay Hostname When LB Defined\n      set_fact:\n        quay_hostname: \"{{ hostvars[groups['lb'][0]]['inventory_hostname'] }}\"\n      when: quay_hostname is undefined and groups['lb'] | length > 0\n    - name: Set Quay Hostname When LB Not Defined\n      set_fact:\n        quay_hostname: \"{{ hostvars[groups['quay_enterprise'][0]]['inventory_hostname'] }}\"\n      when: quay_hostname is undefined and groups['lb'] | length == 0\n    - name: Install Quay\n      include_role:\n        name: config-quay-enterprise\n      vars:\n        quay_database_username: \"{{ hostvars[groups['database'][0]]['quay_database_username'] }}\"\n        quay_database_password: \"{{ hostvars[groups['database'][0]]['quay_database_password'] }}\"\n        quay_database_name: \"{{ hostvars[groups['database'][0]]['quay_database_name'] }}\"\n        quay_database_port: \"{{ hostvars[groups['database'][0]]['quay_database_port'] }}\"\n        quay_database_host: \"{{ hostvars[groups['database'][0]]['ansible_eth0']['ipv4']['address'] }}\"\n        redis_host: \"{{ quay_hostname | default(hostvars[groups['lb'][0]]['inventory_hostname']) if groups['lb'] | length > 0 else hostvars[groups['redis'][0]]['ansible_eth0']['ipv4']['address'] }}\"\n        quay_server_hostname: \"{{ quay_hostname | default(inventory_hostname) }}\"\n        quay_clair_enable: \"{{ (groups['clair']| length > 0) | ternary('True','False') }}\"\n        quay_clair_endpoint: \"http://{{ hostvars[groups['clair'][0]]['ansible_eth0']['ipv4']['address'] if (groups['clair']| length > 0) else '' }}:{{ clair_endpoint_port | default('6060') }}\"\n        quay_builder_enable: \"{{ (groups['quay_builder']| length > 0) | ternary('True','False') }}\"\n\n- name: Install Clair Database\n  hosts: database\n  become: True\n  tasks:\n    - name: Install and Configure PostgreSQL for Clair\n      include_role:\n        name: config-postgresql\n      vars:\n        mode: containerized\n        postgresql_name: \"{{ clair_database_service_name | default('postgresql-clair') }}\"\n        postgresql_username: \"{{ clair_database_username }}\"\n        postgresql_password: \"{{ clair_database_password }}\"\n        postgresql_admin_user: \"{{ clair_database_admin_username }}\"\n        postgresql_admin_password: \"{{ clair_database_admin_password }}\"\n        postgresql_host_port: \"{{ clair_database_port | default('5433') }}\"\n        postgresql_database: \"{{ clair_database_name }}\"\n      when: groups['clair']| length > 0\n\n- name: Install Clair\n  hosts: clair\n  become: True\n  tasks:\n    - name: Gather facts from machine\n      setup:\n      with_items:\n        - \"{{ groups['quay_enterprise'] }}\"\n    - name: Set Quay Hostname When LB Defined\n      set_fact:\n        quay_hostname: \"{{ hostvars[groups['lb'][0]]['inventory_hostname'] }}\"\n      when: quay_hostname is undefined and groups['lb'] | length > 0\n    - name: Set Quay Hostname When LB Defined\n      set_fact:\n        quay_hostname: \"{{ hostvars[groups['quay_enterprise'][0]]['inventory_hostname'] }}\"\n      when: quay_hostname is undefined and groups['lb'] | length == 0\n\n    - name: Install Clair\n      include_role:\n        name: config-clair\n      vars:\n        database_host: \"{{ hostvars[groups['database'][0]]['ansible_eth0']['ipv4']['address'] }}\"\n        quay_enterprise_address: \"{{ hostvars[groups['quay_enterprise'][0]]['quay_http_protocol'] }}://{{ quay_hostname }}\"\n        clair_ssl_trust_configure: \"{{ hostvars[groups['quay_enterprise'][0]]['quay_ssl_enable']|bool }}\"\n        clair_ssl_trust_src_file: \"{{ hostvars[groups['quay_enterprise'][0]]['quay_ssl_cert_file'] if hostvars[groups['quay_enterprise'][0]]['quay_ssl_enable'] is defined and hostvars[groups['quay_enterprise'][0]]['quay_ssl_enable']|bool else '' }}\"\n        postgresql_port: \"{{ clair_database_port | default('5433') }}\"\n        clair_host_proxy_port: \"{{ clair_endpoint_port | default('6060') }}\"\n\n- name: Install Quay Builder\n  hosts: quay_builder\n  tasks:\n    - name: Gather facts from machine\n      setup:\n      with_items:\n        - \"{{ groups['quay_enterprise'] }}\"\n    - name: Install Quay Enterprise\n      include_role:\n        name: config-quay-builder\n      vars:\n        quay_enterprise_hostname: \"{{ quay_hostname }}\"\n        quay_builder_ssl_trust_configure: \"{{ hostvars[groups['quay_enterprise'][0]]['quay_ssl_enable']|bool }}\"\n        quay_builder_ssl_trust_src_file: \"{{ hostvars[groups['quay_enterprise'][0]]['quay_ssl_cert_file'] if hostvars[groups['quay_enterprise'][0]]['quay_ssl_enable'] is defined and hostvars[groups['quay_enterprise'][0]]['quay_ssl_enable']|bool else '' }}\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "f08240881af61ed76d392dddd20e767d3e001d96", "filename": "roles/manage-aws-infra/tasks/update_dns.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "# Create/update Route 53 zone with new records fro Infra and Master\n---\n- name: Ensure Route53 zone is present\n  route53_zone:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    zone: \"{{ dns_domain }}\"\n    state: present\n\n- name: Update Route53 with OCP master record\n  route53:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    zone: \"{{ dns_domain }}.\"\n    record: \"master-0.{{ env_id }}.{{ dns_domain }}.\"\n    type: A\n    value: \"{{ master_eip.public_ip }}\"\n    command: create\n    overwrite: yes\n    ttl: 300\n  when: not ha_mode\n\n- name: Update Route53 with OCP infra wildcard record\n  route53:\n    aws_access_key: \"{{ aws_access_key }}\"\n    aws_secret_key: \"{{ aws_secret_key }}\"\n    zone: \"{{ dns_domain }}.\"\n    record: \"*.apps.{{ env_id }}.{{ dns_domain }}.\"\n    type: A\n    value: \"{{ infra_eip.public_ip }}\"\n    command: create\n    overwrite: yes\n    ttl: 300\n  when: not ha_mode\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7ec2b8458ab7be6df39dcc57dae5e92087c46aa1", "filename": "roles/ansible/tower/manage-inventories/tasks/process-host.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Load up the inventory (host)\"\n  uri:\n    url: \"{{ ansible_tower.url | default(default_ansible_tower_url) }}/api/v2/hosts/\"\n    user: \"{{ ansible_tower.admin_username | default(default_ansible_tower_admin_username) }}\"\n    password: \"{{ ansible_tower.admin_password }}\"\n    force_basic_auth: yes\n    method: POST\n    body: \"{{ lookup('template', 'host.j2') }}\"\n    body_format: 'json'\n    headers:\n      Content-Type: \"application/json\"\n      Accept: \"application/json\"\n    validate_certs: no\n    status_code: 200,201,400\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "2272403cbb2628b4fe9e918c9ffe8f399cbed7b5", "filename": "roles/common/handlers/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: restart rsyslog\n  service: name=rsyslog state=restarted\n\n- name: restart ipfw\n  service: name=ipfw state=restarted\n\n- name: flush routing cache\n  shell: echo 1 > /proc/sys/net/ipv4/route/flush\n\n- name: restart loopback\n  shell: ifdown lo:100 && ifup lo:100\n\n- name: restart loopback bsd\n  shell: >\n    ifconfig lo100 destroy || true &&\n    ifconfig lo100 create &&\n    ifconfig lo100 inet {{ local_service_ip }} netmask 255.255.255.255 &&\n    ifconfig lo100 inet6 FCAA::1/64; echo $?\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "66d6656c2834e260167f94b1b9e987ca7651d107", "filename": "ops/playbooks/install_nfs_clients.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- hosts: docker\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n\n  environment: \"{{ env }}\"\n\n  tasks:\n    - name: Install NFS client\n      yum:\n        name: nfs-utils\n        state: latest\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "f4f42761092f3d86ce5d7b45a692e248b1a39d72", "filename": "roles/ovirt-engine-remote-db/vars/postgres95.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\npostgres_service_name: \"rh-postgresql95-postgresql\"\npostgres_server: \"rh-postgresql95-postgresql-server\"\npostgres_data_dir: \"/var/opt/rh/rh-postgresql95/lib/pgsql/data\"\npostgres_config_file: \"/var/opt/rh/rh-postgresql95/lib/pgsql/data/postgresql.conf\"\npostgres_setup_cmd: \"/opt/rh/rh-postgresql95/root/usr/bin/postgresql-setup --initdb\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "a32e89ffceda299a55f478ed58d4a333ddb3a859", "filename": "roles/user-management/manage-atlassian-users/tasks/create_atlassian_groups.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Create Group\n  uri:\n    url: '{{ atlassian.url }}/rest/api/2/group'\n    method: POST\n    status_code: [201, 400]\n    user: '{{ atlassian.username }}'\n    password: '{{ atlassian.password }}'\n    force_basic_auth: yes\n    body_format: json\n    body: \"{'name': '{{ group }}' }\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "895e796618d5eb5d7c7363674305f93f61aa5fcd", "filename": "roles/notifications/send-email/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Create the 'To:' list of addresses\"\n  set_fact:\n    mail: \"{{ mail | combine({ 'to': list_of_mail_to }) }}\"\n  when:\n  - list_of_mail_to is defined\n\n- name: \"Create the 'CC:' list of addresses\"\n  set_fact:\n    mail: \"{{ mail | combine({ 'cc': list_of_mail_cc }) }}\"\n  when:\n  - list_of_mail_cc is defined\n\n- name: \"Create the 'BCC:' list of addresses\"\n  set_fact:\n    mail: \"{{ mail | combine({ 'bcc': list_of_mail_bcc }) }}\"\n  when:\n  - list_of_mail_bcc is defined\n\n- name: \"Send out e-mail content to users\"\n  mail:\n    subject: \"{{ mail.subject }}\"\n    body: \"{{ mail.body | default(omit) }}\"\n    host: \"{{ mail.host | default(omit) }}\"\n    port: \"{{ mail.port | default (omit) }}\"\n    username: \"{{ mail.username | default(omit) }}\"\n    password: \"{{ mail.password | default(omit) }}\"\n    to: \"{{ mail.to | default(omit) }}\"\n    cc: \"{{ mail.cc | default(omit) }}\"\n    bcc: \"{{ mail.bcc | default(omit) }}\"\n    from: \"{{ mail.from | default(omit)}}\"\n    headers: \"{{ mail.headers | default(omit)}}\"\n    secure: \"{{ mail.secure | default(omit) }}\"\n    subtype: \"{{ mail.subtype | default(omit) }}\"\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "027a9f29b87b685d23bad20bc5e3db0df9681ac7", "filename": "roles/kafka/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n- name: Install packages\n  yum:\n    name:\n      - kafka\n      - kafkacat\n      - java-11-openjdk-headless\n    state: present\n\n- name: Create kafka data directory\n  file:\n    path: \"{{ kafka_data_dir }}\"\n    mode: 0755\n    owner: \"{{ kafka_user }}\"\n    group: \"{{ kafka_group }}\"\n    state: directory\n\n- name: Set kafka retention\n  lineinfile:\n    dest: \"{{ kafka_config_path }}\"\n    regexp: \"log.retention.hours=\"\n    line: \"log.retention.hours={{ kafka_retention }}\"\n    state: present\n\n- name: Set kafka data directory\n  lineinfile:\n    dest: \"{{ kafka_config_path }}\"\n    regexp: \"log.dirs=\"\n    line: \"log.dirs={{ kafka_data_dir }}\"\n\n- name: Discover facts about data mount\n  set_fact:\n    rock_mounts:\n      mount: \"{{ item.mount }}\"\n      device: \"{{ item.device }}\"\n      size_total: \"{{ item.size_total }}\"\n  loop:\n    \"{{ ansible_mounts }}\"\n  when: (default_mount is defined and item.mount == default_mount and rock_mounts is not defined)\n\n- name: Determining if quotas are enabled\n  shell: grep \"{{ default_mount }}\" /etc/fstab | awk /prjquota/\n  register: prjquota\n  changed_when: false\n\n- name: Create kafka quota project id\n  getent:\n    database: group\n    split: ':'\n    key: kafka\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Map kafka quota project id to name\n  lineinfile:\n    create: true\n    state: present\n    insertafter: EOF\n    path: /etc/projid\n    line: \"kafka:{{ getent_group.kafka[1] }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Define kafka quota project directories\n  lineinfile:\n    create: true\n    state: present\n    insertafter: EOF\n    path: /etc/projects\n    line: \"{{ getent_group.kafka[1] }}:{{ kafka_data_dir }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: set kafka weight\n  set_fact:\n    kafka_weight: \"{{ rock_services | selectattr('name', 'equalto', 'kafka') | map(attribute='quota_weight') | first }}\"\n  when: kafka_quota is not defined and (prjquota.stdout|length>0)\n\n- name: set kafka quota if not user defined\n  set_fact:\n    kafka_quota: \"{{ rock_mounts.size_total | int / xfs_quota_weight | int * kafka_weight | int }}\"\n  when: kafka_quota is not defined and (prjquota.stdout|length>0)\n\n- name: set kafka project quota\n  xfs_quota:\n    type: project\n    name: kafka\n    bhard: \"{{ kafka_quota }}\"\n    state: present\n    mountpoint: \"{{ rock_mounts.mount }}\"\n  when: rock_mounts is defined and (prjquota.stdout|length>0)\n\n- name: Enable and start kafka\n  service:\n    name: kafka\n    state: \"{{ 'started' if rock_services | selectattr('name', 'equalto', 'kafka') | map(attribute='enabled') | bool else 'stopped' }}\"\n    enabled: \"{{ rock_services | selectattr('name', 'equalto', 'kafka') | map(attribute='enabled') | bool }}\"\n\n- name: Configure firewall ports\n  firewalld:\n    port: \"{{ item }}/tcp\"\n    permanent: true\n    state: enabled\n    immediate: true\n  loop:\n    - 9092\n  when: groups['kafka'] | difference(groups['logstash']) | count > 0\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "da439162d0659eaa16c6b85d649e3f6a68fd5a86", "filename": "playbooks/selinux.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "- hosts: all\n  gather_facts: no\n  vars:\n    selinux: \"permissive\"\n  tasks:\n    - name: \"setenforce {{ selinux }}\"\n      shell: \"setenforce {{ selinux }}\"\n      become: true\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "79dd479cc73104c67611a2f0f66fc0ad696d5a78", "filename": "roles/postgresql/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "postgresql_locale: \"en_US.UTF-8\"\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "90a1d368fc781f49227c55d41ad3cff8e005ea19", "filename": "roles/network/tasks/edit_ifcfg.yml", "repository": "iiab/iiab", "decoded_content": "- name: Turn off ISP nameservers\n  lineinfile: state=present\n              backrefs=yes\n              regexp='^PEERDNS'\n              line='PEERDNS=\"no\"'\n              dest={{ has_ifcfg_gw }}\n\n- name: Turn on local nameserver\n  lineinfile: state=present\n              line='DNS1=\"127.0.0.1\"'\n              dest={{ has_ifcfg_gw }}\n\n- name: Remove the UUID\n  lineinfile: state=absent\n              regexp='^UUID'\n              dest={{ has_ifcfg_gw }}\n\n# Leave wifi as is NAME=<AP> needs to match keyring name.\n- name: Fix the NM name\n  lineinfile: state=present\n              backrefs=yes\n              regexp='^NAME'\n              line='NAME=\"iiab-WAN\"'\n              dest={{ has_ifcfg_gw }}\n  when: has_wifi_gw == \"none\"\n\n# testpoint - quoting and present\n# note DEVICE can change what is displayed via \"ip and friends\"\n- name: Fix the DEVICE\n  lineinfile: state=present\n              backrefs=yes\n              regexp='^DEVICE'\n              line='DEVICE=\"{{ iiab_wan_iface }}\"'\n              dest={{ has_ifcfg_gw }}\n  when: iiab_wan_iface != \"none\" and has_wifi_gw == \"none\"\n\n- name: Add marker\n  lineinfile: state=present\n              line=\"# Modified by IIAB\"\n              dest={{ has_ifcfg_gw }}\n\n- name: Rename supplied gateway ifcfg file to WAN if present\n  shell: mv \"{{ has_ifcfg_gw }}\" /etc/sysconfig/network-scripts/ifcfg-WAN\n  when: has_wifi_gw == \"none\"\n\n- name: Now setting ifcfg-WAN True after moving file\n  set_fact:\n    has_WAN: True\n  when: has_wifi_gw == \"none\"\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "fbf82a8ac7522fd245328cfb86124da67e537d2d", "filename": "roles/zookeeper/handlers/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# handlers file for zookeeper\n"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "1324b8aa27b19ca1d84c7759470c7a507a1d00b3", "filename": "tasks/fetch/web.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Download artifact from web\"\n  get_url:\n    url: \"{{ transport_web }}\"\n    dest: \"{{ download_path }}\"\n  until: \"'OK' in FILE_DOWNLOADED.msg\"\n  retries: 3\n  delay: 2\n  register: FILE_DOWNLOADED\n  delegate_to: \"localhost\"\n  connection: \"local\"\n\n- name: \"Downloaded artifact\"\n  set_fact:\n    oracle_artifact: \"{{ FILE_DOWNLOADED.dest }}\"\n    oracle_artifact_basename: \"{{ FILE_DOWNLOADED.dest | basename }}\"\n\n- name: \"Getting java version variables\"\n  set_fact:\n    java_package: \"{{ oracle_artifact_basename.split('-')[0] }}\"\n    java_major_version: \"{{ oracle_artifact_basename.split('-')[1].split('u')[0] }}\"\n    java_minor_version: \"{{ oracle_artifact_basename.split('-')[1].split('u')[1] }}\"\n    java_arch: \"{{ oracle_artifact_basename.split('-')[3] }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5492bfd076b0b39fc31e34e540abeb480662a3f5", "filename": "roles/config-container-storage-setup/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- block:\n    - name: create the docker-storage config file\n      template:\n        src: docker-storage-setup-overlayfs.j2\n        dest: /etc/sysconfig/docker-storage-setup\n        owner: root\n        group: root\n        mode: 0644\n  when:\n    - ansible_distribution_version is version_compare('7.4', '>=')\n    - ansible_distribution == \"RedHat\"\n\n- block:\n    - name: create the docker-storage-setup config file\n      template:\n        src: docker-storage-setup-dm.j2\n        dest: /etc/sysconfig/docker-storage-setup\n        owner: root\n        group: root\n        mode: 0644\n  when:\n    - ansible_distribution_version is version_compare('7.4', '<')\n    - ansible_distribution == \"RedHat\"\n\n- block:\n    - name: create the docker-storage-setup config file for CentOS\n      template:\n        src: docker-storage-setup-dm.j2\n        dest: /etc/sysconfig/docker-storage-setup\n        owner: root\n        group: root\n        mode: 0644\n  when:\n    - ansible_distribution == \"CentOS\""}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "8350c11de9c4e601180b144e548addbfc9920cbe", "filename": "playbooks/dns/vars/views.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\nnamed_config_views:\n- name: \"casl-private\"\n  zone:\n  - \"dns_domain\": \"private.example.com\"\n- name: \"casl-public\"\n  zone:\n  - \"dns_domain\": \"public.example.com\"\n  forwarder:\n  - \"8.8.8.8\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "99a95e4ca30f9a55dc8cbb957bb241a01451652f", "filename": "playbooks/openstack/openshift-cluster/filter_plugins", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "../../../filter_plugins"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "2bb6db8a47aa238370851772dc3795a46f8f2a5a", "filename": "roles/manage-confluence-space/tasks/copy_confluence_attachments.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: Get attachment data\n  uri:\n    url: '{{ confluence_source_url }}/wiki/rest/api/content/{{ confluence_content_ids.key }}/child/attachment'\n    method: GET\n    user: '{{ confluence_source_username }}'\n    password: '{{ confluence_source_password }}'\n    force_basic_auth: yes\n    status_code: 200\n    return_content: yes\n  register: attachments_json\n\n- name: Create temp directory for downloaded attachments\n  tempfile:\n    state: directory\n  register: attachment_tempdir\n\n- include_tasks: download_attachment.yml\n  with_items: '{{ attachments_json.json.results }}'\n  loop_control:\n    loop_var: attachment_data\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "f66401b80ce88cc9237662f169bcaeff52b70a50", "filename": "roles/teamviewer/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "teamviewer_url: \"{{ iiab_download_url }}\"\nteamviewer_rpm_file: teamviewer_10.0.41499.i686.rpm\nteamviewer_install: True\nteamviewer_enabled: False\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "f3d19204720c1bb828a85c0369dbab1611e6e539", "filename": "roles/vpn/meta/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\n\ndependencies:\n  - { role: common, tags: common }\n\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "2aa063bb353e89c5f6ba558f790fc20fee1bd874", "filename": "examples/playbooks/cleanup_engine.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# example playbook for cleanup engines\n# use with cleanup_engine.inv\n- hosts: engine\n  remote_user: root\n  roles:\n    - {role: ovirt-engine-cleanup}\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "d8773365b0af21afd366d39754a67c6b55c4cd56", "filename": "roles/osp/packstack-install/tasks/host-prep.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Disable firewalld and NetworkManager\"\n  service:\n    name: \"{{ item }}\"\n    state: stopped\n    enabled: no\n  with_items:\n  - 'NetworkManager'\n  - 'firewalld'\n\n- name: \"Enable and start 'network' service\"\n  service:\n    name: \"{{ item }}\"\n    state: started\n    enabled: yes\n  with_items:\n  - 'network'\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ffbad2e3f1b541d9a49ee855729e95a502f92aeb", "filename": "roles/dns-views/tasks/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: \"Generate ACL list for DNS server\"\n  set_fact:\n    acl_list: \"{{ acl_list | default([]) + [ (hostvars[item]['private_v4'] + '/32') ] }}\"\n  with_items: \"{{ groups['cluster_hosts'] }}\"\n\n- name: \"Generate the private view\"\n  set_fact:\n    private_named_view:\n      - name: \"private\"\n        recursion: \"{{ named_private_recursion }}\"\n        acl_entry: \"{{ acl_list }}\"\n        zone:\n          - dns_domain: \"{{ full_dns_domain }}\"\n        forwarder: \"{{ public_dns_nameservers }}\"\n  when: external_nsupdate_keys['private'] is undefined\n\n- name: \"Generate the public view\"\n  set_fact:\n    public_named_view:\n      - name: \"public\"\n        recursion: \"{{ named_public_recursion }}\"\n        zone:\n          - dns_domain: \"{{ full_dns_domain }}\"\n        forwarder: \"{{ public_dns_nameservers }}\"\n  when: external_nsupdate_keys['public'] is undefined\n\n- name: \"Generate the final named_config_views\"\n  set_fact:\n    named_config_views: \"{{ private_named_view|default([]) + public_named_view|default([]) }}\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "50c684bcfaa2f73e9e96f5aeacef07f5496144ec", "filename": "roles/create_users/tasks/create_users.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n - name: \"Create {{ users.num_users }} htpasswd users\"\n   htpasswd:\n     path: \"{{ users.passwd_file }}\"\n     name: \"{{ item }}\"\n     password: \"{{ users.password }}\"\n     owner: root\n     group: root\n     mode: 0600\n   with_sequence: start=0 end=\"{{ users.num_users }}\" format=\"{{ users.prefix }}%02d\"\n\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "7988b842844dc7ac309465c63133935ec9b31e63", "filename": "playbooks/group_vars/all.yml", "repository": "rocknsm/rock", "decoded_content": "%YAML 1.1\n---\nhttp_proxy: \"{{ lookup('env','http_proxy') }}\"\nhttps_proxy: \"{{ lookup('env', 'https_proxy') }}\"\n\nrock_version: 2.4.0\nelastic:\n  major_version: 7\n  suffix: \"x\"\nelastic_version: \"{{ elastic.major_version }}.{{ elastic.suffix }}\"\nrock_online_install: true\nrock_enable_testing: false\nrock_disable_offline_repo: true\nrock_sysctl_file: /etc/sysctl.d/10-ROCK.conf\nrock_data_dir: \"{{ default_mount | default('/data')}}\" \nrock_conf_dir: /etc/rocknsm\nrock_config: \"{{ rock_conf_dir }}/config.yml\"\nrocknsm_dir: /opt/rocknsm\nrock_data_user: root\nrock_data_group: root\nrock_monifs: \"{{ ansible_interfaces | difference(['lo', ansible_default_ipv4.interface | default('lo') ])| list }}\"\nrock_mgmt_nets: [ \"0.0.0.0/0\" ]\nrock_cache_dir: /srv/rocknsm/support\nrock_debug: \"{{ lookup('env', 'DEBUG') }}\"\n\n#### Retention Configuration ####\nelastic_close_interval: 15\nelastic_delete_interval: 60\nkafka_retention: 168\nsuricata_retention: 3\nbro_log_retention: 7\nbro_stats_retention: 0\n\n# Feature options - Don't flip these unless you know what you're doing\n# These control if the service is installed\n\nrock_services:\n  - name: bro\n    quota_weight: 1\n    installed: true\n    enabled: true\n  - name: stenographer\n    quota_weight: 8\n    installed: true\n    enabled: true\n  - name: docket\n    quota_weight: 0\n    installed: true\n    enabled: true\n  - name: suricata\n    quota_weight: 2\n    installed: true\n    enabled: true\n  - name: elasticsearch\n    quota_weight: 4\n    installed: true\n    enabled: true\n  - name: kibana\n    quota_weight: 0\n    installed: true\n    enabled: true\n  - name: zookeeper\n    quota_weight: 0\n    installed: true\n    enabled: true\n  - name: kafka\n    quota_weight: 4\n    installed: true\n    enabled: true\n  - name: lighttpd\n    quota_weight: 0\n    installed: true\n    enabled: true\n  - name: fsf\n    quota_weight: 1\n    installed: true\n    enabled: true\n  - name: filebeat\n    quota_weight: 0\n    installed: true\n    enabled: true\n\nrocknsm_package_list:\n  - jq\n  - tcpreplay\n  - tcpdump\n  - bats\n  - policycoreutils-python\n  - htop\n  - vim\n  - git\n  - tmux\n  - nmap-ncat\n  - logrotate\n  - firewalld\n  - chrony\n  - libselinux-python\n\nhttp_tls_crt: /etc/pki/tls/certs/http_tls_crt.pem\nhttp_tls_pub: /etc/pki/tls/certs/http_tls_pub.pem\nhttp_tls_key: /etc/pki/tls/private/http_tls_key.pem\nhttp_tls_combined: /etc/pki/tls/private/httpd-combined.pem\nhttp_tls_dhparams: /etc/pki/tls/misc/http_tls_dhparams.pem\n\ndocket_web_pemfile: \"{{ http_tls_combined }}\"\ndocket_web_dhparams: \"{{ http_tls_dhparams }}\"\n\nepel_baseurl: http://download.fedoraproject.org/pub/epel/$releasever/$basearch/\nepel_gpgurl: https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7\nelastic_baseurl: https://artifacts.elastic.co/packages/{{ elastic_version }}/yum\nelastic_gpgurl: https://artifacts.elastic.co/GPG-KEY-elasticsearch\n\nrocknsm_baseurl: https://packagecloud.io/rocknsm/2_4/el/7/$basearch\nrocknsm_srpm_baseurl: https://packagecloud.io/rocknsm/2_4/el/7/SRPMS\nrocknsm_testing_baseurl: https://copr-be.cloud.fedoraproject.org/results/@rocknsm/testing/epel-7-$basearch/\nrocknsm_gpgurl: https://packagecloud.io/rocknsm/2_4/gpgkey\nrocknsm_local_baseurl: file:///srv/rocknsm\nrock_offline_gpgcheck: 0\nbro_user: bro\nbro_group: bro\nbro_data_dir: \"{{ rock_data_dir }}/bro\"\nbro_prefix: /usr\nbro_sysconfig_dir: /etc/bro\nbro_site_dir: /usr/share/bro/site\nbro_cpu: \"{{ (ansible_processor_vcpus|int // 2) if (ansible_processor_vcpus|int <= 16) else 8 }}\"\nbro_rockscripts_repo: https://github.com/rocknsm/rock-scripts.git\nbro_rockscripts_branch: master\nbro_rockscripts_filename: \"rock-scripts_{{ bro_rockscripts_branch | replace('/', '-') }}.tar.gz\"\nrock_dashboards_repo: https://github.com/rocknsm/rock-dashboards.git\nrock_dashboards_branch: master\nrock_dashboards_url: \"https://github.com/rocknsm/rock-dashboards/archive/{{ rock_dashboards_branch }}.tar.gz\"\nrock_dashboards_filename: \"rock-dashboards_{{ rock_dashboards_branch | replace('/', '-') }}.tar.gz\"\nrock_dashboards_version: 2.4\nrock_module_dir: \"/opt/rocknsm/rock-dashboards-{{ rock_dashboards_branch }}\"\nstenographer_user: stenographer\nstenographer_group: stenographer\nstenographer_data_dir: \"{{ rock_data_dir }}/stenographer\"\nsuricata_user: suricata\nsuricata_group: suricata\nsuricata_data_dir: \"{{ rock_data_dir }}/suricata\"\nsuricata_var_dir: /var/lib/suricata\n\nfsf_user: fsf\nfsf_group: fsf\nfsf_data_dir: \"{{ rock_data_dir }}/fsf\"\nfsf_archive_dir: \"{{ fsf_data_dir }}/archive\"\nfsf_client_logfile: \"{{ fsf_data_dir }}/client.log\"\nfsf_retention: 3\nkafka_user: kafka\nkafka_group: kafka\nkafka_data_dir: \"{{ rock_data_dir }}/kafka\"\nkafka_config_path: /etc/kafka/server.properties\nes_user: elasticsearch\nes_group: elasticsearch\nes_data_dir: \"{{ rock_data_dir }}/elasticsearch\"\nes_cluster_name: rocknsm\nes_node_name: \"{{ ansible_hostname }}\"\nes_network_host: \"{{ '_site:ipv4_' if ( groups['elasticsearch'] | length ) > 1 else '_local:ipv4_' }}\"\nes_url: \"http://{{ groups['elasticsearch'][0] if ( groups['elasticsearch'] | length ) > 1 else '127.0.0.1' }}:9200\"\nes_action_auto_create_index: true\nes_min_master_nodes: \"{{ 2 if ( groups['es_masters'] | length ) == 3 else 1 }}\"\nes_mem: \"{{ (ansible_memtotal_mb|int // 1024 // 2) if (ansible_memtotal_mb|int // 1024) < 64 else 31 }}\"\nes_log_dir: /var/log/elasticsearch\nes_memlock_override: |\n  [Service]\n  LimitMEMLOCK=infinity\nlogstash_user: logstash\nlogstash_group: logstash\nkibana_host: \"127.0.0.1\"\nkibana_port: \"5601\"\nkibana_url: \"http://{{ kibana_host }}:{{ kibana_port }}\"\n\n\n###############################################################################\n#                       XFS Quota Configuration\n###############################################################################\n# Set default mount point for Storage\ndefault_mount: /data\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "3ee7ee20017cc592d55bfa824de58c4967a8ce30", "filename": "tasks/create_repo_docker_proxy_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_docker_proxy\n    args: \"{{ _nexus_repos_docker_defaults|combine(item) }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ee365be305dda2fc460c1874c6cbd066c9edbd7a", "filename": "reference-architecture/gcp/ansible/playbooks/roles/ssh-proxy/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: configure ssh proxy via bastion host\n  blockinfile:\n    dest: '{{ ssh_config_file }}'\n    create: true\n    mode: 0600\n    marker: '# {mark} OPENSHIFT ON GCP BLOCK'\n    state: present\n    block: |\n      Host {{ prefix }}-bastion\n      HostName {{ hostvars[prefix + '-bastion']['gce_public_ip'] }}\n      User cloud-user\n      IdentityFile ~/.ssh/google_compute_engine\n      UserKnownHostsFile ~/.ssh/google_compute_known_hosts\n      HostKeyAlias compute.{{ hostvars[prefix + '-bastion']['gce_id'] }}\n      IdentitiesOnly yes\n      CheckHostIP no\n      StrictHostKeyChecking no\n      {% for item in groups['tag_' + prefix] %}\n      Host {{ item }}\n        User cloud-user\n        IdentityFile ~/.ssh/google_compute_engine\n        proxycommand ssh {{ prefix }}-bastion -W %h:%p\n      {% endfor %}\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "cfe3be2ebfd8f30a2a483ad9c5f6f0a3100f5dbc", "filename": "archive/roles/openshift-common/tasks/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- name: Setting OpenShift Common Facts\n  set_fact:\n    openshift_storage_disk_volume: \"{{ openshift_storage_disk_volume | default(default_openshift_storage_disk_volume) }}\"\n    openshift_master_count: \"{{ openshift_master_count | default(default_openshift_master_count) }}\"\n    openshift_node_count: \"{{ openshift_node_count | default(default_openshift_node_count) }}\"\n    openshift_app_domain: \"{{ openshift_app_domain | default(default_openshift_app_domain) }}\"\n    openshift_openstack_flavor_name: \"{{ openshift_openstack_flavor_name | default(default_openshift_openstack_flavor_name) }}\"\n    openshift_openstack_image_name: \"{{ openshift_openstack_image_name | default(default_openshift_openstack_image_name) }}\"\n    openshift_openstack_master_storage_size: \"{{ openshift_openstack_master_storage_size | default(default_openshift_openstack_master_storage_size) }}\"\n    openshift_openstack_node_storage_size: \"{{ openshift_openstack_node_storage_size | default(default_openshift_openstack_node_storage_size) }}\"\n    openshift_openstack_master_security_groups: \"{{ openshift_openstack_master_security_groups | default(default_openshift_openstack_master_security_groups) }}\"\n    openshift_openstack_node_security_groups: \"{{ openshift_openstack_node_security_groups | default(default_openshift_openstack_node_security_groups) }}\"\n    openshift_openstack_dns_security_groups: \"{{ openshift_openstack_dns_security_groups | default(default_openshift_openstack_dns_security_groups) }}\"\n    openshift_openstack_nfs_security_groups: \"{{ openshift_openstack_nfs_security_groups | default(default_openshift_openstack_nfs_security_groups) }}\"\n"}, {"commit_sha": "e9fb46dc84b9c815a69f6de1347c9ece5db01cc8", "sha": "d41bdc3c772d2fb6fa64821d717614bc361422b9", "filename": "tasks/nodejs.yml", "repository": "fubarhouse/ansible-role-nodejs", "decoded_content": "---\n# Tasks file for NodeJS\n\n- name: \"NodeJS | Check\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell:  \"{{ fubarhouse_npm.nvm_symlink_exec }} ls | cat\"\n  register: installed_nodejs_versions\n  changed_when: false\n\n- name: \"NodeJS | Install all requested versions\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell:  \"{{ fubarhouse_npm.nvm_symlink_exec }} install {{ item }}\"\n  when: '\"{{ item }}\" in nodejs_available_versions.stdout and item not in installed_nodejs_versions.stdout'\n  with_items:\n  - \"{{ node_version }}\"\n  - \"{{ node_versions }}\"\n\n- name: \"NodeJS | Switching\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell:  \"{{ fubarhouse_npm.nvm_symlink_exec }} use {{ node_version }}\"\n  register: fubarhouse_npm_switch\n  changed_when: false\n  when: '\"{{ node_version }}\" in nodejs_available_versions.stdout'\n\n- name: \"NodeJS | Linking\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell: \"{{ fubarhouse_npm.nvm_symlink_exec }} alias default {{ node_version }}\"\n  changed_when: false\n  when: '\"{{ node_version }}\" in nodejs_available_versions.stdout'\n\n- name: \"NodeJS | Linking binaries\"\n  file:\n    src: \"{{ fubarhouse_npm.user_dir }}/.nvm/v{{ node_version }}/bin/{{ item }}\"\n    dest: \"/usr/local/bin/{{ item }}\"\n    state: link\n    force: yes\n  with_items:\n    - node\n    - npm\n  when: '\"{{ node_version }}\" in nodejs_available_versions.stdout'\n\n- name: \"NodeJS | Import exports\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  lineinfile:\n    dest: \"{{ fubarhouse_npm.user_dir }}/{{ item.filename }}\"\n    line: \"export PATH=$PATH:$(npm config --global get prefix)/bin\"\n    state: present\n  with_items: \"{{ fubarhouse_npm.shell_profiles }}\"\n  when: '\"{{ node_version }}\" in nodejs_available_versions.stdout'\n\n- name: \"NodeJS | Verify version in use\"\n  shell: \"{{ fubarhouse_npm.nvm_symlink_exec }} ls | grep current | cat\"\n  register: node_current_version\n  changed_when: false\n  failed_when: 'node_current_version.stdout.find(\"{{ node_version }}\") == -1'\n  when: '\"{{ node_version }}\" in nodejs_available_versions.stdout'"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "edeffb7772a4a662f6fd46afbb63f61ce6ec67ab", "filename": "tasks/configure-docker/configure-docker-plugins.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Fetch Docker daemon status\n  become: yes\n  service:\n    name: docker\n  register: _docker_status\n\n- name: Start Docker daemon\n  become: yes\n  service:\n    name: docker\n    state: started\n  when:\n    - _docker_status.status is defined\n    - _docker_status.status.SubState is defined\n    - _docker_status.status.SubState != \"running\"\n\n- name: Wait for Docker daemon to started\n  become: yes\n  shell: docker info\n  register: _docker_info\n  until: _docker_info.rc == 0\n  retries: 10\n  changed_when: false\n  tags:\n    - skip_ansible_lint\n\n- name: Install Docker plugins\n  become: yes\n  shell: \"(docker plugin install --grant-all-permissions --alias {{ item.alias | default(item.name) }} {{ item.name }} \\\n    && echo 'installed') || echo 'nop'\"\n  loop: \"{{ docker_plugins }}\"\n  register: _docker_plugin_install\n  changed_when: _docker_plugin_install.stdout_lines | last == 'installed'\n  when:\n    - docker_network_access | bool\n\n- name: Reset list of authorization plugins\n  set_fact:\n    _authz_plugins: []\n\n- name: Create list of authorization plugins\n  set_fact:\n    _authz_plugins: \"{{ _authz_plugins + [item.alias | default(item.name)] }}\"\n  loop: \"{{ docker_plugins }}\"\n  when:\n    - item.type == 'authz'\n\n- name: Update Docker daemon configuration with authorization plugins\n  set_fact:\n    _docker_daemon_config: \"{{ docker_daemon_config | combine(_updated_item, recursive=true) }}\"\n  vars:\n    _updated_item: \"{ 'authorization-plugins': {{ _authz_plugins | list }} }\"\n\n- name: Update Docker daemon (variables)\n  become: yes\n  copy:\n    content: \"{{ _docker_daemon_config | to_nice_json }}\"\n    dest: /etc/docker/daemon.json\n  notify: restart docker\n  when:\n    - docker_daemon_config_file is not defined\n    - docker_daemon_config is defined\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "0d247100d57ac2ee720dcf6af9f1d58015ea3acc", "filename": "tasks/configure-RedHat.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n\nhttpd_package_name: \"httpd\"\nhttpd_config_dir: \"/etc/{{ httpd_package_name }}/conf.d\"\ncertificate_file_dest: \"/etc/pki/tls/certs\"\ncertificate_key_dest: \"/etc/pki/tls/private\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "7d8324770f7b61b664a29e9ae6af5f9f3e993e05", "filename": "roles/dns/manage-dns-zones/tasks/route53/process-views.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- include_tasks: process-zones.yml\n  with_items:\n    - \"{{ dns_data.views }}\"\n  loop_control:\n    loop_var: \"view\"\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "dffa4cc5e8077c6681d4708d37f259f49d74db6d", "filename": "roles/config-postgresql/tasks/install_containerized.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Generate Random DB Username\n  set_fact:\n    postgresql_username: \"{{ lookup('password', '/dev/null length=5 chars=ascii_letters') | lower }}\"\n  when: postgresql_username is undefined or postgresql_username|trim == \"\"\n\n- name: Generate Random DB Admin Username\n  set_fact:\n    postgresql_admin_user: \"{{ lookup('password', '/dev/null length=5 chars=ascii_letters') | lower }}\"\n  when: postgresql_admin_user is undefined or postgresql_admin_user|trim == \"\"\n\n\n- name: Generate Random DB Password\n  set_fact:\n    postgresql_password: \"{{ lookup('password', '/dev/null length=10 chars=ascii_letters,digits,hexdigits') }}\"\n  when: postgresql_password is undefined or postgresql_password|trim == \"\"\n\n- name: Generate Random Admin DB Password\n  set_fact:\n    postgresql_admin_password: \"{{ lookup('password', '/dev/null length=10 chars=ascii_letters,digits,hexdigits') }}\"\n  when: postgresql_admin_password is undefined or postgresql_admin_password|trim == \"\"\n\n- name: Configure Storage Directory\n  file:\n    state: directory\n    owner: root\n    group: root\n    mode: g+rw\n    path: \"{{ postgresql_storage_dir }}\"\n  notify: \"Restart PostgreSQL Service\"\n\n\n- name: Configure systemd environment files\n  template:\n    src: \"postgresql.j2\"\n    dest: \"{{ systemd_environmentfile_dir}}/{{ postgresql_name }}\"\n  notify: \"Restart PostgreSQL Service\"\n\n- name: Configure systemd unit files\n  template:\n    src: \"postgresql.service.j2\"\n    dest: \"{{ systemd_service_dir}}/{{ postgresql_service }}\"\n  notify: \"Restart PostgreSQL Service\"\n\n- name: Include firewall tasks\n  include_tasks: firewall.yml"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "06eb979579c508ad4b2a64c0d3be548ce6a9029e", "filename": "tasks/create_repo_pypi_hosted_each.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n- include: call_script.yml\n  vars:\n    script_name: create_repo_pypi_hosted\n    args: \"{{ _nexus_repos_pypi_defaults|combine(item) }}\""}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "b5869feff31d0d5e5760d16e71b7cd5a8d7c2045", "filename": "reference-architecture/vmware-ansible/playbooks/roles/docker-storage-setup/templates/docker-storage-setup-dm.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "DEVS=\"{{ docker_dev }}\"\nVG=\"{{ docker_vg }}\"\nDATA_SIZE=\"{{ docker_data_size }}\"\nEXTRA_DOCKER_STORAGE_OPTIONS=\"--storage-opt dm.basesize={{ docker_dm_basesize }}\"\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "8bb65a5a9bb832bbc81b50393153479ac3296c8b", "filename": "roles/config-vnc-server/tasks/vnc-server.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Install additional packages for VNC server\"\n  package:\n    name: \"{{ item }}\"\n    state: installed\n  with_items:\n  - tigervnc-server\n  - policycoreutils-python-utils\n  - checkpolicy\n\n- name: \"Ensure .vnc dir exists\"\n  file:\n    path: \"{{ vnc_home_dir }}/{{ main_user }}/.vnc\"\n    state: directory\n    owner: \"{{ main_user }}\"\n\n- name: \"Check to see if a VNC password already exists\"\n  stat:\n    path: \"{{ vnc_home_dir }}/{{ main_user }}/.vnc/passwd\"\n  register: passwd_info\n\n- name: \"Set a vnc password\"\n  shell: \"echo {{ vnc_password | default('vncpasswd01') }} | vncpasswd -f > {{ vnc_home_dir }}/{{ main_user }}/.vnc/passwd\"\n  when: passwd_info.stat.exists == False\n\n- name: \"Ensure correct ownership of the vnc password file\"\n  file:\n    path: \"{{ vnc_home_dir }}/{{ main_user }}/.vnc/passwd\"\n    owner: \"{{ main_user }}\"\n    mode: 0600\n\n- name: \"Add the xstartup (gnome) configuration to the main user\"\n  copy :\n    src: xstartup-gnome\n    dest: \"{{ vnc_home_dir }}/{{ main_user }}/.vnc/xstartup\"\n    force: no\n    owner: \"{{ main_user }}\"\n    mode: 0755\n  when:\n  - gnome_install|default(False)\n\n- name: \"Add the xstartup (XFCE) configuration to the main user\"\n  copy :\n    src: xstartup-xfce\n    dest: \"{{ vnc_home_dir }}/{{ main_user }}/.vnc/xstartup\"\n    force: no\n    owner: \"{{ main_user }}\"\n    mode: 0755\n  when:\n  - xfce_install|default(False)\n\n- name: \"Add the xstartup (LXDE) configuration to the main user\"\n  copy :\n    src: xstartup-lxde\n    dest: \"{{ vnc_home_dir }}/{{ main_user }}/.vnc/xstartup\"\n    force: no\n    owner: \"{{ main_user }}\"\n    mode: 0755\n  when:\n  - lxde_install|default(False)\n\n- name: \"Add the xstartup (MATE) configuration to the main user\"\n  copy :\n    src: xstartup-mate\n    dest: \"{{ vnc_home_dir }}/{{ main_user }}/.vnc/xstartup\"\n    force: no\n    owner: \"{{ main_user }}\"\n    mode: 0755\n  when:\n  - mate_install|default(False)\n\n- name: \"Copy VNC service file into place\"\n  copy:\n    src: /usr/lib/systemd/system/vncserver@.service\n    dest: \"/etc/systemd/system/vncserver-{{ main_user }}@.service\"\n    remote_src: True\n\n- name: \"Ensure the user config is set for the vnc service\"\n  replace:\n    path: \"/etc/systemd/system/vncserver-{{ main_user }}@.service\"\n    regexp: \"{{ item.0 }}\"\n    replace: \"{{ item.1 }}\"\n  with_together:\n  - ['<USER>', '/home/' ]\n  - [ \"{{ main_user }}\", \"{{ vnc_home_dir }}/\" ]\n\n- name: \"Reload systemctl daemon\"\n  command: systemctl daemon-reload\n\n- name: \"Copy SELinux .te file to the host - used to build the module\"\n  copy:\n    src: SELinuxVNC.te\n    dest: /tmp/SELinuxVNC.te\n\n- name: \"Build SELinux module (.mod) to allow VNC\"\n  command: checkmodule -M -m -o SELinuxVNC.mod /tmp/SELinuxVNC.te\n\n- name: \"Build SELinux module (.pp) to allow VNC\"\n  command: semodule_package -m SELinuxVNC.mod -o SELinuxVNC.pp\n\n- name: \"Load SELinux module to allow VNC\"\n  command: semodule -i SELinuxVNC.pp\n\n- name: \"Enable and start VNC server for user\"\n  service:\n    name: \"vncserver-{{ main_user }}@:1\"\n    enabled: yes\n    state: started\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "63e7b48cdf2511088a66aa5cf4d12897f85c5e6c", "filename": "roles/wireguard/tasks/freebsd.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- name: BSD | WireGuard installed\n  package:\n    name: wireguard\n    state: present\n\n- set_fact:\n    service_name: wireguard\n  tags: always\n\n- name: BSD | Configure rc script\n  copy:\n    src: wireguard.sh\n    dest: /usr/local/etc/rc.d/wireguard\n    mode: \"0755\"\n  notify: restart wireguard\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "7345250d6612f7738fd729834fad5179bdbe7105", "filename": "roles/storage-demo/tasks/provision.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- name: Login As Super User\n  command: \"oc login -u {{ admin_user }} -p {{ admin_password }}\"\n  when: cluster==\"openshift\"\n        and admin_user is defined\n        and admin_password is defined\n\n- name: Check if namespace {{ namespace }} exists\n  shell: kubectl get ns | grep -w {{ namespace }} | awk '{ print $1 }'\n  register: ns\n\n- name: Create {{ namespace }} namespace\n  shell: kubectl create namespace {{ namespace }}\n  when: ns.stdout != namespace\n\n- name: Check for storage-demo serviceaccount\n  command: kubectl get serviceaccount storage-demo -n {{ namespace }}\n  register: user\n  failed_when: user.rc > 1\n\n- name: Create storage-demo serviceaccount\n  command: kubectl create serviceaccount storage-demo -n {{ namespace }}\n  when: user.stdout == \"\"\n\n- name: Grant privileged access to storage-demo serviceaccount\n  command: oc adm policy add-scc-to-user privileged system:serviceaccount:{{ namespace }}:storage-demo\n  when: cluster==\"openshift\"\n\n- name: Select a target node\n  command: kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type==\"Hostname\")].address}'\n  register: node_hostname\n  when: storage_demo_node_hostname is not defined\n\n- name: Set the target node\n  set_fact:\n    storage_demo_node_hostname: \"{{ node_hostname.stdout }}\"\n  when: storage_demo_node_hostname is not defined\n\n- name: Render storage-demo deployment yaml\n  template:\n    src: \"{{ storage_demo_template_dir }}/storage-demo.yml\"\n    dest: /tmp/storage-demo.yml\n\n- name: Create storage-demo Resources\n  command: kubectl apply -f /tmp/storage-demo.yml\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "29d9f4b19d47af385b27f6d8581bdb35c21caf0f", "filename": "archive/playbooks/registry/provision-openstack.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- hosts: localhost\n  pre_tasks:\n  - include: roles/common/pre_tasks/pre_tasks.yml\n  roles:\n    - role: common\n    - role: openshift-common\n    # Provision Master\n    - role: openstack-create\n      type: \"registry\"\n      key_name: \"{{ openstack_key_name }}\"\n      image_name: \"ose3_1-base\"\n      flavor_name: \"{{ openshift_openstack_flavor_name }}\"\n      security_groups: \"docker-registry,default\"\n      register_host_group: \"registry\"\n      node_count: \"1\"\n- hosts: registry\n  roles:\n    - role: registry\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "56cea8a4e705e4dc7171672c1282bd1edeca7453", "filename": "roles/config-chrony/tests/test.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- hosts: ntp_servers\n  roles:\n  - config-chrony\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "21e746d773f9e798c0abb838e175482770433ccb", "filename": "playbooks/roles/stenographer/tasks/install.yml", "repository": "rocknsm/rock", "decoded_content": "- name: Install packages\n  yum:\n    name: \"{{ stenographer_packagename }}\"\n    state: \"present\"\n  tags:\n    - yum\n    - stenographer\n    - install\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "2f01092f3da29187161d97f4ebf3923d682dfd12", "filename": "reference-architecture/gcp/ansible/playbooks/roles/gcp-ssh-key/defaults/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\ngcp_ssh_key: '~/.ssh/google_compute_engine'\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "45e9005cc9a814591e58049a1f002fdd45742d9d", "filename": "playbooks/provisioning/openstack/pre-install.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n###############################\n# OpenShift Pre-Requisites\n\n# - subscribe hosts\n# - prepare docker\n# - other prep (install additional packages, etc.)\n#\n- hosts: OSEv3\n  become: true\n  roles:\n    - { role: subscription-manager, when: hostvars.localhost.rhsm_register|default(False), tags: 'subscription-manager', ansible_sudo: true }\n    - { role: docker, tags: 'docker' }\n    - { role: openshift-prep, tags: 'openshift-prep' }\n\n- hosts: localhost:cluster_hosts\n  become: False\n  tasks:\n    - include: pre_tasks.yml\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "2de15f4574e0eb2b2eb0d0677860f598adf0da5e", "filename": "reference-architecture/vmware-ansible/playbooks/roles/haproxy-server/handlers/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: restart haproxy\n  service: name=haproxy state=restarted\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "230314c5efbd3b3214e184f6a5f59084388ac773", "filename": "playbooks/provision-satellite-server/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_playbook: ../prep.yml\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../osp/manage-user-network.yml\n  when:\n  - hosting_infrastructure == 'openstack'\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../osp/provision-osp-instance.yml\n  when:\n  - hosting_infrastructure == 'openstack'\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: ../rhsm.yml\n  tags:\n  - 'never'\n  - 'install'\n\n- hosts: satellite-server\n  roles:\n  - role: update-host\n  tags:\n  - 'never'\n  - 'install'\n\n- import_playbook: configure-satellite-server.yml\n  tags:\n  - 'always'\n"}, {"commit_sha": "b2591b9333f6e7e70f6b9d99e55356b30d7e173c", "sha": "005dd424b6075ffbf39573f2c1ac0c1e0d4bb76a", "filename": "tasks/main.yml", "repository": "inkatze/wildfly", "decoded_content": "---\n# task file for wildfly\n\n- include: install.yml\n- include: configure.yml\n- include: users.yml\n\n- include: ssl.yml\n  when: wildfly_enable_ssl\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "d4a417898382efd20c1d6187a503c3c6930eb6b0", "filename": "roles/config-satellite/tasks/activation_keys.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Get current subscriptions\"\n  shell: hammer -u \"{{ satellite_username }}\" -p \"{{ satellite_password }}\" --csv subscription list --organization \"{{ satellite_organization }}\" | grep \"{{ item.value.subscription }}\" | awk -F, '{print $1}'\n  register: subids\n  with_dict: \"{{ satellite_activation_keys }}\"\n\n- name: \"Create activation key(s)\"\n  command: > \n    hammer\n      -u \"{{ satellite_username }}\"\n      -p \"{{ satellite_password }}\"\n      activation-key create\n      --name {{ item.key }}\n      --lifecycle-environment Library\n      --organization \"{{ satellite_organization }}\"\n  with_dict: \"{{ satellite_activation_keys }}\"\n  register: chk_ak\n  failed_when:\n  - chk_ak.rc != 65\n  - chk_ak.rc != 0\n\n- name: \"Add subs to activation key\"\n  command: >\n     hammer\n       -u \"{{ satellite_username }}\"\n       -p \"{{ satellite_password }}\"\n       activation-key add-subscription\n       --name \"{{ item.item.key }}\"\n       --subscription-id \"{{ item.stdout }}\"\n       --organization \"{{ satellite_organization }}\"\n  with_items:\n  - \"{{ subids.results }}\"\n  register: ak_chk\n  failed_when:\n  - ak_chk.rc != 128\n  - ak_chk.rc != 0\n\n- name: \"Turn off autoattach on activation key(s)\"\n  command: >\n    hammer\n      -u \"{{ satellite_username }}\"\n      -p \"{{ satellite_password }}\"\n      activation-key update\n      --auto-attach false\n      --name \"{{ item.key }}\"\n      --organization \"{{ satellite_organization }}\"\n  with_dict: \"{{ satellite_activation_keys }}\"\n\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "1affd1db7f7d2f5e5e5523faed743cf323859443", "filename": "roles/ansible/tower/config-ansible-tower-ldap/tests/inventory/group_vars/tower.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nansible_connection: local\n\n# NOTE: below is an example on how these params and files can be specified\n#       - please replace with valid values and files\n\nansible_tower:\n  admin_password: \"admin01\"\n  ldap:\n    ca_cert: \"{{ inventory_dir }}/../files/ca.crt\"\n    uri: \"ldaps://idm.test.example.com:636\"\n    bind_dn: \"uid=bind-user,cn=users,cn=accounts,dc=test,dc=example,dc=com\"\n    bind_password: \"my-bind-secret\"\n    user_search_dn: \"cn=users,cn=accounts,dc=test,dc=example,dc=com\"\n    user_dn_template: \"uid=%(user)s,cn=users,cn=accounts,dc=test,dc=example,dc=com\"\n    group_search_dn: \"cn=groups,cn=accounts,dc=test,dc=example,dc=com\"\n    require_group: \"cn=tower-users,cn=groups,cn=accounts,dc=test,dc=example,dc=com\"\n    admin_group: \"cn=tower-admins,cn=groups,cn=accounts,dc=test,dc=example,dc=com\"\n    organization_map:\n    - name: \"My Admin Org\"\n      admin_group: \"cn=tower-admins,cn=groups,cn=accounts,dc=test,dc=example,dc=com\"\n      user_groups:\n      - \"cn=tower-users,cn=groups,cn=accounts,dc=test,dc=example,dc=com\"\n    - name: \"My Support Org\"\n      admin_group: \"cn=tower-admins,cn=groups,cn=accounts,dc=test,dc=example,dc=com\"\n      user_groups:\n      - \"cn=tower-users,cn=groups,cn=accounts,dc=test,dc=example,dc=com\"\n      - \"cn=tower-support,cn=groups,cn=accounts,dc=test,dc=example,dc=com\"\n    team_map:\n    - name: \"My First Team\"\n      organization: \"First Org\"\n      user_groups:\n      - \"cn=users1,cn=groups,cn=accounts,dc=test,dc=example,dc=com\"\n      - \"cn=users2,cn=groups,cn=accounts,dc=test,dc=example,dc=com\"\n    - name: \"My Second Team\"\n      organization: \"Second Org\"\n      user_groups:\n      - \"cn=users1,cn=groups,cn=accounts,dc=test,dc=example,dc=com\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "d8b4a0276d8efd8fd2a0fb92237d994389b259a9", "filename": "reference-architecture/vmware-ansible/playbooks/roles/docker-storage-setup/templates/docker-storage-setup-overlayfs.j2", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "DEVS=\"{{ docker_dev }}\"\nVG=\"{{ docker_vg }}\"\nDATA_SIZE=\"{{ docker_data_size }}\"\nSTORAGE_DRIVER=overlay2\nCONTAINER_ROOT_LV_NAME=\"{{ container_root_lv_name }}\"\nCONTAINER_ROOT_LV_MOUNT_PATH=\"{{ container_root_lv_mount_path }}\"\nCONTAINER_ROOT_LV_SIZE=100%FREE\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "380888cd3dfcf724e6c20995620883a8be3fc759", "filename": "roles/scm/github.com/handlers/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- name: 'github.com cleanup temp'\n  delegate_to: localhost\n  file:\n    path: '{{ tmp_dir.path }}'\n    state: absent"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "22c7b030e8a59f242c98062fd0e00c583f5dca57", "filename": "vagrant/tasks/install_bootstrap_origin.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- package:\n    name: git\n    state: present\n- git:\n    repo: https://github.com/openshift/openshift-ansible\n    dest: ~/openshift-ansible\n    force: yes\n    update: yes\n  become: yes\n  become_user: vagrant\n- package:\n    name: https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n    state: present\n- replace:\n    dest: /etc/yum.repos.d/epel.repo\n    regexp: '^enabled=1'\n    replace: 'enabled=0'\n- yum:\n    name: \"{{ item }}\"\n    enablerepo: epel\n    state: present\n  when: \"{{ ansible_distribution != 'Fedora' }}\"\n  with_items:\n  - ansible\n  - pyOpenSSL\n- dnf:\n    name: \"{{ item }}\"\n    enablerepo: epel\n    state: present\n  when: \"{{ ansible_distribution == 'Fedora' }}\"\n  with_items:\n  - ansible\n  - pyOpenSSL\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "eb5c4894afbaa767d36bb2d6b2a4f0d57b0ecfe3", "filename": "roles/ipaserver/vars/default.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "# defaults file for ipaserver\n# vars/default.yml\nipaserver_packages: [ \"ipa-server\", \"python3-libselinux\" ]\nipaserver_packages_dns: [ \"ipa-server-dns\" ]\nipaserver_packages_adtrust: [ \"freeipa-server-trust-ad\" ]\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "f10179dfcc33832dd20bc239c9886885b112d500", "filename": "archive/roles/cicd/tasks/groovy.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n  \n- name: Download Groovy Artifact\n  get_url: \n    url: \"{{ groovy_url }}\"\n    dest: \"{{ groovy_local_archive }}\"\n  tags: groovy\n \n- name: Extract Groovy Artifact\n  unarchive: \n      src: \"{{ groovy_local_archive }}\"\n      dest: \"{{ groovy_base_dir }}\"\n      copy: no \n      creates: \"{{ groovy_install_dir }}\"\n  tags: groovy\n\n- name: Create Groovy Symbolic Link\n  file: \n      src: \"{{ groovy_install_dir }}\"\n      dest: \"{{ groovy_base_dir }}/groovy\"\n      state: link\n  tags: groovy\n\n- name: Add GROOVY_HOME Variable\n  lineinfile: \n    dest: /etc/profile \n    regexp: \"^export GROOVY_HOME=/usr/local/groovy-{{groovy_version}}\"\n    line: \"export GROOVY_HOME=/usr/local/groovy-{{groovy_version}}\"\n  tags: groovy\n  \n- name: Add GROOVY_HOME to PATH\n  lineinfile: \n    dest: /etc/profile\n    regexp: \"^export PATH=$PATH:$GROOVY_HOME/bin\"\n    line: \"export PATH=$PATH:$GROOVY_HOME/bin\"\n  tags: groovy"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "66a1015b9129d1ae6904d3313ab1c62d198364c8", "filename": "roles/ovirt-guest-agent/defaults/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_guest_agent_pkg_prefix: ovirt\n"}, {"commit_sha": "218cdc58f9fe9d7ece7d43e5f100fe9631fde5cc", "sha": "1897f2e3869f8a19ee09afb755a8c0e72cf4d120", "filename": "tasks/install-source.yml", "repository": "fubarhouse/ansible-role-golang", "decoded_content": "---\n\n# This file needs to be rewritten\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "61bcc7684474b03422f9ef84169e0dce53d349bb", "filename": "roles/idm-host-cert/tasks/register-host.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Register host\"\n  uri: \n    url: \"https://{{ idm_fqdn }}/ipa/session/json\"\n    method: POST\n    body: '{\"method\":\"host_add\",\"params\":[[\"{{ host_name }}\"],{\"force\": {{ host_force_add }}, \"description\": \"{{ host_description }}\", \"version\": \"{{ api_version }}\" }],\"id\":0}'\n    body_format: json\n    validate_certs: no\n    headers:\n      Cookie: \"{{ idm_session.set_cookie }}\"\n      referer: \"https://{{ idm_fqdn }}/ipa\"\n      Content-Type: \"application/json\"\n      Accept: \"application/json\"\n  register: reg_host\n\n- name: \"Error out if the request returned an error\"\n  fail:\n    msg: \"ERROR: request failed with message: {{ reg_host.json.error.message }}\"\n  when:\n  - reg_host.json.error is defined\n  - reg_host.json.error.message is defined\n  - reg_host.json.error.name != \"DuplicateEntry\"\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "e084739366aba14e3a41b0622ab309e04810a166", "filename": "roles/stenographer/vars/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# vars file for stenographer\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "a781dc00c4a86e72c8da531b979ae9a3ccbd0bf0", "filename": "roles/ovirt-common/meta/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\ngalaxy_info:\n  author: \"Petr Kubica\"\n  description: \"oVirt common stuff\"\n  company: \"Red Hat\"\n  license: \"GPLv3\"\n  min_ansible_version: 1.9\n  platforms:\n  - name: EL\n    versions:\n    - all\n  galaxy_tags:\n    - installer\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "17f3b5d668be90290fc3450c84ebe8a4b040d623", "filename": "roles/user-management/manage-user-passwd/tasks/generate-passwords.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n- include_tasks: generate-one-password.yml\n  with_items:\n    - \"{{ users }}\"\n\n- name: \"Add password to users dict\"\n  set_fact:\n    users_tmp: \"{{ users_tmp|default([]) + [ item | combine( { 'password': user_passwords[item.user_name] } )] }}\"\n  with_items:\n    - \"{{ users }}\"\n\n- name: \"Copy rebuilt structure to users structure\"\n  set_fact:\n    users: \"{{ users_tmp }}\"\n\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "90da64f5ab6e9dc21534cf4d2a948a20a8d41417", "filename": "roles/wireguard/defaults/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\nwireguard_client_ip: \"{{ wireguard_network_ipv4['clients_range'] }}.{{ wireguard_network_ipv4['clients_start'] + index|int + 1 }}/{{ wireguard_network_ipv4['prefix'] }}{% if ipv6_support %},{{ wireguard_network_ipv6['clients_range'] }}{{ wireguard_network_ipv6['clients_start'] + index|int + 1 }}/{{ wireguard_network_ipv6['prefix'] }}{% endif %}\"\nwireguard_server_ip: \"{{ wireguard_network_ipv4['gateway'] }}/{{ wireguard_network_ipv4['prefix'] }}{% if ipv6_support %},{{ wireguard_network_ipv6['gateway'] }}/{{ wireguard_network_ipv6['prefix'] }}{% endif %}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "10f8f1da87135393a2a3ba191e6d633944e01f97", "filename": "reference-architecture/vmware-ansible/playbooks/roles/haproxy-server/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Ensure HAProxy is installed.\n  yum: name=haproxy state=installed\n\n- name: Get HAProxy version.\n  command: haproxy -v\n  register: haproxy_version_result\n  changed_when: false\n  always_run: yes\n\n- name: open firewall for Openshift services\n  command: iptables -I INPUT -p tcp --dport {{item}} -j ACCEPT\n  with_items:\n    - \"{{ console_port }}\"\n    - 443\n    - 80\n\n- name: Save the iptables rules\n  command: iptables-save\n\n- name: Ensure Firewalld is disabled\n  service: name=firewalld state=stopped enabled=no\n\n- name: Ensure HAProxy is started and enabled on boot.\n  service: name=haproxy state=started enabled=yes\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "4368862663608976cb5d5ed4963e76d25e1ceeb0", "filename": "roles/config-nagios-target/tasks/nrpe_mem.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Copy in additional Nagios Memory plugins\n  copy: \n    src: plugins/check_mem\n    dest: /usr/lib64/nagios/plugins/check_mem\n    owner: root\n    group: root\n    mode: 0755\n\n- name: Copy nrpe.d Memory configuration files\n  copy: \n    src: nrpe.d/check_mem.cfg\n    dest: /etc/nrpe.d/check_mem.cfg\n    owner: root\n    group: root\n    mode: 0644\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "04c66bdf5d8917c88915c5601b76b89f51deecf0", "filename": "roles/network/tasks/wondershaper.yml", "repository": "iiab/iiab", "decoded_content": "- name: Copy Wondershaper service script\n  template: backup=yes\n            src=roles/network/templates/wondershaper/wondershaper.service\n            dest=/etc/systemd/system/wondershaper.service\n            mode=0644\n\n- name: Copy Wondershaper script\n  template: backup=yes\n            src=roles/network/templates/wondershaper/wondershaper.j2\n            dest=/usr/bin/wondershaper\n            owner=root\n            group=root\n            mode=0744\n\n- name: Create conf.d directory\n  file: path=/etc/conf.d\n        owner=root\n        group=root\n        mode=0755\n        state=directory\n\n- name: Copy Wondershaper config script\n  template: src=roles/network/templates/wondershaper/wondershaper.conf\n            dest=/etc/conf.d/wondershaper.conf\n            owner=root\n            group=root\n            mode=0600\n\n- name: Create fact for Wondershaper config file\n  file: src=/etc/conf.d/wondershaper.conf\n        dest=/etc/ansible/facts.d/wondershaper.fact\n        owner=root\n        group=root\n        state=link\n\n- name: Add 'wondershaper' to service list\n  ini_file: dest='{{ service_filelist }}'\n            section=wondershaper\n            option='{{ item.option }}'\n            value='{{ item.value }}'\n  with_items:\n    - option: name\n      value: wondershaper\n    - option: description\n      value: '\"Wondershaper is a command line tool to set maximum transfer rates for network adapters.\"'\n    - option: enabled\n      value: \"{{ wondershaper_enabled }}\"\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "ea1841e32c98e411bad39a9a0337aaec2f179d89", "filename": "reference-architecture/vmware-ansible/playbooks/roles/keepalived_haproxy/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- block:\n    - name: Install packages\n      yum: name={{ item }} state=present\n      with_items:\n        - keepalived\n        - psmisc\n\n    - name: Determine interface name on single node\n      set_fact:\n        external_interface: \"{{ ansible_default_ipv4.interface }}\"\n\n    - name: Allow connections between haproxy nodes\n      template:\n        src: firewall.sh.j2\n        dest: /tmp/firewall.sh\n        mode: \"u=rwx,g=,o=\"\n\n    - command: /tmp/firewall.sh\n\n    - file:\n        path: /tmp/firewall.sh\n        state: absent\n\n    - name: Generate OCP public IP for play\n      set_fact:\n        openshift_master_cluster_public_ip: \"{{ lb_ha_ip }}\"\n\n    - name: Generate random external password\n      shell: uuidgen\n      run_once: true\n      register: keepalived_pass\n\n    - name: Start keepalived\n      service:\n        name: keepalived\n        state: started\n        enabled: yes\n\n    - name: Configure keepalived\n      template:\n        src: keepalived.conf.j2\n        dest: /etc/keepalived/keepalived.conf\n      notify: restart keepalived\n\n  when:\n    - lb_ha_ip is defined\n    - lb_ha_ip|trim != ''\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "e2b799b9d705b8545e7242a920921d8221791d4b", "filename": "playbooks/provisioning/openstack/roles", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "../../../roles/"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "0e8eb90c11796802ad4c950357ed93860974df34", "filename": "playbooks/aws/openshift-cluster/add_nodes.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: Launch instance(s)\n  hosts: localhost\n  connection: local\n  become: no\n  gather_facts: no\n  vars_files:\n  - vars.yml\n  vars:\n    oo_extend_env: True\n  tasks:\n  - include: ../../common/openshift-cluster/tasks/set_node_launch_facts.yml\n    vars:\n      type: \"compute\"\n      count: \"{{ num_nodes }}\"\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ node_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"{{ sub_host_type }}\"\n\n  - include: ../../common/openshift-cluster/tasks/set_node_launch_facts.yml\n    vars:\n      type: \"infra\"\n      count: \"{{ num_infra }}\"\n  - include: tasks/launch_instances.yml\n    vars:\n      instances: \"{{ node_names }}\"\n      cluster: \"{{ cluster_id }}\"\n      type: \"{{ k8s_type }}\"\n      g_sub_host_type: \"{{ sub_host_type }}\"\n\n- include: scaleup.yml\n- include: list.yml\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "b318c173e39a815eaea6ee1d8b5c82a574df07cc", "filename": "roles/virt-install/tasks/create_vm.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: 'Initialize Facts for VM deployments'\n  set_fact:\n   virt_install_commands: []\n   mounted_iso: {}\n\n- name: 'Gather the list of existing VMs'\n  command: virsh list --name\n  register: existing_vms\n\n- name: 'Compose Command for new VM provisioning'\n  include_tasks: 'virt-install.yml'\n  when:\n  - inventory_hostname == hostvars[vm]['infrahost']\n  - hostvars[vm]['libvirt_name'] not in existing_vms.stdout_lines\n  with_items:\n  - \"{{ groups['infra_vms'] }}\"\n  loop_control:\n    loop_var: vm\n\n- name: \"Call virt-install to create VMs\"\n  shell: \"{{ item }}\"\n  with_items: \"{{ virt_install_commands }}\"\n  register: vm_instances\n  async: 7200\n  poll: 0\n  when: virt_install_commands|length > 0\n\n- name: 'Wait for VMs creation to complete'\n  async_status: \n    jid: \"{{ item.ansible_job_id }}\"\n  register: vm_jobs\n  until: vm_jobs.finished\n  retries: 300\n  with_items: \"{{ vm_instances.results }}\"\n  when: virt_install_commands|length > 0\n\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "fe0d216383832c3ec933ebb2d9892721a738f0e1", "filename": "archive/roles/registry/handlers/main.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n- name: reload systemd\n  command: systemctl daemon-reload\n\n- name: restart docker registry\n  service:\n    name: docker-distribution\n    state: restarted\n\n- name: reload nginx\n  service:\n    name: nginx\n    state: reloaded\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "9401499cae440dda7d8a7ebb13da9738ee657755", "filename": "roles/manage-sshd-config/test/playbook.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n\n# Test the role to update the access keys\n- name: \"Update  access\"\n  hosts: all\n  roles:\n    - role: manage-local-user-ssh-authkeys\n    - role: manage-local-user-password\n    - role: manage-sshd-config\n\n# Test the SSH Key access by running a remote command on machine\n#\n\n- name: \"Testing authorized ssh keyfile\" \n  hosts: all\n  tasks:\n  - name: \"Test authorized key for {{ user_name }} on {{ ansible_host }}\"\n    raw: \"ssh -v -i id_rsa_user1 {{ user_name }}@{{ ansible_host }} /bin/true\"\n    delegate_to: localhost\n    register: result\n    become: False\n    changed_when: False\n    failed_when:\n      result.rc != 0\n\n  - name: \"Test password for {{ user_name }} on {{ ansible_host }}\"\n    raw: \"sshpass -p \\\"{{ clear_text_password }}\\\" ssh {{ user_name }}@{{ansible_host }} /bin/true\"\n    delegate_to: localhost\n    become: False\n    register: result\n    changed_when: False\n    failed_when:\n      result.rc != 0\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "3716bf8b4704dd36d0c237291886740b1bf6a1ba", "filename": "roles/ovirt-collect-logs/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n- name: Include correct variables based on system\n  include_vars: \"{{ ovirt_collect_logs_from_system }}.yml\"\n\n- name: Ensure required archive tools\n  yum:\n    name: \"{{ item }}\"\n    state: \"present\"\n  with_items:\n    - gzip\n    - tar\n\n# Prepare place to store data\n\n- name: Clean temporary directory and previous archive\n  file:\n    path: \"{{ item }}\"\n    state: \"absent\"\n  with_items:\n    - \"{{ ovirt_collect_logs_tmp_dir }}\"\n    - \"{{ ovirt_collect_logs_archive }}\"\n\n- name: Create temporary directory\n  file:\n    path: \"{{ ovirt_collect_logs_tmp_dir }}\"\n    state: \"directory\"\n    mode: \"0777\"\n\n# Collect common stuff\n- name: Dump system information using shell commands\n  shell: \"{{ item.value }} &> {{ ovirt_collect_logs_tmp_dir }}/{{ item.key }}.txt\"\n  with_dict: \"{{ ovirt_collect_logs_shell_commands }}\"\n  ignore_errors: true\n  tags:\n    - skip_ansible_lint # check for shell module is not working correctly in Lint\n\n\n- name: Link common logs\n  file:\n    src: \"{{ item.src }}\"\n    dest: \"{{ ovirt_collect_logs_tmp_dir }}/{{ item.dest }}\"\n    state: link\n  with_items:\n    - { src: \"/var/log/messages\", dest: \"messages\" }\n\n# Collect system specific stuff\n\n- include: \"{{ ovirt_collect_logs_from_system }}.yml\"\n\n# Fetch stuff to local system\n- name: Archive logs\n  shell: \"tar {{ ovirt_collect_logs_tar_optional_params}}\n            -hczf {{ ovirt_collect_logs_archive }}\n            {{ ovirt_collect_logs_tmp_dir }}\"\n  ignore_errors: true\n  tags:\n    - skip_ansible_lint # achive module has insufficient functionality\n\n- name: Fetch logs\n  fetch:\n    src: \"{{ ovirt_collect_logs_archive }}\"\n    dest: \"logs/{{ ovirt_collect_logs_from_system }}-{{ ansible_hostname }}/ovirt-engine-logs.tar.gz\"\n    flat: yes\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "dbd82f368557bca6d6b1f7baa43608383bb040b6", "filename": "roles/cloud-azure/defaults/main.yml", "repository": "trailofbits/algo", "decoded_content": "---\nazure_venv: \"{{ playbook_dir }}/configs/.venvs/azure\"\n_azure_regions: >\n  [\n    {\n      \"displayName\": \"East Asia\",\n      \"latitude\": \"22.267\",\n      \"longitude\": \"114.188\",\n      \"name\": \"eastasia\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"Southeast Asia\",\n      \"latitude\": \"1.283\",\n      \"longitude\": \"103.833\",\n      \"name\": \"southeastasia\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"Central US\",\n      \"latitude\": \"41.5908\",\n      \"longitude\": \"-93.6208\",\n      \"name\": \"centralus\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"East US\",\n      \"latitude\": \"37.3719\",\n      \"longitude\": \"-79.8164\",\n      \"name\": \"eastus\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"East US 2\",\n      \"latitude\": \"36.6681\",\n      \"longitude\": \"-78.3889\",\n      \"name\": \"eastus2\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"West US\",\n      \"latitude\": \"37.783\",\n      \"longitude\": \"-122.417\",\n      \"name\": \"westus\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"North Central US\",\n      \"latitude\": \"41.8819\",\n      \"longitude\": \"-87.6278\",\n      \"name\": \"northcentralus\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"South Central US\",\n      \"latitude\": \"29.4167\",\n      \"longitude\": \"-98.5\",\n      \"name\": \"southcentralus\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"North Europe\",\n      \"latitude\": \"53.3478\",\n      \"longitude\": \"-6.2597\",\n      \"name\": \"northeurope\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"West Europe\",\n      \"latitude\": \"52.3667\",\n      \"longitude\": \"4.9\",\n      \"name\": \"westeurope\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"Japan West\",\n      \"latitude\": \"34.6939\",\n      \"longitude\": \"135.5022\",\n      \"name\": \"japanwest\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"Japan East\",\n      \"latitude\": \"35.68\",\n      \"longitude\": \"139.77\",\n      \"name\": \"japaneast\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"Brazil South\",\n      \"latitude\": \"-23.55\",\n      \"longitude\": \"-46.633\",\n      \"name\": \"brazilsouth\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"Australia East\",\n      \"latitude\": \"-33.86\",\n      \"longitude\": \"151.2094\",\n      \"name\": \"australiaeast\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"Australia Southeast\",\n      \"latitude\": \"-37.8136\",\n      \"longitude\": \"144.9631\",\n      \"name\": \"australiasoutheast\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"South India\",\n      \"latitude\": \"12.9822\",\n      \"longitude\": \"80.1636\",\n      \"name\": \"southindia\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"Central India\",\n      \"latitude\": \"18.5822\",\n      \"longitude\": \"73.9197\",\n      \"name\": \"centralindia\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"West India\",\n      \"latitude\": \"19.088\",\n      \"longitude\": \"72.868\",\n      \"name\": \"westindia\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"Canada Central\",\n      \"latitude\": \"43.653\",\n      \"longitude\": \"-79.383\",\n      \"name\": \"canadacentral\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"Canada East\",\n      \"latitude\": \"46.817\",\n      \"longitude\": \"-71.217\",\n      \"name\": \"canadaeast\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"UK South\",\n      \"latitude\": \"50.941\",\n      \"longitude\": \"-0.799\",\n      \"name\": \"uksouth\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"UK West\",\n      \"latitude\": \"53.427\",\n      \"longitude\": \"-3.084\",\n      \"name\": \"ukwest\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"West Central US\",\n      \"latitude\": \"40.890\",\n      \"longitude\": \"-110.234\",\n      \"name\": \"westcentralus\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"West US 2\",\n      \"latitude\": \"47.233\",\n      \"longitude\": \"-119.852\",\n      \"name\": \"westus2\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"Korea Central\",\n      \"latitude\": \"37.5665\",\n      \"longitude\": \"126.9780\",\n      \"name\": \"koreacentral\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"Korea South\",\n      \"latitude\": \"35.1796\",\n      \"longitude\": \"129.0756\",\n      \"name\": \"koreasouth\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"France Central\",\n      \"latitude\": \"46.3772\",\n      \"longitude\": \"2.3730\",\n      \"name\": \"francecentral\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"France South\",\n      \"latitude\": \"43.8345\",\n      \"longitude\": \"2.1972\",\n      \"name\": \"francesouth\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"Australia Central\",\n      \"latitude\": \"-35.3075\",\n      \"longitude\": \"149.1244\",\n      \"name\": \"australiacentral\",\n      \"subscriptionId\": null\n    },\n    {\n      \"displayName\": \"Australia Central 2\",\n      \"latitude\": \"-35.3075\",\n      \"longitude\": \"149.1244\",\n      \"name\": \"australiacentral2\",\n      \"subscriptionId\": null\n    }\n  ]\n"}, {"commit_sha": "4a9aaf0951e383c57077cf651b93e78eeea1b5ac", "sha": "bff4894f01450fa4a5bb1708f6cdc976e8298286", "filename": "tasks/main.yml", "repository": "geerlingguy/ansible-role-solr", "decoded_content": "---\n- import_tasks: user.yml\n  when: solr_create_user\n\n- name: Set solr_filename for Solr 4+.\n  set_fact:\n    solr_filename: \"solr-{{ solr_version }}\"\n  when: \"solr_version.split('.')[0] >= '4'\"\n\n- name: Set solr_filename for Solr 3.x.\n  set_fact:\n    solr_filename: \"apache-solr-{{ solr_version }}\"\n  when: \"solr_version.split('.')[0] == '3'\"\n\n- name: Check if Solr has been installed already.\n  stat:\n    path: \"{{ solr_install_path }}\"\n  register: solr_install_path_status\n\n- name: Download Solr.\n  get_url:\n    url: \"{{ solr_mirror }}/lucene/solr/{{ solr_version }}/{{ solr_filename }}.tgz\"\n    dest: \"{{ solr_workspace }}/{{ solr_filename }}.tgz\"\n    force: false\n  when: solr_install_path_status.stat.isdir is not defined\n  register: solr_download_status\n\n- name: Expand Solr.\n  unarchive:\n    src: \"{{ solr_workspace }}/{{ solr_filename }}.tgz\"\n    dest: \"{{ solr_workspace }}\"\n    copy: false\n  when: solr_download_status.changed\n  tags: ['skip_ansible_lint']\n\n# Install Solr < 5.\n- include_tasks: install-pre5.yml\n  when: \"solr_version.split('.')[0] < '5'\"\n\n# Install Solr 5+.\n- include_tasks: install.yml\n  when: \"solr_version.split('.')[0] >= '5'\"\n\n- name: Ensure solr is started and enabled on boot if configured.\n  service:\n    name: \"{{ solr_service_name }}\"\n    state: \"{{ solr_service_state }}\"\n    enabled: true\n  when: solr_service_manage\n\n# Configure solr.\n- include_tasks: configure.yml\n  when: \"solr_version.split('.')[0] >= '5'\"\n\n# Create cores, if any are configured.\n- include_tasks: cores.yml\n  when: \"solr_cores and solr_version.split('.')[0] >= '5'\"\n\n- include_tasks: trim-fat.yml\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "d97afb19623abf7db4f1ac6c95ac365a9b46eb03", "filename": "roles/ipaserver/vars/Fedora-25.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "ipaserver_packages: [ \"ipa-server\", \"libselinux-python\" ]\nipaserver_packages_dns: [ \"ipa-server-dns\" ]\nipaserver_packages_adtrust: [ \"ipa-server-trust-ad\" ]"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "5a8bc0e6118f5e8b5177cbf68b6c95deb7d20137", "filename": "tasks/section_08_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n#  - name: 8.2 Configure rsyslog\n#  The rsyslog software is recommended as a replacement for the default syslogd daemon and\n#  provides improvements over syslogd, such as connection-oriented (i.e. TCP) transmission\n#  of logs, the option to log to database formats, and the encryption of log data en route to a\n#  central logging server.\n#  tags:\n#    - section8\n#    - section8.2\n\n\n  - name: 8.2.1 Install the rsyslog package (Scored)\n    apt: name=rsyslog state=present\n    tags:\n      - section8\n      - section8.2\n      - section8.2.1\n\n  - name: 8.2.2 Ensure the rsyslog Service is activated (Scored)\n    command: grep 'start on filesystem' /etc/rsyslog.conf\n    register: startonfilesystem\n    changed_when: False\n    failed_when: False\n    always_run: True\n    tags:\n      - section8\n      - section8.2\n      - section8.2.5\n\n  - name: 8.2.5.2 Configure rsyslog to Send Logs to a Remote Log Host (start rsyslog) (Scored)\n    lineinfile:\n       dest=/etc/rsyslog.conf\n       line='start on filesystem'\n       insertafter=EOF\n       state=present\n    when: startonfilesystem.rc == 1\n    tags:\n      - section8\n      - section8.2\n      - section8.2.5\n\n  - name: 8.2.3 Configure /etc/rsyslog.conf (Not Scored)\n    lineinfile:\n       dest=/etc/rsyslog.conf\n       line=\"{{ item }}\"\n       insertafter=EOF\n    with_items:\n      - '*.emerg :omusrmsg:*'\n      - 'mail.* -/var/log/mail'\n      - 'mail.info -/var/log/mail.info'\n      - 'mail.warning -/var/log/mail.warn'\n      - 'mail.err /var/log/mail.err'\n      - 'news.crit -/var/log/news/news.crit'\n      - 'news.err -/var/log/news/news.err'\n      - 'news.notice -/var/log/news/news.notice'\n      - '*.=warning;*.=err -/var/log/warn'\n      - '*.crit /var/log/warn'\n      - '*.*;mail.none;news.none -/var/log/messages'\n      - 'local0,local1.* -/var/log/localmessages'\n      - 'local2,local3.* -/var/log/localmessages'\n      - 'local4,local5.* -/var/log/localmessages'\n      - 'local6,local7.* -/var/log/localmessages'\n    changed_when: False\n    notify: restart rsyslog\n    tags:\n      - section8\n      - section8.2\n      - section8.2.3\n\n  - name: 8.2.4.1 Create and Set Permissions on rsyslog Log Files (Scored)\n    shell: awk '{ print $NF }' /etc/rsyslog.d/* /etc/rsyslog.conf | grep /var/log | sed 's/^-//' | sed 's/)$//' \n    register: result\n    changed_when: False\n    always_run: True\n    tags:\n      - section8\n      - section8.2\n      - section8.2.4\n\n  - name: 8.2.4.2 Create and Set Permissions on rsyslog Log Files (Scored)\n    shell: 'mkdir -p -- \"$(dirname -- {{ item }})\"; touch -- {{ item }}' \n    with_items: result.stdout_lines          \n    changed_when: False\n    tags:\n      - section8\n      - section8.2\n      - section8.2.4\n\n  - name: 8.2.4.3 Create and Set Permissions on rsyslog Log Files (Scored)\n    file: >\n        path='{{item}}' \n        owner=root \n        group=root \n        mode=\"og-rwx\" \n    with_items: result.stdout_lines\n    tags:\n      - section8\n      - section8.2\n      - section8.2.4\n\n  - name: 8.2.5.1 Configure rsyslog to Send Logs to a Remote Log Host (Scored)\n    command: grep \"^*.*[^I][^I]*@\" /etc/rsyslog.conf\n    register: remoteloghost \n    changed_when: False\n    failed_when: False\n    always_run: True\n    tags:\n      - section8\n      - section8.2\n      - section8.2.5\n\n  - name: 8.2.5.2 Configure rsyslog to Send Logs to a Remote Log Host (Scored)\n    lineinfile:\n       dest=/etc/rsyslog.conf\n       line=\"*.* @@{{remote_logs_host_address}}\"\n       insertafter=EOF\n       state=present\n    when: set_rsyslog_remote == True and remoteloghost.rc == 1\n    tags:\n      - section8\n      - section8.2\n      - section8.2.5\n\n  - name: 8.2.6.1 Accept Remote rsyslog Messages Only on Designated Log Hosts (Not Scored)\n    command: grep '^$ModLoad imtcp' /etc/rsyslog.conf\n    register: moadloadpresent\n    changed_when: False\n    failed_when: False\n    always_run: True\n    tags:\n      - section8\n      - section8.2\n      - section8.2.6\n\n  - name: 8.2.6.2 Accept Remote rsyslog Messages Only on Designated Log Hosts (Not Scored)\n    lineinfile: >\n        dest=/etc/rsyslog.conf\n        regexp='^#({{ item }})'\n        line='{{ item }}'\n        state=present\n    with_items:\n        - '$ModLoad imtcp'\n        - '$InputTCPServerRun 514'\n    when: set_rsyslog_remote == True and moadloadpresent.rc == 1\n    changed_when: False\n    notify: restart rsyslog\n    tags:\n      - section8\n      - section8.2\n      - section8.2.6\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "1d78f1127b3f9a036315c4c53c249a590a5b3093", "filename": "roles/rhsm/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- block:\n    - name: Allow rhsm a longer timeout to help out with subscription-manager\n      lineinfile:\n        dest: /etc/rhsm/rhsm.conf\n        line: 'server_timeout=600'\n        insertafter: '^proxy_password ='\n\n    - name: Check for sat config file\n      stat: path=/etc/rhsm/rhsm.conf.kat-backup\n      register: sat_cfg\n\n    - name: Remove satellite configuration if using RH CDN\n      command: \"mv -f /etc/rhsm/rhsm.conf.kat-backup /etc/rhsm/rhsm.conf\"\n      when: rhsm_user is defined and rhsm_user and sat_cfg.stat.exists == True\n      ignore_errors: yes\n\n    - name: Remove satellite SSL if using RH CDN\n      command: \"rpm -e $(rpm -qa katello-ca-consumer*)\"\n      when: rhsm_user is defined and rhsm_user and sat_cfg.stat.exists == True\n      ignore_errors: yes\n\n    - name: \"Install Satellite certificate (if applicable)\"\n      command: \"rpm -Uh --force {{ rhsm_katello_url }}\"\n      when:\n        - rhsm_katello_url is defined\n        - rhsm_katello_url|trim != ''\n\n    - name: \"Is the system already registered?\"\n      command: \"subscription-manager version\"\n      register: subscribed\n      ignore_errors: yes\n\n    - name: 'Register system using Red Hat Subscription Manager'\n      redhat_subscription:\n        state: present\n        username: \"{{ rhsm_user | default(omit) }}\"\n        password: \"{{ rhsm_password | default(omit) }}\"\n        pool: \"{{ rhsm_pool | default(omit) }}\"\n        server_hostname: \"{{ rhsm_satellite | default(omit) }}\"\n        activationkey: \"{{ rhsm_activation_key | default(omit) }}\"\n        org_id: \"{{ rhsm_org_id | default(omit) }}\"\n      when: \"'not registered' in subscribed.stdout\"\n      register: rhn\n      until: rhn|success\n      retries: 5\n\n    - name: \"Obtain currently enabled repos\"\n      shell: 'subscription-manager repos --list-enabled | sed -ne \"s/^Repo ID:[^a-zA-Z0-9]*\\(.*\\)/\\1/p\"'\n      register: enabled_repos\n\n    - name: \"Disable repositories that should not be enabled\"\n      shell: \"subscription-manager repos --disable={{ item }}\"\n      with_items:\n        - \"{{ enabled_repos.stdout_lines | difference(openshift_required_repos) }}\"\n      when: provider is not defined\n\n    - name: \"Enable specified repositories not already enabled\"\n      command: \"subscription-manager repos --enable={{ item }}\"\n      with_items:\n        - \"{{ openshift_required_repos | difference(enabled_repos.stdout_lines) }}\"\n\n  when: ansible_distribution == \"RedHat\"\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "1c6eb9d1e678793942bc1849be67f29067d9f76a", "filename": "playbooks/cluster/openshift/vars/3.9.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "openshift_image_tag: v3.9.0-alpha.4\nopenshift_service_catalog_image_version: \"{{ openshift_image_tag }}\"\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "a97143443b438d1bcb4fd69fc7f41060e9a429d3", "filename": "archive/playbooks/cicd-provision.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n- hosts: localhost\n  pre_tasks:\n  - include: roles/common/pre_tasks/pre_tasks.yml\n  roles:\n    - role: common\n    - role: cicd-common\n    # Provision CICD Environment\n    - role: openstack-create\n      type: \"cicd\"\n      image_name: \"{{ cicd_openstack_image_name }}\"\n      security_groups: \"{{ cicd_openstack_security_groups }}\"\n      key_name: \"{{ openstack_key_name }}\"\n      flavor_name: \"{{ cicd_openstack_flavor_name }}\"\n      register_host_group: \"cicd\"\n      node_count: \"{{ cicd_instance_count }}\"\n      disk_volume: \"{{ cicd_storage_disk_volume }}\"\n      volume_size: \"{{ cicd_openstack_storage_size }}\"\n\n- hosts: cicd\n  remote_user: \"cloud-user\"\n  vars:\n    ansible_ssh_user: cloud-user\n  tasks:\n  - name: \"Enable direct root access\"\n    shell: \"cat ~/.ssh/authorized_keys | sudo tee /root/.ssh/authorized_keys >/dev/null\"\n\n- hosts: cicd\n  roles:\n    - role: subscription-manager\n\n- hosts: cicd\n  roles:\n  - cicd-common\n  - cicd\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "0bba2d3dfdf41e39d7b7e5ab5e7b7dfef2b434cb", "filename": "roles/config-bonding/tests/inventory/group_vars/infra_hosts.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nbonds: \n- device: bond0.mgmt\n  bonding_opts: 'mode=4 miimon=100'\n  slaves: \n  - device: eth0\n  - device: eth1\n  ipaddr: '{{ mgmt_net_ip }}'\n  netmask: '{{ mgmt_net_netmask }}'\n  gateway: '{{ mgmt_net_gateway }}'\n  dns1: '{{ mgmt_net_dns1 }}'\n  dns2: '{{ mgmt_net_dns2 }}'\n- device: bond1.vms\n  bonding_opts: 'mode=4 miimon=100'\n  slaves: \n  - device: eth2\n  - device: eth3\n\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "02d60c8015caeeef434b9dab81c9033ea0b730ed", "filename": "reference-architecture/aws-ansible/playbooks/roles/cloudformation-infra/files/add-crs-storage-iops.json", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "{\n  \"AWSTemplateFormatVersion\": \"2010-09-09\",\n  \"Parameters\": {\n    \"KeyName\": {\n      \"Type\": \"AWS::EC2::KeyPair::KeyName\"\n    },\n    \"Vpc\": {\n      \"Type\": \"String\"\n    },\n    \"Route53HostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"PublicHostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"AmiId\": {\n      \"Type\": \"AWS::EC2::Image::Id\"\n    },\n    \"InstanceType\": {\n      \"Type\": \"String\",\n      \"Default\": \"m4.2xlarge\"\n    },\n    \"NodeRootVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"30\"\n    },\n    \"NodeDockerVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"NodeDockerVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"GlusterVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"StorageIops\": {\n      \"Type\": \"Number\"\n    },\n    \"GlusterVolSize\": {\n      \"Type\": \"Number\",\n      \"Default\": \"500\"\n    },\n    \"NodeUserData\": {\n      \"Type\": \"String\"\n    },\n    \"NodeEmptyVolSize\": {\n      \"Type\": \"String\",\n      \"Default\": \"25\"\n    },\n    \"NodeEmptyVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"NodeRootVolType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gp2\"\n    },\n    \"PublicHostedZone\": {\n      \"Type\": \"String\"\n    },\n    \"NodeType\": {\n      \"Type\": \"String\",\n      \"Default\": \"gluster\"\n    },\n    \"GlusterNodeDns1\": {\n      \"Type\": \"String\"\n    },\n    \"GlusterNodeDns2\": {\n      \"Type\": \"String\"\n    },\n    \"GlusterNodeDns3\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet1\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet2\": {\n      \"Type\": \"String\"\n    },\n    \"PrivateSubnet3\": {\n      \"Type\": \"String\"\n    },\n    \"BastionSG\": {\n      \"Type\": \"String\"\n    },\n    \"NodeSG\": {\n      \"Type\": \"String\"\n    }\n  },\n  \"Resources\": {\n    \"Route53Records\": {\n      \"Type\": \"AWS::Route53::RecordSetGroup\",\n      \"DependsOn\": [\n        \"GlusterNode1\",\n        \"GlusterNode2\",\n        \"GlusterNode3\"\n      ],\n      \"Properties\": {\n        \"HostedZoneName\": { \"Ref\": \"Route53HostedZone\" },\n        \"RecordSets\": [\n          {\n            \"Name\":  {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns1\"},{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t            \"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"GlusterNode1\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\":  {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns2\"},{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t            \"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"GlusterNode2\", \"PrivateIp\"] }]\n          },\n          {\n            \"Name\":  {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns3\"},{\"Ref\": \"Route53HostedZone\"}]]},\n            \"Type\": \"A\",\n\t            \"TTL\": \"300\",\n\t\t    \"ResourceRecords\": [{ \"Fn::GetAtt\" : [\"GlusterNode3\", \"PrivateIp\"] }]\n          }\n        ]\n      }\n    },\n    \"GlusterSG\": {\n      \"Type\": \"AWS::EC2::SecurityGroup\",\n      \"Properties\": {\n        \"GroupDescription\": \"CRS Gluster SG\",\n        \"VpcId\": { \"Ref\": \"Vpc\" },\n        \"Tags\": [ {\"Key\": \"Name\", \"Value\": \"gluster_crs_sg\"} ]\n      }\n    },\n    \"GlusterDaemon\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24007\",\n        \"ToPort\": \"24007\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] }\n      }\n    },\n    \"ClientDaemon\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24007\",\n        \"ToPort\": \"24007\",\n        \"SourceSecurityGroupId\": { \"Ref\": \"NodeSG\" }\n      }\n    },\n    \"GlusterManagement\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24008\",\n        \"ToPort\": \"24008\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterConnect\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"24008\",\n        \"ToPort\": \"24008\",\n        \"SourceSecurityGroupId\": { \"Ref\": \"NodeSG\" }\n      }\n    },\n    \"BastionSsh\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"22\",\n        \"ToPort\": \"22\",\n        \"SourceSecurityGroupId\": {\"Ref\": \"BastionSG\" }\n      }\n    },\n    \"GlusterCommonSsh\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"22\",\n        \"ToPort\": \"22\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterSsh\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"2222\",\n        \"ToPort\": \"2222\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterWeb\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"8080\",\n        \"ToPort\": \"8080\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] }\n      }\n    },\n    \"GlusterWebFromNode\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"8080\",\n        \"ToPort\": \"8080\",\n        \"SourceSecurityGroupId\": { \"Ref\": \"NodeSG\" }\n      }\n    },\n    \"GlusterNfs\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"49152\",\n        \"ToPort\": \"49664\",\n        \"SourceSecurityGroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] }\n      }\n    },\n    \"ClientGlusterNfs\": {\n      \"Type\": \"AWS::EC2::SecurityGroupIngress\",\n      \"Properties\":{\n        \"GroupId\": { \"Fn::GetAtt\": [ \"GlusterSG\", \"GroupId\" ] },\n        \"IpProtocol\": \"tcp\",\n        \"FromPort\": \"49152\",\n        \"ToPort\": \"49664\",\n        \"SourceSecurityGroupId\": { \"Ref\": \"NodeSG\" }\n      }\n    },\n    \"GlusterNode1\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InstanceType\"},\n\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"GlusterSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet1\"},\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns1\"},{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"StorageType\",\n              \"Value\": \"crs\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"GlusterVolSize\"},\n              \"VolumeType\": {\"Ref\": \"GlusterVolType\"},\n              \"Iops\": {\"Ref\": \"StorageIops\"}\n            }\n          }\n         ]\n     }\n   },\n    \"GlusterNode2\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InstanceType\"},\n\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"GlusterSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet2\"},\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns2\"},{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"StorageType\",\n              \"Value\": \"crs\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"GlusterVolSize\"},\n              \"VolumeType\": {\"Ref\": \"GlusterVolType\"},\n              \"Iops\": {\"Ref\": \"StorageIops\"}\n            }\n          }\n         ]\n     }\n   },\n    \"GlusterNode3\" : {\n       \"Type\" : \"AWS::EC2::Instance\",\n       \"Properties\" : {\n          \"ImageId\" : {\"Ref\": \"AmiId\"},\n          \"UserData\": {\"Ref\": \"NodeUserData\"},\n          \"KeyName\" : {\"Ref\": \"KeyName\"},\n\t  \"InstanceType\": {\"Ref\": \"InstanceType\"},\n\t  \"SecurityGroupIds\": [{ \"Fn::GetAtt\" : [\"GlusterSG\", \"GroupId\"] }],\n          \"SubnetId\" : {\"Ref\": \"PrivateSubnet3\"},\n          \"Tags\": [\n            { \"Key\": \"Name\",\n              \"Value\": {\"Fn::Join\": [\".\", [{\"Ref\": \"GlusterNodeDns3\"},{\"Ref\": \"PublicHostedZone\"}]]}\n            },\n            { \"Key\": \"StorageType\",\n              \"Value\": \"crs\"\n            }\n          ],\n          \"BlockDeviceMappings\" : [\n          {\n            \"DeviceName\": \"/dev/sda1\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"NodeRootVolSize\"},\n              \"VolumeType\": {\"Ref\": \"NodeRootVolType\"}\n            }\n          },\n          {\n            \"DeviceName\": \"/dev/xvdb\",\n            \"Ebs\": {\n              \"DeleteOnTermination\": \"true\",\n              \"VolumeSize\": {\"Ref\": \"GlusterVolSize\"},\n              \"VolumeType\": {\"Ref\": \"GlusterVolType\"},\n              \"Iops\": {\"Ref\": \"StorageIops\"}\n            }\n          }\n         ]\n     }\n   }\n }\n}\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "6ae8dd6bcea3a10ecb3b9f055b9156b318aefd45", "filename": "roles/openvpn/defaults/main.yml", "repository": "iiab/iiab", "decoded_content": "vpn_presence: xscenet.net\nopenvpn_server_virtual_ip: 10.8.0.1\nopenvpn_server_port: 1194\nopenvpn_install: True\nopenvpn_enable: False\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "2e671be353d5dbbaed787f46bd7e41ae5936dd21", "filename": "ops/files/splunk/linux/SPLUNK_HOME/etc/apps/learned/metadata/local.meta", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "[props/first_install-too_small]\nowner = nobody\nversion = 7.0.0\nmodtime = 1521819552.611380000\n\n[props/splunkd_stdout-too_small]\nowner = nobody\nversion = 7.0.0\nmodtime = 1521819552.629670000\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "d2f8bb5ed5679096ca73bbac786107ebbb483017", "filename": "playbooks/auth-mgmt.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n- hosts: all\n  become: true\n  vars:\n    - public_keys: \"{{ lookup('file', '~/.ssh/id_rsa.pub') }}\"\n  vars_files:\n    - \"{{ rock_config }}\"\n  tasks:\n    - name: Generate SSH keys\n      become: false\n      command: \"ssh-keygen -t rsa -b 4096 -N '' -f ~/.ssh/id_rsa\"\n      args:\n        creates: ~/.ssh/id_rsa.pub\n      delegate_to: localhost\n      run_once: true\n\n    - name: Set authorized keys\n      authorized_key:\n        user: \"{{ ansible_env.SUDO_USER }}\"\n        state: present\n        key: \"{{ public_keys }}\"\n\n    - name: Enable sudo w/o password\n      lineinfile:\n        path: /etc/sudoers\n        state: present\n        regexp: '^{{ ansible_env.SUDO_USER }}\\s'\n        line: '{{ ansible_env.SUDO_USER }} ALL=(ALL) NOPASSWD: ALL'\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "e19026f9e1bd87d394c33100acab96f69cca71f0", "filename": "roles/ipaclient/meta/main.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "dependencies: []\n\ngalaxy_info:\n  author: Florence Blanc-Renaud, Thomas Woerner\n  description: A role to join a machine to an IPA domain\n  company: Red Hat, Inc\n  license: GPLv3\n  min_ansible_version: 2.5.0\n  platforms:\n  - name: Fedora\n    versions:\n    - all\n  - name: EL\n    versions:\n    - 7\n    # - 8\n  galaxy_tags:\n    - identity\n    - ipa\n    - freeipa\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "9c8b201d7b26bcfddee25f458f99b96090477688", "filename": "dev/playbooks/install_dtr_nodes.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n### \n---\n- hosts: dtr\n  serial: 1\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n\n  vars:\n    dtr_lb: \"{{ groups['dtr_lb'][0] }}.{{ domain_name }}\"\n    ucp_main: \"{{ groups['ucp_main'][0] }}.{{ domain_name }}\"\n    dtr_main: \"{{ groups['dtr_main'][0] }}\"\n    nfs_server: \"{{ hostvars[groups['nfs'][0]].ip_addr | ipaddr('address') }}\"\n    http_proxy_switch:  \"{% if  env.http_proxy is defined %} --http-proxy {{ env.http_proxy }} {% endif %}\"\n    https_proxy_switch:  \"{% if  env.https_proxy is defined %} --https-proxy {{ env.https_proxy }} {% endif %}\"\n    no_proxy_switch:  \"{% if  env.no_proxy is defined %} --no-proxy '{{ env.no_proxy }}' {% endif %}\"\n\n  tasks:\n    - name: Open required ports for DTR\n      command: firewall-cmd --permanent --zone=public --add-port=80/tcp --add-port=443/tcp --add-port=2377/tcp --add-port=4789/tcp --add-port=4789/udp --add-port=7946/tcp --add-port=7946/udp --add-port=12376/tcp\n\n    - name: Reload firewalld configuration\n      command: firewall-cmd --reload\n\n    - name: Get worker token\n      include_vars:\n        file: /tmp/worker_token\n        name: add_worker\n\n    - name: Check if node already belongs to the swarm\n      shell: docker info | grep \"Swarm{{':'}} inactive\" | wc -l\n      register: swarm_inactive\n\n    - name: Add DTR nodes to the swarm\n      command: \"{{ add_worker.token }}\"\n      when: swarm_inactive.stdout == \"1\"\n\n    - name: Install first DTR node\n      command: docker run --rm docker/dtr:{{ dtr_version }} install --nfs-storage-url nfs://{{ nfs_server }}{{ images_folder }} --ucp-node {{ inventory_hostname }}.{{ domain_name }} --ucp-insecure-tls --dtr-external-url https://{{ dtr_lb }} --ucp-url https://{{ ucp_main }} --ucp-username {{ ucp_username }} --ucp-password {{ ucp_password }}  {{ http_proxy_switch }} {{ https_proxy_switch }} {{ no_proxy_switch }}\n      when: inventory_hostname in groups.dtr_main and swarm_inactive.stdout == \"1\"\n      register: dtrlog\n      until: dtrlog.rc == 0\n      retries: 20\n      delay: 20\n\n    - name: Get replica ID\n      shell: docker ps --format \"{{ '{{' }}.Names{{ '}}' }}\" | grep dtr | awk -F'-' '{ print $NF }' | uniq\n      register: replica_id\n      when: inventory_hostname in groups.dtr_main\n\n    - name: Add DTR nodes\n      command: docker run --rm docker/dtr:{{ dtr_version }} join --ucp-node {{ inventory_hostname }}.{{ domain_name }} --ucp-url https://{{ ucp_main }} --ucp-username {{ ucp_username }} --ucp-password {{ ucp_password }} --ucp-insecure-tls --existing-replica-id {{ hostvars[dtr_main]['replica_id']['stdout'] }} \n      register: task_result\n      until: task_result.rc == 0\n      retries: 20\n      delay: 60\n      when: inventory_hostname not in groups.dtr_main and swarm_inactive.stdout == \"1\"\n\n# This task is not needed any more from DTR version 2.2.0 since it's enabled by default\n    - name: Enable image scanning\n      uri:\n        url: \"https://{{ inventory_hostname }}.{{ domain_name }}/api/v0/meta/settings\"\n        method: POST\n        user: \"{{ ucp_username }}\"\n        password: \"{{ ucp_password }}\"\n        body: {\"scanningEnabled\":true}\n        status_code: 202\n        body_format: json\n        force_basic_auth: yes\n        validate_certs: no\n      when: inventory_hostname in groups.dtr_main\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e419e037a695a856042cbe2b70f7fc0fdca85d9a", "filename": "roles/virt-install/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: \"prereq.yml\"\n- import_tasks: \"create_vm.yml\"\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "ce89572f81ad8fe540ea373a5bf239b7605a1208", "filename": "roles/consul/handlers/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# handlers file for consul\n- name: Restart consul\n  shell: restart consul\n  sudo: yes\n\n- name: Start consul\n  shell: start consul\n  sudo: yes\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "78e52d1a06d8e57039017812bae74011a5738ef9", "filename": "roles/storage-demo-nodeconfig/tasks/deprovision.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- name: \"Remove rule: Allow ceph OSD traffic\"\n  iptables:\n    state: absent\n    chain: INPUT\n    protocol: tcp\n    destination_port: 6789\n    jump: ACCEPT\n\n- name: \"Remove rule: Allow ceph MDS traffic\"\n  iptables:\n    state: absent\n    chain: INPUT\n    protocol: tcp\n    destination_port: 6800:7300\n    jump: ACCEPT\n\n- name: Save iptables configuration\n  command: iptables-save\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "2eaefa8b78dfc3fd0f1f0e2b5fd6d4dbabaae86b", "filename": "reference-architecture/aws-ansible/playbooks/roles/cloudformation-infra/files/user_data_node.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "#cloud-config\ncloud_config_modules:\n- disk_setup\n- mounts\n\nfs_setup:\n- label: emptydir\n  filesystem: xfs\n  device: /dev/xvdc\n  partition: auto\n\nruncmd:\n- mkdir -p /var/lib/origin/openshift.local.volumes\n\nmounts:\n- [ /dev/xvdc, /var/lib/origin/openshift.local.volumes, xfs, \"defaults,gquota\" ]\n\nwrite_files:\n- content: |\n    DEVS='/dev/xvdb'\n    VG=docker_vol\n    DATA_SIZE=95%VG\n    STORAGE_DRIVER=overlay2\n    CONTAINER_ROOT_LV_NAME=dockerlv\n    CONTAINER_ROOT_LV_MOUNT_PATH=/var/lib/docker\n    CONTAINER_ROOT_LV_SIZE=100%FREE\n    ROOT_SIZE=45G\n    GROWPART=true\n  path: /etc/sysconfig/docker-storage-setup\n  owner: root:root\n\nusers:\n- default\n\nsystem_info:\n  default_user:\n    name: ec2-user\n\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "29f9b3265081c0d31188495240933d1d0a1f55eb", "filename": "roles/calibre/tasks/py-installer.yml", "repository": "iiab/iiab", "decoded_content": "# roles/calibre/tasks/main.yml requires calibre_via_python (to be True) before calling this script.\n\n# Seems to work with just about any Linux (Fedora 18 on OLPC XO Laptops??) and deals with dependencies.\n# BUT IS ARCH DEPENDENT: requires x86_64 or i686 as of early 2018.\n\n- name: Download latest linux-installer.py from GitHub to calibre-installer.py\n  get_url:\n    url: \"{{ calibre_src_url }}\"\n    dest: \"{{ downloads_dir }}/calibre-installer.py\"\n    mode: 0755\n    force: yes\n    backup: yes\n    timeout: \"{{ download_timeout }}\"\n  register: calibre_download_output\n  when: internet_available\n\n# ALWAYS DEFINED, DESPITE get_url DOCUMENTATION CLAIM...\n# - debug:\n#     msg: \"{{ calibre_download_output.src }}\"\n#\n# DEFINED ONLY WHEN /opt/iiab/downloads/calibre-installer.py CHANGES\n# - debug:\n#    msg: \"{{ calibre_download_output.backup_file }}\"\n\n# OOPS BAD IDEA: changes in https://github.com/kovidgoyal/calibre/commits/master/setup/linux-installer.py are not sync'd with Calibre releases!\n# - name: FORCE AN UPGRADE IF calibre-installer.py HAS CHANGED, IF SO ORIGINAL IS SAVED TO {{ calibre_download_output.backup_file }}\n#   file:\n#     path: /usr/bin/calibre-uninstall\n#     state: absent\n#   when: calibre_download_output.backup_file is defined\n\n- name: Check if calibre-installer.py exists in /opt/iiab/downloads\n  stat:\n    path: \"{{ downloads_dir }}/calibre-installer.py\"\n  register: calib_inst\n\n# MOVED UP TO roles/calibre/tasks/main.yml (now checks for /usr/bin/calibre)\n#- name: Check if calibre-uninstall exists in /usr/bin\n#  stat:\n#    path: \"/usr/bin/calibre-uninstall\"\n#  register: calib_uninst\n\n- name: FAIL (force Ansible to exit) IF /opt/iiab/downloads/calibre-installer.py OR Internet ARE MISSING\n  # meta: end_play\n  fail:\n    msg: \"{{ downloads_dir }}/calibre-installer.py and an Internet connection are REQUIRED in order to install Calibre!\"\n  when: (not calib_inst.stat.exists) or (not internet_available)\n  #when: (not calib_inst.stat.exists) or (not internet_available and not calib_uninst.stat.exists)\n\n# INSTALL THE LATEST CALIBRE (calibre, calibredb, calibre-server etc)\n\n- name: Run calibre-installer.py to install Calibre programs into /usr/bin\n  shell: \"{{ downloads_dir }}/calibre-installer.py >> /dev/null\"\n  #args:\n  #  creates: /usr/bin/calibre-uninstall\n  when: internet_available\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "4b937b4394fc0a506c28c519f81eb3c3679a506d", "filename": "playbooks/roles/sensor-common/files/RPM-GPG-KEY-RockNSM-2.1-Testing", "repository": "rocknsm/rock", "decoded_content": "-----BEGIN PGP PUBLIC KEY BLOCK-----\n\nmQENBFlvv3sBCACmWFXdLI6OvV/vbfuq5r9rXoVQl3XP5e/AZFaqanheRJGixT5j\nF6xbUvOwoej0RhJi1jSx7PXqtjAe30DGwlqK1pd07PupL2m3JizTPrNcEa/uODBg\nAFpJDudxT5h0d0krZPKN6fsvIyI0PV6wipSzv7WGEr/BVeWfUIwqmY8b0fjc4wGg\nCLoq9Jsbu1rVrUKhq4Wr9QIzugbazwWNvgBbbmQXWAE1LmmnZjc1CcBU/nLHPG9G\nG3PD8FQaufnxNi3xqQd5C7NGI2eID05aV7W2EYh/NMs+X4JccEuvexv08uHwDUYh\nbo7k6co3usaQJMuLVz4HtUq/IJcEDM6r++xbABEBAAG0SEByb2NrbnNtX3JvY2tu\nc20tMi4xIChOb25lKSA8QHJvY2tuc20jcm9ja25zbS0yLjFAY29wci5mZWRvcmFo\nb3N0ZWQub3JnPokBPQQTAQgAJwUCWW+/ewIbLwUJCWYBgAULCQgHAgYVCAkKCwIE\nFgIDAQIeAQIXgAAKCRDeDJmNdJs4dRmtB/95rrE9qCy1IHhspyJI0t0VRW+/Bw1v\n9kaOJTOfINNWAS1JJhyGTKloA7GEaaEpNFuOXqoyGpAGQCl5pfYmsjYKF1OR7t2H\nzNA5YAeGADyJxDvUU9NdBwZLDXK7/vNQxcAQUQ5CiYm2zXj5rDnW2Rv4b45a59ax\n4WAluDkYOKvaIl8Q4ouoGqDT5bB50az5Xd9nGsn/sXrkU67Vt33Py66V2UZRXN1d\nHTq6EAYBoTrMa2rRGwgWFdaJ/TlI5qtTzj8fj6Nu/MH27q4nFKLx7RdgVjO/aQfa\nxX3YBtLsqlIVEhNCMt7ujSJnRD7EAqCs9tspRo0rpuAcC2M5onk7ERrF\n=z0ux\n-----END PGP PUBLIC KEY BLOCK-----\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "c53e562052e350316ca987ab8d38c133ece1871c", "filename": "roles/common/tasks/configure-pipelining.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# Remove requiretty to make ssh pipelining work\n- name: Remove require tty\n  lineinfile:\n    regexp: '^\\w+\\s+requiretty'\n    dest: /etc/sudoers\n    state: absent\n"}, {"commit_sha": "bf6e08dcb2440421477b6536ff6a8d11adc2be17", "sha": "ebc231fccf37cc8ca48720e74be75a00f803bb22", "filename": "roles/weave/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# tasks file for weave\n- name: include all interfaces.d\n  sudo: yes\n  lineinfile:\n    dest: /etc/network/interfaces\n    state: present\n    line: 'source /etc/network/interfaces.d/*.cfg'\n  tags:\n    - weave\n\n# Start docker as it is a requirement for weave create-bridge.\n- name: Start up docker\n  service:\n    name: docker\n    state: started\n  tags:\n    - weave\n\n- name: configure weave interface\n  sudo: yes\n  template:\n    src: interfaces.j2\n    dest: /etc/network/interfaces.d/weave.cfg\n    owner: root\n    group: root\n    mode: 0644\n  tags:\n    - weave\n\n# Create weave bridge.\n- name: bring up weave bridge\n  command: ifup weave\n  sudo: yes\n\n- name: upload weave template service\n  template:\n    src: weave.conf.j2\n    dest: \"/etc/init/weave.conf\"\n    mode: 0755\n  sudo: yes\n  tags:\n    - weave\n\n# Restart docker with weave bridge available and triggers weave service.\n- name: configure weave bridge for docker\n  sudo: yes\n  lineinfile:\n    dest: /etc/default/docker\n    state: present\n    regexp: ^DOCKER_OPTS=.*--bridge=weave.*\n    line: 'DOCKER_OPTS=\\\"$DOCKER_OPTS {{ weave_docker_opts }}\\\"'\n  notify:\n    - restart docker\n  tags:\n    - weave\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "eb675cb2b072da0dea717b1cd66a60f7e468b3ba", "filename": "playbooks/roles/docket/vars/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# vars file for rocknsm.docket"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "9eeddb4f62692d993a16119e846eb15308670e16", "filename": "roles/network/tasks/debian.yml", "repository": "iiab/iiab", "decoded_content": "# debian.yml\n# Start out making simplifying assumptions\n#   1. we are dealing with a rpi3\n#   2. Gui inputs define the config -- auto config is more difficult\n#      a. gui_desired_network_role\n#      b. hostapd_enabled\n#      c. gui_static_wan_ip\n#   3. In appliance mode: wan (and wlan0) is either static or dhcp under br0, and hostapd off\n#   4. In lan_controller: wan is off, eth0 and wlan0 under br0\n#   5. In gateway: eth0 is wan, and wlan0 is under br0 (only one adapter under br0)\n#   6. As a slight concess to auto config, if eth1 exists, make it wan, and force gateway\n\n#- name: In upgrade from earlier IIAB 6.2, delete the resolvconf\n#  package: name=resolvconf\n#           state=absent\n#           enabled=False\n#  ignore_errors: True\n\n#- name: Get the dhcp client daemon used in recent raspbian\n#  package: name=dhcpcd5\n#           state=present\n\n- name: For upgrades from earlier IIAB 6.2, remove br0 file\n  file:\n    path: /etc/network/interfaces.d/br0\n    state: absent\n  when: iiab_lan_iface != \"br0\" and wan_ip == \"dhcp\"\n\n- name: Supply resolvconf.conf\n  template:\n    dest: /etc/resolvconf.conf\n    src: network/resolvconf.j2\n\n- name: Supply dhcpcd.conf\n  template:\n    dest: /etc/dhcpcd.conf\n    src: network/dhcpcd.conf.j2\n  when: dhcpcd_result == \"enabled\"\n\n- name: Copy the bridge script\n  template:\n    dest: /etc/network/interfaces.d/iiab\n    src: network/systemd.j2\n  when: not is_rpi and (iiab_lan_iface == \"br0\" or wan_ip != \"dhcp\" or gui_static_wan_ip == \"undefined\")\n\n- name: Copy the bridge script for RPi\n  template:\n    dest: /etc/network/interfaces.d/iiab\n    src: network/rpi.j2\n  when: is_rpi and iiab_lan_iface == \"br0\"\n\n- name: Workaround auto issue (debian-9)\n  template:\n    dest: /etc/network/interfaces.d/patch_auto\n    src: network/debian-auto.j2\n  when: iiab_wan_iface != \"none\" and is_debian_9\n\n- name: Clearing out /etc/network/interfaces for static addresses (debian-9)\n  lineinfile:\n    state: absent\n    path: /etc/network/interfaces\n    regexp: \"{{ iiab_wan_iface }}\"\n  when: wan_ip != \"dhcp\" and iiab_wan_iface != \"none\" and is_debian_9\n\n- name: BIND may be affected\n  service:\n    name: \"{{ dns_service }}\"\n    state: stopped\n  when: named_install and dnsmasq_enabled\n\n# dhcpd_server release the interface\n- name: dhcpd_server may be affected - stopping dhcpd\n  service:\n    name: dhcpd\n    state: stopped\n  when: dhcpd_install\n\n- name: dhcpd_server may be affected - stopping dnsmasq\n  service:\n    name: dnsmasq\n    state: stopped\n  when: dnsmasq_install\n\n- name: Reload systemd\n  systemd:\n    daemon_reload: yes\n\n# now pick up denyinterfaces\n- name: Restart dhcpcd\n  service:\n    name: dhcpcd\n    state: restarted\n  when: dhcpcd_result == \"enabled\"\n\n- name: Restart the networking service\n  service:\n    name: networking\n    state: restarted\n  when: not nobridge is defined and not no_net_restart\n"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "5611b3197853b8f5a72bf6beb761066feba6f15d", "filename": "reference-architecture/gcp/ansible/playbooks/roles/rhel-image/tasks/main.yaml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: check if rhel image is already present in gce\n  command: gcloud --project {{ gcloud_project }} compute images describe {{ rhel_image_gce }}\n  register: rhel_image_gce_exists\n  changed_when: false\n  ignore_errors: true\n\n- block:\n  - name: convert qcow2 image to raw image\n    command: qemu-img convert -p -S 4096 -f qcow2 -O raw '{{ rhel_image_path }}' '{{ rhel_image_raw }}' creates='{{ rhel_image_raw }}'\n\n  - name: archive raw image\n    command: /usr/bin/tar -Szcf {{ rhel_image_archive }} -C '{{ rhel_image_dir }}' {{ rhel_image_raw | basename }} creates='{{ rhel_image_archive }}'\n\n  - name: check if a bucket in gcs exists\n    command: gsutil ls -p {{ gcloud_project }} {{ rhel_image_bucket }}\n    register: bucket_exists\n    ignore_errors: true\n\n  - name: create a bucket in gcs\n    command: gsutil mb -p {{ gcloud_project }} -l {{ gcloud_region }} {{ rhel_image_bucket }}\n    when: bucket_exists | failed\n\n  - name: check if the bucket contains image\n    command: gsutil ls -p {{ gcloud_project }} {{ rhel_image_in_bucket }}\n    register: image_in_bucket_exists\n    ignore_errors: true\n\n  - name: upload image to the bucket\n    command: gsutil -o GSUtil:parallel_composite_upload_threshold=150M cp '{{ rhel_image_archive }}' {{ rhel_image_bucket }}\n    when: image_in_bucket_exists | failed\n\n  - name: create gce image from the uploaded archive\n    command: gcloud --project {{ gcloud_project }} compute images create {{ rhel_image_gce }} --source-uri {{ rhel_image_in_bucket }} --family {{ rhel_image_gce_family }}\n\n  - name: delete bucket with uploaded archive\n    command: gsutil -m rm -r {{ rhel_image_bucket }}\n\n  - name: delete temporary archive and raw image\n    file:\n      path: '{{ item }}'\n      state: absent\n    with_items:\n    - '{{ rhel_image_archive }}'\n    - '{{ rhel_image_raw }}'\n  when: rhel_image_gce_exists | failed\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "5aa35b925b6408217bc6c80845226c4f0f1ef644", "filename": "roles/config-nagios-target/tasks/nrpe.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: Include Docker related configuration (if enabled)\n  when: '\"docker\" in nagios_services'\n  include_tasks: nrpe_docker.yml\n\n- name: Include DNS related configuration (if enabled)\n  when: '\"dns\" in nagios_services'\n  include_tasks: nrpe_dns.yml\n\n- name: Include NFS related configuration (if enabled)\n  when: '\"nfs\" in nagios_services'\n  include_tasks: nrpe_nfs.yml\n\n- name: Include Memory related configuration\n  import_tasks: nrpe_mem.yml\n\n- name: Include Disk related configuration\n  import_tasks: nrpe_disk.yml\n\n- name: Include OpenShift Master related configuration (if enabled)\n  when: '\"openshift-master\" in nagios_services'\n  include_tasks: nrpe_openshift_master.yml\n\n- name: Include OpenShift Node related configuration (if enabled)\n  when: '\"openshift-node\" in nagios_services'\n  include_tasks: nrpe_openshift_node.yml\n\n- name: Adjust the number of total processes allowed\n  lineinfile:\n    dest: /etc/nagios/nrpe.cfg\n    regexp: '^command.check_total_procs.=.+check_procs .*'\n    line: 'command[check_total_procs]=/usr/lib64/nagios/plugins/check_procs -w 350 -c 400'\n    state: present\n\n- name: Build list of nagios servers for use with access list\n  set_fact:\n    access_servers: \"{{ item }},{{ access_servers | default('') }}\"\n  with_items: \"{{groups['nagios-servers']}}\"\n\n- name: Ensure access is allowed for the nagios server(s)\n  lineinfile:\n    dest: /etc/nagios/nrpe.cfg\n    regexp: '^allowed_hosts='\n    line: \"allowed_hosts={{ access_servers }}\"\n\n- name: Set Firewall Configuration\n  set_fact:\n    firewall_port: 5666\n    firewall_protocol: \"tcp\"\n \n- name: Update the firewall configuration\n  import_tasks: firewall.yml\n\n- name: Ensure NRPE is enabled at boot\n  service:\n    name: nrpe\n    enabled: yes\n\n- name: Restart NRPE\n  service:\n    name: nrpe\n    state: restarted\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "5f533d15792019cb1644aa77555c5704b9d7a9a1", "filename": "ops/playbooks/roles/hpe.haproxy/templates/10-grafana", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "frontend grafana\n  bind *:3000\n  log global\n  mode http\n  option httplog\n\nbackend grafana\n  mode http\n  balance source\n\n"}, {"commit_sha": "4a9aaf0951e383c57077cf651b93e78eeea1b5ac", "sha": "7ae5675059589364682d8588be17afe852c1d084", "filename": "tasks/install-pre5.yml", "repository": "geerlingguy/ansible-role-solr", "decoded_content": "---\n# Install Solr.\n- name: Check if Solr is already installed.\n  stat: \"path={{ solr_install_path }}/dist/{{ solr_filename }}.war\"\n  register: solr_war_file\n\n- name: Copy Solr into place.\n  command: \"cp -r {{ solr_workspace }}/{{ solr_filename }} {{ solr_install_path }}\"\n  when: not solr_war_file.stat.exists\n\n- name: Ensure Solr install files are owned by the solr_user.\n  file:\n    path: \"{{ solr_install_path }}\"\n    owner: \"{{ solr_user }}\"\n    group: \"{{ solr_user }}\"\n    recurse: true\n  when: not solr_war_file.stat.exists\n\n# Set up solr_home.\n- name: Check if solr_home is already set up.\n  stat: \"path={{ solr_home }}/solr.xml\"\n  register: solr_example\n\n- name: Ensure solr_home directory exists.\n  file:\n    path: \"{{ solr_home }}\"\n    state: directory\n    owner: \"{{ solr_user }}\"\n    group: \"{{ solr_user }}\"\n    mode: 0755\n  when: not solr_example.stat.exists\n\n- name: Copy Solr example into solr_home.\n  shell: \"cp -r {{ solr_install_path }}/example/solr/* {{ solr_home }}\"\n  when: not solr_example.stat.exists\n\n- name: Fix the example solrconfig.xml file.\n  replace:\n    dest: \"{{ solr_home }}/collection1/conf/solrconfig.xml\"\n    regexp: ^.+solr\\.install\\.dir.+$\n    replace: \"\"\n  when: \"not solr_example.stat.exists and solr_version.split('.')[0] == '4'\"\n\n- name: Ensure Solr home files are owned by the solr_user.\n  file:\n    path: \"{{ solr_home }}\"\n    owner: \"{{ solr_user }}\"\n    group: \"{{ solr_user }}\"\n    recurse: true\n  when: not solr_example.stat.exists\n\n# Set up Solr init script.\n- name: Ensure log file is created and has proper permissions.\n  file:\n    path: \"/var/log/solr.log\"\n    state: touch\n    owner: \"{{ solr_user }}\"\n    group: root\n    mode: 0664\n  changed_when: false\n  when: ansible_service_mgr != 'systemd'\n\n- name: Copy solr init script into place.\n  template:\n    src: \"solr-init-{{ ansible_os_family }}-pre5.j2\"\n    dest: \"/etc/init.d/{{ solr_service_name }}\"\n    mode: 0755\n  when: ansible_service_mgr != 'systemd'\n\n- name: Ensure daemon is installed (Debian).\n  apt: name=daemon state=present\n  when:\n    - ansible_os_family == \"Debian\"\n    - ansible_service_mgr != 'systemd'\n\n- name: Copy solr systemd unit file into place (for systemd systems).\n  template:\n    src: solr-pre5.unit.j2\n    dest: /etc/systemd/system/{{ solr_service_name }}.service\n    owner: root\n    group: root\n    mode: 0755\n  when: ansible_service_mgr == 'systemd'\n"}]