[{"commit_sha": "6d10af54bdbf8e81c3d90a70ffea87b4d2c20eb2", "sha": "82032fd8f43a39349ed4da82507458c4a4c2de88", "filename": "tasks/plugins.yml", "repository": "Oefenweb/ansible-wordpress", "decoded_content": "# tasks file for wordpress, plugins\n---\n- name: identify installation (plugin)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1 }}\"\n  register: check_installation_plugins\n  failed_when: False\n  changed_when: False\n  with_subelements:\n    - wordpress_installs\n    - plugins\n  when: item.1\n  tags: [configuration, wordpress, wordpress-plugins, wordpress-is-installed-plugin]\n\n- name: install (plugin)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.item.0.path }}' plugin install {{ item.item.1 }} --activate\"\n  with_items: check_installation_plugins.results\n  when: check_installation_plugins is defined and item.item.1 and item.rc != 0\n  tags: [configuration, wordpress, wordpress-plugins, wordpress-install-plugin]\n\n- name: check install (plugin)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.0.path }}' plugin is-installed {{ item.1 }}\"\n  changed_when: False\n  with_subelements:\n    - wordpress_installs\n    - plugins\n  when: item.1\n  tags: [configuration, wordpress, wordpress-plugins, wordpress-install-plugin-check]\n"}, {"commit_sha": "a10c5f4577e6e74feb1fadec4bcbab039b8b180a", "sha": "c2d8c7433623060ed107702d6f41acd8286ad892", "filename": "tasks/configure-docker/configure-systemd.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Combine all systemd service configuration options\n  set_fact:\n    _systemd_service_config: \"{{ docker_systemd_service_config_tweaks + docker_systemd_service_config }}\"\n\n- name: Ensure /etc/systemd/system/docker.service.d directory exists\n  become: true\n  file:\n    path: /etc/systemd/system/docker.service.d\n    state: directory\n    mode: 0755\n\n- name: Setup default Docker drop-in to enable use of environment file\n  become: true\n  template:\n    src: drop-ins/default.conf.j2\n    dest: /etc/systemd/system/docker.service.d/default.conf\n  register: _systemd_docker_dropin\n  notify: restart docker\n  vars:\n    systemd_envs_dir: \"{{ docker_envs_dir[_docker_os_dist] }}\"\n    systemd_service_conf: \"{{ _systemd_service_config }}\"\n\n- name: Combine Docker daemon environment variable configuration\n  set_fact:\n    docker_service_envs: \"{{ docker_service_envs | combine(_docker_service_opts) | combine(docker_daemon_envs) }}\"\n  vars:\n    _docker_service_opts:\n      DOCKER_OPTS: \"{{ docker_daemon_opts }}\"\n\n- name: Setup Docker environment file {{ docker_envs_dir[_docker_os_dist] }}/docker-envs\n  become: true\n  template:\n    src: docker-envs.j2\n    dest: \"{{ docker_envs_dir[_docker_os_dist] }}/docker-envs\"\n  notify: restart docker\n  vars:\n    docker_envs: \"{{ docker_service_envs }}\"\n\n- name: Force daemon reload of systemd\n  become: true\n  systemd:\n    daemon_reload: yes\n  notify: restart docker\n  when: _systemd_docker_dropin|changed\n  tags:\n    - skip_ansible_lint"}, {"commit_sha": "92dabcd706e72a0dc15ce13086fb9d59f1a8760e", "sha": "500eeb9844edd97d5d639e052f696c12de92a275", "filename": "tasks/mongodb.yml", "repository": "RocketChat/Rocket.Chat.Ansible", "decoded_content": "---\n# tasks/mongodb.yml: MongoDB configuration for RocketChat.Ansible\n  - name: Install Official MongoDB.org packages\n    block:\n\n    - name: Set official package names\n      set_fact:\n        rocket_chat_mongodb_packages:\n          - mongodb-org\n          - mongodb-org-server\n        rocket_chat_mongodb_config: /etc/mongod.conf\n\n    - name: Debian/Ubuntu MongoDB.org official pkgs tasks\n      block:\n\n      - name: Ensure the MongoDB repository key has been imported [Debian]\n        apt_key:\n          keyserver: \"{{ rocket_chat_mongodb_keyserver }}\"\n          id: \"{{ rocket_chat_mongodb_gpg_key }}\"\n        tags: repo\n\n      - name: Ensure the MongoDB repository is present [Debian]\n        apt_repository:\n          repo: \"{{ rocket_chat_mongodb_apt_repo }}\"\n          state: present\n        tags: repo\n        register: rocket_chat_mongodb_repo_state\n\n      when:\n        - (rocket_chat_mongodb_apt_repo is defined)\n        - (rocket_chat_mongodb_apt_repo)\n        - ((ansible_os_family | lower) == \"debian\")\n\n    - name: RHEL-based MongoDB.org official pkgs tasks\n      block:\n\n        - name: Ensure the MongoDB repository key has been imported [RHEL]\n          rpm_key:\n            key: \"{{ rocket_chat_mongodb_rpm_repo.pgp_key }}\"\n            state: present\n          tags: repo\n\n        - name: Ensure the MongoDB repository is present [RHEL]\n          yum_repository:\n            name: \"{{ rocket_chat_mongodb_rpm_repo.name }}\"\n            baseurl: \"{{ rocket_chat_mongodb_rpm_repo.baseurl }}\"\n            state: present\n            description: \"{{ rocket_chat_mongodb_rpm_repo.desc }}\"\n            gpgcheck: \"{{ rocket_chat_mongodb_rpm_repo.gpgcheck }}\"\n            gpgkey:  \"{{ rocket_chat_mongodb_rpm_repo.pgp_key }}\"\n          tags: repo\n          register: rocket_chat_mongodb_repo_state\n\n      when:\n      - (rocket_chat_mongodb_rpm_repo is defined)\n      - (rocket_chat_mongodb_rpm_repo)\n      - ((ansible_os_family | lower) == \"redhat\")\n\n    when:\n      - (rocket_chat_mongodb_org_pkgs is defined)\n      - (rocket_chat_mongodb_org_pkgs | bool)\n    tags: repo\n\n  - name: Ensure MongoDB Server is installed\n    package:\n      name: \"{{ rocket_chat_mongodb_packages }}\"\n      state: \"{{ (rocket_chat_mongodb_repo_state | changed) | ternary('latest','present') }}\"\n\n  - name: Deploy MongoDB service configuration\n    template:\n      src: \"{{ rocket_chat_mongodb_config_template }}\"\n      dest: \"{{ rocket_chat_mongodb_config }}\"\n    notify: Restart the MongoDB service\n\n  - meta: flush_handlers\n\n  - name: Ensure the MongoDB service is started/enabled\n    service:\n      name: \"{{ rocket_chat_mongodb_service_name }}\"\n      state: started\n      enabled: yes\n\n  - name: Wait for MongoDB to come online\n    wait_for:\n      port: \"{{ rocket_chat_mongodb_port }}\"\n      host: \"{{ rocket_chat_mongodb_server }}\"\n      state: started\n\n  - name: Ensure the MongoDB replSets have been initiated\n    shell: >-\n      mongo --quiet --eval\n      'rs.initiate({_id:\"{{ rocket_chat_mongodb_repl_setname }}\",\n      members: [{\"_id\":1, \"host\":\n      \"{{ rocket_chat_mongodb_server }}:{{ rocket_chat_mongodb_port }}\"}]})'\n    become: yes\n    become_user: \"{{ rocket_chat_mongodb_service_user }}\"\n    args:\n      executable: /bin/bash\n    register: replSet_result\n    changed_when:\n      - not (replSet_result.stdout | search(' Object'))\n      - ('ok' in (replSet_result.stdout | from_json))\n      - (((replSet_result.stdout | from_json).ok | int) == 1)\n\n  - name: Reset replSet config when /etc/hosts changes\n    shell: >-\n       mongo --quiet --eval\n       'cfg = rs.conf();\n       cfg.members[0].host = \"{{ ansible_nodename }}\";\n       rs.reconfig(cfg, {force: true})'\n    become: yes\n    become_user: \"{{ rocket_chat_mongodb_service_user }}\"\n    when:\n      - (hosts_change_result | changed)\n      - ((replSet_result is undefined) or (not (replSet_result | changed)))\n    args:\n      executable: /bin/bash\n    notify:\n      - Restart the MongoDB service\n"}, {"commit_sha": "92dabcd706e72a0dc15ce13086fb9d59f1a8760e", "sha": "5cb702cf669ac8ccd5ed98328f0674fc22ba714e", "filename": "tasks/letsencrypt.yml", "repository": "RocketChat/Rocket.Chat.Ansible", "decoded_content": "---\n\n# possibly just copy the script into files and include it w/ the role?\n- name: Clone acme-tiny to /opt [Let's Encrypt!]\n  git:\n    dest: \"{{ rocket_chat_letsencrypt_acmetiny_path }}\"\n    repo: https://github.com/diafygi/acme-tiny.git\n    force: yes\n    update: yes\n\n- name: Ensure letsencrypt well-known dir exists [Let's Encrypt!]\n  file:\n    path: \"{{ rocket_chat_letsencrypt_wellknown_path }}\"\n    state: directory\n    owner: \"{{ rocket_chat_nginx_process_user }}\"\n    setype: httpd_sys_content_t\n    recurse: yes\n\n- name: Restore SELinux contexts for well-know dir [Let's Encrypt!:SELinux]\n  command: restorecon -R \"{{ rocket_chat_letsencrypt_wellknown_path }}\"\n  when: ('status' in ansible_selinux)\n         and ((ansible_selinux.status | lower) == \"enabled\")\n\n- name: Generate acme-tiny Let's Encrypt account key [Let's Encrypt!]\n  shell: >-\n    openssl genrsa -out {{ rocket_chat_letsencrypt_account_key }} 4096\n  args:\n    creates: \"{{ rocket_chat_letsencrypt_account_key }}\"\n\n- name: Check if acme-tiny Let's Encrypt CSR exists [Let's Encrypt!]\n  stat:\n    path: \"{{ rocket_chat_letsencrypt_csr }}\"\n  register: csr_path\n\n- name: Generate acme-tiny Let's Encrypt CSR [Let's Encrypt!]\n  shell: >-\n    openssl req -new -sha256 -key {{ rocket_chat_ssl_key_path }}\n    -subj \"/CN={{ rocket_chat_letsencrypt_domain | default(rocket_chat_service_host) }}\"\n    -out {{ rocket_chat_letsencrypt_csr }}\n  register: csr_gen_result\n  when:\n    - (key_gen_result | changed)\n       or (('stat' in csr_path)\n            and (not (csr_path.stat.exists | bool)))\n\n- name: Setup script in cron.daily [Let's Encrypt!]\n  copy:\n    dest: /etc/cron.monthly/acme-tiny_renew.sh\n    mode: 0755\n    content: |\n      #!/bin/bash\n      python {{ rocket_chat_letsencrypt_acmetiny_path }}/acme_tiny.py \\\n      --account-key {{ rocket_chat_letsencrypt_account_key }}  \\\n      --csr {{ rocket_chat_letsencrypt_csr }}  \\\n      --acme-dir {{ rocket_chat_letsencrypt_wellknown_path  }} \\\n      > {{ rocket_chat_ssl_cert_path }} || exit\n      curl -s {{ rocket_chat_letsencrypt_ca_cert }} \\\n      >> {{ rocket_chat_ssl_cert_path }} &&\n      nginx -t && nginx -s reload\n\n- name: Run acme-tiny_renew.sh (first run cert creation) [Let's Encrypt!]\n  shell: /etc/cron.monthly/acme-tiny_renew.sh\n  notify: Reload the Nginx service\n  when: (csr_gen_result | changed)\n         or (rocket_chat_letsencrypt_force_renew | bool)\n"}, {"commit_sha": "8802c6d50d54583955be4354e6bfebf3f0e776c6", "sha": "2005602191f27455d2f2f1343f1d9766589d9c14", "filename": "tasks/install/main.yml", "repository": "HanXHX/ansible-mysql", "decoded_content": "---\n\n- name: INCLUDE | Use Percona repository\n  include: 'percona/apt.yml'\n  when: mariadb_use_percona_apt\n\n- name: INCLUDE | Install MariaDB from Debian repo\n  include: 'mariadb/default.yml'\n  when: mariadb_origin == 'default' and mariadb_vendor == 'mariadb'\n\n- name: INCLUDE | Install MariaDB from MariaDB repo\n  include: 'mariadb/upstream.yml'\n  when: mariadb_origin == 'upstream'\n\n- name: APT | Install few MariaDB related tools\n  apt:\n    pkg: \"{{ item }}\"\n    install_recommends: no\n  with_items: \"{{ mariadb_tools }}\"\n\n- name: APT | Install percona-xtrabackup if needed\n  apt:\n    pkg: \"{{ mariadb_xtrabackup_package }}\"\n  when: mariadb_install_xtrabackup_package\n"}, {"commit_sha": "bf6e08dcb2440421477b6536ff6a8d11adc2be17", "sha": "5f6617e8aa4ba6625c4304b534ca54fce0477f41", "filename": "roles/consul/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for consul\nconsul_is_server: no\nconsul_dc: dc1\nconsul_servers_group: consul_servers\nconsul_advertise: \"{{ inventory_hostname }}\"\nconsul_bind_addr: \"{{ ansible_default_ipv4.address }}\"\nconsul_retry_join: \"{% for host in groups[consul_servers_group] %}\\\"{{ hostvars[host].ansible_default_ipv4.address }}\\\"{% if not loop.last %}, {% endif %}{% endfor %}\"\nconsul_bootstrap_expect: \"{{ groups[consul_servers_group] | length }}\"\nconsul_client_addr: \"0.0.0.0\"\nconsul_atlas_join: false\nconsul_node_name: \"{{ ansible_hostname }}\"\n"}, {"commit_sha": "b11c4477d973b0cc87a296f6b028eaf9abab4686", "sha": "213fd17474dd1b0fd967eed89d6894fd9ba06f98", "filename": "tasks/main-Fedora.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# tasks file for ansible-role-docker-ce\n\n- name: Add Docker CE repository\n  shell: dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo\n  args:\n    creates: /etc/yum.repos.d/docker-ce.repo\n  become: true\n  register: dnf_repo\n\n- name: Update dnf cache\n  shell: dnf makecache fast\n  become: true\n  when: dnf_repo.changed\n\n- name: Install python and deps for ansible modules\n  raw: dnf install -y python2 python2-dnf libselinux-python\n  become: true\n  changed_when: false\n\n- include: main-Generic.yml"}, {"commit_sha": "99eb1124a889eee7b5a71f45015599b368d166de", "sha": "e83911cd7ce2488cceb08561351732b5af0332ef", "filename": "roles/docker/tasks/main.yml", "repository": "openshift/openshift-ansible-contrib", "decoded_content": "---\n- name: \"Setting Docker Facts\"\n  set_fact:\n    docker_storage_block_device: \"{{ docker_storage_block_device | default(default_docker_storage_block_device) }}\"\n    docker_storage_volume_group: \"{{ docker_storage_volume_group | default(default_docker_storage_volume_group) }}\"\n\n- name: \"Install Docker\"\n  yum:\n    name: docker\n    state: latest\n  notify:\n    - enable docker\n\n- name: \"Check for existing Docker Storage device\"\n  command: pvs\n  register: pvs\n\n- name: \"Set Docker Storage fact if already configured\"\n  set_fact:\n    docker_storage_setup: true\n    when: pvs.stdout | search(docker_storage_block_device ~ '.*' ~ docker_storage_volume_group)\n\n- name: \"Configure Docker Storage Setup\"\n  template:\n    src: docker-storage-setup.j2\n    dest: /etc/sysconfig/docker-storage-setup\n  when: docker_storage_setup is undefined\n  notify:\n    - restart docker\n"}, {"commit_sha": "9e7bed4aaef56159f148d24e4f7d7e8b53be632b", "sha": "9d4f5c692a03b880d39d60aded85503292f05423", "filename": "tasks/main-Storage.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Check if docker-storage-setup service already is present\n  stat:\n    path: /usr/lib/systemd/system/docker-storage-setup.service\n  become: true\n  register: docker_stat_storage\n\n- name: Ensure git is present\n  package:\n    name: git\n    state: present\n  become: true\n\n- name: Clone project Atomic container-storage-setup\n  git:\n    repo: https://github.com/projectatomic/container-storage-setup.git\n    version: 9b77bcb2cba8e272799fa21e2d484e9f6e7c34d0\n    dest: \"/tmp/container-storage-setup\"\n  become: yes\n  when: docker_stat_storage.stat.exists == false\n\n- name: Install container-storage-setup\n  make:\n    chdir: /tmp/container-storage-setup\n    target: install\n  become: yes\n  when: docker_stat_storage.stat.exists == false\n\n- name: Ensure /tmp/container-storage-setup directory is removed\n  file:\n    path: /tmp/container-storage-setup\n    state: absent\n  become: true\n\n- name: Start docker-storage-setup service\n  service:\n    name: docker-storage-setup\n    enabled: true\n    state: started\n  become: true\n  when: docker_stat_storage.stat.exists == false\n\n- name: Ensure Docker default user namespace is defined in subuid and subgid\n  lineinfile:\n    path: \"{{ item }}\"\n    regexp: '^dockremap'\n    line: 'dockremap:500000:65536'\n  become: yes\n  with_items:\n    - /etc/subuid\n    - /etc/subgid\n\n- name: Ensure /etc/systemd/system/docker.service.d directory exists\n  file: \n    path: /etc/systemd/system/docker.service.d\n    state: directory\n    mode: 0755\n  become: yes\n\n- name: Copy systemd drop-in for Docker storage configuration\n  copy:\n    src: files/etc/systemd/system/docker.service.d/docker-storage.conf\n    dest: /etc/systemd/system/docker.service.d/docker-storage.conf\n  become: yes\n"}, {"commit_sha": "3c8d04f3e0875a9baf1f1282f6665b2e7d6871a8", "sha": "f80c938e73e8646c6201eb7be662be8aebd71680", "filename": "meta/main.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\ndependencies: []\n\ngalaxy_info:\n  author: geerlingguy\n  description: Security software installation and configuration.\n  company: \"Midwestern Mac, LLC\"\n  license: \"license (BSD, MIT)\"\n  min_ansible_version: 2.0\n  platforms:\n    - name: EL\n      versions:\n      - all\n    - name: Fedora\n      versions:\n      - all\n    - name: Debian\n      versions:\n      - all\n    - name: Ubuntu\n      versions:\n      - all\n  galaxy_tags:\n    - system\n    - security\n    - fail2ban\n    - automatic-updates\n    - yum\n    - apt\n    - dnf\n    - hardening\n"}, {"commit_sha": "4a9aaf0951e383c57077cf651b93e78eeea1b5ac", "sha": "a6bbac75c07cc51f318c9d0fb1ce290e85825ae9", "filename": "meta/main.yml", "repository": "geerlingguy/ansible-role-solr", "decoded_content": "---\ndependencies: []\n\ngalaxy_info:\n  author: geerlingguy\n  description: Apache Solr for Linux.\n  company: \"Midwestern Mac, LLC\"\n  license: \"license (BSD, MIT)\"\n  min_ansible_version: 2.4\n  platforms:\n    - name: EL\n      versions:\n        - 6\n        - 7\n    - name: Debian\n      versions:\n        - all\n    - name: Ubuntu\n      versions:\n        - all\n  galaxy_tags:\n    - development\n    - solr\n    - search\n    - lucene\n    - container\n    - apache\n    - text\n"}, {"commit_sha": "4dff2dcf95ce42d98c947dbed66f45920314ae3e", "sha": "703b1d0420cb6ee7f00f59b754d6439262470281", "filename": "roles/cloud-ec2/tasks/main.yml", "repository": "trailofbits/algo", "decoded_content": "- name: Locate official Ubuntu 16.04 AMI for region\n  ec2_ami_find:\n    aws_access_key: \"{{ aws_access_key | default(lookup('env','AWS_ACCESS_KEY_ID'))}}\"\n    aws_secret_key: \"{{ aws_secret_key | default(lookup('env','AWS_SECRET_ACCESS_KEY'))}}\"\n    name: \"ubuntu/images/hvm-ssd/ubuntu-xenial-16.04-amd64-server-*\"\n    owner:  099720109477\n    sort: creationDate\n    sort_order: descending\n    sort_end: 1\n    region: \"{{ region }}\"\n  register: ami_search\n\n- set_fact:\n    ami_image: \"{{ ami_search.results[0].ami_id }}\"\n\n- include: encrypt_image.yml\n  tags: [encrypted]\n\n- name: Add ssh public key\n  ec2_key:\n    aws_access_key: \"{{ aws_access_key | default(lookup('env','AWS_ACCESS_KEY_ID'))}}\"\n    aws_secret_key: \"{{ aws_secret_key | default(lookup('env','AWS_SECRET_ACCESS_KEY'))}}\"\n    name: VPNKEY\n    region: \"{{ region }}\"\n    key_material: \"{{ item }}\"\n  with_file: \"{{ SSH_keys.public }}\"\n  register: keypair\n\n- name: Configure EC2 virtual private clouds\n  ec2_vpc:\n    aws_access_key: \"{{ aws_access_key | default(lookup('env','AWS_ACCESS_KEY_ID'))}}\"\n    aws_secret_key: \"{{ aws_secret_key | default(lookup('env','AWS_SECRET_ACCESS_KEY'))}}\"\n    state: present\n    resource_tags: { \"Environment\":\"Algo\" }\n    region: \"{{ region }}\"\n    cidr_block: \"{{ ec2_vpc_nets.cidr_block }}\"\n    internet_gateway: yes\n    subnets:\n      - cidr: \"{{ ec2_vpc_nets.subnet_cidr }}\"\n        resource_tags: { \"Environment\":\"Algo\" }\n  register: vpc\n\n- name: Set up Public Subnets Route Table\n  ec2_vpc_route_table:\n    aws_access_key: \"{{ aws_access_key | default(lookup('env','AWS_ACCESS_KEY_ID'))}}\"\n    aws_secret_key: \"{{ aws_secret_key | default(lookup('env','AWS_SECRET_ACCESS_KEY'))}}\"\n    vpc_id: \"{{ vpc.vpc_id }}\"\n    region: \"{{ region }}\"\n    state: present\n    tags:\n      Environment: Algo\n    subnets:\n      - \"{{ ec2_vpc_nets.subnet_cidr }}\"\n    routes:\n      - dest: 0.0.0.0/0\n        gateway_id: \"{{ vpc.igw_id }}\"\n  register: public_rt\n\n- name: Configure EC2 security group\n  ec2_group:\n    aws_access_key: \"{{ aws_access_key | default(lookup('env','AWS_ACCESS_KEY_ID'))}}\"\n    aws_secret_key: \"{{ aws_secret_key | default(lookup('env','AWS_SECRET_ACCESS_KEY'))}}\"\n    name: vpn-secgroup\n    description: Security group for VPN servers\n    region: \"{{ region }}\"\n    vpc_id: \"{{ vpc.vpc_id }}\"\n    rules:\n      - proto: udp\n        from_port: 4500\n        to_port: 4500\n        cidr_ip: 0.0.0.0/0\n      - proto: udp\n        from_port: 500\n        to_port: 500\n        cidr_ip: 0.0.0.0/0\n      - proto: tcp\n        from_port: 22\n        to_port: 22\n        cidr_ip: 0.0.0.0/0\n    rules_egress:\n      - proto: all\n        from_port: 0-65535\n        to_port: 0-65535\n        cidr_ip: 0.0.0.0/0\n\n- name: Launch instance\n  ec2:\n    aws_access_key: \"{{ aws_access_key | default(lookup('env','AWS_ACCESS_KEY_ID'))}}\"\n    aws_secret_key: \"{{ aws_secret_key | default(lookup('env','AWS_SECRET_ACCESS_KEY'))}}\"\n    keypair: \"VPNKEY\"\n    vpc_subnet_id: \"{{ vpc.subnets[0].id }}\"\n    group: vpn-secgroup\n    instance_type: t2.micro\n    image: \"{{ ami_image }}\"\n    wait: true\n    region: \"{{ region }}\"\n    instance_tags:\n      name: \"{{ aws_server_name }}\"\n      Environment: Algo\n    exact_count: 1\n    count_tag:\n      name: \"{{ aws_server_name }}\"\n    assign_public_ip: yes\n    instance_initiated_shutdown_behavior: terminate\n  register: ec2\n\n- name: Add new instance to host group\n  add_host:\n    hostname: \"{{ item.public_ip }}\"\n    groupname: vpn-host\n    ansible_ssh_user: ubuntu\n    ansible_python_interpreter: \"/usr/bin/python2.7\"\n    ansible_ssh_private_key_file: \"{{ SSH_keys.private }}\"\n    cloud_provider: ec2\n    ipv6_support: no\n  with_items: \"{{ ec2.tagged_instances }}\"\n\n- set_fact:\n    cloud_instance_ip: \"{{ ec2.tagged_instances[0].public_ip }}\"\n\n- name: Get EC2 instances\n  ec2_remote_facts:\n    aws_access_key: \"{{ aws_access_key | default(lookup('env','AWS_ACCESS_KEY_ID'))}}\"\n    aws_secret_key: \"{{ aws_secret_key | default(lookup('env','AWS_SECRET_ACCESS_KEY'))}}\"\n    region: \"{{ region }}\"\n    filters:\n      instance-state-name: running\n      \"tag:Environment\": Algo\n  register: algo_instances\n\n- name: Ensure the group ec2 exists in the dynamic inventory file\n  lineinfile:\n    state: present\n    dest: configs/inventory.dynamic\n    line: '[ec2]'\n\n- name: Populate the dynamic inventory\n  lineinfile:\n    state: present\n    dest: configs/inventory.dynamic\n    insertafter: '\\[ec2\\]'\n    regexp: \"^{{ item.public_ip_address }}.*\"\n    line: \"{{ item.public_ip_address }}\"\n  with_items:\n    - \"{{ algo_instances.instances }}\"\n"}, {"commit_sha": "a10c5f4577e6e74feb1fadec4bcbab039b8b180a", "sha": "5b4382bd73b00f6748569669c6c8a4eff16afbf8", "filename": "meta/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "galaxy_info:\n  author: Bjorn Oscarsson\n  company: none\n  description: \"Installs and configures Docker Community Edition (CE)\"\n  min_ansible_version: 2.4\n  license: MIT\n  platforms:\n  - name: Fedora\n    versions:\n      - 25\n      - 26\n      - 27\n      - 28\n      - 29\n\n  - name: EL\n    versions:\n      - 7\n\n  - name: Debian\n    versions:\n      - wheezy\n      - jessie\n      - stretch\n      - buster\n\n  - name: Ubuntu\n    versions:\n      - trusty\n      - artful\n      - xenial\n      - bionic\n      - cosmic\n\n  galaxy_tags:\n    - docker\n    - containers\n    - virtualization\n    - compose\n    - orchestration\n    - system\n\ndependencies: []\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "38b008750a7ed3d9528c9211e7f6861c38706063", "filename": "roles/config-lvm/tasks/lvm.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Setup and create PV & VG\"\n  lvg:\n    vg: \"{{ item.vg_name }}\"\n    pvs: \"{{ item.storage_device }}\"\n\n- name: \"Setup LV\"\n  lvol: \n    vg: \"{{ item.vg_name }}\"\n    lv: \"{{ item.lv_name }}\"\n    size: \"100%VG\"\n\n- name: \"Create file system on share\"\n  filesystem:\n    fstype: \"{{ lvm_fstype }}\"\n    dev: \"/dev/mapper/{{ item.vg_name }}-{{ item.lv_name }}\"\n\n- name: \"Ensure the mount dir exists\" \n  file:\n    path: \"{{ item.mount_path }}\"\n    state: directory\n\n- name: \"Mount LVM to directory\"\n  mount:\n    src: \"/dev/mapper/{{ item.vg_name }}-{{ item.lv_name }}\" \n    path: \"{{ item.mount_path }}\"\n    fstype: \"{{ lvm_fstype }}\" \n    state: mounted\n"}, {"commit_sha": "b2591b9333f6e7e70f6b9d99e55356b30d7e173c", "sha": "cb0a66602252f977ac7fa146c29716eb457a8572", "filename": "tasks/configure.yml", "repository": "inkatze/wildfly", "decoded_content": "---\n# task file for wildfly\n\n- name: Create wildfly etc directory\n  file:\n    path: '{{ wildfly_conf_dir }}'\n    state: directory\n    owner: '{{ wildfly_user }}'\n    group: '{{ wildfly_group }}'\n    mode: '0750'\n\n- name: Copy wildfly configuration\n  template:\n    src: wildfly.conf.j2\n    dest: '{{ wildfly_conf_dir }}/wildfly.conf'\n    owner: root\n    group: root\n    mode: '0640'\n  notify:\n    - restart wildfly\n    - change standalone data mode\n\n- name: Copy wildfly properties file\n  template:\n    src: wildfly.properties.j2\n    dest: '{{ wildfly_conf_dir }}/wildfly.properties'\n    owner: '{{ wildfly_user }}'\n    group: '{{ wildfly_group }}'\n    mode: '0640'\n  notify:\n    - restart wildfly\n    - change standalone data mode\n\n- name: Copy wildfly init script\n  template: src=wildfly.init.j2 dest={{ wildfly_init_dir }}/wildfly owner=root\n            group=root mode=0750\n  when: ansible_service_mgr == 'init'\n  notify:\n    - restart wildfly\n    - change standalone data mode\n\n- name: Copy wildfly systemd unit file\n  template: src=wildfly.service.j2 dest={{ wildfly_systemd_dir }}/wildfly owner=root\n            group=root mode=0640\n  when: ansible_service_mgr == 'systemd'\n  notify:\n    - restart wildfly\n    - change standalone data mode\n\n  firewalld:\n    port: '{{ wildfly_manage_http_port }}/tcp'\n    permanent: yes\n    immediate: yes\n    state: enabled\n\n- name: Open wildfly management https tcp port\n  firewalld:\n    port: '{{ wildfly_manage_https_port }}/tcp'\n    permanent: yes\n    immediate: yes\n    state: enabled\n\n- name: Open wildfly http tcp port\n  firewalld:\n    port: '{{ wildfly_http_port }}/tcp'\n    permanent: yes\n    immediate: yes\n    state: enabled\n\n- name: Open wildfly https tcp port\n  firewalld:\n    port: '{{ wildfly_https_port }}/tcp'\n    permanent: yes\n    immediate: yes\n    state: enabled\n\n- name: Enable and start the service\n  service:\n    name: wildfly\n    state: started\n    enabled: yes\n\n- name: Change standalone data mode\n  file:\n    path: '{{ wildfly_dir }}/standalone/data'\n    owner: '{{ wildfly_user }}'\n    group: '{{ wildfly_group }}'\n    mode: '0750'\n    recurse: yes\n\n- name: Delete wildfly tar file\n  file:\n    path: '{{ wildfly_download_dir }}/{{ wildfly_download_file }}'\n    state: absent\n\n- name: Create a version file\n  template:\n    src: version.j2\n    dest: '{{ wildfly_version_file }}'\n    owner: '{{ wildfly_user }}'\n    group: '{{ wildfly_group }}'\n    mode: '0750'\n"}, {"commit_sha": "a10c5f4577e6e74feb1fadec4bcbab039b8b180a", "sha": "71e625e9a34f5d72cf9c4cad36ec94c202dff6e4", "filename": "tasks/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Set distribution facts\n  set_fact:\n    _docker_os_dist: \"{{ ansible_distribution }}\"\n    _docker_os_dist_release: \"{{ ansible_distribution_release }}\"\n    _docker_os_dist_major_version: \"{{ ansible_distribution_major_version }}\"\n    _docker_os_dist_check: yes\n  tags: [\"always\"]\n\n- name: Reinterpret distribution facts for Linux Mint 18\n  set_fact:\n    _docker_os_dist: \"Ubuntu\"\n    _docker_os_dist_release: \"xenial\"\n    _docker_os_dist_major_version: \"16\"\n  when:\n    _docker_os_dist == \"Linux Mint\" and\n    _docker_os_dist_major_version == \"18\"\n  tags: [\"always\"]\n\n- name: Reset role variables\n  set_fact:\n    docker_systemd_service_config_tweaks: []\n    docker_service_envs: {}\n  tags: [\"always\"]\n\n- name: Temporary handling of deprecated variable docker_enable_ce_edge (#54)\n  set_fact:\n    docker_channel: edge\n  when:\n    - docker_enable_ce_edge is defined\n    - docker_enable_ce_edge\n  tags: [\"always\"]\n\n- name: Temporary handling of deprecated variable docker_pkg_name\n  set_fact:\n    docker_version: \"{{ docker_pkg_name | regex_replace('^docker-ce.(.+)$', '\\\\1') }}\"\n  when: docker_pkg_name | match('docker-ce' + docker_os_pkg_version_separator[_docker_os_dist])\n  tags: [\"always\"]\n\n- name: Compatibility and distribution checks\n  include_tasks: checks.yml\n  when: docker_do_checks | bool\n  tags: [\"always\"]\n\n- name: Install and configure Docker CE\n  block:\n    - name: Setup Docker package repositories\n      include_tasks: setup-repository.yml\n      tags: [\"install\"]\n\n    - name: Remove Docker versions before Docker CE\n      include_tasks: remove-pre-docker-ce.yml\n      when: docker_remove_pre_ce | bool\n      tags: [\"install\"]\n\n    - name: Install Docker\n      include_tasks: install-docker.yml\n      tags: [\"install\"]\n\n    - name: Configure audit logging\n      include_tasks: setup-audit.yml\n      tags: [\"configure\"]\n\n    - name: Apply workarounds for bugs and/or tweaks\n      include_tasks: bug-tweaks.yml\n      tags: [\"configure\"]\n\n    - name: Configure Docker\n      include_tasks: configure-docker.yml\n      tags: [\"configure\"]\n\n    - name: Postinstall tasks\n      include_tasks: postinstall.yml\n      tags: [\"install\", \"postinstall\"]\n  when: not docker_remove | bool\n\n- name: Remove Docker CE and related configuration\n  include_tasks: remove-docker.yml\n  when: docker_remove | bool\n"}, {"commit_sha": "c91b6076e3a957fb0a165131d0ff3b3b208ed419", "sha": "fbb96546071be36bc0ab85da1eaf587ab24f55f0", "filename": "meta/main.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\nallow_duplicates: no\n\n\ngalaxy_info:\n  author: Aur\u00e9lien Wailly\n  description: Ansible role to meet CIS (Center for Internet Security) requirements on ubuntu\n  license: GPLv2\n  min_ansible_version: 1.6\n  platforms:\n    - name: Ubuntu\n      versions:\n        14.04\n  categories:\n    #- cloud\n    #- cloud:ec2\n    #- cloud:gce\n    #- cloud:rax\n    #- clustering\n    #- database\n    #- database:nosql\n    #- database:sql\n    #- development\n    - monitoring\n    #- networking\n    #- packaging\n    - system\n    #- web\n\n\n\ndependencies: []\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "cf49d2c4c4c67daa050b1fbfa9a93dd86447fa06", "filename": "roles/ipaclient/tasks/uninstall.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "---\n# tasks to uninstall IPA client\n\n# - name: Uninstall - Include Python2/3 import test\n#   import_tasks: \"{{ role_path }}/tasks/python_2_3_test.yml\"\n\n- name: Uninstall - Uninstall IPA client\n  command: >\n    /usr/sbin/ipa-client-install\n    --uninstall\n    -U\n  register: uninstall\n  # 2 means that uninstall failed because IPA client was not configured\n  failed_when: uninstall.rc != 0 and uninstall.rc != 2\n  changed_when: uninstall.rc == 0\n\n# - name: Remove IPA client package\n#   package:\n#     name: \"{{ item }}\"\n#     state: absent\n#   with_items: \"{{ ipaclient_packages }}\"\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "08f7d837e68f0bf8b8f42b48d22f6efcde33d2ab", "filename": "playbooks/manage-users/add-users.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- name: \"Import user data from {{ csv_doc_file_name }} and create users\"\n  hosts: identity-hosts\n  gather_facts: no\n  roles:\n    - user-management/populate-users\n    - user-management/manage-users\n    - user-management/manage-user-passwd\n\n- name: \"Notify users\"\n  import_playbook: ../notifications/email-notify-users.yml\n"}, {"commit_sha": "45971be8249cc4627ef8ddfacf55a661b7fc13ca", "sha": "8c8ff044409886d99dbd09b6e67b69c7c5d55fc5", "filename": "tasks/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Set distribution facts\n  set_fact:\n    _docker_os_dist: \"{{ ansible_distribution }}\"\n    _docker_os_dist_release: \"{{ ansible_distribution_release }}\"\n    _docker_os_dist_major_version: \"{{ ansible_distribution_major_version }}\"\n    _docker_os_dist_check: yes\n  tags: [\"install\", \"configure\"]\n\n- name: Reinterpret distribution facts for Linux Mint 18\n  set_fact:\n    _docker_os_dist: \"Ubuntu\"\n    _docker_os_dist_release: \"xenial\"\n    _docker_os_dist_major_version: \"16\"\n  when:\n    _docker_os_dist == \"Linux Mint\" and\n    _docker_os_dist_major_version == \"18\"\n  tags: [\"install\", \"configure\"]\n\n# https://wiki.ubuntu.com/SystemdForUpstartUsers\n# Important! systemd is only fully supported in Ubuntu 15.04 and later releases\n- name: Determine usage of systemd\n  shell: \"ps -p1 | grep systemd 1>/dev/null && echo systemd || echo upstart\"\n  become: true\n  changed_when: no  \n  register: _determine_systemd_usage\n\n- name: Set fact to indicate systemd is not used\n  set_fact:\n    _docker_systemd_used: \"{{ _determine_systemd_usage is defined and _determine_systemd_usage.stdout == 'systemd' }}\"\n\n- name: Compatibility and distribution checks\n  include_tasks: checks.yml\n  tags: [\"install\", \"configure\"]\n\n- name: Setup Docker package repositories\n  include_tasks: setup-repository.yml\n  tags: [\"install\"]\n\n- name: Remove Docker versions before Docker CE\n  include_tasks: remove-pre-docker-ce.yml\n  when: docker_remove_pre_ce | bool\n  tags: [\"install\"]\n\n- name: Install Docker\n  include_tasks: install-docker.yml\n  tags: [\"install\"]\n\n- name: Configure audit logging\n  include_tasks: setup-audit.yml\n  tags: [\"configure\"]\n\n- name: Apply workarounds for bugs and/or tweaks\n  include_tasks: bug-tweaks.yml\n  tags: [\"configure\"]\n\n- name: Configure systemd service\n  include_tasks: configure-systemd.yml\n  when: _docker_systemd_used | bool  \n  tags: [\"configure\"]\n\n- name: Configure non-systemd service\n  include_tasks: configure-non-systemd.yml\n  when: _docker_systemd_used | bool == false \n  tags: [\"configure\"]\n\n- name: Configure Docker\n  include_tasks: configure-docker.yml\n  tags: [\"configure\"]\n"}, {"commit_sha": "6d10af54bdbf8e81c3d90a70ffea87b4d2c20eb2", "sha": "e1c829f76e06d24fdac063f96eaca7c6a361f338", "filename": "tasks/themes.yml", "repository": "Oefenweb/ansible-wordpress", "decoded_content": "# tasks file for wordpress, themes\n---\n- name: identify installation (theme)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.0.path }}' theme is-installed {{ item.1 }}\"\n  register: check_installation_themes\n  failed_when: False\n  changed_when: False\n  with_subelements:\n    - wordpress_installs\n    - themes\n  when: item.1\n  tags: [configuration, wordpress, wordpress-themes, wordpress-is-installed-theme]\n\n- name: install (theme)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.item.0.path }}' theme install {{ item.item.1 }} --activate\"\n  with_items: check_installation_themes.results\n  when: check_installation_themes is defined and item.item.1 and item.rc != 0\n  tags: [configuration, wordpress, wordpress-themes, wordpress-install-theme]\n\n- name: check install (theme)\n  shell: \"wp-cli --allow-root --no-color --path='{{ item.0.path }}' theme is-installed {{ item.1 }}\"\n  changed_when: False\n  with_subelements:\n    - wordpress_installs\n    - themes\n  when: item.1\n  tags: [configuration, wordpress, wordpress-themes, wordpress-install-theme-check]\n"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "a78e134a9afe8bce40dbaa2e5deddf602ec32670", "filename": "playbooks/group_vars/all.yml", "repository": "rocknsm/rock", "decoded_content": "%YAML 1.1\n---\nrock_version: 2.1.0\nrock_online_install: true\nrock_enable_testing: false\nrock_sysctl_file: /etc/sysctl.d/10-ROCK.conf\nrock_data_dir: /data\nrocknsm_dir: /opt/rocknsm\nrock_data_user: root\nrock_data_group: root\nrock_monifs: \"{{ ansible_interfaces | difference(['lo', ansible_default_ipv4.interface | default('lo') ])| list }}\"\nrock_hostname: \"{{ inventory_hostname_short }}\"\nrock_fqdn: \"{{ inventory_hostname }}\"\nrock_mgmt_nets: [ \"0.0.0.0/0\" ]\nrock_cache_dir: /srv/rocknsm/support\npulledpork_rules:\n  - { url: \"https://snort.org/downloads/community/\", file: \"community-rules.tar.gz\", apikey: \"Community\", test: \"{{not with_suricata and with_snort}}\" }\n  - { url: \"https://www.snort.org/reg-rules/\", file: \"snortrules-snapshot.tar.gz\", apikey: \"796f26a2188c4c953ced38ff3ec899d8ae543350\", test: \"{{not with_suricata and with_snort}}\" }\n  - { url: \"https://rules.emergingthreats.net/\", file: \"emerging.rules.tar.gz\", apikey: \"open-nogpl\" }\n  - { url: \"http://talosintel.com/feeds/ip-filter.blf\", file: \"IPBLACKLIST\", apikey: \"open\" }\n\n#### Retention Configuration ####\nelastic_close_interval: 15\nelastic_delete_interval: 60\nkafka_retention: 168\nsuricata_retention: 3\nbro_log_retention: 0\nbro_stats_retention: 0\n\n# Feature options - Don't flip these unless you know what you're doing\n# These control if the service is installed\nwith_stenographer: true\nwith_docket: true\nwith_bro: true\nwith_suricata: true\nwith_snort: false\nwith_pulledpork: false\nwith_logstash: true\nwith_elasticsearch: true\nwith_kibana: true\nwith_filebeat: true\nwith_zookeeper: true\nwith_kafka: true\nwith_nginx: true\nwith_lighttpd: true\nwith_fsf: true\n\n# Feature options - Don't flip these unless you know what you're doing\n# These control if the systemd service is enabled\nenable_stenographer: false\nenable_docket: false\nenable_bro: true\nenable_suricata: true\nenable_snort: false\nenable_pulledpork: false\nenable_logstash: true\nenable_elasticsearch: true\nenable_kibana: true\nenable_filebeat: true\nenable_zookeeper: true\nenable_kafka: true\nenable_nginx: true\nenable_lighttpd: true\nenable_fsf: false\n\nrocknsm_package_list:\n  - java-1.8.0-openjdk-headless\n  - jq\n  - GeoIP\n  - GeoIP-update\n  - tcpreplay\n  - tcpdump\n  - bats\n  - policycoreutils-python\n  - htop\n  - vim\n  - git\n  - tmux\n  - nmap-ncat\n  - logrotate\n  - perl-LWP-Protocol-https\n  - perl-Sys-Syslog\n  - perl-Crypt-SSLeay\n  - perl-Archive-Tar\n\n\nhttp_tls_crt: /etc/pki/tls/certs/http_tls_crt.pem\nhttp_tls_pub: /etc/pki/tls/certs/http_tls_pub.pem\nhttp_tls_key: /etc/pki/tls/private/http_tls_key.pem\nhttp_tls_combined: /etc/pki/tls/private/httpd-combined.pem\nhttp_tls_dhparams: /etc/pki/tls/misc/http_tls_dhparams.pem\n\ndocket_web_pemfile: \"{{ http_tls_combined }}\"\ndocket_web_dhparams: \"{{ http_tls_dhparams }}\"\n\nepel_baseurl: http://download.fedoraproject.org/pub/epel/$releasever/$basearch/\nepel_gpgurl:  https://dl.fedoraproject.org/pub/epel/RPM-GPG-KEY-EPEL-7\nelastic_baseurl: https://artifacts.elastic.co/packages/6.x/yum\nelastic_gpgurl: https://artifacts.elastic.co/GPG-KEY-elasticsearch\npulledpork_release: 0.7.2\npulledpork_url: \"https://github.com/shirkdog/pulledpork/archive/{{ pulledpork_release }}.tar.gz\"\npulledpork_filename: \"pulledpork-{{ pulledpork_release }}.tar.gz\"\npulledpork_engine_basepath: \"/etc/{{ \\\"suricata\\\" if with_suricata else \\\"snort\\\" }}\"\n\nrocknsm_baseurl: https://packagecloud.io/rocknsm/2_1/el/7/$basearch\nrocknsm_srpm_baseurl: https://packagecloud.io/rocknsm/2_1/el/7/SRPMS\nrocknsm_testing_baseurl: https://copr-be.cloud.fedoraproject.org/results/@rocknsm/rocknsm-2.1/epel-7-$basearch/\nrocknsm_gpgurl: https://packagecloud.io/rocknsm/2_1/gpgkey\nrocknsm_local_baseurl: file:///srv/rocknsm\nbro_user: bro\nbro_group: bro\nbro_data_dir: \"{{ rock_data_dir }}/bro\"\nbro_prefix: /usr\nbro_sysconfig_dir: /etc/bro\nbro_site_dir: /usr/share/bro/site\nbro_cpu: \"{{ (ansible_processor_vcpus // 2) if (ansible_processor_vcpus <= 16) else 8 }}\"\nbro_rockscripts_repo: https://github.com/rocknsm/rock-scripts.git\nbro_rockscripts_branch: master\nbro_rockscripts_filename: \"rock-scripts_{{ bro_rockscripts_branch | replace('/', '-') }}.tar.gz\"\nrock_dashboards_repo: https://github.com/rocknsm/rock-dashboards.git\nrock_dashboards_branch: devel\nrock_dashboards_url: \"https://github.com/rocknsm/rock-dashboards/archive/{{ rock_dashboards_branch }}.tar.gz\"\nrock_dashboards_filename: \"rock-dashboards_{{ rock_dashboards_branch | replace('/', '-') }}.tar.gz\"\nrock_dashboards_version: 2.1\nrock_module_dir: \"/opt/rocknsm/rock-dashboards-{{ rock_dashboards_branch }}\"\nstenographer_user: stenographer\nstenographer_group: stenographer\nstenographer_data_dir: \"{{ rock_data_dir }}/stenographer\"\nsuricata_user: suricata\nsuricata_group: suricata\nsuricata_data_dir: \"{{ rock_data_dir }}/suricata\"\nsuricata_var_dir: /var/lib/suricata\n\npulled_pork_repo: https://github.com/shirkdog/pulledpork.git\npulled_pork_oinkcode: 796f26a2188c4c953ced38ff3ec899d8ae543350\nfsf_user: fsf\nfsf_group: fsf\nfsf_data_dir: \"{{ rock_data_dir }}/fsf\"\nfsf_archive_dir: \"{{ fsf_data_dir }}/archive\"\nfsf_client_logfile: \"{{ fsf_data_dir }}/client.log\"\nkafka_user: kafka\nkafka_group: kafka\nkafka_data_dir: \"{{ rock_data_dir }}/kafka\"\nkafka_config_path: /etc/kafka/server.properties\nes_user: elasticsearch\nes_group: elasticsearch\nes_data_dir: \"{{ rock_data_dir }}/elasticsearch\"\nes_cluster_name: rocknsm\nes_node_name: \"{{ rock_hostname }}\"\nes_mem: \"{{ (ansible_memtotal_mb // 1024 // 2) if (ansible_memtotal_mb // 1024) < 64 else 31 }}\"\nes_url: \"http://127.0.0.1:9200\"\nes_log_dir: /var/log/elasticsearch\nes_memlock_override: |\n  [Service]\n  LimitMEMLOCK=infinity\nlogstash_user: logstash\nlogstash_group: logstash\nkibana_url: \"http://127.0.0.1:5601\"\n"}, {"commit_sha": "a10c5f4577e6e74feb1fadec4bcbab039b8b180a", "sha": "32e7ca262c90a7c700f6d81bad1456756d151a76", "filename": "tasks/bug-tweaks.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Configuration to avoid 'Device or resource busy' (CentOS/RedHat)\n  include_tasks: bug-tweaks/bug-centos7-resource-busy.yml\n  when:\n    - _docker_os_dist == \"CentOS\" or _docker_os_dist == \"RedHat\"\n    - ansible_kernel | version_compare('4', '<')\n\n- name: Best effort handling to directlvm for Debian 8 to get uniform behavior across distributions\n  include_tasks: bug-tweaks/tweak-debian8-directlvm.yml\n  when:\n    - _docker_os_dist == \"Debian\"\n    - _docker_os_dist_major_version == '8'\n    - docker_daemon_config['storage-opts'] is defined\n    - docker_daemon_config['storage-opts'] | select('match', '^dm.directlvm_device.+')\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "13d7d6c115a3ea0a61756707c129238ff9028b71", "filename": "roles/mesos/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for mesos\nmesos_zk_port: 2181\nmesos_zookeeper_group: zookeeper_servers\nmesos_master_port: 5050\nconsul_dir: /etc/consul.d\nmesos_local_address: \"{{ ansible_default_ipv4.address }}\"\n"}, {"commit_sha": "1054e6b543fd736ac9c3616cb81e484bb0af134c", "sha": "186fb20d29f34abc7029c700652d94ddcc61a497", "filename": "tasks/main.yml", "repository": "antoiner77/caddy-ansible", "decoded_content": "---\n- name: Create Caddy user\n  user: name=caddy system=yes createhome=yes\n\n- name: Get all Caddy releases\n  get_url: url=https://api.github.com/repos/mholt/caddy/git/refs/tags dest=/home/caddy/releases.txt force=yes\n  when: caddy_update\n  register: caddy_releases_cache\n\n- name: Set Caddy features\n  copy: content=\"{{caddy_features}}\" dest=/home/caddy/features.txt\n  when: caddy_update\n  register: caddy_features_cache\n\n- name: Download Caddy\n  get_url: url=https://caddyserver.com/download/build?os=linux&arch=amd64&features={{caddy_features}} dest=/home/caddy/caddy.tar.gz force=yes\n  when: caddy_releases_cache.changed or caddy_features_cache.changed\n  register: caddy_binary_cache\n\n- name: Download caddy\n  get_url: url=https://caddyserver.com/download/build?os=linux&arch=amd64&features={{caddy_features}} dest=/home/caddy/caddy.tar.gz\n\n- name: Extract caddy\n  unarchive: src=/home/caddy/caddy.tar.gz dest=/usr/bin/ copy=no\n  when: caddy_binary_cache.changed\n\n- name: Extract caddy\n  unarchive: src=/home/caddy/caddy.tar.gz dest=/usr/bin/ creates=/usr/bin/caddy copy=no\n\n- name: Check if the binary can bind to TCP port <1024\n  shell: getcap /usr/bin/caddy | grep cap_net_bind_service\n  failed_when: False\n  changed_when: False\n  register: caddy_bind_cap\n\n- name: Set capability on the binary file to be able to bind to TCP port <1024\n  command: setcap cap_net_bind_service=+ep /usr/bin/caddy\n  when: caddy_bind_cap.rc > 0\n\n- name: Create Caddy dirs\n  file: path={{ item }} state=directory owner=caddy\n  with_items:\n    - /etc/caddy\n    - /var/www\n\n- name: Create Caddy config file\n  copy: content=\"{{caddy_config}}\" dest=/etc/caddy/Caddyfile\n  notify:\n    - restart caddy\n\n- name: apt | Install git\n  apt: name=git update_cache=yes\n  when: ansible_pkg_mgr == 'apt' and caddy_features | search('git')\n\n- name: yum | Install git\n  yum: name=git\n  when: ansible_pkg_mgr == 'yum' and caddy_features | search('git')\n\n- name: Upstart service\n  template: src=caddy.conf dest=/etc/init/caddy.conf\n  when: ansible_distribution == 'Debian' or ansible_distribution == 'Ubuntu'\n  notify: restart caddy\n\n- name: Systemd service\n  template: src=caddy.service dest=/etc/systemd/system/caddy.service\n  when: ansible_distribution == 'CentOS' or ansible_distribution == 'Red Hat Enterprise Linux'\n\n- name: Start Caddy service\n  service: name=caddy state=started enabled=yes\n"}, {"commit_sha": "584d564218d6e2e63d3ecca157daf817fe4d533c", "sha": "855667b99059b585b7dccf1f6cb7e71e9890daa2", "filename": "meta/main.yml", "repository": "mikolak-net/ansible-raspi-config", "decoded_content": "---\ngalaxy_info:\n  author: Miko\u0142aj Koziarkiewicz\n  description: Sets up basic configuration for a headless Raspberry Pi Raspbian server.\n  company: miko\u0142ak\n  license: BSD\n  min_ansible_version: 1.8\n  platforms:\n  - name: Raspbian\n  categories:\n  - development\n  - system\ndependencies:\n  - { role: \"knopki.locale\", version: \"v1.0.3\", locale_all: \"{{raspi_config_locale}}\" }\n  - { role: \"Stouts.timezone\", version: \"2.0.1\", timezone_timezone: \"{{raspi_config_timezone}}\" }\n  - { role: \"Stouts.hostname\", version: \"1.0.3\", hostname_hostname: \"{{raspi_config_hostname}}\" }"}, {"commit_sha": "9e7bed4aaef56159f148d24e4f7d7e8b53be632b", "sha": "21e7709739185e0e50c917f3f947c9d33090d764", "filename": "tasks/main-Generic.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# tasks file for ansible-role-docker-ce\n\n- name: Copy Docker audit rules\n  copy:\n    src: files/etc/audit/rules.d/docker.rules\n    dest: /etc/audit/rules.d/docker.rules\n  become: yes\n  notify: restart auditd\n  when: docker_enable_audit == true\n\n- name: Ensure Docker audit rules are removed\n  file:\n    path: /etc/audit/rules.d/docker.rules\n    state: absent\n  become: yes\n  notify: restart auditd\n  when: docker_enable_audit == false\n\n- name: Determine Docker version\n  command: bash -c \"docker version | grep Version | awk '{print $2}'\"\n  ignore_errors: yes\n  changed_when: false\n  register: cmd_docker_version\n\n- name: Set fact if old Docker installation shall be removed\n  set_fact:\n    remove_old_docker: \"{{docker_remove_pre_ce | bool }} == true and {{ cmd_docker_version.stdout_lines[0] | search('-ce') }} == false\"\n  when: cmd_docker_version.stdout_lines is defined and cmd_docker_version.stdout_lines[0] is defined\n\n- name: Check if Docker is running\n  command: systemctl status docker\n  ignore_errors: yes\n  changed_when: false\n  register: service_docker_status\n  when: remove_old_docker | default(false) | bool == true\n  become: true\n\n- name: Stop Docker service\n  service:\n    name: docker\n    state: stopped\n  when: \"service_docker_status.rc | default(1) == 0\"\n\n- name: Remove old Docker installation before Docker CE\n  package:\n    name: \"{{ item }}\"\n    state: absent\n  become: true\n  when: remove_old_docker|default(false) | bool == true\n  with_items:\n    - docker\n    - docker-common\n    - container-selinux\n    - docker-selinux\n    - docker-engine\n\n- name: Ensure docker-ce is the latest version\n  package:\n    name: docker-ce\n    state: latest\n  become: true\n  notify: restart docker\n\n- name: Ensure /etc/docker directory exists\n  file:\n    path: /etc/docker\n    state: directory\n    mode: 0755\n  become: true\n\n- name: Configure Docker daemon (file)\n  copy:\n    src: \"{{ docker_daemon_config_file }}\"\n    dest: /etc/docker/daemon.json\n  become: true\n  notify: restart docker\n  when: docker_daemon_config_file is defined\n\n- name: Configure Docker daemon (variables)\n  copy:\n    content: \"{{ docker_daemon_config | to_nice_json }}\"\n    dest: /etc/docker/daemon.json\n  become: true\n  notify: restart docker\n  when: docker_daemon_config_file is not defined and \n        docker_daemon_config is defined\n\n- name: Enable and start Docker service\n  service:\n    name: docker\n    state: started\n    enabled: true\n  become: true\n\n"}, {"commit_sha": "1054e6b543fd736ac9c3616cb81e484bb0af134c", "sha": "6584718d0bb3d77f31599c03356e1126b2311b79", "filename": "handlers/main.yml", "repository": "antoiner77/caddy-ansible", "decoded_content": "---\n# handlers file for caddy-ansible\n- name: restart caddy\n  service: name=caddy state=restarted\n"}, {"commit_sha": "92dabcd706e72a0dc15ce13086fb9d59f1a8760e", "sha": "d65fe4be582c59683b39413d857065bcd134787f", "filename": "tasks/main.yml", "repository": "RocketChat/Rocket.Chat.Ansible", "decoded_content": "---\n# tasks/main.yml: Main tasks for RocketChat.Ansible\n\n  - include_vars: \"{{ item }}\"\n    with_first_found:\n      - \"{{ ansible_distribution }}.yml\"\n      - \"{{ ansible_os_family }}.yml\"\n    tags:\n      - vars\n      - always\n\n  - include_vars: \"{{ item }}\"\n    with_first_found:\n      # Below is for example: Fedora_2x.yml = 20-29\n      - \"{{ ansible_distribution }}_{{ ansible_distribution_major_version[:1] ~ 'x' }}.yml\"\n      - \"{{ ansible_distribution }}_{{ ansible_distribution_major_version }}.yml\"\n      - \"{{ ansible_os_family }}_{{ ansible_distribution_major_version }}.yml\"\n    tags:\n      - vars\n      - always\n\n  - name: Ensure the Rocket.Chat service group is present\n    group:\n      name: \"{{ rocket_chat_service_group }}\"\n      state: present\n      system: true\n\n  - name: Ensure the Rocket.Chat service user is present\n    user:\n      comment: Rocket.Chat Service User\n      name: \"{{ rocket_chat_service_user }}\"\n      group: \"{{ rocket_chat_service_group }}\"\n      home: \"{{ rocket_chat_application_path }}\"\n      createhome: true\n      shell: /bin/false\n      state: present\n      system: true\n\n  - name: Ensure APT cache has been updated recently\n    apt:\n      update_cache: yes\n      cache_valid_time: 3600\n    when: ((ansible_pkg_mgr | lower) == \"apt\")\n\n  - name: Install EPEL for RHEL based distros (CentOS/RHEL)\n    block:\n\n      - name: Ensure the EPEL repository is present\n        package:\n          name: epel-release\n          state: present\n\n      - name: Ensure the EPEL repository GPG key is imported\n        rpm_key:\n          key: /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7\n          state: present\n\n    when: ((ansible_distribution | lower) == \"centos\")\n            or ((ansible_distribution | lower) == \"redhat\")\n    tags: repo\n\n  # https://docs.ansible.com/ansible/latest/become.html#becoming-an-unprivileged-user\n  - name: Install acl controls\n    package:\n      name: acl\n      state: present\n\n  - name: Check for adequate privilege escalation rights\n    ping:\n    become: yes\n    become_user: \"{{ rocket_chat_service_user }}\"\n    ignore_errors: true\n    register: priv_check_result\n  - assert:\n      that: (priv_check_result | succeeded)\n      msg: >\n        Check your sudo configuration to ensure that your connecting user\n        can assume the identities of other users without prompting.\n\n  - name: \"Configure /etc/hosts\"\n    lineinfile:\n      dest: /etc/hosts\n      line:  \"{{ item.line }}\"\n      regexp: \"{{ item.regexp }}\"\n      insertafter: \"{{ item.insertafter | default(omit) }}\"\n      insertbefore: \"{{ item.insertbefore | default(omit) }}\"\n      backrefs: \"{{ item.backrefs | default(True) }}\"\n    when: ansible_virtualization_type != \"docker\"\n    with_items:\n      - regexp: '^127\\.0\\.0\\.1(.*){{ ansible_nodename }}(.*)'\n        line:   '127.0.0.1 \\1 \\2'\n      - regexp: '^127\\.0\\.0\\.1(.*){{ ansible_hostname }}(.*)'\n        line:   '127.0.0.1 \\1 \\2'\n      - regexp: '^127\\.0\\.0\\.1[ ]*([^ ].*)[ ]+localhost[ ]+([^ ].*)'\n        line:   '127.0.0.1    localhost \\1 \\2'\n      - regexp: '^127\\.0\\.1\\.1'\n        line:   \"127.0.1.1    {{ ansible_nodename }} {{ ansible_hostname }}\"\n        insertafter: '^127\\.0\\.0\\.1'\n        backrefs: no\n    register: hosts_change_result\n\n  - import_tasks: mongodb.yml\n    when: (rocket_chat_include_mongodb | bool)\n    tags: mongodb\n\n  - name: Redefine rocket_chat_dep_packages to add dist specific packages if needed\n    set_fact:\n      rocket_chat_dep_packages:\n        \"{{ rocket_chat_dep_packages | union(rocket_chat_dist_specific_packages) | unique }}\"\n    when:\n      - (rocket_chat_dist_specific_packages is defined)\n      - (rocket_chat_dist_specific_packages)\n\n  - name: Ensure Rocket.Chat dependencies are installed\n    package:\n      name:\n        \"{{\n            (ansible_virtualization_type != 'docker') |\n            ternary(\n              rocket_chat_dep_packages,\n              rocket_chat_dep_packages | difference('[\\\"cron\\\"]')\n              )\n          }}\"\n      state: present\n    register: dep_install_result\n    until: (dep_install_result | succeeded)\n    retries: 2\n    tags: packages\n\n  - import_tasks: nodejs.yml\n    tags: nodejs\n\n  - name: Check to see if this is the initial Rocket.Chat deployment\n    stat:\n      path: \"{{ rocket_chat_application_path }}/bundle\"\n    register: rocket_chat_deploy_state\n\n  - name: Set the initial Rocket.Chat upgrade status\n    set_fact:\n      rocket_chat_upgraded: false\n\n  - name: Setup PGP for verifying the Rocket.Chat tarball\n    block:\n\n    - name: \"Import RochetChat PGP Key from keyserver: {{ rocket_chat_tarball_gpg_keyserver }}\"\n      shell: |\n        {{ rocket_chat_pgp_command }} \\\n            --keyserver \"{{ rocket_chat_tarball_gpg_keyserver }}\" \\\n            --recv-keys \"{{ rocket_chat_tarball_gpg_key }}\"\n      changed_when: \"'not changed' not in key_recv_result.stderr\"\n      register: key_recv_result\n      until:  (key_recv_result | succeeded)\n      retries: 4\n\n    - name: Fetch the Rocket.Chat binary tarball PGP signature\n      get_url:\n        url: \"{{ rocket_chat_tarball_asc_remote }}\"\n        force: yes\n        dest: \"{{ rocket_chat_application_path }}/rocket.chat-{{ rocket_chat_version }}.asc\"\n        timeout: \"{{ rocket_chat_tarball_fetch_timeout }}\"\n        validate_certs: \"{{ rocket_chat_tarball_validate_remote_cert }}\"\n        owner: \"{{ rocket_chat_service_user }}\"\n        group: \"{{ rocket_chat_service_group }}\"\n      register: get_pgp_asc_result\n      until: (get_pgp_asc_result | succeeded)\n      retries: 2\n    when: (rocket_Chat_tarball_check_pgp | bool)\n    tags: pgp\n\n  - name: Fetch the Rocket.Chat binary tarball\n    get_url:\n      url: \"{{ rocket_chat_tarball_remote }}\"\n      checksum: \"{{ (rocket_chat_tarball_check_checksum == false) | ternary(omit, 'sha256: ' + (rocket_chat_tarball_sha256sum|string)) }}\"\n      force: yes\n      dest: \"{{ rocket_chat_application_path }}/rocket.chat-{{ rocket_chat_version }}.tgz\"\n      timeout: \"{{ rocket_chat_tarball_fetch_timeout }}\"\n      validate_certs: \"{{ rocket_chat_tarball_validate_remote_cert }}\"\n      owner: \"{{ rocket_chat_service_user }}\"\n      group: \"{{ rocket_chat_service_group }}\"\n    # Temp fix for ansible/ansible#15915 ( Broken include in handlers )\n    # https://github.com/ansible/ansible/issues/15915\n    #notify: Upgrade Rocket.Chat\n    register: download_result\n    until: (download_result | succeeded)\n    retries: 2\n    changed_when: (download_result | changed)\n                   or (not (rocket_chat_tarball_check_checksum | bool))\n    tags:\n      - download\n      - pgp\n\n  - name: Verify Rocket.Chat binary tarball with GPG\n    shell: |\n      {{ rocket_chat_pgp_command }} \\\n        --verify rocket.chat-{{ rocket_chat_version }}.asc \\\n                 rocket.chat-{{ rocket_chat_version }}.tgz\n    args:\n      chdir: \"{{ rocket_chat_application_path }}\"\n    when: (rocket_Chat_tarball_check_pgp | bool)\n    changed_when: false\n    tags: pgp\n\n  - name: Upgrade Rocket.Chat\n    include_tasks: upgrade.yml\n    when:\n      - (download_result | changed)\n      - ('stat' in rocket_chat_deploy_state)\n      - (rocket_chat_deploy_state.stat.exists | bool)\n    tags:\n      - upgrade\n\n  - meta: flush_handlers\n\n  - name: Unpack the Rocket.Chat binary tarball\n    unarchive:\n      copy: false\n      src: \"{{ rocket_chat_application_path }}/rocket.chat-{{ rocket_chat_version }}.tgz\"\n      dest: \"{{ rocket_chat_application_path }}\"\n      creates: \"{{ rocket_chat_application_path }}/bundle\"\n      owner: \"{{ rocket_chat_service_user }}\"\n      group: \"{{ rocket_chat_service_group }}\"\n    tags: build\n\n  - name: Install Rocket.Chat via NPM\n    npm:\n      state: present\n      path: \"{{ rocket_chat_application_path }}/bundle/programs/server\"\n      executable: \"{{ rocket_chat_npm_path }}\"\n      production: true\n    become: true\n    become_user: \"{{ rocket_chat_service_user }}\"\n    environment:\n      PATH: \"{{ rocket_chat_node_prefix }}/bin:{{ ansible_env.PATH }}\"\n    tags: build\n\n  - name: Ensure the Rocket.Chat log file symlink is present [Ubuntu 14]\n    file:\n      path: /var/log/rocketchat.log\n      src: /var/log/upstart/rocketchat.log\n      state: link\n      force: yes\n    when:\n      - ((ansible_distribution | lower) == \"ubuntu\")\n      - ((ansible_distribution_major_version | int) == \"14\")\n    tags: build\n\n  - name: Ensure the Rocket.Chat application data permissions are correct\n    command: >-\n      chown {{ rocket_chat_service_user }}.{{ rocket_chat_service_group }}\n      -R {{ rocket_chat_application_path | quote }}\n    args:\n      warn: no\n    tags: build\n\n  - name: Deploy the Rocket.Chat service file\n    template:\n      src: \"{{ rocket_chat_service_template.src }}\"\n      dest: \"{{ rocket_chat_service_template.dest }}\"\n    notify:\n      - Update the Rocket.Chat service configuration\n      - Restart the Rocket.Chat service\n    tags: service\n\n  - meta: flush_handlers\n\n  - name: Restart the Rocket.Chat service [UPGRADE]\n    service:\n      name: rocketchat\n      state: restarted\n    when: (rocket_chat_upgraded | bool)\n    tags: service\n\n  - name: Ensure the Rocket.Chat service is running/enabled\n    service:\n      name: rocketchat\n      state: started\n      enabled: true\n    tags: service\n\n  - import_tasks: nginx.yml\n    when: (rocket_chat_include_nginx | bool)\n    tags: nginx\n\n  - import_tasks: letsencrypt.yml\n    when: (rocket_chat_include_letsencrypt | bool)\n    tags: letsencrypt\n\n  - meta: flush_handlers\n"}, {"commit_sha": "45971be8249cc4627ef8ddfacf55a661b7fc13ca", "sha": "f15b95e43a7f730317eb0f3eae7ee364e0a4a18e", "filename": "tasks/bug-tweaks.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "# Configuration to avoid 'Device or resource busy'\n- block:\n  - name: Stat /proc/sys/fs/may_detach_mounts (CentOS/RedHat)\n    stat:\n      path: /proc/sys/fs/may_detach_mounts\n    register: may_detach_mounts\n\n  - name: Ensure fs.may_detach_mounts is set to avoid 'Device or resource busy' (CentOS/RedHat)\n    sysctl:\n      name: fs.may_detach_mounts\n      value: 1\n      sysctl_file: /etc/sysctl.d/99-docker.conf\n      reload: yes\n    become: yes\n    when: may_detach_mounts.stat.exists\n  \n  # Keep for compatibility reasons of this role. Now everything is in the same file.\n  - name: Remove systemd drop-in for Docker Mount Flags slave configuration (CentOS/RedHat)\n    file:\n      path: /etc/systemd/system/docker.service.d/mountflags-slave.conf\n      state: absent\n    become: yes\n    notify: restart docker\n\n  - name: Set systemd service MountFlags option to \"slave\" to prevent \"device busy\" errors on CentOS/RedHat 7.3 kernels (CentOS/RedHat)\n    set_fact:\n      docker_systemd_service_config_tweaks: \"{{ docker_systemd_service_config_tweaks + _systemd_service_config_tweaks }}\"\n    vars:\n      _systemd_service_config_tweaks:\n        - 'MountFlags=slave'\n  when:\n    - _docker_os_dist == \"CentOS\" or _docker_os_dist == \"RedHat\"\n    - docker_enable_mount_flag_fix | bool\n    - ansible_kernel | version_compare('4', '<')\n\n\n"}, {"commit_sha": "85225262433ef633502568ddf4af026ab0276bc6", "sha": "ffeb168e695a6131fef708175282c1faba0d5920", "filename": "tasks/install-docker.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Set version string\n  set_fact:\n    _docker_version_string: \"{{ docker_os_pkg_version_separator[_docker_os_dist] }}{{ docker_version }}\"\n  when: docker_version != ''\n\n- name: Set packages state to latest\n  set_fact:\n    _docker_pkg_state: 'latest'\n  when: docker_latest_version | bool and docker_version == ''\n\n- name: Filter out packages to match older Docker CE versions\n  set_fact:\n    _docker_packages:\n      - docker-ce\n  when:\n    - docker_version != ''\n    - docker_version is match('17.') or docker_version is match('18.03') or docker_version is match('18.06')\n\n- name: Ensure some kind of compatibility for no longer officially supported distributions since Docker CE 18.09\n  set_fact:\n    _docker_packages:\n      - docker-ce\n  when:\n    - _docker_packages is not defined\n    - (_docker_os_dist == \"Debian\" and _docker_os_dist_major_version | int < 9) or\n      (_docker_os_dist == \"Fedora\" and _docker_os_dist_major_version | int < 27) or\n      (_docker_os_dist == \"Ubuntu\" and _docker_os_dist_major_version | int < 16) or\n      (_docker_os_dist == \"Ubuntu\" and _docker_os_dist_major_version | int == 17)\n\n- name: Ensure Docker CE is installed\n  become: true\n  package:\n    name: \"{{ (item is search('docker')) | ternary((item + _docker_version_string | default('')), item) }}\"\n    state: \"{{ _docker_pkg_state | default('present') }}\"\n  loop: \"{{ _docker_packages | default(docker_packages) }}\"\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  notify: restart docker\n"}, {"commit_sha": "f9310d340d7ef77d1b47df05e6d09619e5baa3f5", "sha": "ee4d6746b44114fc557a675a2eeb7182f1d8a34b", "filename": "meta/main.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\ngalaxy_info:\n  role_name: \"java\"\n  author: \"Lean Delivery team <team@lean-delivery.com>\"\n  description: \"Lean Delivery Java install\"\n  company: \"Epam Systems\"\n  license: \"Apache\"\n  min_ansible_version: \"2.7\"\n  issue_tracker_url: \"https://github.com/lean-delivery/ansible-role-java/issues\"\n  platforms:\n    - name: \"Ubuntu\"\n      versions:\n        - \"trusty\"\n        - \"xenial\"\n        - \"bionic\"\n    - name: \"Debian\"\n      versions:\n        - \"stretch\"\n    - name: \"EL\"\n      versions:\n        - \"6\"\n        - \"7\"\n    - name: \"Amazon\"\n      versions:\n        - \"2017.12\"\n        - \"Candidate\"\n    - name: \"Windows\"\n      versions:\n        - \"all\"\n\n  galaxy_tags:\n    - \"development\"\n    - \"system\"\n    - \"packaging\"\n    - \"java\"\n    - \"oracle\"\n    - \"jdk\"\n    - \"openjdk\"\n    - \"sapjvm\"\n    - \"windows\"\n\ndependencies: []\n"}, {"commit_sha": "45971be8249cc4627ef8ddfacf55a661b7fc13ca", "sha": "6610a68efe253e94a5af8d5b3449b73039591ba3", "filename": "handlers/main.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# handlers file for ansible-role-docker-ce\n\n- name: restart docker\n  service:\n    name: docker\n    state: restarted\n  become: yes\n  tags: [\"install\", \"configure\"]\n\n# Workaround because systemd cannot be used: https://github.com/ansible/ansible/issues/22171\n- name: restart auditd\n  shell: service auditd restart\n  args:\n    warn: no\n  become: yes\n  tags: [\"install\", \"configure\"]\n"}, {"commit_sha": "01c4359d8ad17ba10149ac898663e598069b9055", "sha": "c167f3901bc24c2300891485d9020960ac99d682", "filename": "tasks/autoupdate-RedHat.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\n- name: Install yum-cron.\n  yum: name=yum-cron state=present\n\n- name: Ensure yum-cron is running and enabled on boot.\n  service: name=yum-cron state=started enabled=yes\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "95dfe1969cfde2d93a87f46a94a7778a2b940569", "filename": "roles/ipareplica/tasks/uninstall.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "---\n# tasks to uninstall IPA replica\n\n# - name: Uninstall - Include Python2/3 import test\n#   import_tasks: \"{{ role_path }}/tasks/python_2_3_test.yml\"\n\n- name: Uninstall - Uninstall IPA replica\n  command: >\n    /usr/sbin/ipa-server-install\n    --uninstall\n    -U\n    {{ \"--ignore-topology-disconnect\" if\n       ipareplica_ignore_topology_disconnect | bool else \"\" }}\n    {{ \"--ignore-last-of-role\" if ipareplica_ignore_last_of_role | bool\n       else \"\" }}\n  register: result_uninstall\n  # 2 means that uninstall failed because IPA replica was not configured\n  failed_when: result_uninstall.rc != 0 and \"'Env' object\n    has no attribute 'basedn'\" not in result_uninstall.stderr\n  # IPA server is not configured on this system\" not in\n  #   result_uninstall.stdout_lines\n  changed_when: result_uninstall.rc == 0\n  # until: result_uninstall.rc == 0\n  retries: 2\n  delay: 1\n\n#- name: Uninstall - Remove all replication agreements and data about replica\n#  command: >\n#    /usr/sbin/ipa-replica-manage\n#    del\n#    {{ ipareplica_hostname | default(ansible_fqdn) }}\n#    --force\n#    --password={{ ipadm_password }}\n#  failed_when: False\n#  delegate_to: \"{{ groups.ipaserver[0] | default(fail) }}\"\n\n# - name: Remove IPA replica packages\n#   package:\n#     name: \"{{ item }}\"\n#     state: absent\n#   with_items: \"{{ ipareplica_packages }}\"\n"}, {"commit_sha": "8d4956fcd97d78caa57ee3e5a36e9c44a23ab2a6", "sha": "f28c8f28eaf65389acd16066b3c15eb6dbb1d844", "filename": "tasks/setup-repository-Ubuntu.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Update APT cache block\n  block:\n    - name: Update APT cache\n      become: true\n      apt:\n        update_cache: yes\n      changed_when: false\n      register: _pkg_result\n      until: _pkg_result is succeeded\n      when:\n        - docker_network_access | bool\n  rescue:\n    - name: Retry APT cache update with allow-releaseinfo-change\n      become: true\n      command: apt-get update --allow-releaseinfo-change\n      args:\n        warn: false\n      changed_when: false\n      register: _pkg_result\n      until: _pkg_result is succeeded\n      when:\n        - docker_network_access | bool\n\n- name: Ensure packages are installed for repository setup\n  become: true\n  package:\n    name: \"{{ item }}\"\n    state: present\n  loop: \"{{ docker_repository_related_packages[_docker_os_dist] }}\"\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when:\n    - docker_network_access | bool\n\n- name: Add Docker official GPG key\n  become: true\n  apt_key:\n    url: https://download.docker.com/linux/{{ _docker_os_dist|lower }}/gpg\n    state: present\n  register: _pkg_result\n  until: _pkg_result is succeeded\n  when:\n    - docker_network_access | bool\n    - (_docker_os_dist == \"Ubuntu\" and _docker_os_dist_major_version | int > 14) or\n      (_docker_os_dist == \"Debian\" and _docker_os_dist_major_version | int > 7)\n\n- name: Add Docker APT key (alternative for older Ubuntu systems without SNI).\n  become: true\n  shell: \"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\"\n  args:\n    warn: false\n  changed_when: false\n  when:\n    - docker_network_access | bool\n    - (_docker_os_dist == \"Ubuntu\" and _docker_os_dist_major_version | int == 14) or\n      (_docker_os_dist == \"Debian\" and  _docker_os_dist_major_version | int == 7)\n  tags:\n    - skip_ansible_lint\n\n- name: Determine channels to be enabled and/or disabled\n  set_fact:\n    _docker_disable_channels: \"{{ docker_channels | difference(_docker_merged_channels) }}\"\n    _docker_enable_channels: \"{{ docker_channels | intersect(_docker_merged_channels) }}\"\n  vars:\n    _docker_mandatory_channel: []\n    _docker_merged_channels: \"{{ _docker_mandatory_channel }} + [ '{{ docker_channel }}' ]\"\n\n- name: Add Docker CE repository with correct channels (Ubuntu/Debian)\n  become: true\n  apt_repository:\n    repo: \"deb [arch={{ _docker_os_arch|lower }}] https://download.docker.com/linux/{{ _docker_os_dist|lower }} \\\n      {{ ansible_lsb.codename }} {{ _docker_enable_channels | join(' ') }}\"\n    state: present\n    filename: 'docker-ce'\n"}, {"commit_sha": "a10c5f4577e6e74feb1fadec4bcbab039b8b180a", "sha": "c926f2bc6c2bfe152ccba97cc8b5c626b1b137da", "filename": "tasks/bug-tweaks/lvm-thinpool.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Ensure lvm2 is installed\n  become: true\n  package:\n    name: lvm2\n    state: present\n  register: _pkg_result\n  until: _pkg_result|succeeded\n\n- name: Create LVM volume group\n  become: true\n  lvg:\n    pvs: '{{ pool.physical_volumes }}'\n    state: present\n    vg: '{{ pool.volume_group }}'\n  when: pool.physical_volumes|default(None)\n\n- name: Check if data volume exists\n  become: true\n  stat:\n    path: '/dev/mapper/{{ pool.volume_group }}-{{ pool.name }}'\n  ignore_errors: true\n  register: _volume\n\n- name: Create data volume\n  become: true\n  lvol:\n    lv: '{{ pool.name }}'\n    size: '{{ pool.data_size }}'\n    vg: '{{ pool.volume_group }}'\n  register: _datavolume_created\n  when: not _volume.stat.exists\n\n- name: Create meta data volume\n  become: true\n  lvol:\n    lv: '{{ pool.name }}meta'\n    size: '{{ pool.metadata_size }}'\n    vg: '{{ pool.volume_group }}'\n  when: _datavolume_created | changed\n\n- name: Convert data volume to thinpool\n  become: true\n  shell:\n    lvconvert\n        -y\n        --zero n\n        -c 512K\n        --thinpool \"{{ pool.volume_group }}/{{ pool.name }}\"\n        --poolmetadata \"{{ pool.volume_group }}/{{ pool.name }}meta\"\n  when: _datavolume_created | changed\n  tags:\n    - skip_ansible_lint"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "d84f899b9f5d3316d6bd13c6d32f9f27bf1574d0", "filename": "roles/ipaclient/tasks/install.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "---\n# tasks file for ipaclient\n\n- name: Install - Ensure that IPA client packages are installed\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items: \"{{ ipaclient_packages }}\"\n  when: ipaclient_install_packages | bool\n\n#- name: Install - Include Python2/3 import test\n#  import_tasks: \"{{ role_path }}/tasks/python_2_3_test.yml\"\n\n- name: Install - Set ipaclient_servers\n  set_fact:\n    ipaclient_servers: \"{{ groups['ipaservers'] | list }}\"\n  when: groups.ipaservers is defined and ipaclient_servers is not defined\n\n- name: Install - Set ipaclient_servers from cluster inventory\n  set_fact:\n    ipaclient_servers: \"{{ groups['ipaserver'] | list }}\"\n  when: ipaclient_no_dns_lookup | bool and groups.ipaserver is defined and\n        ipaclient_servers is not defined\n\n- name: Install - Check that either principal or keytab is set\n  fail: msg=\"ipaadmin_principal and ipaadmin_keytab cannot be used together\"\n  when: ipaadmin_keytab is defined and ipaadmin_principal is defined\n\n- name: Install - Set default principal if no keytab is given\n  set_fact:\n    ipaadmin_principal: admin\n  when: ipaadmin_principal is undefined and ipaclient_keytab is undefined\n\n- name: Install - IPA client test\n  ipaclient_test:\n    ### basic ###\n    domain: \"{{ ipaserver_domain | default(ipaclient_domain) | default(omit) }}\"\n    servers: \"{{ ipaclient_servers | default(omit) }}\"\n    realm: \"{{ ipaserver_realm | default(ipaclient_realm) | default(omit) }}\"\n    hostname: \"{{ ipaclient_hostname | default(ansible_fqdn) }}\"\n    ntp_servers: \"{{ ipaclient_ntp_servers | default(omit) }}\"\n    ntp_pool: \"{{ ipaclient_ntp_pool | default(omit) }}\"\n    no_ntp: \"{{ ipaclient_no_ntp }}\"\n    force_ntpd: \"{{ ipaclient_force_ntpd }}\"\n    nisdomain: \"{{ ipaclient_nisdomain | default(omit) }}\"\n    no_nisdomain: \"{{ ipaclient_no_nisdomain }}\"\n    kinit_attempts: \"{{ ipaclient_kinit_attempts }}\"\n    ca_cert_files: \"{{ ipaclient_ca_cert_file | default(omit) }}\"\n    configure_firefox: \"{{ ipaclient_configure_firefox }}\"\n    firefox_dir: \"{{ ipaclient_firefox_dir | default(omit) }}\"\n    ip_addresses: \"{{ ipaclient_ip_addresses | default(omit) }}\"\n    all_ip_addresses: \"{{ ipaclient_all_ip_addresses }}\"\n    on_master: \"{{ ipaclient_on_master }}\"\n    ### sssd ###\n    enable_dns_updates: \"{{ ipassd_enable_dns_updates }}\"\n  register: result_ipaclient_test\n\n- block:\n  - name: Install - Cleanup leftover ccache\n    file:\n      path: \"/etc/ipa/.dns_ccache\"\n      state: absent\n\n  - name: Install - Configure NTP\n    ipaclient_setup_ntp:\n      ### basic ###\n      ntp_servers: \"{{ ipaclient_ntp_servers | default(omit) }}\"\n      ntp_pool: \"{{ ipaclient_ntp_pool | default(omit) }}\"\n      no_ntp: \"{{ ipaclient_no_ntp }}\"\n      # force_ntpd: \"{{ ipaclient_force_ntpd }}\"\n      on_master: \"{{ ipaclient_on_master }}\"\n      ### additional ###\n      servers: \"{{ result_ipaclient_test.servers }}\"\n      domain: \"{{ result_ipaclient_test.domain }}\"\n\n  - name: Install - Disable One-Time Password for on_master\n    set_fact:\n      ipaclient_use_otp: \"no\"\n    when: ipaclient_use_otp | bool and ipaclient_on_master | bool\n\n  - name: Install - Test if IPA client has working krb5.keytab\n    ipaclient_test_keytab:\n      servers: \"{{ result_ipaclient_test.servers }}\"\n      domain: \"{{ result_ipaclient_test.domain }}\"\n      realm: \"{{ result_ipaclient_test.realm }}\"\n      hostname: \"{{ result_ipaclient_test.hostname }}\"\n      kdc: \"{{ result_ipaclient_test.kdc }}\"\n      kinit_attempts: \"{{ ipaclient_kinit_attempts | default(omit) }}\"\n    register: result_ipaclient_test_keytab\n\n  - name: Install - Disable One-Time Password for client with working\n          krb5.keytab\n    set_fact:\n      ipaclient_use_otp: \"no\"\n    when: ipaclient_use_otp | bool and\n          result_ipaclient_test_keytab.krb5_keytab_ok and\n          not ipaclient_force_join | bool\n\n  # The following block is executed when using OTP to enroll IPA client\n  # ie when ipaclient_use_otp is set.\n  # It connects to ipaserver and add the host with --random option in order\n  # to create a OneTime Password\n  # If a keytab is specified in the hostent, then the hostent will be disabled\n  # if ipaclient_use_otp is set.\n  - block:\n    - name: Install - Keytab or password is required for otp\n      fail: msg=\"Keytab or password is required for otp\"\n      when: ipaadmin_keytab is undefined and ipaadmin_password is undefined\n\n    - name: Install - Save client ansible_python_interpreter setting\n      set_fact:\n        ipaclient_ansible_python_interpreter: \"{{ ansible_python_interpreter }}\"\n\n    #- name: Install - Include Python2/3 import test\n    #  import_tasks: \"{{ role_path }}/tasks/python_2_3_test.yml\"\n    #  delegate_to: \"{{ result_ipaclient_test.servers[0] }}\"\n\n    - name: Install - Get One-Time Password for client enrollment\n      no_log: yes\n      ipaclient_get_otp:\n        state: present\n        principal: \"{{ ipaadmin_principal | default('admin') }}\"\n        password: \"{{ ipaadmin_password | default(omit) }}\"\n        keytab: \"{{ ipaadmin_keytab | default(omit) }}\"\n        fqdn: \"{{ result_ipaclient_test.hostname }}\"\n        lifetime: \"{{ ipaclient_lifetime | default(omit) }}\"\n        random: True\n        ansible_python_interpreter: \"{{ ansible_python_interpreter }}\"\n      register: result_ipaclient_get_otp\n      # If the host is already enrolled, this command will exit on error\n      # The error can be ignored\n      failed_when: result_ipaclient_get_otp is failed and\n                   \"Password cannot be set on enrolled host\" not\n                       in result_ipaclient_get_otp.msg\n      delegate_to: \"{{ result_ipaclient_test.servers[0] }}\"\n      delegate_facts: yes\n\n    - name: Install - Store the previously obtained OTP\n      no_log: yes\n      set_fact:\n        ipaadmin_orig_password: \"{{ ipaadmin_password }}\"\n        ipaadmin_password: \"{{ result_ipaclient_get_otp.host.randompassword\n                               if result_ipaclient_get_otp.host is defined }}\"\n\n    - name: Install - Restore client ansible_python_interpreter setting\n      set_fact:\n        ansible_python_interpreter: \"{{ ipaclient_ansible_python_interpreter }}\"\n\n    when: ipaclient_use_otp | bool\n\n  - block:\n    # This block is executed only when\n    # not (not ipaclient_on_master | bool and\n    #      not result_ipaclient_join.changed and\n    #      not ipaclient_allow_repair | bool and\n    #      (result_ipaclient_test_keytab.krb5_keytab_ok or\n    #       (result_ipaclient_join.already_joined is defined and\n    #        result_ipaclient_join.already_joined)))\n\n    - name: Install - Check if principal and keytab are set\n      fail: msg=\"Principal and keytab cannot be used together\"\n      when: ipaadmin_principal is defined and ipaadmin_principal|length > 0\n            and ipaclient_keytab is defined and ipaclient_keytab|length > 0\n\n    - name: Install - Check if one of password and keytab are set\n      fail: msg=\"At least one of password or keytab must be specified\"\n      when: not result_ipaclient_test_keytab.krb5_keytab_ok and\n            (ipaadmin_password is undefined or ipaadmin_password|length == 0)\n            and (ipaclient_keytab is undefined or ipaclient_keytab|length == 0)\n    when: not ipaclient_on_master | bool\n\n  - name: Install - Purge {{ result_ipaclient_test.realm }} from host keytab\n    command: >\n      /usr/sbin/ipa-rmkeytab\n      -k /etc/krb5.keytab\n      -r \"{{ result_ipaclient_test.realm }}\"\n    register: result_ipa_rmkeytab\n    # Do not fail on error codes 3 and 5:\n    #   3 - Unable to open keytab\n    #   5 - Principal name or realm not found in keytab\n    failed_when: result_ipa_rmkeytab.rc != 0 and\n                 result_ipa_rmkeytab.rc != 3 and result_ipa_rmkeytab.rc != 5\n    when: ipaclient_use_otp | bool or ipaclient_force_join | bool\n\n  - name: Install - Backup and set hostname\n    ipaclient_set_hostname:\n      hostname: \"{{ result_ipaclient_test.hostname }}\"\n    when: not ipaclient_on_master | bool\n\n  - name: Install - Join IPA\n    ipaclient_join:\n      servers: \"{{ result_ipaclient_test.servers }}\"\n      domain: \"{{ result_ipaclient_test.domain }}\"\n      realm: \"{{ result_ipaclient_test.realm }}\"\n      kdc: \"{{ result_ipaclient_test.kdc }}\"\n      basedn: \"{{ result_ipaclient_test.basedn }}\"\n      hostname: \"{{ result_ipaclient_test.hostname }}\"\n      force_join: \"{{ ipaclient_force_join | default(omit) }}\"\n      principal: \"{{ ipaadmin_principal if not ipaclient_use_otp | bool and\n                     ipaclient_keytab is not defined else '' }}\"\n      password: \"{{ ipaadmin_password | default(omit) }}\"\n      keytab: \"{{ ipaclient_keytab | default(omit) }}\"\n      # ca_cert_file: \"{{ ipaclient_ca_cert_file | default(omit) }}\"\n      kinit_attempts: \"{{ ipaclient_kinit_attempts | default(omit) }}\"\n    register: result_ipaclient_join\n    when: not ipaclient_on_master | bool and\n          (not result_ipaclient_test_keytab.krb5_keytab_ok or\n              ipaclient_force_join)\n\n  - block:\n    - fail:\n        msg: >\n          The krb5 configuration is not correct, please enable allow_repair\n          to fix this.\n      when: not result_ipaclient_test_keytab.krb5_conf_ok\n    - fail:\n        msg: \"The IPA test failed, please enable allow_repair to fix this.\"\n      when: not result_ipaclient_test_keytab.ping_test_ok\n    - fail:\n        msg: >\n          The ca.crt file is missing, please enable allow_repair to fix this.\n      when: not result_ipaclient_test_keytab.ca_crt_exists\n    when: not ipaclient_on_master | bool and\n          not result_ipaclient_join.changed and\n          not ipaclient_allow_repair | bool and\n          (result_ipaclient_test_keytab.krb5_keytab_ok or\n              (result_ipaclient_join.already_joined is defined and\n                  result_ipaclient_join.already_joined))\n\n  - block:\n    - name: Install - Configure IPA default.conf\n      ipaclient_ipa_conf:\n        servers: \"{{ result_ipaclient_test.servers }}\"\n        domain: \"{{ result_ipaclient_test.domain }}\"\n        realm: \"{{ result_ipaclient_test.realm }}\"\n        hostname: \"{{ result_ipaclient_test.hostname }}\"\n        basedn: \"{{ result_ipaclient_test.basedn }}\"\n      when: not ipaclient_on_master | bool\n\n    - name: Install - Configure SSSD\n      ipaclient_setup_sssd:\n        servers: \"{{ result_ipaclient_test.servers }}\"\n        domain: \"{{ result_ipaclient_test.domain }}\"\n        realm: \"{{ result_ipaclient_test.realm }}\"\n        hostname: \"{{ result_ipaclient_test.hostname }}\"\n        on_master: \"{{ ipaclient_on_master }}\"\n        no_ssh: \"{{ ipaclient_no_ssh }}\"\n        no_sshd: \"{{ ipaclient_no_sshd }}\"\n        no_sudo: \"{{ ipaclient_no_sudo }}\"\n        all_ip_addresses: \"{{ ipaclient_all_ip_addresses }}\"\n        fixed_primary: \"{{ ipassd_fixed_primary }}\"\n        permit: \"{{ ipassd_permit }}\"\n        enable_dns_updates: \"{{ ipassd_enable_dns_updates }}\"\n        preserve_sssd: \"{{ ipassd_preserve_sssd }}\"\n        no_krb5_offline_passwords: \"{{ ipassd_no_krb5_offline_passwords }}\"\n\n    - name: Install - Configure krb5 for IPA realm\n      ipaclient_setup_krb5:\n        realm: \"{{ result_ipaclient_test.realm }}\"\n        domain: \"{{ result_ipaclient_test.domain }}\"\n        servers: \"{{ result_ipaclient_test.servers }}\"\n        kdc: \"{{ result_ipaclient_test.kdc }}\"\n        dnsok: \"{{ result_ipaclient_test.dnsok }}\"\n        client_domain: \"{{ result_ipaclient_test.client_domain }}\"\n        hostname: \"{{ result_ipaclient_test.hostname }}\"\n        sssd: \"{{ result_ipaclient_test.sssd }}\"\n        force: \"{{ ipaclient_force }}\"\n        # on_master: \"{{ ipaclient_on_master }}\"\n      when: not ipaclient_on_master | bool\n\n    - name: Install - IPA API calls for remaining enrollment parts\n      ipaclient_api:\n        servers: \"{{ result_ipaclient_test.servers }}\"\n        realm: \"{{ result_ipaclient_test.realm }}\"\n        hostname: \"{{ result_ipaclient_test.hostname }}\"\n        # debug: yes\n      register: result_ipaclient_api\n\n    - name: Install - Fix IPA ca\n      ipaclient_fix_ca:\n        servers: \"{{ result_ipaclient_test.servers }}\"\n        realm: \"{{ result_ipaclient_test.realm }}\"\n        basedn: \"{{ result_ipaclient_test.basedn }}\"\n        allow_repair: \"{{ ipaclient_allow_repair }}\"\n      when: not ipaclient_on_master | bool and\n            result_ipaclient_test_keytab.krb5_keytab_ok and\n            not result_ipaclient_test_keytab.ca_crt_exists\n\n    - name: Install - Create IPA NSS database\n      ipaclient_setup_nss:\n        servers: \"{{ result_ipaclient_test.servers }}\"\n        domain: \"{{ result_ipaclient_test.domain }}\"\n        realm: \"{{ result_ipaclient_test.realm }}\"\n        basedn: \"{{ result_ipaclient_test.basedn }}\"\n        hostname: \"{{ result_ipaclient_test.hostname }}\"\n        subject_base: \"{{ result_ipaclient_api.subject_base }}\"\n        principal: \"{{ ipaadmin_principal | default(omit) }}\"\n        mkhomedir: \"{{ ipaclient_mkhomedir }}\"\n        ca_enabled: \"{{ result_ipaclient_api.ca_enabled }}\"\n        on_master: \"{{ ipaclient_on_master }}\"\n        enable_dns_updates: \"{{ ipassd_enable_dns_updates }}\"\n        all_ip_addresses: \"{{ ipaclient_all_ip_addresses }}\"\n        ip_addresses: \"{{ ipaclient_ip_addresses | default(omit) }}\"\n        request_cert: \"{{ ipaclient_request_cert }}\"\n        preserve_sssd: \"{{ ipassd_preserve_sssd }}\"\n        no_ssh: \"{{ ipaclient_no_ssh }}\"\n        no_sshd: \"{{ ipaclient_no_sshd }}\"\n        no_sudo: \"{{ ipaclient_no_sudo }}\"\n        fixed_primary: \"{{ ipassd_fixed_primary }}\"\n        permit: \"{{ ipassd_permit }}\"\n        no_krb5_offline_passwords: \"{{ ipassd_no_krb5_offline_passwords }}\"\n        no_dns_sshfp: \"{{ ipaclient_no_dns_sshfp }}\"\n\n    - name: Install - Configure SSH and SSHD\n      ipaclient_setup_ssh:\n        servers: \"{{ result_ipaclient_test.servers }}\"\n        sssd: \"{{ result_ipaclient_test.sssd }}\"\n        no_ssh: \"{{ ipaclient_no_ssh }}\"\n        ssh_trust_dns: \"{{ ipaclient_ssh_trust_dns }}\"\n        no_sshd: \"{{ ipaclient_no_sshd }}\"\n\n    - name: Install - Configure automount\n      ipaclient_setup_automount:\n        servers: \"{{ result_ipaclient_test.servers }}\"\n        sssd: \"{{ result_ipaclient_test.sssd }}\"\n        automount_location: \"{{ ipaautomount_location | default(omit) }}\"\n\n    - name: Install - Configure firefox\n      ipaclient_setup_firefox:\n        firefox_dir: \"{{ ipaclient_firefox_dir | default(omit) }}\"\n      when: ipaclient_configure_firefox | bool\n\n    - name: Install - Configure NIS\n      ipaclient_setup_nis:\n        domain: \"{{ result_ipaclient_test.domain }}\"\n        nisdomain: \"{{ ipaclient_nisdomain | default(omit) }}\"\n      when: not ipaclient_no_nisdomain | bool\n\n    when: not (not ipaclient_on_master | bool and\n          not result_ipaclient_join.changed and\n          not ipaclient_allow_repair | bool\n              and (result_ipaclient_test_keytab.krb5_keytab_ok\n              or (result_ipaclient_join.already_joined is defined\n              and result_ipaclient_join.already_joined)))\n\n  when: not ansible_check_mode and\n        not (result_ipaclient_test.client_already_configured and\n            not ipaclient_allow_repair | bool and not ipaclient_force_join | bool)\n\n  always:\n  - name: Install - Restore original admin password if overwritten by OTP\n    no_log: yes\n    set_fact:\n        ipaadmin_password: \"{{ ipaadmin_orig_password }}\"\n    when: ipaclient_use_otp | bool and ipaadmin_orig_password is defined\n\n  - name: Cleanup leftover ccache\n    file:\n      path: \"/etc/ipa/.dns_ccache\"\n      state: absent\n"}, {"commit_sha": "1471601bb120a0e15aa0a66e608985830b4c083e", "sha": "189e7e7705ea86f103158fcf83d0d7f6deeec7e9", "filename": "roles/ovirt-engine-remote-db/tasks/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\n# main file for remote DB task\n# based on https://fedoraproject.org/wiki/PostgreSQL\n\n- name: Include postgres params\n  include_vars: default.yml\n\n- name: Override postgres params for CentOs or Red Hat when ovirt >= 4.2\n  include_vars: postgres95.yml\n  when:\n    - ovirt_engine_version >= '4.2'\n    - ansible_distribution in ('CentOS', 'Red Hat')\n\n# install libselinux-python on machine - selinux policy\n- name: install SELinux requirements to run ansible modules managing SELinux.\n  yum:\n    name: \"{{ item }}\"\n    state: \"present\"\n  with_items:\n    - libselinux-python\n    - policycoreutils-python\n\n- name: install psycopg2 requirements to run ansible modules managing postgres.\n  yum:\n    name: \"python-psycopg2\"\n    state: \"present\"\n\n- name: check PostgreSQL service\n  service:\n    name: \"{{ postgres_service_name }}\"\n    state: started\n  register: postgresql_status\n  ignore_errors: True\n\n- name: yum install PostgreSQL\n  yum:\n    name: \"{{ postgres_server }}\"\n    state: installed\n    update_cache: yes\n  when: postgresql_status|failed\n\n- name: enable sudo without tty\n  lineinfile:\n    path: /etc/sudoers\n    state: present\n    regexp: '^Defaults *requiretty$'\n    line: 'Defaults    !requiretty'\n  when: postgresql_status|failed\n\n- name: scl enable\n  shell: 'scl enable rh-postgresql95 bash'\n  when:\n    - postgresql_status|failed\n    - ovirt_engine_version >= '4.2'\n    - ansible_distribution in ('CentOS', 'Red Hat')\n  tags:\n    - skip_ansible_lint\n\n- name: run PostgreSQL DB config\n  become_user: postgres\n  become: yes\n  shell: '{{ postgres_setup_cmd }}'\n  args:\n    creates: \"{{ postgres_config_file }}\"\n  when: ovirt_engine_version < '4.2'\n  tags:\n    - skip_ansible_lint\n\n- name: run PostgreSQL DB config\n  shell: '{{ postgres_setup_cmd }}'\n  args:\n    creates: \"{{ postgres_config_file }}\"\n  when: ovirt_engine_version >= '4.2'\n  tags:\n    - skip_ansible_lint\n\n- name: start PostgreSQL service\n  service:\n    name: \"{{ postgres_service_name }}\"\n    state: started\n    enabled: yes\n\n# allow access engine database access from outside\n- name: \"update pg_hba.conf to allow connection for ovirt_engine_remote_db\"\n  lineinfile:\n    dest: '{{ postgres_data_dir }}/pg_hba.conf'\n    line: >\n      {{ item.type }} {{ ovirt_engine_db_name }} {{ ovirt_engine_db_user }}\n      {{ item.address | default(' ') }} {{ item.method }}\n    insertafter: EOF\n  with_items: \"{{ ovirt_engine_remote_db_access | list }}\"\n  when: ovirt_engine_remote_db == True\n\n# allow access engine dwh database access from outside\n- name: \"update pg_hba.conf to allow connection for ovirt_engine_dwh_remote_db\"\n  lineinfile:\n    dest: '{{ postgres_data_dir }}/pg_hba.conf'\n    line: >\n      {{ item.type }} {{ ovirt_engine_dwh_db_name }}\n      {{ ovirt_engine_dwh_db_user }} {{ item.address | default(' ') }}\n      {{ item.method }}\n    insertafter: EOF\n  with_items: \"{{ ovirt_engine_remote_db_access | list }}\"\n  when: ovirt_engine_dwh_remote_db == True\n\n# listen on specific address\n- name: update postgresql.conf -> listen_addresses='*'\n  lineinfile:\n    dest: \"{{ postgres_config_file }}\"\n    regexp: \"^listen_addresses *=.*$\"\n    line: \"listen_addresses='{{ovirt_engine_remote_db_listen_address}}'\"\n    insertafter: EOF\n  when: postgresql_status|failed\n\n# listen on specific port\n- name: update postgresql.conf -> port number\n  lineinfile:\n    dest: \"{{ postgres_config_file }}\"\n    regexp: \"^port *=.*$\"\n    line: \"port={{ ovirt_engine_remote_db_port }}\"\n    insertafter: EOF\n  when: postgresql_status|failed and ovirt_engine_remote_db_port != 5432\n\n# postgresql.conf: (el7)\n# Note: In RHEL/Fedora installations, you can't set the port number here;\n#   adjust it in the service file instead.\n#   /usr/lib/systemd/system/postgresql.service\n#    - Environment=PGPORT=5432\n- name: update postgresql.conf -> port number in service file (Fedora & RHEL)\n  lineinfile:\n    dest: '/usr/lib/systemd/system/{{ postgres_service_name }}.service'\n    backrefs: yes\n    regexp: \"^Environment=PGPORT *=.*$\"\n    line: \"Environment=PGPORT={{ ovirt_engine_remote_db_port }}\"\n  register: port_update\n  when: postgresql_status|failed and ovirt_engine_remote_db_port != 5432\n  ignore_errors: True\n\n# daemon reload - service file was changed\n- name: systemctl daemon-reload (el7)\n  shell: 'systemctl daemon-reload'\n  when: postgresql_status|failed and ovirt_engine_remote_db_port != 5432 and port_update|success\n  tags:\n    - skip_ansible_lint\n\n# el6 use only service (systemctl not present)\n- name: update postgresql.conf -> port number in service file (el6)\n  lineinfile:\n    dest: '/etc/init.d/postgresql'\n    backrefs: yes\n    regexp: \"^PGPORT *=.*$\"\n    line: \"PGPORT={{ ovirt_engine_remote_db_port }}\"\n  when: postgresql_status|failed and ovirt_engine_remote_db_port != 5432 and port_update|failed\n  ignore_errors: True\n\n# Required for vacuum feature\n- name: set vacuum configuration for postgresql\n  ini_file:\n    path: \"{{ postgres_config_file }}\"\n    option: \"{{ item.key }}\"\n    value: \"{{ item.value }}\"\n    section: null\n  with_dict: \"{{ ovirt_engine_remote_db_vacuum_config }}\"\n  when: ovirt_engine_remote_db_vacuum\n\n# allow selinux for postgresql non-standard port\n- name: allow selinux for non-standard port\n  seport:\n    ports: \"{{ ovirt_engine_remote_db_port }}\"\n    proto: \"tcp\"\n    setype: \"postgresql_port_t\"\n    state: present\n  when: postgresql_status|failed and ovirt_engine_remote_db_port != 5432\n  ignore_errors: True\n\n# first check of PostgreSQL - if fail, setup\n- name: PostgreSQL reload configuration\n  service:\n    name: \"{{ postgres_service_name }}\"\n    state: restarted\n\n- name: check iptables service\n  service:\n    name: iptables\n    state: started\n  register: iptables_status\n  when: postgresql_status|failed\n  ignore_errors: True\n\n- name: open port for PostgreSQL in iptables\n  shell: \"iptables -I INPUT -p tcp -m state --state NEW -m tcp --dport {{ovirt_engine_remote_db_port}} -j ACCEPT\"\n  when: postgresql_status|failed and not iptables_status|failed\n  tags:\n    - skip_ansible_lint\n\n- name: save iptables rules\n  shell: \"/sbin/iptables-save\"\n  when: postgresql_status|failed and not iptables_status|failed\n  tags:\n    - skip_ansible_lint\n\n- name: check firewalld service\n  service:\n    name: firewalld\n    state: started\n  register: firewalld_status\n  when: postgresql_status|failed\n  ignore_errors: True\n\n- name: open port for PostgreSQL in firewalld\n  firewalld:\n    port: \"{{ovirt_engine_remote_db_port|int}}/tcp\"\n    permanent: True\n    state: enabled\n  when: postgresql_status|failed and not firewalld_status|failed\n\n- name: reload firewalld\n  shell: \"firewall-cmd --reload\"\n  when: postgresql_status|failed and not firewalld_status|failed\n  tags:\n    - skip_ansible_lint\n\n- name: create DWH DB user\n  become: true\n  become_user: postgres\n  postgresql_user:\n    name: \"{{ item.user }}\"\n    password: \"{{ item.password }}\"\n  with_items:\n    - user: \"{{ ovirt_engine_db_user }}\"\n      password: \"{{ ovirt_engine_db_password }}\"\n    - user: \"{{ ovirt_engine_dwh_db_user }}\"\n      password: \"{{ ovirt_engine_dwh_db_password }}\"\n  when: ovirt_engine_dwh_remote_db == True\n\n- name: create engine & DWH DBs\n  become: true\n  become_user: postgres\n  postgresql_db:\n    name: \"{{ item.db_name }}\"\n    owner: \"{{ item.user }}\"\n    encoding: UTF-8\n    lc_collate: en_US.UTF-8\n    lc_ctype: en_US.UTF-8\n    template: template0\n  with_items:\n    - db_name: \"{{ ovirt_engine_db_name }}\"\n      user: \"{{ ovirt_engine_db_user }}\"\n    - db_name: \"{{ ovirt_engine_dwh_db_name }}\"\n      user: \"{{ ovirt_engine_dwh_db_user }}\"\n  when: ovirt_engine_dwh_remote_db == True\n\n- name: check PostgreSQL service\n  service:\n    name: \"{{ postgres_service_name }}\"\n    state: started\n    enabled: yes\n\n- name: clean tmp files\n  file:\n    path: '/tmp/ansible-sql'\n    state: 'absent'\n"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "3e08995cfe301f3a43cc3cf174032d2cd5d2eaab", "filename": "tasks/system/Win32NT.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Install {{ java_package }} from chocolatey\"\n  win_chocolatey:\n    name: \"{{ java_package }}{{ java_major_version }}\"\n    params: \"/InstallationPath:C:\\\\Program Files\\\\Java /Force /Machine\"\n  when: transport == \"win-chocolatey\"\n\n- name: \"Non-chocolatey installation\"\n  block:\n\n    - name: \"Path to the remote installation package\"\n      set_fact:\n        java_artifact: \"{{ windows_temp_path }}\\\n{{ oracle_artifact | win_basename }}\"\n\n    - name: \"Copy installation package to the remote\"\n      win_copy:\n        src: \"{{ oracle_artifact }}\"\n        dest: \"{{ java_artifact }}\"\n\n    - name: \"Install package from the remote source\"\n      win_package:\n        path: \"{{ java_artifact }}\"\n        state: \"present\"\n        creates_path: \"C:\\\\Program Files\\\\Java\"\n\n    - name: \"Get installed java directory\"\n      win_shell: \"Get-ChildItem -Path 'C:\\\\Program Files\\\\Java' |\\\n                  Select-Object -Last 1 -ExpandProperty FullName\"\n      register: java_folder\n      failed_when: not java_folder.stdout_lines[0]\n\n    - name: \"Set java environment variable\"\n      win_environment:\n        name: \"JAVA_HOME\"\n        state: \"present\"\n        value: \"{{ java_folder.stdout_lines[0] }}\"\n        level: \"machine\"\n\n    - name: \"Ensure that 'JAVA_HOME/bin' present in 'Path' variable\"\n      win_path:\n        elements: \"{{ java_folder.stdout_lines[0] }}\\\\bin\"\n        state: \"present\"\n        scope: \"machine\"\n\n  when: transport != \"win-chocolatey\"\n"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "520f4a55a5641739f764b9560f45afdce787f2b7", "filename": "tasks/system/Linux.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Load a variable file based on distribution or OS family\"\n  include_vars: \"{{ item }}\"\n  with_first_found:\n    - \"{{ ansible_distribution }}.yml\"\n    - \"{{ ansible_os_family }}.yml\"\n    - \"default.yml\"\n\n- name: \"Become section\"\n  block:\n    - name: \"Install requirements\"\n      package:\n        name: \"{{ requirements }}\"\n        state: \"present\"\n\n    - name: \"Mkdir for java installation\"\n      file:\n        path: \"{{ java_path }}\"\n        state: \"directory\"\n        owner: \"root\"\n        group: \"root\"\n        mode: \"0755\"\n\n    - name: \"Install java {{ java_major_version }}.{{ java_minor_version }}\"\n      unarchive:\n        src: \"{{ oracle_artifact }}\"\n        dest: \"{{ java_path }}\"\n        owner: \"root\"\n        group: \"root\"\n        mode: \"0755\"\n        creates: \"{{ java_path }}{{ java_package }}1.{{ java_major_version }}.0_{{ java_minor_version }}/bin/\"\n\n    - name: \"Check for java binaries existence\"\n      stat:\n        path: \"{{ java_path }}{{ java_package }}1.{{ java_major_version }}.0_{{ java_minor_version }}/bin/{{ binary }}\"\n      register: java_binary_collection\n      loop:\n        - \"java\"\n        - \"javac\"\n        - \"jar\"\n      loop_control:\n        loop_var: binary\n\n    - name: \"Update alternatives\"\n      alternatives:\n        name: \"{{ item.binary }}\"\n        path: \"{{ java_path }}{{ java_package }}1.{{ java_major_version }}.0_{{ java_minor_version }}/bin/{{ item.binary }}\"\n        link: \"/usr/bin/{{ item.binary }}\"\n        priority: 100\n      with_items: \"{{ java_binary_collection.results }}\"\n      when: item.stat.exists\n\n    - name: \"Put java profile\"\n      template:\n        src: \"java.sh.j2\"\n        dest: \"/etc/profile.d/java.sh\"\n        owner: \"root\"\n        group: \"root\"\n        mode: \"0555\"\n\n  become: True\n"}, {"commit_sha": "45971be8249cc4627ef8ddfacf55a661b7fc13ca", "sha": "d5d3aba5ca8cf654c78865d5530d9d3c6cc1e21e", "filename": "tasks/configure-systemd.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "- name: Combine all systemd service configuration options\n  set_fact:\n    _systemd_service_config: \"{{ docker_systemd_service_config_tweaks + docker_systemd_service_config }}\"\n\n- name: Ensure /etc/systemd/system/docker.service.d directory exists\n  file:\n    path: /etc/systemd/system/docker.service.d\n    state: directory\n    mode: 0755\n  become: yes\n\n- name: Setup default Docker drop-in to enable use of environment file\n  template:\n    src: drop-ins/default.conf.j2\n    dest: /etc/systemd/system/docker.service.d/default.conf\n  become: yes\n  notify: restart docker\n  register: _systemd_docker_dropin\n  vars:\n    systemd_envs_dir: \"{{ docker_envs_dir[_docker_os_dist] }}\"\n    systemd_service_conf: \"{{ _systemd_service_config }}\"\n\n- name: Combine Docker daemon environment variable configuration\n  set_fact:\n    docker_service_envs: \"{{ docker_service_envs | combine(_docker_service_opts) | combine(docker_daemon_envs) }}\"\n  vars:\n    _docker_service_opts:\n      DOCKER_OPTS: \"{{ docker_daemon_opts }}\"\n\n- name: Setup Docker environment file {{ docker_envs_dir[_docker_os_dist] }}/docker-envs\n  template:\n    src: docker-envs.j2\n    dest: \"{{ docker_envs_dir[_docker_os_dist] }}/docker-envs\"\n  become: yes\n  notify: restart docker\n  vars:\n    docker_envs: \"{{ docker_service_envs }}\"\n\n- name: Force daemon reload of systemd\n  systemd: \n    daemon_reload: yes\n  become: yes\n  notify: restart docker\n  when: _systemd_docker_dropin|changed"}, {"commit_sha": "56b8e72732a363ef01fc4ffa2332459ad39b10b7", "sha": "2522bfc1d6c598c4f88312c4cb051c0de08d0f65", "filename": "playbooks/deploy-rock.yml", "repository": "rocknsm/rock", "decoded_content": "---\n# Everything that needs to satisfy dependencies should be run in this play\n- hosts: all\n  vars:\n    rock_debug: \"{{ lookup('env', 'DEBUG') }}\"\n    http_proxy: \"{{ lookup('env','http_proxy') }}\"\n    https_proxy: \"{{ lookup('env', 'https_proxy') }}\"\n  roles:\n    - { role: sensor-common }\n\n- hosts: all\n  gather_facts: False\n  vars:\n    rock_debug: \"{{ lookup('env', 'DEBUG') }}\"\n    http_proxy: \"{{ lookup('env','http_proxy') }}\"\n    https_proxy: \"{{ lookup('env', 'https_proxy') }}\"\n  roles:\n    - role: stenographer\n      when: with_stenographer | bool\n      stenographer_monitor_interfaces: \"{{rock_monifs}}\"\n    - role: docket\n      when: with_docket | bool\n      docket_install: offline\n      docket_enable: \"{{enable_docket | bool}}\"\n\n  tasks:\n  - name: Apply override settings, if available\n    include_vars: /etc/rocknsm/config.yml\n    ignore_errors: true\n    failed_when: false\n\n  - name: Debug variables\n    import_tasks: debug.yml\n    when: rock_debug is defined and rock_debug\n\n    #######################################################\n    ############# Install/Remove Packages #################\n    #######################################################\n  - name: Install packages\n    yum:\n      name: \"{{ item.pkg }}\"\n      state: \"{{ item.state }}\"\n    when: (item.test is undefined) or (item.test)\n    with_items:\n      - { pkg: \"{{ rocknsm_package_list }}\", state: installed }\n\n  - name: Install optional packages\n    yum:\n      name: \"{{ item.pkg }}\"\n      state: \"{{ item.state }}\"\n    when: (item.test is undefined) or (item.test)\n    with_items:\n    - { pkg: elasticsearch, test: \"{{with_elasticsearch}}\", state: installed }\n    - { pkg: logstash, test: \"{{with_logstash}}\", state: installed }\n    - { pkg: kibana, test: \"{{with_kibana}}\", state: installed }\n    - { pkg: filebeat, test: \"{{with_suricata or with_fsf}}\", state: installed }\n    - { pkg: nginx, test: \"{{with_nginx}}\", state: installed }\n    - { pkg: python2-xkcdpass, test: \"{{with_nginx}}\", state: installed }\n    - { pkg: bro, test: \"{{with_bro}}\", state: installed }\n    - { pkg: bro-plugin-af_packet, test: \"{{with_bro}}\", state: installed }\n    - { pkg: bro-plugin-kafka, test: \"{{(with_bro and with_kafka)}}\", state: installed }\n    - { pkg: docket, test: \"{{with_docket}}\", state: installed }\n    - { pkg: suricata, test: \"{{with_suricata}}\", state: installed }\n    - { pkg: suricata-update, test: \"{{with_suricata}}\", state: installed}\n    - { pkg: snort, test: \"{{with_snort}}\", state: installed }\n    - { pkg: daq, test: \"{{with_snort}}\", state: installed }\n    - { pkg: zookeeper, test: \"{{with_zookeeper}}\", state: installed }\n    - { pkg: kafka, test: \"{{with_kafka}}\", state: installed }\n    - { pkg: kafkacat, test: \"{{with_kafka}}\", state: installed }\n    - { pkg: fsf, test: \"{{with_fsf}}\", state: installed }\n    - { pkg: postfix, state: installed }\n\n  - name: Ensure cache directory exists\n    file:\n      dest: \"{{ rock_cache_dir }}\"\n      state: directory\n      mode: 0755\n\n  - name: Download Pulled Pork\n    get_url:\n      url: \"{{ pulledpork_url }}\"\n      dest: \"{{ rock_cache_dir }}/{{ pulledpork_filename }}\"\n      mode: 0644\n    when: with_pulledpork and rock_online_install\n\n  - name: Install Pulled Pork\n    unarchive:\n      src: \"{{ rock_cache_dir }}/{{ pulledpork_filename }}\"\n      dest: /opt\n      owner: root\n      group: root\n      creates: \"/opt/pulledpork-{{ pulledpork_release }}\"\n      remote_src: yes\n    when: with_pulledpork\n\n    #######################################################\n    ################ Configure firewall ###################\n    #######################################################\n  - name: Enable and start firewalld\n    service:\n      name: firewalld\n      enabled: yes\n      state: started\n\n  - name: Configure firewalld\n    firewalld:\n      port: \"{{ item[1].port }}\"\n      source: \"{{ item[0] }}\"\n      permanent: yes\n      state: enabled\n      immediate: yes\n    when: (item[1].test is undefined) or item[1].test\n    with_nested:\n      - \"{{ rock_mgmt_nets }}\"\n      -\n        - { port: \"22/tcp\" }\n        - { port: \"443/tcp\",  test: \"{{ with_kibana }}\" }\n        - { port: \"8443/tcp\", test: \"{{ with_docket }}\" }\n\n    ######################################################\n    ############## Configure GeoIP Databases #############\n    ######################################################\n  - name: Configure GeoIP Update\n    copy: src=GeoIP.conf dest=/etc/GeoIP.conf\n\n    # There's an issue w/ geoipupdate when env is empty\n  - name: Update GeoIP\n    shell: >\n      if [ \"x$HTTP_PROXY\" == \"x\" ]; then\n          unset HTTP_PROXY;\n      fi\n      if [ \"x$http_proxy\" == \"x\" ]; then\n          unset http_proxy;\n      fi\n      if [ \"x$HTTPS_PROXY\" == \"x\" ]; then\n          unset HTTPS_PROXY;\n      fi\n      if [ \"x$https_proxy\" == \"x\" ]; then\n          unset https_proxy;\n      fi\n      /usr/bin/geoipupdate\n    args:\n      creates: /usr/share/GeoIP/GeoLiteASNum.dat\n    register: result\n    failed_when: (result.rc != 0) and (result.rc != 1)\n\n  - name: Create GeoIP symlinks\n    file:\n      src: \"/usr/share/GeoIP/{{ item.src }}\"\n      dest: \"/usr/share/GeoIP/{{ item.dest }}\"\n      force: yes\n      state: link\n    with_items:\n      - { src: 'GeoLiteCity.dat', dest: 'GeoIPCity.dat' }\n      - { src: 'GeoLiteCountry.dat', dest: 'GeoIPCountry.dat' }\n      - { src: 'GeoLiteASNum.dat', dest: 'GeoIPASNum.dat' }\n      - { src: 'GeoLiteCityv6.dat', dest: 'GeoIPCityv6.dat' }\n\n    ######################################################\n    ################### Setup Zookeeper ##################\n    ######################################################\n  - name: Enable and start zookeeper\n    service:\n      name: zookeeper\n      state: \"{{ 'started' if enable_zookeeper else 'stopped' }}\"\n      enabled: \"{{ enable_zookeeper }}\"\n    when: with_zookeeper\n\n    ######################################################\n    ##################### Setup Kafka ####################\n    ######################################################\n  - name: Create Kafka data dir\n    file:\n      path: \"{{ kafka_data_dir }}\"\n      mode: 0755\n      owner: \"{{ kafka_user }}\"\n      group: \"{{ kafka_group }}\"\n      state: directory\n    when: with_kafka\n\n  - name: Set kafka retention\n    lineinfile:\n      dest: \"{{ kafka_config_path }}\"\n      regexp: \"log.retention.hours=\"\n      line:  \"log.retention.hours={{ kafka_retention }}\"\n      state: present\n    when: with_kafka\n\n  - name: Set kafka data dir\n    lineinfile:\n      dest: \"{{ kafka_config_path }}\"\n      regexp: \"log.dirs=\"\n      line: \"log.dirs={{ kafka_data_dir }}\"\n    when: with_kafka\n\n  - name: Enable and start kafka\n    service:\n      name: kafka\n      state: \"{{ 'started' if enable_kafka else 'stopped' }}\"\n      enabled: \"{{ enable_kafka }}\"\n    when: with_kafka\n\n    ######################################################\n    ################# Setup Elasticsearch ################\n    ######################################################\n  - name: Create Elasticsearch directory\n    file:\n      path: \"{{ es_data_dir }}\"\n      mode: 0755\n      owner: \"{{ es_user }}\"\n      group: \"{{ es_group }}\"\n      state: directory\n    when: with_elasticsearch\n\n  - name: Setup elasticsearch config\n    template:\n      src: templates/elasticsearch.yml.j2\n      dest: /etc/elasticsearch/elasticsearch.yml\n      owner: root\n      group: \"{{ es_group }}\"\n      mode: 0640\n    when: with_elasticsearch\n\n  - name: Create elasticsearch systemd override dir\n    file:\n      path: /etc/systemd/system/elasticsearch.service.d\n      owner: root\n      group: root\n      mode: 0755\n      state: directory\n    when: with_elasticsearch\n\n  - name: Enable elasticsearch memlock in service override\n    copy:\n      content: \"{{ es_memlock_override }}\"\n      dest: /etc/systemd/system/elasticsearch.service.d/override.conf\n      mode: 0644\n      owner: root\n      group: root\n    when: with_elasticsearch\n\n  - name: Setup elasticsearch jvm options\n    template:\n      src: templates/es-jvm.options.j2\n      dest: /etc/elasticsearch/jvm.options\n      mode: 0640\n      owner: root\n      group: \"{{ es_group }}\"\n    when: with_elasticsearch\n\n  - name: Install ROCK Elasticsearch cleanup script\n    template:\n      src: templates/es_cleanup.sh.j2\n      dest: /usr/local/bin/es_cleanup.sh\n      mode: 0755\n      owner: root\n      group: root\n    when: with_elasticsearch\n\n  - name: Set elasticsearch cleanup cron job\n    cron:\n      name: \"ES maintenance\"\n      cron_file: rocknsm_es_maintenance\n      hour: 0\n      minute: 1\n      user: root\n      job: /usr/local/bin/es_cleanup.sh > /dev/null 2>&1\n    when: with_elasticsearch\n\n  # TODO: This has to be started for now so that the configuration ca\n  # occur. In the future, we can do this in stages and expect a \"running config\"\n  # phase to execute. Which will allow an install phase, reboot, come up and\n  # configure the services live. We're not there yet.\n  - name: Enable and start Elasticsearch\n    service:\n      name: elasticsearch\n      state: \"started\"\n      enabled: \"{{ enable_elasticsearch }}\"\n    when: with_elasticsearch\n    notify:\n      - es maintenance\n\n  - name: Wait for Elasticsearch to become ready\n    wait_for: host=localhost port=9200\n    when: with_elasticsearch\n\n    ######################################################\n    ################### Setup Kibana #####################\n    ######################################################\n  # TODO: See note above on Elasticsearch\n  - name: Enable and start Kibana\n    service:\n      name: kibana\n      state: \"started\"\n      enabled: \"{{ enable_kibana }}\"\n    when: with_kibana\n\n  - name: Configure Kibana templates\n    uri:\n      method: PUT\n      url: http://localhost:9200/_template/kibana-config\n      body: >\n        { \"order\" : 0, \"template\" : \".kibana\",\n          \"settings\" :\n            { \"index.number_of_replicas\" : \"0\",\n              \"index.number_of_shards\" : \"1\" },\n          \"mappings\" : { }, \"aliases\" : { } }\n      body_format: json\n      status_code: 200,201\n    when: with_kibana\n\n  - name: Add the kibanapw shell function\n    copy:\n      src: profile.d-kibanapw.sh\n      dest: /etc/profile.d/kibanapw.sh\n      mode: 0644\n      owner: root\n      group: root\n    when: with_kibana\n\n  - name: Set initial Kibana credentials\n    shell: >\n      export kibuser=$(getent passwd 1000 | awk -F: '{print $1}') && \\\n      export kibpw=$(xkcdpass -a rock) && \\\n      echo -e \"U: ${kibuser}\\nP: ${kibpw}\" > /home/${kibuser}/KIBANA_CREDS.README && \\\n      printf \"${kibuser}:$(echo ${kibpw} | openssl passwd -apr1 -stdin)\\n\" | \\\n      sudo tee -a /etc/nginx/htpasswd.users > /dev/null 2>&1\n    args:\n      creates: /etc/nginx/htpasswd.users\n    when: with_kibana\n\n    ######################################################\n    ############## Setup RockNSM dataflow ################\n    ######################################################\n\n  - name: Download RockNSM Elastic configs\n    get_url:\n      url: \"{{ rock_dashboards_url }}\"\n      dest: \"{{ rock_cache_dir }}/{{ rock_dashboards_filename }}\"\n      mode: 0644\n    when: (with_kibana or with_elasticsearch or with_logstash) and rock_online_install\n\n  - name: Extract RockNSM Elastic configs\n    unarchive:\n      src: \"{{ rock_cache_dir }}/{{ rock_dashboards_filename }}\"\n      dest: /opt/rocknsm\n      owner: root\n      group: root\n      creates: \"{{ rock_module_dir }}\"\n      remote_src: yes\n    when: (with_kibana or with_elasticsearch or with_logstash)\n\n    ############### Elasticsearch Mappings ####################\n    # TODO customize mappings install per sensor features (i.e. bro, suricata, etc)...maybe\n    # TODO: Fix `changed_when`\n  - name: Blanket install/update Elasticsearch mappings\n    command: ./import-index-templates.sh \"{{ es_url }}\"\n    args:\n      chdir: \"{{ rock_module_dir }}/configuration/elasticsearch\"\n    changed_when: false\n    when: with_elasticsearch\n\n    ############### Logstash Config ####################\n  - name: Install Bro-Kafka configuration for Logstash\n    copy:\n      src: \"{{rock_module_dir}}/configuration/logstash/{{item}}\"\n      dest: \"/etc/logstash/conf.d/{{item}}\"\n      mode: 0640\n      owner: \"{{ logstash_user }}\"\n      group: \"{{ logstash_group }}\"\n      remote_src: \"yes\"\n    when: with_logstash and with_bro and with_kafka\n    notify: Restart Logstash\n    with_items:\n      - logstash-100-input-kafka-bro.conf\n      - logstash-500-filter-bro.conf\n      - logstash-999-output-es-bro.conf\n\n  - name: Install Suricata-Kafka configuration for Logstash\n    copy:\n      src: \"{{rock_module_dir}}/configuration/logstash/{{item}}\"\n      dest: \"/etc/logstash/conf.d/{{item}}\"\n      mode: 0640\n      owner: \"{{ logstash_user }}\"\n      group: \"{{ logstash_group }}\"\n      remote_src: \"yes\"\n    when: with_logstash and with_suricata and with_kafka\n    notify: Restart Logstash\n    with_items:\n      - logstash-100-input-kafka-suricata.conf\n      - logstash-500-filter-suricata.conf\n      - logstash-999-output-es-suricata.conf\n\n  - name: Install FSF-Kafka configuration for Logstash\n    copy:\n      src: \"{{rock_module_dir}}/configuration/logstash/{{item}}\"\n      dest: \"/etc/logstash/conf.d/{{item}}\"\n      mode: 0640\n      owner: \"{{ logstash_user }}\"\n      group: \"{{ logstash_group }}\"\n      remote_src: \"yes\"\n    when: with_logstash and with_fsf and with_kafka\n    notify: Restart Logstash\n    with_items:\n      - logstash-100-input-kafka-fsf.conf\n      - logstash-500-filter-fsf.conf\n      - logstash-999-output-es-fsf.conf\n\n  - name: Install Parse Failure configuration for Logstash\n    copy:\n      src: \"{{rock_module_dir}}/configuration/logstash/{{item}}\"\n      dest: \"/etc/logstash/conf.d/{{item}}\"\n      mode: 0640\n      owner: \"{{ logstash_user }}\"\n      group: \"{{ logstash_group }}\"\n      remote_src: \"yes\"\n    when: with_logstash\n    notify: Restart Logstash\n    with_items:\n      - logstash-998-filter-parsefailures.conf\n      - logstash-999-output-es-parsefailures.conf\n\n    ############### Kibana Config ####################\n  - name: Blanket install/update Kibana saved objects\n    command: ./import-saved-items.sh \"{{ kibana_url }}\"\n    args:\n      chdir: \"{{rock_module_dir}}/configuration/kibana\"\n    changed_when: false\n    # TODO: Fix this ^^\n    when: with_kibana\n\n  # - name: Get Kibana Bro index mapping\n  #   uri:\n  #     method: GET\n  #     url: \"{{ kibana_url }}/api/saved_objects/index-pattern?per_page=1000\"\n  #   when: with_kibana\n  #   register:\n\n    ######################################################\n    ################### Setup Logstash ###################\n    ######################################################\n\n  # - name: Check for Parse Failure mapping template\n  #   uri:\n  #     method: \"GET\"\n  #     url: http://localhost:9200/_template/failure_index\n  #   failed_when: False\n  #   register: failure_mapping\n  #   when: (with_elasticsearch and with_logstash)\n  #\n  # - name: Load Parse Failure Elasticsearch mapping templates\n  #   uri:\n  #     method: PUT\n  #     url: http://localhost:9200/_template/failure_index\n  #     body: \"{{ lookup('file', 'es-parse-failures-mappings.json')}}\"\n  #     body_format: json\n  #   when: (with_elasticsearch and with_logstash) and failure_mapping.status == 404\n\n  - name: Enable and start Logstash\n    service:\n      name: logstash\n      state: \"{{ 'started' if enable_logstash else 'stopped' }}\"\n      enabled: \"{{ enable_logstash }}\"\n    when: with_logstash\n\n\n    ######################################################\n    ################### Setup Filebeat ###################\n    ######################################################\n  - name: Add Filebeat configuration file\n    template:\n      src: filebeat.yml.j2\n      dest: /etc/filebeat/filebeat.yml\n    notify: Restart Filebeat\n\n  - name: Enable and start Filebeat\n    service:\n      name: filebeat\n      state: \"{{ 'started' if enable_filebeat else 'stopped' }}\"\n      enabled: \"{{ enable_filebeat }}\"\n    when: with_filebeat\n\n    #######################################################\n    ###################### Setup Bro  #####################\n    #######################################################\n  - name: Create bro group\n    group:\n      name: \"{{ bro_group }}\"\n      state: present\n      system: yes\n    when: with_bro\n\n  - name: Create bro user and group\n    user:\n      name: \"{{ bro_user }}\"\n      comment: \"bro service account\"\n      createhome: no\n      group: \"{{ bro_group }}\"\n      home: /var/spool/bro\n      shell: /sbin/nologin\n      system: yes\n      state: present\n    when: with_bro\n\n  - name: Create Bro directories\n    file:\n      path: \"{{ item }}\"\n      mode: 0755\n      owner: \"{{ bro_user }}\"\n      group: \"{{ bro_group }}\"\n      state: directory\n    with_items:\n      - \"{{ bro_data_dir }}\"\n      - \"{{ bro_data_dir }}/logs\"\n      - \"{{ bro_data_dir }}/spool\"\n    when: with_bro\n\n  - name: Create /opt/bro dir for wandering users\n    file:\n      dest: \"/opt/bro\"\n      state: directory\n    when: with_bro\n\n  - name: Create note to wandering users\n    copy:\n      dest: \"/opt/bro/README.md\"\n      content: |\n        Hey! Where's my Bro?\n        =========================\n\n        RockNSM has aligned the Bro package to be inline with Fedora packaging\n        guidelines in an effort to push the package upstream for maintenance.\n        Fedora and EPEL have a great community and we believe others can benefit\n        from our hard work.\n\n        Here's where you can find your stuff:\n\n        Bro configuration files\n        -----------------------\n        /opt/bro/etc -> /etc/bro\n\n        Bro site scripts\n        -----------------------\n        /opt/bro/share/bro/site -> /usr/share/bro/site\n\n        Bro logs and spool dirs (same as previous ROCK iterations)\n        -----------------------\n        /opt/bro/logs -> /data/bro/logs\n        /opt/bro/spool -> /data/bro/spool\n    when: with_bro\n\n  - name: Create Bro node.cfg\n    template:\n      src: templates/bro-node.cfg.j2\n      dest: \"{{ bro_sysconfig_dir }}/node.cfg\"\n      mode: 0644\n      owner: root\n      group: root\n    when: with_bro\n    notify: reload bro\n\n  - name: Create broctl.cfg\n    template:\n      src: templates/bro-broctl.cfg.j2\n      dest: \"{{ bro_sysconfig_dir }}/broctl.cfg\"\n      mode: 0644\n      owner: root\n      group: root\n    when: with_bro\n    notify: reload bro\n\n  - name: Create bro networks.cfg\n    copy:\n      src: bro-networks.cfg\n      dest: \"{{ bro_sysconfig_dir }}/networks.cfg\"\n      mode: 0644\n      owner: root\n      group: root\n    when: with_bro\n    notify: reload bro\n\n  - name: Add bro custom scripts dir\n    file:\n      path: \"{{ bro_site_dir }}/scripts\"\n      owner: root\n      group: root\n      mode: 0755\n      state: directory\n    when: with_bro\n\n  - name: Set permissions on broctl scripts dir\n    file:\n      path: \"{{ bro_prefix }}/share/broctl/scripts\"\n      owner: \"{{ bro_user }}\"\n      group: \"{{ bro_user }}\"\n      mode: 0755\n      state: directory\n    when: with_bro\n\n  - name: Add README to scripts dir\n    copy:\n      src: bro-scripts-readme.txt\n      dest: \"{{ bro_site_dir }}/scripts/README.txt\"\n      mode: 0644\n      owner: root\n      group: root\n    when: with_bro\n\n  - name: Checkout ROCK Bro scripts\n    git:\n      repo: \"{{ bro_rockscripts_repo }}\"\n      dest: \"{{ bro_site_dir }}/scripts/rock\"\n      version: \"{{ bro_rockscripts_branch }}\"\n    when: with_bro and rock_online_install\n\n  - name: Deploy offline ROCK Bro scripts\n    unarchive:\n      src: \"{{ rock_cache_dir }}/{{ bro_rockscripts_filename }}\"\n      dest: \"{{ bro_site_dir }}/scripts/\"\n      owner: root\n      group: root\n      creates: \"{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}\"\n      remote_src: yes\n    when: with_bro and not rock_online_install\n\n  - name: Symlink offline ROCK bro scripts\n    file:\n      src: \"{{ bro_site_dir }}/scripts/rock-scripts-{{ bro_rockscripts_branch | replace ('/', '-') }}\"\n      dest: \"{{ bro_site_dir }}/scripts/rock\"\n      state: link\n      force: yes\n    when: with_bro and not rock_online_install\n\n  - name: Update owner for ROCK NSM Bro scripts\n    file:\n      path: \"{{ bro_site_dir }}/scripts/rock\"\n      owner: \"{{ bro_user }}\"\n      group: \"{{ bro_group }}\"\n      state: directory\n      recurse: yes\n      follow: yes\n    tags:\n      - bro_scripts\n    when: with_bro\n\n  - name: Add ROCK scripts to local.bro\n    lineinfile:\n      dest: \"{{ bro_site_dir }}/local.bro\"\n      line: \"@load scripts/rock # ROCK NSM customizations\"\n      state: present\n    when: with_bro\n\n  - name: Enable Bro Kafka output to local.bro\n    lineinfile:\n      dest: \"{{ bro_site_dir }}/local.bro\"\n      line: \"@load scripts/rock/plugins/kafka\"\n      state: present\n    when: with_bro and with_kafka\n\n  - name: Add bro aliases\n    copy:\n      src: profile.d-bro.sh\n      dest: /etc/profile.d/bro.sh\n      mode: 0644\n      owner: root\n      group: root\n    when: with_bro\n\n  - name: Add broctl wrapper for admin use\n    copy:\n      src: broctl.sh\n      dest: /usr/sbin/broctl\n      mode: 0754\n      owner: root\n      group: root\n    when: with_bro\n\n  - name: Set bro capabilities\n    capabilities:\n      path: /usr/bin/bro\n      capability: \"{{ item }}\"\n      state: present\n    with_items:\n      - \"cap_net_raw+eip\"\n      - \"cap_net_admin+eip\"\n    when: with_bro\n\n  - name: Set capstats capabilities\n    capabilities:\n      path: /usr/bin/capstats\n      capability: \"{{ item }}\"\n      state: present\n    with_items:\n      - \"cap_net_raw+eip\"\n      - \"cap_net_admin+eip\"\n    when: with_bro\n\n  - name: Set broctl cron\n    cron:\n      name: \"broctl maintenance\"\n      minute: \"*/5\"\n      cron_file: rocknsm_broctl\n      user: \"{{ bro_user }}\"\n      job: \"/usr/bin/broctl cron >/dev/null 2>&1\"\n    when: with_bro\n\n  - name: Initialize bro scripts for workers\n    command: /usr/bin/broctl install\n    args:\n      creates: \"{{ bro_data_dir }}/spool/broctl-config.sh\"\n    become: yes\n    become_user: \"{{ bro_user }}\"\n    when: with_bro\n\n  - name: Enable and start broctl\n    service:\n      name: bro\n      enabled: \"{{ enable_bro }}\"\n    when: with_bro\n    notify: reload bro\n\n\n    ######################################################\n    ################## Setup Suricata ####################\n    ######################################################\n  - name: Create Suricata directories\n    file:\n      path: \"{{ suricata_data_dir }}/\"\n      mode: 0755\n      owner: \"{{ suricata_user }}\"\n      group: \"{{ suricata_group }}\"\n      state: directory\n    when: with_suricata\n\n  - name: Remove suricata sysconfig file\n    file:\n      path: /etc/sysconfig/suricata\n      state: absent\n    when: with_suricata\n\n  - name: Install suricata service files\n    copy:\n      src: \"suricata.service\"\n      dest: \"/etc/systemd/system/suricata.service\"\n      mode: 0644\n      owner: root\n      group: root\n    when: with_suricata\n\n  - name: Setup suricata tmpfiles\n    copy:\n      src: \"suricata.tmpfiles\"\n      dest: \"/etc/tmpfiles.d/suricata.conf\"\n      mode: 0644\n      owner: root\n      group: root\n    when: with_suricata\n\n  - name: Install suricata overrides\n    template:\n      src: templates/suricata_overrides.yaml.j2\n      dest: /etc/suricata/rocknsm-overrides.yaml\n      mode: 0640\n      owner: \"root\"\n      group: \"{{ suricata_group }}\"\n    when: with_suricata\n\n  - name: Create IP reputation config dir\n    file:\n      path: /etc/suricata/rules/iplists\n      state: directory\n      owner: root\n      group: root\n      mode: 0755\n    when: with_suricata\n\n  - name: Create Suricata dirs for suricata-update\n    file:\n      path: \"{{ suricata_var_dir }}/{{ item }}\"\n      state: directory\n      owner: \"{{ suricata_user }}\"\n      group: \"{{ suricata_group }}\"\n      mode: 0755\n      recurse: \"yes\"\n    when: with_suricata\n    with_items:\n      - rules\n      - update\n\n  - name: Set suricata overrides include in main config\n    lineinfile:\n      dest: /etc/suricata/suricata.yaml\n      line: \"include: rocknsm-overrides.yaml\"\n      state: present\n    when: with_suricata\n\n  - name: Enable and start suricata\n    service:\n      name: suricata\n      enabled: \"{{ enable_suricata }}\"\n      state: \"{{ 'started' if enable_suricata else 'stopped' }}\"\n    when: with_suricata\n\n  - name: Configure logrotate for suricata logs\n    template:\n      src: templates/logrotate-suricata.conf.j2\n      dest: /etc/logrotate.d/suricata.conf\n      mode: 0644\n      owner: root\n      group: root\n    when: with_suricata\n\n    ######################################################\n    ################# Setup PulledPork  ##################\n    ######################################################\n  - name: Create pulledpork directory symlink\n    file:\n      src: \"/opt/pulledpork-{{ pulledpork_release }}\"\n      dest: \"/opt/pulledpork\"\n      state: link\n      force: yes\n    when: with_pulledpork\n\n  - name: Set pulledpork executable\n    file:\n      path: /opt/pulledpork/pulledpork.pl\n      mode: 0755\n      owner: root\n      group: root\n    when: with_pulledpork\n\n  - name: Create pulledpork config dir\n    file:\n      path: /etc/pulledpork\n      mode: 0755\n      owner: root\n      group: root\n      state: directory\n    when: with_pulledpork\n\n  - name: Configure pulledpork\n    template:\n      src: templates/pulledpork.conf.j2\n      dest: /etc/pulledpork/pulledpork.conf\n      owner: root\n      group: root\n      mode: 0644\n    when: with_pulledpork\n\n  - name: Check stats of rules files\n    stat:\n      path: \"{{ pulledpork_engine_basepath }}/rules/pulledpork.rules\"\n    register: rules_file\n    when: with_pulledpork\n\n  - name: Create initial pulledpork rules-related files\n    file:\n      path: \"{{ pulledpork_engine_basepath }}/rules/pulledpork.rules\"\n      owner: root\n      group: root\n      mode: 0644\n      state: touch\n    when: with_pulledpork and not rules_file.stat.exists\n\n  - name: Schedule pulledpork to run daily\n    cron:\n      name: \"pulledpork update\"\n      cron_file: rocknsm_pulledpork\n      user: root\n      hour: \"12\"\n      minute: \"0\"\n      job: /opt/pulledpork/pulledpork.pl\n        -c /etc/pulledpork/pulledpork.conf\n        -l > /var/log/pulledpork.log 2>&1;\n        {{ \"/usr/bin/systemctl kill -s USR2 suricata;\" if with_suricata else None }}\n        {{ \"/usr/bin/systemctl restart snortd;\" if with_snort else None }}\n    when: with_pulledpork\n\n    #######################################################\n    ######################## FSF ##########################\n    #######################################################\n  - name: Create FSF data dir\n    file:\n      path: \"{{ fsf_data_dir }}\"\n      mode: 0755\n      owner: \"{{ fsf_user }}\"\n      group: \"{{ fsf_group }}\"\n      state: directory\n    when: with_fsf\n\n  - name: Create FSF archive dir\n    file:\n      path: \"{{ fsf_archive_dir }}\"\n      mode: 0755\n      owner: \"{{ fsf_user }}\"\n      group: \"{{ fsf_group }}\"\n      state: directory\n    when: with_fsf\n\n  - name: Configure logrotate for FSF logs\n    copy:\n      src: logrotate-fsf.conf\n      dest: /etc/logrotate.d/fsf.conf\n      mode: 0644\n      owner: root\n      group: root\n    when: with_fsf\n\n  - name: Configure fsf-server\n    template:\n      src: templates/fsf-server-config.j2\n      dest: /opt/fsf/fsf-server/conf/config.py\n      owner: \"{{ fsf_user }}\"\n      group: \"{{ fsf_group }}\"\n      mode: 0644\n    when: with_fsf\n\n  - name: Configure fsf-client\n    template:\n      src: templates/fsf-client-config.j2\n      dest: /opt/fsf/fsf-client/conf/config.py\n      owner: \"{{ fsf_user }}\"\n      group: \"{{ fsf_group }}\"\n      mode: 0644\n    when: with_fsf\n\n  - name: Enable and start FSF\n    service:\n      name: fsf\n      state: \"{{ 'started' if enable_fsf else 'stopped' }}\"\n      enabled: \"{{ enable_fsf }}\"\n    when: with_fsf\n\n    ######################################################\n    ################### Setup nginx ######################\n    ######################################################\n  - name: Install ROCK nginx configuration\n    template:\n      src: templates/nginx-rock.conf.j2\n      dest: /etc/nginx/conf.d/rock.conf\n      mode: 0644\n      owner: root\n      group: root\n    when: with_nginx and with_kibana\n\n  - name: Install nginx base configuration\n    copy:\n      src: nginx.conf\n      dest: /etc/nginx/nginx.conf\n      mode: 0644\n      owner: root\n      group: root\n    when: with_nginx\n\n  - name: Enable nginx to perform proxy connect\n    seboolean:\n      name: httpd_can_network_connect\n      state: yes\n      persistent: yes\n    when: with_nginx and with_kibana\n\n  - name: Generate sensor private key\n    openssl_privatekey:\n      path: \"{{ http_tls_key }}\"\n    when: with_kibana and with_nginx\n    notify:\n      - Enable and start nginx\n\n  - name: Generate sensor public key\n    openssl_publickey:\n      path: \"{{ http_tls_pub }}\"\n      privatekey_path: \"{{ http_tls_key }}\"\n    when: with_kibana and with_nginx\n    notify:\n      - Enable and start nginx\n\n  - name: Generate sensor CSR\n    openssl_csr:\n      path: \"{{ http_tls_pub }}.csr\"\n      privatekey_path: \"{{ http_tls_key }}\"\n      country_name: US\n      state_or_province_name: MO\n      locality_name: St. Louis\n      organization_name: RockNSM\n      organizational_unit_name: NSM Ninjas\n      email_address: info@rocknsm.io\n      common_name: \"{{ rock_fqdn }}\"\n    when: with_kibana and with_nginx\n    notify:\n      - Enable and start nginx\n\n  - name: Generate sensor certificate\n    openssl_certificate:\n      path: \"{{ http_tls_crt }}\"\n      privatekey_path: \"{{ http_tls_key }}\"\n      csr_path: \"{{ http_tls_pub }}.csr\"\n      provider: selfsigned\n    when: with_kibana and with_nginx\n    notify:\n      - Enable and start nginx\n\n  - name: Combine sensor cert and key\n    shell: >\n      cat {{http_tls_key}} {{http_tls_crt}} > {{http_tls_combined}}\n    args:\n      creates: \"{{ http_tls_combined }}\"\n    when: with_lighttpd\n    notify:\n      - Restart lighttpd\n\n  - name: Generate DH parameters\n    command: >\n      openssl dhparam -out {{http_tls_dhparams}} 2048\n    args:\n      creates: \"{{http_tls_dhparams}}\"\n    when: with_kibana and with_nginx\n    notify:\n      - Enable and start nginx\n\n    ######################################################\n    ############### Setup ROCKNSM Scripts ################\n    ######################################################\n  - name: Install rock start script\n    copy:\n      src: rock_start\n      dest: /usr/local/bin/rock_start\n      mode: 0700\n      owner: root\n      group: root\n\n  - name: Install rock stop script\n    copy:\n      src: rock_stop\n      dest: /usr/local/bin/rock_stop\n      mode: 0700\n      owner: root\n      group: root\n\n  - name: Install rock status script\n    copy:\n      src: rock_status\n      dest: /usr/local/bin/rock_status\n      mode: 0755\n      owner: root\n      group: root\n\n  - name: Create rock script symlinks\n    file:\n      src: \"/usr/local/bin/{{ item.src }}\"\n      dest: \"/usr/sbin/{{ item.dest }}\"\n      force: yes\n      state: link\n    with_items:\n      - { src: 'rock_start', dest: 'rock_start' }\n      - { src: 'rock_stop', dest: 'rock_stop' }\n      - { src: 'rock_status', dest: 'rock_status' }\n\n  # Training mode / Service mode not needed for AF_PACKET\n  ######################################################\n  ############### ROCKNSM Customization ################\n  ######################################################\n  - name: Set ROCK NSM Version\n    copy:\n      content: \"{{ rock_version }}\"\n      dest: /etc/rocknsm/rock-version\n      mode: 0644\n      owner: root\n      group: root\n\n  - name: Install ROCK NSM /etc/issue\n    copy:\n      src: etc-issue.in\n      dest: /etc/issue.in\n      mode: 0644\n      owner: root\n      group: root\n\n  - name: NetworkManager ROCK NSM hook\n    copy:\n      src: nm-issue-update\n      dest: /etc/NetworkManager/dispatcher.d/50-rocknsm-issue-update\n      mode: 0755\n      owner: root\n      group: root\n\n  #######################################################\n  #####################  Handlers  ######################\n  #######################################################\n  handlers:\n    - name: force sync time\n      command: >\n        chronyc -a 'burst 3/4'; sleep 5; chronyc -a makestep\n\n    - name: configure monitor interfaces\n      shell: >\n        for intf in {{ rock_monifs | join(' ') }}; do\n          /sbin/ifup ${intf};\n        done\n\n    - name: sshd restart\n      service: name=sshd state=restarted\n\n    - name: es maintenance\n      command: /usr/local/bin/es_cleanup.sh\n\n    - name: reload bro\n      service: name=bro state=\"{{ 'started' if enable_bro else 'stopped' }}\"\n\n    - name: create kafka bro topic\n      command: >\n        /opt/kafka/bin/kafka-topics.sh\n           --zookeeper 127.0.0.1:2181\n           --create\n           --replication-factor 1\n           --topic bro-raw\n           --partitions 1\n\n    - name: create kafka suricata topic\n      command: >\n        /opt/kafka/bin/kafka-topics.sh\n           --zookeeper 127.0.0.1:2181\n           --create\n           --replication-factor 1\n           --topic suricata-raw\n           --partitions 1\n\n    - name: create kafka fsf topic\n      command: >\n        /opt/kafka/bin/kafka-topics.sh\n           --zookeeper 127.0.0.1:2181\n           --create\n           --replication-factor 1\n           --topic fsf-raw\n           --partitions 1\n\n    - name: reload systemd\n      systemd:\n        daemon_reload: yes\n\n    - name: Restart Logstash\n      systemd:\n        name: logstash\n        state: restarted\n\n    - name: Restart Filebeat\n      systemd:\n        name: filebeat\n        state: restarted\n\n    - name: Enable and start nginx\n      systemd:\n        name: nginx\n        state: \"{{ 'started' if enable_nginx else 'stopped' }}\"\n        enabled: \"{{ enable_nginx }}\"\n      when: with_nginx\n\n    - name: Restart lighttpd\n      service:\n        name: lighttpd\n        state: \"{{ 'started' if enable_lighttpd else 'stopped' }}\"\n        enabled: \"{{ enable_lighttpd }}\"\n      when: with_lighttpd\n\n\n  environment:\n   http_proxy:  \"{{ http_proxy }}\"\n   https_proxy: \"{{ https_proxy }}\"\n   HTTP_PROXY:  \"{{ http_proxy }}\"\n   HTTPS_PROXY: \"{{ https_proxy }}\"\n"}, {"commit_sha": "11051981ef7064c86ca9b70ff9f3e45ea03dced9", "sha": "b248ea533e19afe177d1d945f51958f7ed067977", "filename": "roles/lighttpd/tasks/main.yml", "repository": "rocknsm/rock", "decoded_content": "---\n\n- name: Install packages\n  yum:\n    name:\n      - lighttpd\n      - python2-xkcdpass\n    state: present\n\n- name: Install ROCK lighttpd configuration\n  template:\n    src: templates/{{ item }}.j2\n    dest: /etc/lighttpd/vhosts.d/{{ item }}\n    mode: 0644\n    owner: root\n    group: root\n  when: \"rock_services | selectattr('name', 'equalto', 'kibana') | map(attribute='installed')\"\n  loop:\n    - 10-rock-auth.conf\n    - 10-tls.conf\n    - 20-rock-vars.conf\n    - 50-rockproxy.conf\n  notify: Enable and restart lighttpd\n\n- name: Enable lighttpd vhosts\n  lineinfile:\n    path: /etc/lighttpd/lighttpd.conf\n    regexp: '^#?\\s*include.*vhosts\\.d/.*$'\n    line: include \"/etc/lighttpd/vhosts.d/*.conf\"\n  notify: Enable and restart lighttpd\n\n- name: Disable lighttpd ipv6\n  lineinfile:\n    path: /etc/lighttpd/lighttpd.conf\n    regexp: '^server.use-ipv6.*$'\n    line: server.use-ipv6 = \"disable\"\n  notify: Enable and restart lighttpd\n\n- name: Enable lighttpd to perform proxy connect\n  seboolean:\n    name: httpd_can_network_connect\n    state: true\n    persistent: true\n  when: \"rock_services | selectattr('name', 'equalto', 'kibana') | map(attribute='installed')\"\n\n- name: Generate sensor private key\n  openssl_privatekey:\n    path: \"{{ http_tls_key }}\"\n  when: \"rock_services | selectattr('name', 'equalto', 'kibana') | map(attribute='installed')\"\n  notify: Enable and restart lighttpd\n\n- name: Generate sensor public key\n  openssl_publickey:\n    path: \"{{ http_tls_pub }}\"\n    privatekey_path: \"{{ http_tls_key }}\"\n  when: \"rock_services | selectattr('name', 'equalto', 'kibana') | map(attribute='installed')\"\n  notify: Enable and restart lighttpd\n\n- name: Generate sensor CSR\n  openssl_csr:\n    path: \"{{ http_tls_pub }}.csr\"\n    privatekey_path: \"{{ http_tls_key }}\"\n    country_name: US\n    state_or_province_name: MO\n    locality_name: St. Louis\n    organization_name: RockNSM\n    organizational_unit_name: NSM Ninjas\n    email_address: info@rocknsm.io\n    common_name: \"{{ ansible_hostname }}\"\n  when: \"rock_services | selectattr('name', 'equalto', 'kibana') | map(attribute='installed')\"\n  notify: Enable and restart lighttpd\n\n- name: Generate sensor certificate\n  openssl_certificate:\n    path: \"{{ http_tls_crt }}\"\n    privatekey_path: \"{{ http_tls_key }}\"\n    csr_path: \"{{ http_tls_pub }}.csr\"\n    provider: selfsigned\n  when: \"rock_services | selectattr('name', 'equalto', 'kibana') | map(attribute='installed')\"\n  notify: Enable and restart lighttpd\n\n- name: Combine sensor cert and key\n  shell: >\n    cat {{ http_tls_key }} {{ http_tls_crt }} > {{ http_tls_combined }}\n  args:\n    creates: \"{{ http_tls_combined }}\"\n  notify: Enable and restart lighttpd\n\n- name: Generate DH parameters\n  command: >\n    openssl dhparam -out {{ http_tls_dhparams }} 2048\n  args:\n    creates: \"{{ http_tls_dhparams }}\"\n  when: \"rock_services | selectattr('name', 'equalto', 'kibana') | map(attribute='installed')\"\n  notify: Enable and restart lighttpd\n\n- name: Configure firewall ports\n  firewalld:\n    port: \"{{ item }}/tcp\"\n    permanent: true\n    state: enabled\n    immediate: true\n  loop:\n    - 443\n\n- name: Check if initial user has already been created\n  stat:\n    path: /etc/lighttpd/rock-htpasswd.user\n  register: rocknsm_initial_user_created\n\n- block:\n    - name: Determine initial username\n      shell: \"getent passwd 1000 | awk -F: '{print $1}'\"\n      register: rocknsm_initial_username\n\n    - name: Determine initial password\n      command: \"xkcdpass -a rock\"\n      register: rocknsm_initial_password\n\n    - name: Set initial credentials\n      include_tasks: add-user.yml\n      vars:\n        lighttpd_user: \"{{ rocknsm_initial_username.stdout }}\"\n        lighttpd_password: \"{{ rocknsm_initial_password.stdout }}\"\n\n    - name: Output initial credentials\n      shell: \"echo -e \\\"U: {{ rocknsm_initial_username.stdout }}\\nP: {{ rocknsm_initial_password.stdout }}\\\"\n              > /home/{{ rocknsm_initial_username.stdout }}/KIBANA_CREDS.README\"\n  when: rocknsm_initial_user_created.stat.exists | bool\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "84dcda2fafddd078ba17d303426eac5cb11e2259", "filename": "roles/ipaserver/tasks/install.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "---\n# tasks file for ipaserver\n\n- block:\n  - name: Install - Ensure that IPA server packages are installed\n    package:\n      name: \"{{ item }}\"\n      state: present\n    with_items: \"{{ ipaserver_packages }}\"\n\n  - name: Install - Ensure that IPA server packages for dns are installed\n    package:\n      name: \"{{ item }}\"\n      state: present\n    with_items: \"{{ ipaserver_packages_dns }}\"\n    when: ipaserver_setup_dns | bool\n\n  - name: Install - Ensure that IPA server packages for adtrust are installed\n    package:\n      name: \"{{ item }}\"\n      state: present\n    with_items: \"{{ ipaserver_packages_adtrust }}\"\n    when: ipaserver_setup_adtrust | bool\n\n  when: ipaserver_install_packages | bool\n\n#- name: Install - Include Python2/3 import test\n#  import_tasks: \"{{ role_path }}/tasks/python_2_3_test.yml\"\n\n- name: Install - Server installation test\n  ipaserver_test:\n    ### basic ###\n    dm_password: \"{{ ipadm_password }}\"\n    password: \"{{ ipaadmin_password }}\"\n    master_password: \"{{ ipaserver_master_password | default(omit) }}\"\n    ip_addresses: \"{{ ipaserver_ip_addresses | default([]) }}\"\n    domain: \"{{ ipaserver_domain | default(omit) }}\"\n    realm: \"{{ ipaserver_realm | default(omit) }}\"\n    hostname: \"{{ ipaserver_hostname | default(ansible_fqdn) }}\"\n    ca_cert_files: \"{{ ipaserver_ca_cert_files | default(omit) }}\"\n    no_host_dns: \"{{ ipaserver_no_host_dns }}\"\n    ### server ###\n    setup_adtrust: \"{{ ipaserver_setup_adtrust }}\"\n    setup_kra: \"{{ ipaserver_setup_kra }}\"\n    setup_dns: \"{{ ipaserver_setup_dns }}\"\n    idstart: \"{{ ipaserver_idstart | default(omit) }}\"\n    idmax: \"{{ ipaserver_idmax | default(omit) }}\"\n    # no_hbac_allow: \"{{ ipaserver_no_hbac_allow }}\"\n    no_pkinit: \"{{ ipaserver_no_pkinit }}\"\n    # no_ui_redirect: \"{{ ipaserver_no_ui_redirect }}\"\n    dirsrv_config_file: \"{{ ipaserver_dirsrv_config_file | default(omit) }}\"\n    ### ssl certificate ###\n    dirsrv_cert_files: \"{{ ipaserver_dirsrv_cert_files | default([]) }}\"\n    http_cert_files: \"{{ ipaserver_http_cert_files | default([]) }}\"\n    pkinit_cert_files: \"{{ ipaserver_pkinit_cert_files | default([]) }}\"\n    # dirsrv_pin\n    # http_pin\n    # pkinit_pin\n    # dirsrv_name\n    # http_name\n    # pkinit_name\n    ### client ###\n    # mkhomedir\n    no_ntp: \"{{ ipaclient_no_ntp }}\"\n    # ssh_trust_dns\n    # no_ssh\n    # no_sshd\n    # no_dns_sshfp\n    ### certificate system ###\n    external_ca: \"{{ ipaserver_external_ca }}\"\n    external_ca_type: \"{{ ipaserver_external_ca_type | default(omit) }}\"\n    external_cert_files: \"{{ ipaserver_external_cert_files | default([]) }}\"\n    subject_base: \"{{ ipaserver_subject_base | default(omit) }}\"\n    ca_subject: \"{{ ipaserver_ca_subject | default(omit) }}\"\n    # ca_signing_algorithm\n    ### dns ###\n    allow_zone_overlap: \"{{ ipaserver_allow_zone_overlap }}\"\n    reverse_zones: \"{{ ipaserver_reverse_zones | default([]) }}\"\n    no_reverse: \"{{ ipaserver_no_reverse }}\"\n    auto_reverse: \"{{ ipaserver_auto_reverse }}\"\n    zonemgr: \"{{ ipaserver_zonemgr | default(omit) }}\"\n    forwarders: \"{{ ipaserver_forwarders | default([]) }}\"\n    no_forwarders: \"{{ ipaserver_no_forwarders }}\"\n    auto_forwarders: \"{{ ipaserver_auto_forwarders }}\"\n    forward_policy: \"{{ ipaserver_forward_policy | default(omit) }}\"\n    no_dnssec_validation: \"{{ ipaserver_no_dnssec_validation }}\"\n    ### ad trust ###\n    enable_compat: \"{{ ipaserver_enable_compat }}\"\n    netbios_name: \"{{ ipaserver_netbios_name | default(omit) }}\"\n    rid_base: \"{{ ipaserver_rid_base | default(omit) }}\"\n    secondary_rid_base: \"{{ ipaserver_secondary_rid_base | default(omit) }}\"\n\n    ### additional ###\n  register: result_ipaserver_test\n\n- block:\n  # This block is executed only when\n  # not ansible_check_mode and\n  # not (not result_ipaserver_test.changed and\n  #      (result_ipaserver_test.client_already_configured is defined or\n  #       result_ipaserver_test.server_already_configured is defined)\n\n  - block:\n    - name: Install - Master password creation\n      no_log: yes\n      ipaserver_master_password:\n        dm_password: \"{{ ipadm_password }}\"\n        master_password: \"{{ ipaserver_master_password | default(omit) }}\"\n      register: result_ipaserver_master_password\n\n    - name: Install - Use new master password\n      no_log: yes\n      set_fact:\n        ipaserver_master_password:\n          \"{{ result_ipaserver_master_password.password }}\"\n\n    when: ipaserver_master_password is undefined\n\n  - name: Install - Server preparation\n    ipaserver_prepare:\n      ### basic ###\n      dm_password: \"{{ ipadm_password }}\"\n      password: \"{{ ipaadmin_password }}\"\n      # ip_addresses: \"{{ result_ipaserver_test.ip_addresses }}\"\n      domain: \"{{ result_ipaserver_test.domain }}\"\n      realm: \"{{ result_ipaserver_test.realm }}\"\n      hostname: \"{{ result_ipaserver_test.hostname }}\"\n      no_host_dns: \"{{ result_ipaserver_test.no_host_dns }}\"\n      ### server ###\n      setup_adtrust: \"{{ result_ipaserver_test.setup_adtrust }}\"\n      setup_kra: \"{{ result_ipaserver_test.setup_kra }}\"\n      setup_dns: \"{{ ipaserver_setup_dns }}\"\n      ### certificate system ###\n      # external_ca\n      # external_cert_files\n      subject_base: \"{{ result_ipaserver_test.subject_base }}\"\n      ca_subject: \"{{ result_ipaserver_test.ca_subject }}\"\n      ### dns ###\n      allow_zone_overlap: \"{{ ipaserver_allow_zone_overlap }}\"\n      reverse_zones: \"{{ result_ipaserver_test.reverse_zones }}\"\n      no_reverse: \"{{ ipaserver_no_reverse }}\"\n      auto_reverse: \"{{ ipaserver_auto_reverse }}\"\n      forwarders: \"{{ ipaserver_forwarders | default([]) }}\"\n      no_forwarders: \"{{ ipaserver_no_forwarders }}\"\n      auto_forwarders: \"{{ ipaserver_auto_forwarders }}\"\n      no_dnssec_validation: \"{{ result_ipaserver_test.no_dnssec_validation }}\"\n      ### ad trust ###\n      enable_compat: \"{{ ipaserver_enable_compat }}\"\n      netbios_name: \"{{ ipaserver_netbios_name | default(omit) }}\"\n      # rid_base\n      # secondary_rid_base\n      ### additional ###\n      setup_ca: \"{{ result_ipaserver_test.setup_ca }}\"\n      _hostname_overridden: \"{{ result_ipaserver_test._hostname_overridden }}\"\n    register: result_ipaserver_prepare\n\n  - name: Install - Setup NTP\n    ipaserver_setup_ntp:\n    when: not ipaclient_no_ntp | bool and (ipaserver_external_cert_files\n          is undefined or ipaserver_external_cert_files|length < 1)\n\n  - name: Install - Setup DS\n    ipaserver_setup_ds:\n      dm_password: \"{{ ipadm_password }}\"\n      password: \"{{ ipaadmin_password }}\"\n      # master_password: \"{{ ipaserver_master_password }}\"\n      domain: \"{{ result_ipaserver_test.domain }}\"\n      realm: \"{{ result_ipaserver_test.realm | default(omit) }}\"\n      hostname: \"{{ result_ipaserver_test.hostname }}\"\n      # ip_addresses: \"{{ result_ipaserver_test.ip_addresses }}\"\n      # reverse_zones: \"{{ result_ipaserver_test.reverse_zones }}\"\n      # setup_adtrust: \"{{ result_ipaserver_test.setup_adtrust }}\"\n      # setup_kra: \"{{ result_ipaserver_test.setup_kra }}\"\n      # setup_dns: \"{{ ipaserver_setup_dns }}\"\n      setup_ca: \"{{ result_ipaserver_test.setup_ca }}\"\n      # no_host_dns: \"{{ result_ipaserver_test.no_host_dns }}\"\n      dirsrv_config_file: \"{{ ipaserver_dirsrv_config_file | default(omit) }}\"\n      dirsrv_cert_files: \"{{ ipaserver_dirsrv_cert_files | default([]) }}\"\n      external_cert_files: \"{{ ipaserver_external_cert_files | default([]) }}\"\n      subject_base: \"{{ result_ipaserver_test.subject_base }}\"\n      ca_subject: \"{{ result_ipaserver_test.ca_subject }}\"\n      # no_reverse: \"{{ ipaserver_no_reverse }}\"\n      # auto_forwarders: \"{{ ipaserver_auto_forwarders }}\"\n      no_pkinit: \"{{ result_ipaserver_test.no_pkinit }}\"\n      no_hbac_allow: \"{{ ipaserver_no_hbac_allow }}\"\n      idstart: \"{{ result_ipaserver_test.idstart }}\"\n      idmax: \"{{ result_ipaserver_test.idmax }}\"\n\n  - name: Install - Setup KRB\n    ipaserver_setup_krb:\n      dm_password: \"{{ ipadm_password }}\"\n      password: \"{{ ipaadmin_password }}\"\n      master_password: \"{{ ipaserver_master_password }}\"\n      domain: \"{{ result_ipaserver_test.domain }}\"\n      realm: \"{{ result_ipaserver_test.realm }}\"\n      hostname: \"{{ result_ipaserver_test.hostname }}\"\n      # ip_addresses: \"{{ result_ipaserver_test.ip_addresses }}\"\n      reverse_zones: \"{{ result_ipaserver_test.reverse_zones }}\"\n      setup_adtrust: \"{{ result_ipaserver_test.setup_adtrust }}\"\n      setup_kra: \"{{ result_ipaserver_test.setup_kra }}\"\n      setup_dns: \"{{ ipaserver_setup_dns }}\"\n      setup_ca: \"{{ result_ipaserver_test.setup_ca }}\"\n      no_host_dns: \"{{ result_ipaserver_test.no_host_dns }}\"\n      external_cert_files: \"{{ ipaserver_external_cert_files | default([]) }}\"\n      subject_base: \"{{ result_ipaserver_test.subject_base }}\"\n      ca_subject: \"{{ result_ipaserver_test.ca_subject }}\"\n      no_reverse: \"{{ ipaserver_no_reverse }}\"\n      auto_forwarders: \"{{ ipaserver_auto_forwarders }}\"\n      no_pkinit: \"{{ result_ipaserver_test.no_pkinit }}\"\n      no_hbac_allow: \"{{ ipaserver_no_hbac_allow }}\"\n      idstart: \"{{ result_ipaserver_test.idstart }}\"\n      idmax: \"{{ result_ipaserver_test.idmax }}\"\n      _pkinit_pkcs12_info: \"{{ result_ipaserver_test._pkinit_pkcs12_info }}\"\n\n  - name: Install - Setup custodia\n    ipaserver_setup_custodia:\n      realm: \"{{ result_ipaserver_test.realm }}\"\n      hostname: \"{{ result_ipaserver_test.hostname }}\"\n      setup_ca: \"{{ result_ipaserver_test.setup_ca }}\"\n\n  - name: Install - Setup CA\n    ipaserver_setup_ca:\n      dm_password: \"{{ ipadm_password }}\"\n      password: \"{{ ipaadmin_password }}\"\n      master_password: \"{{ ipaserver_master_password }}\"\n      # ip_addresses: \"{{ result_ipaserver_test.ip_addresses }}\"\n      domain: \"{{ result_ipaserver_test.domain }}\"\n      realm: \"{{ result_ipaserver_test.realm }}\"\n      hostname: \"{{ result_ipaserver_test.hostname }}\"\n      no_host_dns: \"{{ result_ipaserver_test.no_host_dns }}\"\n      setup_adtrust: \"{{ result_ipaserver_test.setup_adtrust }}\"\n      setup_kra: \"{{ result_ipaserver_test.setup_kra }}\"\n      setup_dns: \"{{ ipaserver_setup_dns }}\"\n      setup_ca: \"{{ result_ipaserver_test.setup_ca }}\"\n      idstart: \"{{ result_ipaserver_test.idstart }}\"\n      idmax: \"{{ result_ipaserver_test.idmax }}\"\n      no_hbac_allow: \"{{ ipaserver_no_hbac_allow }}\"\n      no_pkinit: \"{{ result_ipaserver_test.no_pkinit }}\"\n      dirsrv_config_file: \"{{ ipaserver_dirsrv_config_file | default(omit) }}\"\n      dirsrv_cert_files: \"{{ ipaserver_dirsrv_cert_files | default([]) }}\"\n      _dirsrv_pkcs12_info: \"{{ result_ipaserver_test._dirsrv_pkcs12_info }}\"\n      external_ca: \"{{ ipaserver_external_ca }}\"\n      external_cert_files: \"{{ ipaserver_external_cert_files | default([]) }}\"\n      subject_base: \"{{ result_ipaserver_test.subject_base }}\"\n      _subject_base: \"{{ result_ipaserver_test._subject_base }}\"\n      ca_subject: \"{{ result_ipaserver_test.ca_subject }}\"\n      _ca_subject: \"{{ result_ipaserver_test._ca_subject }}\"\n      ca_signing_algorithm: \"{{ ipaserver_ca_signing_algorithm |\n                                default(omit) }}\"\n      reverse_zones: \"{{ result_ipaserver_test.reverse_zones }}\"\n      no_reverse: \"{{ ipaserver_no_reverse }}\"\n      auto_forwarders: \"{{ ipaserver_auto_forwarders }}\"\n\n  - name: Install - Setup otpd\n    ipaserver_setup_otpd:\n      realm: \"{{ result_ipaserver_test.realm }}\"\n      hostname: \"{{ result_ipaserver_test.hostname }}\"\n      setup_ca: \"{{ result_ipaserver_test.setup_ca }}\"\n\n  - name: Install - Setup HTTP\n    ipaserver_setup_http:\n      dm_password: \"{{ ipadm_password }}\"\n      password: \"{{ ipaadmin_password }}\"\n      master_password: \"{{ ipaserver_master_password }}\"\n      domain: \"{{ result_ipaserver_test.domain }}\"\n      realm: \"{{ result_ipaserver_test.realm }}\"\n      hostname: \"{{ result_ipaserver_test.hostname }}\"\n      # ip_addresses: \"{{ result_ipaserver_test.ip_addresses }}\"\n      reverse_zones: \"{{ result_ipaserver_test.reverse_zones }}\"\n      setup_adtrust: \"{{ result_ipaserver_test.setup_adtrust }}\"\n      setup_kra: \"{{ result_ipaserver_test.setup_kra }}\"\n      setup_dns: \"{{ ipaserver_setup_dns }}\"\n      setup_ca: \"{{ result_ipaserver_test.setup_ca }}\"\n      no_host_dns: \"{{ result_ipaserver_test.no_host_dns }}\"\n      dirsrv_cert_files: \"{{ ipaserver_dirsrv_cert_files | default([]) }}\"\n      external_cert_files: \"{{ ipaserver_external_cert_files | default([]) }}\"\n      subject_base: \"{{ result_ipaserver_test.subject_base }}\"\n      _subject_base: \"{{ result_ipaserver_test._subject_base }}\"\n      ca_subject: \"{{ result_ipaserver_test.ca_subject }}\"\n      _ca_subject: \"{{ result_ipaserver_test._ca_subject }}\"\n      no_reverse: \"{{ ipaserver_no_reverse }}\"\n      auto_forwarders: \"{{ ipaserver_auto_forwarders }}\"\n      no_pkinit: \"{{ result_ipaserver_test.no_pkinit }}\"\n      no_hbac_allow: \"{{ ipaserver_no_hbac_allow }}\"\n      idstart: \"{{ result_ipaserver_test.idstart }}\"\n      idmax: \"{{ result_ipaserver_test.idmax }}\"\n      http_cert_files: \"{{ ipaserver_http_cert_files | default([]) }}\"\n      no_ui_redirect: \"{{ ipaserver_no_ui_redirect }}\"\n\n  - name: Install - Setup KRA\n    ipaserver_setup_kra:\n      hostname: \"{{ result_ipaserver_test.hostname }}\"\n      setup_ca: \"{{ result_ipaserver_test.setup_ca }}\"\n      dm_password: \"{{ ipadm_password }}\"\n      setup_kra: \"{{ result_ipaserver_test.setup_kra }}\"\n      realm: \"{{ result_ipaserver_test.realm }}\"\n    when: result_ipaserver_test.setup_kra | bool\n\n  - name: Install - Setup DNS\n    ipaserver_setup_dns:\n      hostname: \"{{ result_ipaserver_test.hostname }}\"\n      setup_ca: \"{{ result_ipaserver_test.setup_ca }}\"\n      setup_dns: \"{{ ipaserver_setup_dns }}\"\n      forwarders: \"{{ result_ipaserver_test.forwarders }}\"\n      forward_policy: \"{{ result_ipaserver_test.forward_policy }}\"\n      zonemgr: \"{{ ipaserver_zonemgr | default(omit) }}\"\n      no_dnssec_validation: \"{{ result_ipaserver_test.no_dnssec_validation }}\"\n      ### additional ###\n      dns_ip_addresses: \"{{ result_ipaserver_test.dns_ip_addresses }}\"\n      dns_reverse_zones: \"{{ result_ipaserver_test.dns_reverse_zones }}\"\n    when: ipaserver_setup_dns | bool\n\n  - name: Install - Setup ADTRUST\n    ipaserver_setup_adtrust:\n      hostname: \"{{ result_ipaserver_test.hostname }}\"\n      setup_ca: \"{{ result_ipaserver_test.setup_ca }}\"\n      setup_adtrust: \"{{ result_ipaserver_test.setup_adtrust }}\"\n      ### ad trust ###\n      enable_compat: \"{{ ipaserver_enable_compat }}\"\n      rid_base: \"{{ result_ipaserver_test.rid_base }}\"\n      secondary_rid_base: \"{{ result_ipaserver_test.secondary_rid_base }}\"\n      ### additional ###\n      adtrust_netbios_name: \"{{ result_ipaserver_test.adtrust_netbios_name }}\"\n      adtrust_reset_netbios_name:\n        \"{{ result_ipaserver_test.adtrust_reset_netbios_name }}\"\n    when: result_ipaserver_test.setup_adtrust\n\n  - name: Install - Set DS password\n    ipaserver_set_ds_password:\n      dm_password: \"{{ ipadm_password }}\"\n      password: \"{{ ipaadmin_password }}\"\n      domain: \"{{ result_ipaserver_test.domain }}\"\n      realm: \"{{ result_ipaserver_test.realm }}\"\n      hostname: \"{{ result_ipaserver_test.hostname }}\"\n      setup_ca: \"{{ result_ipaserver_test.setup_ca }}\"\n      subject_base: \"{{ result_ipaserver_test.subject_base }}\"\n      ca_subject: \"{{ result_ipaserver_test.ca_subject }}\"\n      no_pkinit: \"{{ result_ipaserver_test.no_pkinit }}\"\n      no_hbac_allow: \"{{ ipaserver_no_hbac_allow }}\"\n      idstart: \"{{ result_ipaserver_test.idstart }}\"\n      idmax: \"{{ result_ipaserver_test.idmax }}\"\n      dirsrv_config_file: \"{{ ipaserver_dirsrv_config_file | default(omit) }}\"\n      _dirsrv_pkcs12_info: \"{{ result_ipaserver_test._dirsrv_pkcs12_info }}\"\n\n  - name: Install - Setup client\n    include_role:\n      name: ipaclient\n    vars:\n      state: present\n      ipaclient_on_master: yes\n      ipaclient_domain: \"{{ result_ipaserver_test.domain }}\"\n      ipaclient_realm: \"{{ result_ipaserver_test.realm }}\"\n      ipaclient_servers: [\"{{ result_ipaserver_test.hostname }}\"]\n      ipaclient_hostname: \"{{ result_ipaserver_test.hostname }}\"\n      ipaclient_no_ntp:\n        \"{{ 'true' if result_ipaserver_test.ipa_python_version >= 40690\n             else 'false' }}\"\n      ipaclient_install_packages: \"{{ ipaserver_install_packages }}\"\n\n  # - name: Install - Setup client\n  #   command: >\n  #     /usr/sbin/ipa-client-install\n  #     --unattended\n  #     --on-master\n  #     --domain \"{{ result_ipaserver_test.domain }}\"\n  #     --realm \"{{ result_ipaserver_test.realm }}\"\n  #     --server \"{{ result_ipaserver_test.hostname }}\"\n  #     --hostname \"{{ result_ipaserver_test.hostname }}\"\n  #     {{ \"--mkhomedir\" if ipaclient_mkhomedir | bool else \"\" }}\n  #   #  {{ \"--no-dns-sshfp\" if ipaclient_no_dns_sshfp | bool else \"\" }}\n  #   #  {{ \"--ssh-trust-dns\" if ipaclient_ssh_trust_dns | bool else \"\" }}\n  #   #  {{ \"--no-ssh\" if ipaclient_no_ssh | bool else \"\" }}\n  #   #  {{ \"--no-sshd\" if ipaclient_no_sshd | bool else \"\" }}\n\n  - name: Install - Enable IPA\n    ipaserver_enable_ipa:\n      hostname: \"{{ result_ipaserver_test.hostname }}\"\n      setup_dns: \"{{ ipaserver_setup_dns }}\"\n      setup_ca: \"{{ result_ipaserver_test.setup_ca }}\"\n    register: result_ipaserver_enable_ipa\n\n  - name: Install - Cleanup root IPA cache\n    file:\n      path: \"/root/.ipa_cache\"\n      state: absent\n    when: result_ipaserver_enable_ipa.changed\n\n  - name: Install - Configure firewalld\n    command: >\n      firewall-cmd\n      --permanent\n      --add-service=freeipa-ldap\n      --add-service=freeipa-ldaps\n      {{ \"--add-service=freeipa-trust\" if ipaserver_setup_adtrust | bool\n         else \"\" }}\n      {{ \"--add-service=dns\" if ipaserver_setup_dns | bool else \"\" }}\n      {{ \"--add-service=ntp\" if not ipaclient_no_ntp | bool else \"\" }}\n    when: ipaserver_setup_firewalld | bool\n\n  - name: Install - Configure firewalld runtime\n    command: >\n      firewall-cmd\n      --add-service=freeipa-ldap\n      --add-service=freeipa-ldaps\n      {{ \"--add-service=freeipa-trust\" if ipaserver_setup_adtrust | bool\n         else \"\" }}\n      {{ \"--add-service=dns\" if ipaserver_setup_dns | bool else \"\" }}\n      {{ \"--add-service=ntp\" if not ipaclient_no_ntp | bool else \"\" }}\n    when: ipaserver_setup_firewalld | bool\n\n  when: not ansible_check_mode and not\n        (not result_ipaserver_test.changed and\n         (result_ipaserver_test.client_already_configured is defined or\n          result_ipaserver_test.server_already_configured is defined))\n"}, {"commit_sha": "e478d31e50786acdba83bfa5ffa99c63b5d1410b", "sha": "dda5dcf975c328a6d7a81de3aa899e7c6adc3667", "filename": "roles/common/tasks/freebsd.yml", "repository": "trailofbits/algo", "decoded_content": "---\n- set_fact:\n    config_prefix: \"/usr/local/\"\n    strongswan_shell: /usr/sbin/nologin\n    strongswan_home: /var/empty\n    root_group: wheel\n    ssh_service_name: sshd\n    apparmor_enabled: false\n    strongswan_additional_plugins:\n      - kernel-pfroute\n      - kernel-pfkey\n    ansible_python_interpreter: /usr/local/bin/python2.7\n    tools:\n      - git\n      - subversion\n      - screen\n      - coreutils\n      - openssl\n      - bash\n      - wget\n    sysctl:\n      - item: net.inet.ip.forwarding\n        value: 1\n      - item: net.inet6.ip6.forwarding\n        value: 1\n  tags:\n    - always\n\n- setup:\n\n- name: Install tools\n  package: name=\"{{ item }}\" state=present\n  with_items:\n    - \"{{ tools|default([]) }}\"\n  tags:\n    - always\n\n- name: Loopback included into the rc config\n  blockinfile:\n    dest: /etc/rc.conf\n    create: yes\n    block: |\n      cloned_interfaces=\"lo100\"\n      ifconfig_lo100=\"inet {{ local_service_ip }} netmask 255.255.255.255\"\n      ifconfig_lo100=\"inet6 FCAA::1/64\"\n  notify:\n    - restart loopback bsd\n  tags:\n    - always\n\n- name: Enable the gateway features\n  lineinfile: dest=/etc/rc.conf regexp='^{{ item.param }}.*' line='{{ item.param }}={{ item.value }}'\n  with_items:\n    - { param: firewall_enable, value: '\"YES\"' }\n    - { param: firewall_type, value: '\"open\"' }\n    - { param: gateway_enable, value: '\"YES\"' }\n    - { param: natd_enable, value: '\"YES\"' }\n    - { param: natd_interface, value: '\"{{ ansible_default_ipv4.device|default() }}\"' }\n    - { param: natd_flags, value: '\"-dynamic -m\"' }\n  notify:\n    - restart ipfw\n  tags:\n    - always\n\n- name: FreeBSD | Activate IPFW\n  shell: >\n    kldstat -n ipfw.ko || kldload ipfw ; sysctl net.inet.ip.fw.enable=0 &&\n    bash /etc/rc.firewall && sysctl net.inet.ip.fw.enable=1\n\n- meta: flush_handlers\n"}, {"commit_sha": "1818facd0a58a2b42203a403130b71825b960653", "sha": "9fcc07fae0acf7b39eef728b3f9208f6f113e5d2", "filename": "tasks/section_12_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 12.1 Verify Permissions on /etc/passwd (Scored)\n    file: path=/etc/passwd mode=0644\n    tags:\n      - section12\n      - section12.1\n\n  - name: 12.2 Verify Permissions on /etc/shadow (Scored)\n    file: path=/etc/shadow mode='o-rwx,g-rw'\n    tags:\n      - section12\n      - section12.2\n\n  - name: 12.3 Verify Permissions on /etc/group (Scored)\n    file: path=/etc/group mode=0644\n    tags:\n      - section12\n      - section12.3\n\n  - name: 12.4 Verify User/Group Ownership on /etc/passwd (Scored)\n    file: path=/etc/passwd owner=root group=root\n    tags:\n      - section12\n      - section12.4\n\n  - name: 12.5 Verify User/Group Ownership on /etc/shadow (Scored)\n    file: path=/etc/shadow owner=root group=shadow\n    tags:\n      - section12\n      - section12.5\n\n  - name: 12.6 Verify User/Group Ownership on /etc/group (Scored)\n    file: path=/etc/group owner=root group=root\n    tags:\n      - section12\n      - section12.6\n\n  - name: 12.7 Find World Writable Files (check) (Not Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -type f -perm -0002 -print\n    changed_when: False\n    failed_when: False\n    register: world_files\n    tags:\n      - section12\n      - section12.7\n\n  - name: 12.7 Find World Writable Files (Not Scored)\n    debug: msg=\"{{ item }}\"\n    with_items:\n        world_files.stdout_lines\n    tags:\n      - section12\n      - section12.7\n\n  - name: 12.8 Find Un-owned Files and Directories (check) (Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -nogroup -ls\n    changed_when: False\n    failed_when: False\n    register: unowned_files\n    tags:\n      - section12\n      - section12.8\n\n  - name: 12.8 Find Un-owned Files and Directories (Scored)\n    debug: msg=\"{{ item }}\"\n    with_items:\n        unowned_files.stdout_lines\n    tags:\n      - section12\n      - section12.9\n\n  - name: 12.9 Find Un-grouped Files and Directories (check) (Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -nogroup -ls\n    changed_when: False\n    failed_when: False\n    register: ungrouped_files\n    tags:\n      - section12\n      - section12.9\n\n  - name: 12.9 Find Un-grouped Files and Directories (Scored)\n    debug: >\n        msg=\"{{ item }}\"\n    with_items:\n        ungrouped_files.stdout_lines\n    tags:\n      - section12\n      - section12.9\n\n  - name: 12.10 Find SUID System Executables (check) (Not Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -type f -perm -4000 -print\n    changed_when: False\n    failed_when: False\n    register: suid_files\n    tags:\n      - section12\n      - section12.10\n\n  - name: 12.10 Find SUID System Executables (Not Scored)\n    debug: msg=\"{{ item }}\"\n    with_items:\n        suid_files.stdout_lines\n    tags:\n      - section12\n      - section12.10\n\n  - name: 12.11 Find SGID System Executables (check) (Not Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -type f -perm -2000 -print\n    changed_when: False\n    failed_when: False\n    register: gsuid_files\n    tags:\n      - section12\n      - section12.11\n\n  - name: 12.11 Find SGID System Executables (Not Scored)\n    debug: msg=\"{{ item }}\"\n    with_items:\n        gsuid_files.stdout_lines\n    tags:\n      - section12\n      - section12.10\n"}, {"commit_sha": "a10c5f4577e6e74feb1fadec4bcbab039b8b180a", "sha": "09a9ca6af0e41cfcde9aa5e09f2a39cb077b451e", "filename": "tasks/install-docker.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Set version string\n  set_fact:\n    _docker_version_string: \"{{ docker_os_pkg_version_separator[_docker_os_dist] }}{{ docker_version }}\"\n  when: docker_version != ''\n\n- name: Set packages state to latest\n  set_fact:\n    _docker_pkg_state: 'latest'\n  when: docker_latest_version | bool\n\n- name: Filter out packages to match older Docker CE versions\n  set_fact:\n    _docker_packages:\n      - docker-ce\n  when:\n    - docker_version != ''\n    - docker_version | match('17.') or docker_version | match('18.03') or docker_version | match('18.06')\n\n- name: Ensure some kind of compatibility for no longer officially supported distributions since Docker CE 18.09\n  set_fact:\n    _docker_packages:\n      - docker-ce\n  when:\n    - _docker_packages is not defined\n    - (_docker_os_dist == \"Debian\" and _docker_os_dist_major_version < '9') or\n      (_docker_os_dist == \"Fedora\" and _docker_os_dist_major_version < '27') or\n      (_docker_os_dist == \"Ubuntu\" and _docker_os_dist_major_version < '16')\n\n- name: Ensure Docker CE is installed\n  become: true\n  package:\n    name: \"{{ (item | search('docker')) | ternary((item + _docker_version_string | default('')), item) }}\"\n    state: \"{{ _docker_pkg_state | default('present') }}\"\n  with_items: \"{{ _docker_packages | default(docker_packages) }}\"\n  register: _pkg_result\n  until: _pkg_result|succeeded\n  notify: restart docker\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "3f4d4ebcd175556e27f229cb5a2d577ca5db9dcc", "filename": "playbooks/openshift/cinder-registry.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "--- \n# Create a volume for the registry\n- hosts: localhost\n  roles: \n  - infra-ansible/roles/osp/admin-volume\n\n- hosts: OSEv3\n  tasks:\n  - name: \"set registry volumeID fact\"\n    set_fact: \n      openshift_hosted_registry_storage_openstack_volumeID: \"{{ disk.volume.id }}\"\n      openshift_hosted_registry_storage_volume_size: \"{{ disk.volume.size }}Gi\"\n    with_items: \n    -  \"{{ hostvars['localhost'].os_volumes.results }}\"\n    when: \n    - disk.item.purpose  == \"ose_registry\"\n    loop_control:\n      loop_var: disk\n\n\n"}, {"commit_sha": "fa8eab8d7ae5ae376827cb0622a0620955a9c64f", "sha": "3f7d34255f3d0f89553453374373f4fa056fe333", "filename": "tasks/main.yml", "repository": "lean-delivery/ansible-role-java", "decoded_content": "---\n- name: \"Enable EPEL repository\"\n  yum:\n    name: \"https://dl.fedoraproject.org/pub/epel/epel-release-latest-\\\n      {{ ansible_distribution_major_version }}.noarch.rpm\"\n    state: \"present\"\n  delegate_to: \"localhost\"\n  connection: \"local\"\n  run_once: True\n  become: True\n  when:\n    - transport == \"s3\"\n    - ansible_os_family == 'RedHat'\n\n- name: \"Install pip package\"\n  package:\n    name: \"{{ ansible_os_family == 'Ubuntu' | ternary('python-pip','python2-pip') }}\"\n    state: \"present\"\n  delegate_to: \"localhost\"\n  connection: \"local\"\n  run_once: True\n  become: True\n  when: transport == \"s3\"\n\n- name: \"Install boto module\"\n  pip:\n    name: \"{{ item }}\"\n    state: present\n    extra_args: --upgrade\n  with_items:\n    - \"pip\"\n    - \"botocore\"\n    - \"boto\"\n    - \"boto3\"\n  delegate_to: \"localhost\"\n  connection: \"local\"\n  run_once: True\n  become: True\n  when: transport == \"s3\"\n\n- name: \"Fetch oracle artifact with {{ transport }} transport\"\n  include_tasks: \"{{ transport_driver }}\"\n  with_first_found:\n    - \"fetch/{{ transport }}.yml\"\n    - \"fetch/unknown-transport.yml\"\n  loop_control:\n    loop_var: transport_driver\n\n- name: \"Choose platform based task \"\n  include_tasks: \"{{ platform }}\"\n  with_first_found:\n    - \"system/{{ ansible_facts.system }}.yml\"\n    - \"system/not-supported.yml\"\n  loop_control:\n    loop_var: platform\n\n- name: \"Fetch oracle security policy with {{ java_unlimited_policy_transport }} transport\"\n  include_tasks: \"{{ transport_driver }}\"\n  with_first_found:\n    - \"security_policy_fetch/{{ java_unlimited_policy_transport }}.yml\"\n    - \"security_policy_fetch/unknown-transport.yml\"\n  loop_control:\n    loop_var: transport_driver\n  when:\n    - java_unlimited_policy_enabled\n    - (java_major_version|int == 8 and java_minor_version|int < 151) or (java_major_version|int < 8)\n\n- name: \"Apply security policy patch\"\n  include_tasks: \"{{ platform }}\"\n  with_first_found:\n    - \"security_policy_apply/{{ ansible_facts.system }}.yml\"\n    - \"security_policy_apply/not-supported.yml\"\n  loop_control:\n    loop_var: platform\n  when:\n    - java_unlimited_policy_enabled\n"}, {"commit_sha": "218cdc58f9fe9d7ece7d43e5f100fe9631fde5cc", "sha": "2fc1c4f7c3f1e203706ed4eddf707712a095f304", "filename": "tasks/setup.yml", "repository": "fubarhouse/ansible-role-golang", "decoded_content": "---\n\n# Define system-specific variables for fubarhouse.golang.\n\n- name: \"Go-Lang | Define GOARCH for amd64 systems\"\n  set_fact:\n    GOARCH: \"amd64\"\n  when:\n    - ansible_architecture == 'x86_64' or ansible_distribution == 'CentOS' or ansible_distribution == 'Debian' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'FreeBSD' or ansible_distribution == 'RedHat' or ansible_distribution == 'Ubuntu' or ansible_distribution == 'Ubuntu'\n    - GOARCH is not defined\n\n- name: \"Go-Lang | Define GOOS for Darwin systems\"\n  set_fact:\n    GOOS: \"darwin\"\n  when:\n    - ansible_os_family == 'Darwin'\n    - ansible_distribution == 'MacOSX'\n    - GOOS is not defined\n\n- name: \"Go-Lang | Define GOOS for linux systems\"\n  set_fact:\n    GOOS: \"linux\"\n  when:\n    - ansible_architecture == 'x86_64' or ansible_distribution == 'CentOS' or ansible_distribution == 'Debian' or ansible_distribution == 'Red Hat Enterprise Linux' or ansible_distribution == 'FreeBSD' or ansible_distribution == 'RedHat' or ansible_distribution == 'Ubuntu' or ansible_distribution == 'Ubuntu'\n    - GOOS is not defined\n\n- name: \"Go-Lang | Define URL for distribution\"\n  set_fact:\n    go_url: \"{{ go_custom_mirror }}/go{{ go_version }}.{{ GOOS }}-{{ GOARCH }}.tar.gz\"\n\n- name: \"Go-Lang | Looking for existing installation\"\n  stat:\n    path: \"{{ GOROOT }}/bin/go\"\n  register: go_binary\n  failed_when: false\n\n- name: \"Go-Lang | Define GOROOT\"\n  set_fact:\n    GOROOT: \"{{ GOROOT }}\"\n  when: GOROOT is defined\n\n- name: \"Go-Lang | Define GOPATH\"\n  set_fact:\n    GOPATH: \"{{ GOROOT }}/bin\"\n  when:\n    - GOPATH is not defined\n    - GOROOT is defined\n\n- name: \"Go-Lang | Getting version information\"\n  become: yes\n  become_user: \"root\"\n  shell: \"{{ GOPATH }}/go version\"\n  environment:\n    GOPATH: \"{{ GOPATH }}\"\n    GOROOT: \"{{ GOROOT }}\"\n  register: current_go_version\n  changed_when: false\n  when: go_binary.stat.exists == true\n"}, {"commit_sha": "1823ab8e2ab72e1af24e977c7ae1829ef158a941", "sha": "4b547d2543bddb8f2e28327593ce05415fce1cce", "filename": "ops/playbooks/splunk_uf.yml", "repository": "HewlettPackard/Docker-SimpliVity", "decoded_content": "###\n# Copyright (2017) Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# You may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###\n---\n- name: Install Splunk Universal Forwarder (Linux)\n  hosts: vms\n  gather_facts: false\n  become_user: root\n  become: true\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - ./includes/internal_vars.yml\n\n  environment:\n    - \"{{ env }}\"\n\n  tasks:\n\n  - block:\n\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n\n    - name: source stack specific variables\n      include_vars:\n        file: ../templates/splunk/{{ monitoring_stack }}/vars.yml\n\n#\n# section for the logger VM which is used to collect syslog from the ESX infrastructure and the UCP syslogs\n#\n    - block:\n\n      - name: Open  ports  in the firewall\n        firewalld:\n          port: \"{{ item }}\"\n          immediate: true\n          permanent: true\n          state: enabled\n        with_items: \"{{ splunk_architecture_syslog_ports }}\"\n\n      when: inventory_hostname in groups.logger\n\n#\n# Section for all Linux hosts\n#\n    - block:\n\n      - name: Copy Universal forwarder for Linux Pkg\n        copy:\n          src: \"../files/splunk/linux/{{ splunk_architecture_universal_forwarder_package }}.rpm\"\n          dest: /root/scripts/monitoring/\n\n      - name: Copy Splunk Universal Forwarder for Linux start script\n        template:\n          src: ../templates/splunk/{{ monitoring_stack }}/start_uf_linux.sh.j2\n          dest: /root/scripts/monitoring/start_uf_linux.sh\n        notify: RestartLinuxSplunkUF\n      - file:\n          path: /root/scripts/monitoring/start_uf_linux.sh\n          mode: 0744\n        notify: RestartLinuxSplunkUF\n\n      - name: Install Universal Forwarder for Linux\n        yum:\n          name:  /root/scripts/monitoring//{{ splunk_architecture_universal_forwarder_package }}.rpm\n          state: present\n\n      - name: Generating hash password\n        command:  /opt/splunkforwarder/bin/splunk hash-passwd {{ splunk_uf_password }}\n        register: splunk_uf_hash\n        no_log: yes\n\n      - name: Templating seed file \n        template:\n          src: ../templates/splunk/{{ monitoring_stack }}/user-seed.conf.j2\n          dest:  /opt/splunkforwarder/etc/system/local/user-seed.conf\n        vars:\n          splunk_uf_hash_password: \"{{  splunk_uf_hash.stdout }}\" \n        notify: RestartLinuxSplunkUF\n\n      - name: Copy Splunk Universal Forwarder for Linux configuration files\n        copy:\n          src: \"../files/splunk/linux/SPLUNK_HOME/\"\n          dest: \"/opt/splunkforwarder/\"\n        notify: RestartLinuxSplunkUF\n\n      - name: Copy Splunk Universal Forwarder for Linux Technical Add-ons on Docker hosts\n        copy:\n          src: \"../files/splunk/linux/DOCKER_TAS/\"\n          dest: \"/opt/splunkforwarder/\"\n          mode: preserve\n        notify: RestartLinuxSplunkUF\n        when: inventory_hostname in groups.docker\n\n      - local_action: stat path=\"../templates/splunk/{{ monitoring_stack }}/outputs.conf.j2\"\n        register: res\n\n      - name: Debug\n        debug: var=res.stat.exists\n        when: _debug is defined\n\n      - name: Copy output.conf from template folder if present\n        template:\n           src: \"../templates/splunk/{{ monitoring_stack }}/outputs.conf.j2\"\n           dest: \"/opt/splunkforwarder/etc/system/local/outputs.conf\"\n        notify: RestartLinuxSplunkUF\n        when: res.stat.exists == true\n\n      when: inventory_hostname in groups.vms\n      \n# end of section of non docker hosts\n\n    when: monitoring_stack is defined\n\n\n  - debug: msg=\"No splunk integration wanted\"\n    when: monitoring_stack is not defined\n\n  handlers:\n  - name: RestartLinuxSplunkUF\n    shell: /root/scripts/monitoring/start_uf_linux.sh\n    args:\n      chdir: /root/scripts/monitoring\n\n#########################################################################\n#\n# Play 2: Install Splunk Universal Forwarder (Windows)\n#\n#########################################################################\n- name: Install Splunk Universal Forwarder (Windows) \n  hosts: logger win_worker\n  gather_facts: false\n  connection: local\n  user: remote\n  become: false\n\n  vars_files:\n    - ../group_vars/vars\n    - ../group_vars/vault\n    - ./includes/internal_vars.yml\n\n  environment:\n    - \"{{ env }}\"\n\n  tasks:\n\n  - block:  # monitoring_stack is defined\n\n    - include_tasks: includes/find_ucp.yml\n      vars:\n        ping_servers: \"{{ groups.ucp }}\"\n\n    - name: source stack specific variables\n      include_vars:\n        file: ../templates/splunk/{{ monitoring_stack }}/vars.yml\n\n#\n# section for the logger VM which is used to collect syslog from the ESX inrastructure and the UCP syslogs\n#\n    - block:\n\n      - name: Open  ports  in the firewall\n        firewalld:\n          port: \"{{ item }}\"\n          immediate: true\n          permanent: true\n          state: enabled\n        with_items: \"{{ splunk_architecture_syslog_ports }}\"\n\n      when: inventory_hostname in groups.logger\n\n#\n# Section for Windows hosts\n#\n    - block:\n\n      - name: Copy Universal forwarder for Windows Pkg\n        win_copy:\n          src: \"../files/splunk/windows/{{ splunk_architecture_universal_forwarder_package }}.msi\"\n          dest: \"./{{ splunk_architecture_universal_forwarder_package }}.msi\"\n\n      - name: Copy Splunk Universal forwarder for Windows install script\n        win_template:\n          src: ../templates/splunk/{{ monitoring_stack }}/install_uf_windows.ps1.j2\n          dest: .\\install_uf_windows.ps1\n\n      - name: Install Universal Forwarder for Windows\n        win_shell: .\\install_uf_windows.ps1\n\n      - name: Copy Splunk Universal Forwarder for Windows configuration files \n        win_copy:\n          src: \"../files/splunk/windows/SPLUNK_HOME/\"\n          dest: \"c:/Program Files/SplunkUniversalForwarder/\"\n        notify: RestartWindowsSplunkUF\n\n      - name: Generating hash password\n        win_shell: |\n          cd \"C:\\Program Files\\SplunkUniversalForwarder\\bin\"\n          get-location\n          .\\splunk.exe hash-passwd {{ splunk_uf_password }}\n        register: splunk_uf_hash\n        #no_log: yes\n\n      - name: Templating seed file\n        template:\n          src: ../templates/splunk/{{ monitoring_stack }}/user-seed.conf.j2\n          dest:  'C:\\Program Files\\SplunkUniversalForwarder\\etc\\system\\local\\user-seed.conf'\n        vars:\n          splunk_uf_hash_password: \"{{  splunk_uf_hash.stdout }}\"\n        notify: RestartWindowsSplunkUF\n\n\n      - local_action: stat path=\"../templates/splunk/{{ monitoring_stack }}/outputs.conf.j2\"\n        register: res\n\n      - name: Debug\n        debug: var=res.stat.exists\n        when: _debug is defined\n\n      - name: Copy output.conf from template folder if present\n        win_template:\n           src: \"../templates/splunk/{{ monitoring_stack }}/outputs.conf.j2\"\n           dest: \"c:/Program Files/SplunkUniversalForwarder/etc/system/local/outputs.conf\"\n        notify: RestartWindowsSplunkUF\n        when: res.stat.exists == true\n\n      - name: Start Splunk Forwarder Service\n        win_service:\n          name: SplunkForwarder\n          state: started\n\n      when: inventory_hostname in groups.win_worker\n\n# end of section for Windows Hosts\n\n    when: monitoring_stack is defined\n\n\n  - debug: msg=\"No splunk integration wanted\"\n    when: monitoring_stack is not defined\n\n\n  handlers:\n  - name: RestartWindowsSplunkUF\n    win_service:\n      name: SplunkForwarder\n      state: restarted\n"}, {"commit_sha": "1b78e048b0eec3ce34b75f32800bef93e8a2a4cd", "sha": "641df26691bf1a8935499869be59ed6c947b1d88", "filename": "playbooks/openshift/post-provision-openstack.yml", "repository": "redhat-cop/casl-ansible", "decoded_content": "---\n\n# Assign hostnames\n- hosts: cluster_hosts\n  pre_tasks:\n  - import_tasks: pre-tasks.yml\n  roles:\n  - role: openshift-ansible-contrib/roles/hostnames\n\n# Build and process DNS Records\n- hosts: localhost\n  pre_tasks:\n  - import_tasks: pre-tasks.yml\n  roles:\n  - role: openshift-ansible-contrib/roles/dns-records\n  - role: infra-ansible/roles/dns\n\n# provision cinder volume\n- import_playbook: cinder-registry.yml\n  when:\n  - openshift_hosted_registry_storage_kind is defined\n  - openshift_hosted_registry_storage_kind == \"openstack\"\n  - openshift_hosted_registry_storage_openstack_volumeID is not defined\n\n"}, {"commit_sha": "a10c5f4577e6e74feb1fadec4bcbab039b8b180a", "sha": "2e28616f7378bdc60055d77883dea5513e2dfaf6", "filename": "tasks/checks/distribution-checks.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Fail if unsupported CentOS/RedHat version\n  fail:\n    msg: \"CentOS/RedHat 7 or later is required!\"\n  when: (_docker_os_dist == \"CentOS\" or _docker_os_dist == \"RedHat\") and\n        _docker_os_dist_major_version < '7'\n\n- name: Fail if unsupported Fedora version\n  fail:\n    msg: \"Fedora 24 or later is required!\"\n  when: _docker_os_dist == \"Fedora\" and\n        _docker_os_dist_major_version < '24'\n\n- name: Fail if unsupported Ubuntu version\n  fail:\n    msg: \"Ubuntu 14 or later is required!\"\n  when: _docker_os_dist == \"Ubuntu\" and\n        _docker_os_dist_major_version < '14'\n\n- name: Fail if unsupported Debian version\n  fail:\n    msg: \"Debian 7 (wheezy) or later is required!\"\n  when: _docker_os_dist == \"Debian\" and\n        _docker_os_dist_major_version < '7'\n\n- name: Fail if kernel version is lower than 3.10\n  fail:\n    msg: \"Kernel version 3.10 or later is required!\"\n  when: ansible_kernel | version_compare(\"3.10\", '<')\n\n- name: Fail if this roles does not support the distribution\n  fail:\n    msg: \"Distribution {{ _docker_os_dist }} is not supported by this role!\"\n  when: _docker_os_dist != \"Fedora\" and\n        _docker_os_dist != \"CentOS\" and\n        _docker_os_dist != \"RedHat\" and\n        _docker_os_dist != \"Ubuntu\" and\n        _docker_os_dist != \"Debian\"\n"}, {"commit_sha": "45971be8249cc4627ef8ddfacf55a661b7fc13ca", "sha": "9c22ecb304a45f42d737174813ff5be24545a258", "filename": "tasks/configure-docker.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Ensure /etc/docker directory exists\n  file:\n    path: /etc/docker\n    state: directory\n    mode: 0755\n  become: true\n\n- name: Configure Docker daemon (file)\n  copy:\n    src: \"{{ docker_daemon_config_file }}\"\n    dest: /etc/docker/daemon.json\n  become: true\n  notify: restart docker\n  when: docker_daemon_config_file is defined\n\n- name: Configure Docker daemon (variables)\n  copy:\n    content: \"{{ docker_daemon_config | to_nice_json }}\"\n    dest: /etc/docker/daemon.json\n  become: true\n  notify: restart docker\n  when: docker_daemon_config_file is not defined and\n        docker_daemon_config is defined\n\n- name: Ensure Docker default user namespace is defined in subuid and subgid\n  lineinfile:\n    path: \"{{ item }}\"\n    regexp: '^dockremap'\n    line: 'dockremap:500000:65536'\n  become: yes\n  with_items:\n    - /etc/subuid\n    - /etc/subgid\n  when: (_docker_os_dist == \"CentOS\" or _docker_os_dist == \"RedHat\") and\n        ((docker_daemon_config is defined and\n        docker_daemon_config['userns-remap'] is defined and\n        docker_daemon_config['userns-remap'] == 'default') or\n        docker_bug_usermod|bool == true)\n\n- name: Ensure thin-provisioning-tools is installed when devicemapper is used (Ubuntu)\n  package:\n    name: thin-provisioning-tools\n    state: present\n  become: yes\n  when: (_docker_os_dist == \"Ubuntu\" or _docker_os_dist == \"Debian\") and\n        docker_daemon_config['storage-driver'] is defined and\n        docker_daemon_config['storage-driver'] == 'devicemapper'\n\n- name: Enable Docker service\n  service:\n    name: docker\n    enabled: yes\n  notify: restart docker\n  register: docker_service\n  become: yes\n\n- name: Trigger start/restart of Docker\n  service:\n    name: docker\n  notify: restart docker\n  changed_when: docker_service.status.SubState != \"running\"\n  when: docker_service.status is defined\n\n"}, {"commit_sha": "fef51771c066386c3ba52991802d95e2e1391a64", "sha": "a8a7cfe0617a36f6b88dbcffe553083fdf1e144b", "filename": "tasks/selinux-RedHat.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n\n- name: Make sure we have the necessary yum packages available for selinux\n  yum:\n    name:\n      - libselinux-python\n      - libsemanage-python\n    state: present\n"}, {"commit_sha": "bbfe2b84a5a9f265b136e89526f4fe314a6e097f", "sha": "7ed23f10eca0bef6bada6123ec44cb164a78d06b", "filename": "roles/ovirt-engine-cleanup/defaults/main.yml", "repository": "rhevm-qe-automation/ovirt-ansible", "decoded_content": "---\novirt_engine_dwh: True\novirt_engine_type: 'ovirt-engine'\novirt_engine_version: '4.0'\n\novirt_engine_db_host: 'localhost'\novirt_engine_db_port: 5432\novirt_engine_db_name: 'engine'\novirt_engine_db_user: 'engine'\n\novirt_engine_dwh_db_host: 'localhost'\novirt_engine_dwh_db_port: 5432\novirt_engine_dwh_db_name: 'ovirt_engine_history'\novirt_engine_dwh_db_user: 'ovirt_engine_history'\n"}, {"commit_sha": "1bb50a6149f6ff7f2e6399411418d088e2c52d01", "sha": "0c355d342c1abbf955ed8b09f41e73fa4e42bb49", "filename": "tasks/section_02_level1.yml", "repository": "awailly/cis-ubuntu-ansible", "decoded_content": "---\n\n  - name: 2.1 Create Separate Partition for /tmp (Scored)\n    debug: msg=\"/!\\ Ensure there is a separate partition dedicated to /tmp\"\n    tags:\n      - section2\n      - section2.1\n\n  - name: 2.2 - 4 Set nodev, nosuid, noexec option for /tmp Partition (Scored)\n    mount: name=\"/tmp\" src=\"/tmp\" state=\"mounted\" opts=\"nodev,nosuid,noexec\" fstype=ext4\n    when: partitioning == True\n    tags:\n      - section2\n      - section2.2\n      - section2.3\n      - section2.4\n\n  - name: 2.5 Create Separate Partition for /var (Scored)\n    debug: msg=\"/!\\ Ensure there is a separate partition dedicated to /var\"\n    tags:\n      - section2\n      - section2.5\n\n  - name: 2.6 Bind Mount the /var/tmp directory to /tmp (Scored)\n    mount: name=\"/var/tmp\" src=\"/tmp\" opts=bind state=mounted fstype=ext4\n    when: partitioning == True\n    tags:\n      - section2\n      - section2.6\n\n  - name: 2.7 Create Separate Partition for /var/log (Scored)\n    debug: msg=\"/!\\ Ensure there is a separate partition dedicated to /var/log\"\n    tags:\n      - section2\n      - section2.7\n\n  - name: 2.8 Create Separate Partition for /var/log/audit (Scored)\n    debug: msg=\"/!\\ Ensure there is a separate partition dedicated to /var/log/audit\"\n    tags:\n      - section2\n      - section2.8\n\n  - name: 2.9 Create Separate Partition for /home (Scored)\n    debug: msg=\"/!\\ Ensure there is a separate partition dedicated to /home\"\n    tags:\n      - section2\n      - section2.9\n\n  - name: 2.10 Add nodev Option to /home (Scored)\n    mount: name=\"/home\" src=\"/home\" state=mounted opts=remount,nodev fstype=\"ext4\"\n    when: partitioning == True\n    tags:\n      - section2\n      - section2.10\n\n  - name: 2.11 Add nodev Option to Removable Media Partitions (Not Scored)\n    debug: msg=\"/!\\ Ensure removable partitions have nodev option\"\n    tags:\n      - section2\n      - section2.11\n\n  - name: 2.12 Add noexec Option to Removable Media Partitions (Not Scored)\n    debug: msg=\"/!\\ Ensure removable partitions have noexec option\"\n    tags:\n      - section2\n      - section2.12\n\n  - name: 2.13 Add nosuid Option to Removable Media Partitions (Not Scored)\n    debug: msg=\"/!\\ Ensure removable partitions have nosuid option\"\n    tags:\n      - section2\n      - section2.13\n\n  - name: 2.14 - 16 Add nodev Option to /run/shm Partition (Scored)\n    mount: name=\"/run/shm\" src=\"/run/shm\" state=\"mounted\" opts=\"nodev,nosuid,noexec\" fstype=\"tmpfs\"\n    when: run_shm_read_only == False\n    tags:\n      - section2\n      - section2.14\n      - section2.15\n      - section2.16\n\n  - name: 2.17.1 Set Sticky Bit on All World-Writable Directories (preparation) (Scored)\n    shell: df --local -P | awk {'if (NR!=1) print $6'} | xargs -I '{}' find '{}' -xdev -type d -perm -0002 -print 2>/dev/null\n    failed_when: False\n    changed_when: False\n    always_run: True\n    register: sticky_bit_dirs\n    tags:\n      - section2\n      - section2.17\n      - section2.17.1\n\n  - name: 2.17.2 Set Sticky Bit on All World-Writable Directories (Scored)\n    file: path='{{ item }}' mode=\"a+t\"\n    with_items: sticky_bit_dirs.stdout_lines\n    tags:\n      - section2\n      - section2.17\n      - section2.17.2\n\n  - name: 2.25 Disable Automounting (stat) (Scored)\n    stat: >\n        path=/etc/init/autofs.conf\n    register: autofs_file\n    tags:\n      - section2\n      - section2.25\n      \n  - name: 2.25 Disable Automounting (Scored)\n    lineinfile: >\n        dest=/etc/init/autofs.conf\n        line='#start on runlevel [2345]'\n        regexp='start on runlevel'\n        state=present\n    when: autofs_file.stat.exists == True\n    tags:\n      - section2\n      - section2.25\n"}, {"commit_sha": "b11c4477d973b0cc87a296f6b028eaf9abab4686", "sha": "75ff6c055fc23df0862174d082b393824af8f750", "filename": "tasks/main-CentOS.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n# tasks file for ansible-role-docker-ce\n\n- name: Ensure yum-utils is installed\n  package:\n    name: yum-utils\n    state: present\n  become: true\n\n- name: Add Docker CE repository\n  shell: yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n  args:\n    creates: /etc/yum.repos.d/docker-ce.repo\n  become: true\n  register: yum_repo\n\n- name: Update yum cache\n  shell: yum makecache fast\n  become: true\n  when: yum_repo.changed\n\n- include: main-Generic.yml\n"}, {"commit_sha": "332a49aab4cd78a3427bf2944906cb452b69e292", "sha": "39d059c1e17b1196ff616ed4b8e503cc3387c4ad", "filename": "tasks/main.yml", "repository": "ansible-ThoTeam/nexus3-oss", "decoded_content": "---\n\n- name: Include OS specific variables.\n  include_vars: \"configure-{{ ansible_os_family }}.yml\"\n\n- name: Include OS specific selinux libs and utils if needed\n  include: \"selinux-{{ ansible_os_family }}.yml\"\n  when: ansible_selinux.status is defined and ansible_selinux.status == \"enabled\"\n\n- name: Check if SystemD service is installed\n  stat:\n    path: /etc/systemd/system/nexus.service\n  register: nexus_systemd_service_file\n\n- include: nexus_purge.yml\n  when: ((purge is defined) and (purge | bool))\n\n- include: nexus_install.yml\n\n- include: httpd_reverse_proxy_config.yml\n  when: httpd_setup_enable\n\n- include: admin_password_setup.yml\n\n- name: Deleting default repositories\n  include: delete_repo_each.yml\n  with_items:\n    - maven-central\n    - maven-public\n    - maven-releases\n    - maven-snapshots\n    - nuget-group\n    - nuget-hosted\n    - nuget.org-proxy\n  when: nexus_data_dir_contents.stdout == \"\" and nexus_delete_default_repos\n\n- name: Deleting default blobstore\n  include: delete_blobstore_each.yml\n  with_items:\n    - name: default\n    - name: \"{{ nexus_blob_names.raw.blob }}\"\n    - name: \"{{ nexus_blob_names.pypi.blob }}\"\n    - name: \"{{ nexus_blob_names.docker.blob }}\"\n    - name: \"{{ nexus_blob_names.ruby.blob }}\"\n    - name: \"{{ nexus_blob_names.bower.blob }}\"\n    - name: \"{{ nexus_blob_names.npm.blob }}\"\n    - name: \"{{ nexus_blob_names.nuget.blob }}\"\n    - name: \"{{ nexus_blob_names.mvn.blob }}\"\n    - name: \"{{ nexus_blob_names.gitlfs.blob }}\"\n  when: nexus_data_dir_contents.stdout == \"\" and nexus_delete_default_blobstore\n\n- include: setup_ldap_each.yml\n  with_items: \"{{ ldap_connections }}\"\n\n- include: setup_privilege_each.yml\n  with_items: \"{{ nexus_privileges }}\"\n\n- include: setup_role_each.yml\n  with_items: \"{{ nexus_roles }}\"\n\n- include: setup_user_each.yml\n  with_items: \"{{ nexus_local_users }}\"\n\n- name: \"Digest splited blob list var\"\n  include_vars: blob_vars.yml\n  when: nexus_blob_split\n\n- include: create_blobstore_each.yml\n  with_items: \"{{ nexus_blobstores }}\"\n  when: nexus_restore_point is undefined\n\n- name: \"Restore nexus backup\"\n  include: nexus-restore.yml\n  when: nexus_restore_point is defined\n\n- include: create_repo_maven_proxy_each.yml\n  with_items: \"{{ nexus_repos_maven_proxy }}\"\n\n- include: create_repo_maven_hosted_each.yml\n  with_items: \"{{ nexus_repos_maven_hosted }}\"\n\n- include: create_repo_maven_group_each.yml\n  with_items: \"{{ nexus_repos_maven_group }}\"\n\n- block:\n    - include: create_repo_docker_hosted_each.yml\n      with_items: \"{{ nexus_repos_docker_hosted }}\"\n\n    - include: create_repo_docker_proxy_each.yml\n      with_items: \"{{ nexus_repos_docker_proxy }}\"\n\n    - include: create_repo_docker_group_each.yml\n      with_items: \"{{ nexus_repos_docker_group }}\"\n  when: nexus_config_docker\n\n- block:\n    - include: create_repo_pypi_proxy_each.yml\n      with_items: \"{{ nexus_repos_pypi_proxy }}\"\n\n    - include: create_repo_pypi_hosted_each.yml\n      with_items: \"{{ nexus_repos_pypi_hosted }}\"\n\n    - include: create_repo_pypi_group_each.yml\n      with_items: \"{{ nexus_repos_pypi_group }}\"\n  when: nexus_config_pypi\n\n- block:\n    - include: create_repo_raw_proxy_each.yml\n      with_items: \"{{ nexus_repos_raw_proxy }}\"\n\n    - include: create_repo_raw_hosted_each.yml\n      with_items: \"{{ nexus_repos_raw_hosted }}\"\n\n    - include: create_repo_raw_group_each.yml\n      with_items: \"{{ nexus_repos_raw_group }}\"\n  when: nexus_config_raw\n\n- block:\n    - include: create_repo_rubygems_proxy_each.yml\n      with_items: \"{{ nexus_repos_rubygems_proxy }}\"\n\n    - include: create_repo_rubygems_hosted_each.yml\n      with_items: \"{{ nexus_repos_rubygems_hosted }}\"\n\n    - include: create_repo_rubygems_group_each.yml\n      with_items: \"{{ nexus_repos_rubygems_group }}\"\n  when: nexus_config_rubygems\n\n- block:\n    - include: create_repo_bower_proxy_each.yml\n      with_items: \"{{ nexus_repos_bower_proxy }}\"\n\n    - include: create_repo_bower_hosted_each.yml\n      with_items: \"{{ nexus_repos_bower_hosted }}\"\n\n    - include: create_repo_bower_group_each.yml\n      with_items: \"{{ nexus_repos_bower_group }}\"\n  when: nexus_config_bower\n\n- block:\n    - include: create_repo_npm_proxy_each.yml\n      with_items: \"{{ nexus_repos_npm_proxy }}\"\n\n    - include: create_repo_npm_hosted_each.yml\n      with_items: \"{{ nexus_repos_npm_hosted }}\"\n\n    - include: create_repo_npm_group_each.yml\n      with_items: \"{{ nexus_repos_npm_group }}\"\n  when: nexus_config_npm\n\n- block:\n    - include: create_repo_nuget_proxy_each.yml\n      with_items: \"{{ nexus_repos_nuget_proxy }}\"\n    - include: create_repo_nuget_hosted_each.yml\n      with_items: \"{{ nexus_repos_nuget_hosted }}\"\n    - include: create_repo_nuget_group_each.yml\n      with_items: \"{{ nexus_repos_nuget_group }}\"\n  when: nexus_config_nuget\n\n\n- include: create_repo_gitlfs_hosted_each.yml\n  with_items: \"{{ nexus_repos_gitlfs_hosted }}\"\n  when: nexus_config_gitlfs\n\n- include: create_repo_yum_proxy_each.yml\n  with_items: \"{{ nexus_repos_yum_proxy }}\"\n  when: nexus_config_yum\n\n- include: call_script.yml\n  vars:\n    script_name: setup_anonymous_access\n    args:\n      anonymous_access: \"{{ nexus_anonymous_access }}\"\n\n- include: call_script.yml\n  vars:\n    script_name: setup_base_url\n    args:\n      base_url: \"https://{{ public_hostname }}/\"\n\n- include: call_script.yml\n  vars:\n    script_name: setup_http_proxy\n    args:\n      with_http_proxy: \"{{ nexus_with_http_proxy }}\"\n      http_proxy_host: \"{{ nexus_http_proxy_host }}\"\n      http_proxy_port: \"{{ nexus_http_proxy_port }}\"\n      http_proxy_username: \"{{ nexus_http_proxy_username }}\"\n      http_proxy_password: \"{{ nexus_http_proxy_password }}\"\n      with_https_proxy: \"{{ nexus_with_https_proxy }}\"\n      https_proxy_host: \"{{ nexus_https_proxy_host }}\"\n      https_proxy_port: \"{{ nexus_https_proxy_port }}\"\n      https_proxy_username: \"{{ nexus_https_proxy_username }}\"\n      https_proxy_password: \"{{ nexus_https_proxy_password }}\"\n      proxy_exclude_hosts: \"{{ nexus_proxy_exclude_hosts }}\"\n\n- include: call_script.yml\n  vars:\n    script_name: setup_realms\n    args:\n      nuget_api_key_realm: \"{{ nexus_nuget_api_key_realm }}\"\n      npm_bearer_token_realm: \"{{ nexus_npm_bearer_token_realm }}\"\n      rut_auth_realm: \"{{ nexus_rut_auth_realm }}\"\n      ldap_realm: \"{{ nexus_ldap_realm }}\"\n      docker_bearer_token_realm: \"{{ nexus_docker_bearer_token_realm }}\"\n\n- include: call_script.yml\n  vars:\n    script_name: setup_email\n    args:\n      email_server_enabled: \"{{ nexus_email_server_enabled }}\"\n      email_server_host: \"{{ nexus_email_server_host }}\"\n      email_server_port: \"{{ nexus_email_server_port }}\"\n      email_server_username: \"{{ nexus_email_server_username }}\"\n      email_server_password: \"{{ nexus_email_server_password }}\"\n      email_from_address: \"{{ nexus_email_from_address }}\"\n      email_subject_prefix: \"{{ nexus_email_subject_prefix }}\"\n      email_tls_enabled: \"{{ nexus_email_tls_enabled }}\"\n      email_tls_required: \"{{ nexus_email_tls_required }}\"\n      email_ssl_on_connect_enabled: \"{{ nexus_email_ssl_on_connect_enabled }}\"\n      email_ssl_check_server_identity_enabled: \"{{ nexus_email_ssl_check_server_identity_enabled }}\"\n      email_trust_store_enabled: \"{{ nexus_email_trust_store_enabled }}\"\n\n- name: Configure branding capability\n  include: call_script.yml\n  vars:\n    script_name: setup_capability\n    args:\n      capability_typeId: \"rapture.branding\"\n      capability_properties:\n        footerHtml: \"{{ nexus_branding_footer }}\"\n        headerHtml: \"{{ nexus_branding_header }}\"\n        footerEnabled: \"{{ nexus_branding_footer != '' }}\"\n        headerEnabled: \"{{ nexus_branding_header != '' }}\"\n\n- include: create_task_each.yml\n  with_items: \"{{ nexus_scheduled_tasks }}\"\n\n- name: Configure nexus backup task\n  include: call_script.yml\n  vars:\n    script_name: create_task\n    args:\n      name: db and blobstores backup\n      typeId: script\n      cron: \"{{ nexus_backup_cron }}\"\n      taskProperties:\n        language: groovy\n        source: \"{{ lookup('template', './templates/backup.groovy.j2') }}\"\n  when: nexus_backup_configure | bool\n"}, {"commit_sha": "b7ee8a0c3030974856d0b2c2df37b8d13935853c", "sha": "e7433589139778ea2f35175a0a2316d5b0264fee", "filename": "roles/config-lvm/defaults/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\nlvm_fstype: \"xfs\"\n"}, {"commit_sha": "ca212d93b7739adc0aa87b93ac58801007b9f458", "sha": "76a4364ef510d69891c3be19e9d0a52b1b085bbc", "filename": "roles/kubernetes-prerequisites/tasks/main.yml", "repository": "kubevirt/kubevirt-ansible", "decoded_content": "---\n- name: set SELinux to permissive mode\n  command: setenforce 0\n\n- name: set SELinux to permissive mode under configuration file\n  selinux:\n    policy: targeted\n    state: permissive\n\n- name: remove firewalld package\n  package:\n    name: firewalld\n\n- name: stop and disable firewalld\n  register: result\n  service:\n    state: stopped\n    enabled: no\n    name: firewalld\n  failed_when: \"result|failed and not 'Could not find the requested service' in result.msg\"\n\n- name: remove firewalld package\n  package:\n    name: firewalld\n    state: absent\n\n- name: Remove current swaps from fstab\n  lineinfile:\n    dest: /etc/fstab\n    regexp: '^/[\\S]+\\s+none\\s+swap '\n    state: absent\n\n- name: Disable swap\n  command: swapoff --all\n  ignore_errors: yes\n\n- name: install epel repository\n  package:\n    name: \"{{ epel_repo_rpm }}\"\n    state: present\n\n- name: import kubernetes rpm keys\n  rpm_key:\n    state: present\n    key: \"{{ item }}\"\n  with_items:\n    - \"{{ kubernetes_gpgkeys }}\"\n\n- name: add kubernetes repository\n  yum_repository:\n    name: kubernetes\n    description: \"Kubernetes repository\"\n    baseurl: \"{{ kubernetes_repo }}\"\n    enabled: yes\n    gpgcheck: yes\n    repo_gpgcheck: yes\n    gpgkey: \"{{ kubernetes_gpgkeys | join(' ') }}\"\n\n- name: update repo cache for kubernetes repo\n  command: yum -q makecache -y --disablerepo=* --enablerepo=kubernetes\n\n- name: install all kubernetes packages\n  package: \n    name: \"{{ item }}\"\n    state: present\n    update_cache: yes\n  with_items:\n    - \"{{ kubernetes_packages }}\"\n\n- name: use systemd as cgroup driver\n  copy:\n    src: 09-kubeadm.conf\n    dest: /etc/systemd/system/kubelet.service.d/09-kubeadm.conf\n\n- name: reload systemd\n  systemd:\n    daemon_reload: yes\n    name: \"{{ item }}\"\n    state: started\n    enabled: yes\n  with_items:\n    - docker\n    - kubelet\n\n- name: enable bridge kernel module\n  modprobe:\n    name: bridge\n    state: present\n\n- name: enable bridge-nf-call-iptables\n  sysctl:\n    name: \"{{ item }}\"\n    value: 1\n    sysctl_set: yes\n    state: present\n  with_items:\n    - net.bridge.bridge-nf-call-iptables\n    - net.bridge.bridge-nf-call-ip6tables\n    - net.ipv4.ip_forward\n\n"}, {"commit_sha": "45971be8249cc4627ef8ddfacf55a661b7fc13ca", "sha": "0ea3f924ee1347e5611d3c715e01e9ffeb78d4e0", "filename": "tasks/setup-repository.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Ensure python and deps for Ansible modules\n  raw: dnf install -y python2 python2-dnf libselinux-python\n  become: true\n  changed_when: false\n  when: _docker_os_dist == \"Fedora\"\n\n- name: Update APT cache\n  apt:\n    update_cache: yes\n  changed_when: false\n  become: true\n  when: _docker_os_dist == \"Ubuntu\" or\n        _docker_os_dist == \"Debian\"\n\n- name: Ensure packages are installed for repository setup\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - \"{{ docker_repository_related_packages[_docker_os_dist] }}\"\n  become: true\n  when: _docker_os_dist == \"Ubuntu\" or\n        _docker_os_dist == \"Debian\" or\n        _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"RedHat\"\n\n- name: Add Docker official GPG key\n  apt_key:\n    url: https://download.docker.com/linux/{{ _docker_os_dist|lower }}/gpg\n    state: present\n  become: true\n  when: (_docker_os_dist == \"Ubuntu\" and _docker_os_dist_major_version > '14')\n        or _docker_os_dist == \"Debian\"\n\n- name: Add Docker APT key (alternative for older Ubuntu systems without SNI).\n  shell: \"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\"\n  args:\n    warn: false\n  become: true\n  changed_when: false\n  when: _docker_os_dist == \"Ubuntu\" and\n        _docker_os_dist_major_version == '14'\n\n- name: Add Docker CE repository (Ubuntu/Debian)\n  apt_repository:\n    repo: deb [arch=amd64] https://download.docker.com/linux/{{ _docker_os_dist|lower }} {{ _docker_os_dist_release }} stable {{ (docker_enable_ce_edge == true) | ternary('edge','') }}\n    state: present\n    filename: 'docker-ce'\n  become: true\n  when: _docker_os_dist == \"Ubuntu\" or\n        _docker_os_dist == \"Debian\"\n\n- name: Add Docker CE repository (Fedora/CentOS/RedHat)\n  get_url:\n    url: \"{{ docker_repository_url[_docker_os_dist] }}\"\n    dest: /etc/yum.repos.d/docker-ce.repo\n    mode: 0644\n  become: true\n  register: docker_repo\n  when: _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"Fedora\" or\n        _docker_os_dist == \"RedHat\"\n\n- name: Determine Docker CE Edge repo status (Fedora/CentOS/RedHat)\n  shell: \"{{ docker_cmd_check_edge_repo_status[_docker_os_dist] }}\"\n  args:\n    warn: false\n  ignore_errors: yes\n  changed_when: false\n  register: cmd_docker_ce_edge_enabled\n  when: _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"Fedora\" or\n        _docker_os_dist == \"RedHat\"\n\n- name: Set current Docker CE Edge repo status fact (Fedora/CentOS/RedHat)\n  set_fact:\n    _fact_docker_ce_edge_enabled: \"{{ cmd_docker_ce_edge_enabled.stdout == 'enabled = True' }}\"\n  when: _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"Fedora\" or\n        _docker_os_dist == \"RedHat\"\n\n- name: Enable/Disable Docker CE Edge Repository (Fedora/CentOS/RedHat)\n  shell: \"{{ docker_cmd_enable_disable_edge_repo[_docker_os_dist] }}\"\n  become: true\n  when: (_docker_os_dist == \"CentOS\" or _docker_os_dist == \"Fedora\" or _docker_os_dist == \"RedHat\") and\n        _fact_docker_ce_edge_enabled != docker_enable_ce_edge\n        \n# disable rt-beta so we don't get a 403 error retrieving repomd.xml\n- name: Check if rhel-7-server-rt-beta-rpms Repository is enabled (RedHat)\n  shell: \"subscription-manager repos --list-enabled | grep rhel-7-server-rt-beta-rpms\"\n  become: true\n  register: cmd_rhel_rt_beta_repo_enabled\n  when: _docker_os_dist == \"RedHat\"\n  changed_when: false\n  failed_when: cmd_rhel_rt_beta_repo_enabled.rc not in [ 0, 1 ]\n\n- name: Disable rhel-7-server-rt-beta-rpms Repository (RedHat)\n  shell: \"subscription-manager repos --disable=rhel-7-server-rt-beta-rpms\"\n  become: true\n  when: _docker_os_dist == \"RedHat\" and cmd_rhel_rt_beta_repo_enabled.rc == 0\n\n# container-selinux package wants this\n- name: Check if rhel-7-server-extras-rpms Repository is enabled (RedHat)\n  shell: \"subscription-manager repos --list-enabled | grep rhel-7-server-extras-rpms\"\n  become: true\n  register: cmd_rhel_extras_repo_enabled\n  when: _docker_os_dist == \"RedHat\"\n  changed_when: false\n  failed_when: cmd_rhel_extras_repo_enabled.rc not in [ 0, 1 ]\n\n- name: Enable rhel-7-server-extras-rpms Repository (RedHat)\n  shell: \"subscription-manager repos --enable=rhel-7-server-extras-rpms\"\n  become: true\n  when: _docker_os_dist == \"RedHat\" and cmd_rhel_extras_repo_enabled.rc == 1\n\n- name: Update repository cache\n  shell: \"{{ docker_cmd_update_repo_cache[_docker_os_dist] }}\"\n  args:\n    warn: false\n  become: true\n  when: docker_repo.changed"}, {"commit_sha": "45971be8249cc4627ef8ddfacf55a661b7fc13ca", "sha": "f1fd40fea6cd8c57e4c62c5ad95db7d3e5426c56", "filename": "tasks/checks.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- include_tasks: distribution-checks.yml\n  when:\n    _docker_os_dist_check | bool\n    \n- include_tasks: compatibility-checks.yml\n\n"}, {"commit_sha": "3c8d04f3e0875a9baf1f1282f6665b2e7d6871a8", "sha": "31200f84cd8dd9f596d1603b6d215bac61b134c4", "filename": "tasks/main.yml", "repository": "geerlingguy/ansible-role-security", "decoded_content": "---\n- name: Include OS-specific variables.\n  include_vars: \"{{ ansible_os_family }}.yml\"\n\n# Fail2Ban\n- include: fail2ban-RedHat.yml\n  when: ansible_os_family == 'RedHat' and security_fail2ban_enabled\n\n- include: fail2ban-Debian.yml\n  when: ansible_os_family == 'Debian' and security_fail2ban_enabled\n\n- name: Ensure fail2ban is running and enabled on boot.\n  service: name=fail2ban state=started enabled=yes\n  when: security_fail2ban_enabled\n\n# SSH\n- include: ssh.yml\n\n# Autoupdate\n- include: autoupdate-RedHat.yml\n  when: ansible_os_family == 'RedHat' and security_autoupdate_enabled\n\n- include: autoupdate-Debian.yml\n  when: ansible_os_family == 'Debian' and security_autoupdate_enabled\n"}, {"commit_sha": "772a200a5ccd3a91eec54fe17ab86086d60269cb", "sha": "3252efa43206a8c26ef6bbe5ac9f684c377f42c1", "filename": "roles/nextcloud/tasks/main.yml", "repository": "iiab/iiab", "decoded_content": "- name: See if Nextcloud version page exists\n  stat:\n    path: \"{{ nextcloud_prefix }}/nextcloud/version.php\"\n#   path: \"{{ nextcloud_prefix }}/nextcloud/index.php\"\n  register: nextcloud_page\n\n- name: FORCE INSTALL OR REINSTALL OR UPGRADE IF /opt/nextcloud/version.php DOESN'T EXIST\n  set_fact:\n    nextcloud_force_install: True\n  when: not nextcloud_page.stat.exists\n\n# - debug:\n#     msg: \"nextcloud_force_install: {{ nextcloud_force_install }}\"\n\n\n- name: Download latest Nextcloud software to /opt/iiab/download/{{ nextcloud_src_file }}\n  get_url:\n    url: \"{{ nextcloud_dl_url }}/{{ nextcloud_orig_src_file }}\"\n    dest: \"{{ downloads_dir }}/{{ nextcloud_src_file }}\"\n    force: yes\n    timeout: \"{{ download_timeout }}\"\n  when: internet_available and nextcloud_force_install\n  async: 900\n  poll: 15\n  tags:\n    - download\n\n- name: Ubuntu and Debian treat names differently (Debian)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - \"libapache2-mod-php{{ php_version }}\"\n    - \"php{{ php_version }}-mbstring\"\n    - \"php{{ php_version }}-zip\"\n  when: is_debian\n\n- name: Ubuntu and Debian treat names differently (Ubuntu)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - libapache2-mod-php\n    - php-imagick\n    - php-zip\n    - php-mbstring\n  when: is_ubuntu\n\n- name: Install list of packages (debuntu)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - \"php{{ php_version }}-gd\"\n    - \"php{{ php_version }}-json\"\n    - \"php{{ php_version }}-mysql\"\n    - \"php{{ php_version }}-curl\"\n    - \"php{{ php_version }}-intl\"\n  when: is_debuntu\n\n- name: In php7.2, php dropped mcrypt\n  package:\n    name: \"php{{ php_version }}-mcrypt\"\n    state: present\n  when: is_debuntu and not is_ubuntu_18\n\n# we need to install the rpm in order to get the dependencies\n# but we only need to do this the first time\n\n- name: Install list of packages (redhat)\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - php\n    - php-gd\n    - php-json\n    - php-mysql\n    - php-curl\n    - php-intl\n    - php-mcrypt\n# centos does not have a package for php-imagick\n#    - php-imagick\n  when: is_redhat\n\n- name: Copy it to permanent location /opt (OS's other than Fedora 18)\n  unarchive:\n    src: \"{{ downloads_dir }}/{{ nextcloud_src_file }}\"\n    dest: \"{{ nextcloud_prefix }}\"\n#   creates: \"{{ nextcloud_prefix }}/nextcloud/version.php\"\n  when: not is_F18 and nextcloud_force_install\n\n# Ansible 1.4.1 does not have \"creates\" (but hopefully has \"when\")\n- name: Copy it to permanent location /opt (Fedora 18)\n  unarchive:\n    src: \"{{ downloads_dir }}/{{ nextcloud_src_file }}\"\n    dest: \"{{ nextcloud_prefix }}\"\n  when: is_F18 and nextcloud_force_install\n\n- name: In CentOS, the following config dir is symlink to /etc/nextcloud\n  file:\n    path: /etc/nextcloud\n    state: directory\n  when: is_centos\n\n- name: Add autoconfig file (CentOS)\n  template:\n    src: autoconfig.php.j2\n    dest: \"{{ nextcloud_prefix }}/nextcloud/config/autoconfig.php\"\n    owner: \"{{ apache_user }}\"\n    group: \"{{ apache_user }}\"\n    mode: 0640\n  when: is_centos\n\n- name: Make Apache owner\n  file:\n    path: \"{{ nextcloud_prefix }}/nextcloud\"\n    owner: \"{{ apache_user }}\"\n    group: \"{{ apache_user }}\"\n    recurse: yes\n    state: directory\n\n- name: Create data directory library\n  file:\n    path: \"{{ item }}\"\n    owner: \"{{ apache_user }}\"\n    group: \"{{ apache_user }}\"\n    mode: 0750\n    state: directory\n  with_items:\n    - \"{{ nextcloud_data_dir }}\"\n\n- name: Create a MySQL database for Nextcloud\n  mysql_db:\n    name: \"{{ nextcloud_dbname }}\"\n  when: mysql_enabled and nextcloud_enabled\n\n- name: Create a user to access the Nextcloud database\n  mysql_user:\n    name: \"{{ nextcloud_dbuser }}\"\n    host: \"{{ item }}\"\n    password: \"{{ nextcloud_dbpassword }}\"\n    priv: \"{{ nextcloud_dbname }}.*:ALL,GRANT\"\n  with_items:\n    - \"{{ nextcloud_dbhost }}\"\n    - 127.0.0.1\n    - ::1\n    - localhost\n  when: mysql_enabled and nextcloud_enabled\n\n\n- name: Restart Apache, so it picks up the new aliases\n  service:\n    name: \"{{ apache_service }}\"\n    state: restarted\n# when: nextcloud_enabled     # taken care of by nextcloud_enabled.yml below\n  when: not nextcloud_enabled\n\n# Enable nextcloud by copying template to httpd config\n\n# following enables and disables\n- include_tasks: nextcloud_enabled.yml\n\n- name: Add 'nextcloud' to list of services at /etc/iiab/iiab.ini\n  ini_file:\n    dest: \"{{ service_filelist }}\"\n    section: Nextcloud\n    option: \"{{ item.option }}\"\n    value: \"{{ item.value }}\"\n  with_items:\n    - option: name\n      value: Nextcloud\n    - option: description\n      value: '\"NextCloud is a local server-based facility for sharing files, photos, contacts, calendars, etc.\"'\n    - option: path\n      value: \"{{ nextcloud_prefix }}/nextcloud\"\n    - option: source\n      value: \"{{ nextcloud_src_file }}\"\n    - option: enabled\n      value: \"{{ nextcloud_enabled }}\"\n"}, {"commit_sha": "80530fde7df1a94ad361434e02816b0816a2c47a", "sha": "971492a532c7d4d91d4506614b9ca25efa2534fe", "filename": "roles/mesos/tasks/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "# used for leader election amongst masters\n- name: Set ZooKeeper URL\n  template: src=zk.j2 dest=/etc/mesos/zk mode=0644\n  sudo: yes\n\n# Tasks for Master nodes\n- name: Set Mesos Master ip\n  copy:\n    content: \"{{mesos_local_address}}\"\n    dest: /etc/mesos-master/ip\n    mode: 0644\n  sudo: yes\n  when: mesos_install_mode == \"master\"\n\n- name: Set Mesos Master hostname\n  copy:\n    content: \"{{mesos_local_address}}\"\n    dest: /etc/mesos-master/hostname\n    mode: 0644\n  sudo: yes\n  when: mesos_install_mode == \"master\"\n\n  # The Mesos quorum value is based on the number of Mesos Masters. Take the\n  # number of masters, divide by 2, and round-up to nearest integer. For example,\n  # if there are 1 or 2 masters the quorum count is 1. If there are 3 or 4\n  # masters then the quorum count is 2. For 5 or 6 masters it's 3 and so on.\n- name: Set Mesos Master quorum count\n  template: src=quorum.j2 dest=/etc/mesos-master/quorum mode=0644\n  sudo: yes\n  when: mesos_install_mode == \"master\"\n\n- name: remove mesos-master override\n  command: /bin/rm -f /etc/init/mesos-master.override\n  when: mesos_install_mode == \"master\"\n\n- name: Set Mesos Master Cluster name\n  copy:\n    content: \"{{mesos_cluster_name}}\"\n    dest: /etc/mesos-master/cluster\n    mode: 0644\n  sudo: yes\n  notify:\n    - Start mesos master\n  when: mesos_install_mode == \"master\"\n\n- name: Set Mesos Master consul service definition\n  sudo: yes\n  template:\n    src: mesos-master-consul.j2\n    dest: \"{{ consul_dir }}/mesos-master.json\"\n  notify:\n    - Restart consul\n  when: mesos_install_mode == \"master\"\n\n# Tasks for Slave nodes\n- name: Set Mesos Slave hostname\n  copy:\n    content: \"{{mesos_local_address}}\"\n    dest: /etc/mesos-slave/hostname\n    mode: 0644\n  sudo: yes\n  when: mesos_install_mode == \"slave\"\n\n- name: remove mesos-slave override\n  command: /bin/rm -f /etc/init/mesos-slave.override\n  when: mesos_install_mode == \"slave\"\n\n- name: Set Mesos Slave ip\n  copy:\n    content: \"{{mesos_local_address}}\"\n    dest: /etc/mesos-slave/ip\n    mode: 0644\n  sudo: yes\n  notify:\n    - Start mesos slave\n  when: mesos_install_mode == \"slave\"\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "61384f83f23800e3c83fd964198c0ebe5e0bad5c", "filename": "roles/ipaserver/tasks/uninstall.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "---\n# tasks to uninstall IPA server\n\n# - name: Uninstall - Include Python2/3 import test\n#   import: \"{{ role_path }}/tasks/python_2_3_test.yml\"\n\n- name: Uninstall - Uninstall IPA server\n  command: >\n    /usr/sbin/ipa-server-install\n    --uninstall\n    -U\n    {{ '--ignore-topology-disconnect' if ipaserver_ignore_topology_disconnect\n       | bool else '' }}\n    {{ '--ignore-last-of-role' if ipaserver_ignore_last_of_role | bool else ''}}\n  register: uninstall\n  # 1 means that uninstall failed because IPA server was not configured\n  failed_when: uninstall.rc != 0 and uninstall.rc != 1\n  changed_when: uninstall.rc == 0\n\n# - name: Remove IPA server packages\n#   package:\n#     name: \"{{ item }}\"\n#     state: absent\n#   with_items: \"{{ ipaserver_packages }}\"\n"}, {"commit_sha": "e9fb46dc84b9c815a69f6de1347c9ece5db01cc8", "sha": "ac9c6db35acbe3774904f5df5516715dc08f34e8", "filename": "tasks/npm.yml", "repository": "fubarhouse/ansible-role-nodejs", "decoded_content": "---\n# Tasks file for NPM\n\n- name: \"NPM | Make sure NPM can be found\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell: which npm | cat\n  changed_when: false\n  register: which_npm\n  failed_when: 'which_npm.stdout.find(\"npm\") == -1'\n\n- name: \"NPM | Configure\"\n  become: yes\n  become_user: \"{{ fubarhouse_user }}\"\n  shell: \"{{ which_npm.stdout }} config set prefix /usr/local\"\n  when: '\"npm\" in \"{{ which_npm.stdout }}\"'\n  changed_when: false\n  failed_when: false\n\n- name: \"NPM | Ensure installed and updated\"\n  become: yes\n  become_user: root\n  npm:\n    name: \"{{ item }}\"\n    executable: \"{{ which_npm.stdout }}\"\n    global: yes\n  with_items:\n    - \"{{ node_packages }}\"\n  when: '\"npm\" in \"{{ which_npm.stdout }}\"'\n"}, {"commit_sha": "155dade983c6ce867dac38a9ae62e471ad13437f", "sha": "2dddc1a488b704a2c24616a54e1242ec0417b989", "filename": "roles/user-management/manage-idm-users/tasks/main.yml", "repository": "redhat-cop/infra-ansible", "decoded_content": "---\n\n- import_tasks: create_user.yml\n\n- import_tasks: create_group.yml\n\n- include_tasks: add_user_to_group.yml\n  with_items: \"{{ user_groups }}\"\n  loop_control:\n    loop_var: \"this_group\"\n\n"}, {"commit_sha": "af3dfdf7cf37437f5609f1a12e4ac7cbaebd4867", "sha": "e425e4a5c2947bf23ec96f059da9777927f8a6e4", "filename": "roles/weave/defaults/main.yml", "repository": "Capgemini/Apollo", "decoded_content": "---\n# defaults file for weave\nweave_bridge: \"10.2.0.1/16\"\nweave_server_group: weave_servers\nweave_docker_subnet: \"\n    {%- for host in groups[weave_server_group] -%}\n      {%- if host == 'default' or host == inventory_hostname or host == ansible_fqdn or host in ansible_all_ipv4_addresses -%}\n        10.2.{{ loop.index }}.0/24\n      {%- endif -%}\n    {%- endfor -%}\n\"\nweave_launch_peers: \"\n  {%- set weave_peers = [] -%}\n  {%- for host in groups[weave_server_group] -%}\n    {%- if host != inventory_hostname and host not in weave_peers -%}\n      {% do weave_peers.append(hostvars[host]['ansible_eth0']['ipv4']['address']) %}\n    {%- endif -%}\n  {%- endfor -%}\n  {{ weave_peers|join(' ') }}\n\"\nweave_docker_opts: \"--bridge=weave --fixed-cidr={{ weave_docker_subnet }} --dns 172.17.42.1 --dns 8.8.8.8 --dns-search service.{{ consul_domain }}\"\nweave_scope_url: https://github.com/weaveworks/scope/releases/download/latest_release/scope\nweave_scope_dest: /usr/local/bin/scope\n"}, {"commit_sha": "b51397eb89ad0dbab1f8b81e58c841834d20fc07", "sha": "64e3e4260dd06f30d21b6a14db7257a50094a8cf", "filename": "roles/ipareplica/tasks/install.yml", "repository": "freeipa/ansible-freeipa", "decoded_content": "---\n# tasks file for ipareplica\n\n- block:\n\n  - name: Install - Ensure IPA replica packages are installed\n    package:\n      name: \"{{ item }}\"\n      state: present\n    with_items: \"{{ ipareplica_packages }}\"\n\n  - name: Install - Ensure IPA replica packages for dns are installed\n    package:\n      name: \"{{ item }}\"\n      state: present\n    with_items: \"{{ ipareplica_packages_dns }}\"\n    when: ipareplica_setup_dns | bool\n\n  - name: Install - Ensure IPA replica packages for adtrust are installed\n    package:\n      name: \"{{ item }}\"\n      state: present\n    with_items: \"{{ ipareplica_packages_adtrust }}\"\n    when: ipareplica_setup_adtrust | bool\n\n  when: ipareplica_install_packages | bool\n\n#- name: Install - Include Python2/3 import test\n#  import_tasks: \"{{ role_path }}/tasks/python_2_3_test.yml\"\n\n- name: Install - Set default principal if no keytab is given\n  set_fact:\n    ipaadmin_principal: admin\n  when: ipaadmin_principal is undefined and ipaclient_keytab is undefined\n\n- name: Install - Replica installation test\n  ipareplica_test:\n    ### basic ###\n    # dm_password: \"{{ ipadm_password | default(omit) }}\"\n    # password: \"{{ ipaadmin_password | default(omit) }}\"\n    ip_addresses: \"{{ ipareplica_ip_addresses | default([]) }}\"\n    domain: \"{{ ipareplica_domain | default(ipaserver_domain) |\n            default(omit) }}\"\n    servers: \"{{ groups.ipaservers | default(groups.ipaserver) |\n              default(omit) }}\"\n    realm: \"{{ ipareplica_realm | default(omit) }}\"\n    hostname: \"{{ ipareplica_hostname | default(ansible_fqdn) }}\"\n    ca_cert_files: \"{{ ipareplica_ca_cert_files | default([]) }}\"\n    hidden_replica: \"{{ ipareplica_hidden_replica }}\"\n    ### server ###\n    setup_adtrust: \"{{ ipareplica_setup_adtrust }}\"\n    setup_kra: \"{{ ipareplica_setup_kra }}\"\n    setup_dns: \"{{ ipareplica_setup_dns }}\"\n    no_pkinit: \"{{ ipareplica_no_pkinit }}\"\n    dirsrv_config_file: \"{{ ipareplica_dirsrv_config_file | default(omit) }}\"\n    ### ssl certificate ###\n    dirsrv_cert_files: \"{{ ipareplica_dirsrv_cert_files | default([]) }}\"\n    http_cert_files: \"{{ ipareplica_http_cert_files | default([]) }}\"\n    pkinit_cert_files: \"{{ ipareplica_pkinit_cert_files | default([]) }}\"\n    ### client ###\n    no_ntp: \"{{ ipaclient_no_ntp }}\"\n    ntp_servers: \"{{ ipaclient_ntp_servers | default([]) }}\"\n    ntp_pool: \"{{ ipaclient_ntp_pool | default(omit) }}\"\n    ### dns ###\n    no_reverse: \"{{ ipareplica_no_reverse }}\"\n    auto_reverse: \"{{ ipareplica_auto_reverse }}\"\n    forwarders: \"{{ ipareplica_forwarders | default([]) }}\"\n    no_forwarders: \"{{ ipareplica_no_forwarders }}\"\n    auto_forwarders: \"{{ ipareplica_auto_forwarders }}\"\n    forward_policy: \"{{ ipareplica_forward_policy | default(omit) }}\"\n    no_dnssec_validation: \"{{ ipareplica_no_dnssec_validation }}\"\n  register: result_ipareplica_test\n\n- block:\n  # This block is executed only when\n  # not ansible_check_mode and\n  # not (result_ipareplica_test.client_already_configured is defined or\n  #      result_ipareplica_test.server_already_configured is defined)\n\n  - name: Install - Setup client\n    include_role:\n      name: ipaclient\n    vars:\n      state: present\n      ipaclient_domain: \"{{ result_ipareplica_test.domain }}\"\n      ipaclient_realm: \"{{ result_ipareplica_test.realm }}\"\n      ipaclient_servers: [\"{{ result_ipareplica_test.server }}\"]\n      ipaclient_hostname: \"{{ result_ipareplica_test.hostname }}\"\n      ipaclient_no_ntp: \"{{ result_ipareplica_test.ipa_python_version\n                            < 40690 }}\"\n      ipaclient_install_packages: \"{{ ipareplica_install_packages }}\"\n    when: not result_ipareplica_test.client_enrolled\n\n  - name: Install - Configure firewalld\n    command: >\n      firewall-cmd\n      --permanent\n      --add-service=freeipa-ldap\n      --add-service=freeipa-ldaps\n      {{ \"--add-service=freeipa-trust\" if result_ipareplica_test.setup_adtrust\n         else \"\" }}\n      {{ \"--add-service=dns\" if ipareplica_setup_dns | bool else \"\" }}\n      {{ \"--add-service=ntp\" if not ipaclient_no_ntp | bool else \"\" }}\n    when: ipareplica_setup_firewalld | bool\n\n  - name: Install - Configure firewalld runtime\n    command: >\n      firewall-cmd\n      --add-service=freeipa-ldap\n      --add-service=freeipa-ldaps\n      {{ \"--add-service=freeipa-trust\" if result_ipareplica_test.setup_adtrust\n         else \"\" }}\n      {{ \"--add-service=dns\" if ipareplica_setup_dns | bool else \"\" }}\n      {{ \"--add-service=ntp\" if not ipaclient_no_ntp | bool else \"\" }}\n    when: ipareplica_setup_firewalld | bool\n\n  - name: Install - Replica preparation\n    ipareplica_prepare:\n      ### basic ###\n      password: \"{{ ipaadmin_password | default(omit) }}\"\n      ip_addresses: \"{{ ipareplica_ip_addresses | default([]) }}\"\n      domain: \"{{ result_ipareplica_test.domain }}\"\n      realm: \"{{ result_ipareplica_test.realm }}\"\n      hostname: \"{{ result_ipareplica_test.hostname }}\"\n      principal: \"{{ ipaadmin_principal | default(omit) }}\"\n      ca_cert_files: \"{{ ipareplica_ca_cert_files | default([]) }}\"\n      no_host_dns: \"{{ ipareplica_no_host_dns }}\"\n      ### replica ###\n      setup_adtrust: \"{{ result_ipareplica_test.setup_adtrust }}\"\n      setup_ca: \"{{ ipareplica_setup_ca }}\"\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      setup_dns: \"{{ ipareplica_setup_dns }}\"\n      ### ssl certificate ###\n      dirsrv_cert_files: \"{{ ipareplica_dirsrv_cert_files | default([]) }}\"\n      dirsrv_pin: \"{{ ipareplica_dirsrv_pin | default(omit) }}\"\n      http_cert_files: \"{{ ipareplica_http_cert_files | default([]) }}\"\n      http_pin: \"{{ ipareplica_http_pin | default(omit) }}\"\n      pkinit_cert_files: \"{{ ipareplica_pkinit_cert_files | default([]) }}\"\n      pkinit_pin: \"{{ ipareplica_pkinit_pin | default(omit) }}\"\n      ### client ###\n      keytab: \"{{ ipaclient_keytab | default(omit) }}\"\n      mkhomedir: \"{{ ipaclient_mkhomedir | default(omit) }}\"\n      force_join: \"{{ ipaclient_force_join | default(omit) }}\"\n      no_ntp: \"{{ ipaclient_no_ntp | default(omit) }}\"\n      ssh_trust_dns: \"{{ ipaclient_ssh_trust_dns | default(omit) }}\"\n      no_ssh: no\n      no_sshd: no\n      no_dns_sshfp: no\n      ### dns ###\n      allow_zone_overlap: \"{{ ipareplica_allow_zone_overlap }}\"\n      reverse_zones: \"{{ ipareplica_reverse_zones | default([]) }}\"\n      no_reverse: \"{{ ipareplica_no_reverse }}\"\n      auto_reverse: \"{{ ipareplica_auto_reverse }}\"\n      forwarders: \"{{ ipareplica_forwarders | default([]) }}\"\n      no_forwarders: \"{{ ipareplica_no_forwarders }}\"\n      auto_forwarders: \"{{ ipareplica_auto_forwarders }}\"\n      forward_policy: \"{{ ipareplica_forward_policy | default(omit) }}\"\n      no_dnssec_validation: \"{{ ipareplica_no_dnssec_validation }}\"\n      ### ad trust ###\n      enable_compat: \"{{ ipareplica_enable_compat }}\"\n      netbios_name: \"{{ ipareplica_netbios_name | default(omit) }}\"\n      rid_base: \"{{ ipareplica_rid_base | default(omit) }}\"\n      secondary_rid_base: \"{{ ipareplica_secondary_rid_base | default(omit) }}\"\n      ### additional ###\n      server: \"{{ result_ipareplica_test.server }}\"\n      skip_conncheck: \"{{ ipareplica_skip_conncheck }}\"\n    register: result_ipareplica_prepare\n\n  - name: Install - Add to ipaservers\n    ipareplica_add_to_ipaservers:\n      ### server ###\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      ### additional ###\n      config_master_host_name:\n        \"{{ result_ipareplica_prepare.config_master_host_name }}\"\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      installer_ccache: \"{{ result_ipareplica_prepare.installer_ccache }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n    when: result_ipareplica_prepare._add_to_ipaservers\n\n  - name: Install - Create dirman password\n    no_log: yes\n    ipareplica_master_password:\n      dm_password: \"{{ ipadm_password }}\"\n      master_password: \"{{ ipareplica_master_password | default(omit) }}\"\n    register: result_ipareplica_master_password\n\n  - name: Install - Set dirman password\n    no_log: yes\n    set_fact:\n      ipareplica_dirman_password:\n        \"{{ result_ipareplica_master_password.password }}\"\n\n  - name: Install - Setup certmonger\n    ipareplica_setup_certmonger:\n    when: result_ipareplica_prepare._ca_enabled\n\n  - name: Install - Install CA certs\n    ipareplica_install_ca_certs:\n      ### basic ###\n      dm_password: \"{{ ipadm_password | default(omit) }}\"\n      password: \"{{ ipaadmin_password | default(omit) }}\"\n      ip_addresses: \"{{ ipareplica_ip_addresses | default([]) }}\"\n      domain: \"{{ result_ipareplica_test.domain }}\"\n      realm: \"{{ result_ipareplica_test.realm }}\"\n      hostname: \"{{ result_ipareplica_test.hostname }}\"\n      ca_cert_files: \"{{ ipareplica_ca_cert_files | default([]) }}\"\n      no_host_dns: \"{{ ipareplica_no_host_dns }}\"\n      ### replica ###\n      setup_adtrust: \"{{ result_ipareplica_test.setup_adtrust }}\"\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      setup_dns: \"{{ ipareplica_setup_dns }}\"\n      ### ssl certificate ###\n      dirsrv_cert_files: \"{{ ipareplica_dirsrv_cert_files | default([]) }}\"\n      ### client ###\n      force_join: \"{{ ipaclient_force_join }}\"\n      ### ad trust ###\n      netbios_name: \"{{ ipareplica_netbios_name | default(omit) }}\"\n      rid_base: \"{{ ipareplica_rid_base | default(omit) }}\"\n      secondary_rid_base: \"{{ ipareplica_secondary_rid_base | default(omit) }}\"\n      ### additional ###\n      server: \"{{ result_ipareplica_test.server }}\"\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      installer_ccache: \"{{ result_ipareplica_prepare.installer_ccache }}\"\n      _ca_enabled: \"{{ result_ipareplica_prepare._ca_enabled }}\"\n      _kra_enabled: \"{{ result_ipareplica_prepare._kra_enabled }}\"\n      _dirsrv_pkcs12_info: \"{{ result_ipareplica_prepare._dirsrv_pkcs12_info }}\"\n      _http_pkcs12_info: \"{{ result_ipareplica_prepare._http_pkcs12_info }}\"\n      _pkinit_pkcs12_info: \"{{ result_ipareplica_prepare._pkinit_pkcs12_info }}\"\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      _add_to_ipaservers: \"{{ result_ipareplica_prepare._add_to_ipaservers }}\"\n      _ca_subject: \"{{ result_ipareplica_prepare._ca_subject }}\"\n      _subject_base: \"{{ result_ipareplica_prepare._subject_base }}\"\n      dirman_password: \"{{ ipareplica_dirman_password }}\"\n      config_setup_ca: \"{{ result_ipareplica_prepare.config_setup_ca }}\"\n      config_master_host_name:\n        \"{{ result_ipareplica_prepare.config_master_host_name }}\"\n      config_ca_host_name: \"{{ result_ipareplica_prepare.config_ca_host_name }}\"\n      config_ips: \"{{ result_ipareplica_prepare.config_ips }}\"\n    register: result_ipareplica_install_ca_certs\n\n  - name: Install - Setup DS\n    ipareplica_setup_ds:\n      ### basic ###\n      dm_password: \"{{ ipadm_password | default(omit) }}\"\n      password: \"{{ ipaadmin_password | default(omit) }}\"\n      ip_addresses: \"{{ ipareplica_ip_addresses | default([]) }}\"\n      domain: \"{{ result_ipareplica_test.domain }}\"\n      realm: \"{{ result_ipareplica_test.realm }}\"\n      hostname: \"{{ result_ipareplica_test.hostname }}\"\n      ca_cert_files: \"{{ ipareplica_ca_cert_files | default([]) }}\"\n      no_host_dns: \"{{ ipareplica_no_host_dns }}\"\n      ### replica ###\n      setup_adtrust: \"{{ result_ipareplica_test.setup_adtrust }}\"\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      setup_dns: \"{{ ipareplica_setup_dns }}\"\n      no_pkinit: \"{{ ipareplica_no_pkinit }}\"\n      dirsrv_config_file: \"{{ ipareplica_dirsrv_config_file | default(omit) }}\"\n      ### ssl certificate ###\n      dirsrv_cert_files: \"{{ ipareplica_dirsrv_cert_files | default([]) }}\"\n      ### client ###\n      force_join: \"{{ ipaclient_force_join }}\"\n      ### ad trust ###\n      netbios_name: \"{{ ipareplica_netbios_name | default(omit) }}\"\n      rid_base: \"{{ ipareplica_rid_base | default(omit) }}\"\n      secondary_rid_base: \"{{ ipareplica_secondary_rid_base | default(omit) }}\"\n      ### additional ###\n      server: \"{{ result_ipareplica_test.server }}\"\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      installer_ccache: \"{{ result_ipareplica_prepare.installer_ccache }}\"\n      _ca_enabled: \"{{ result_ipareplica_prepare._ca_enabled }}\"\n      _kra_enabled: \"{{ result_ipareplica_prepare._kra_enabled }}\"\n      _dirsrv_pkcs12_info: \"{{ result_ipareplica_prepare._dirsrv_pkcs12_info }}\"\n      _http_pkcs12_info: \"{{ result_ipareplica_prepare._http_pkcs12_info }}\"\n      _pkinit_pkcs12_info: \"{{ result_ipareplica_prepare._pkinit_pkcs12_info }}\"\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      _add_to_ipaservers: \"{{ result_ipareplica_prepare._add_to_ipaservers }}\"\n      _ca_subject: \"{{ result_ipareplica_prepare._ca_subject }}\"\n      _subject_base: \"{{ result_ipareplica_prepare._subject_base }}\"\n      dirman_password: \"{{ ipareplica_dirman_password }}\"\n      config_setup_ca: \"{{ result_ipareplica_prepare.config_setup_ca }}\"\n      config_master_host_name:\n        \"{{ result_ipareplica_install_ca_certs.config_master_host_name }}\"\n      config_ca_host_name: \"{{ result_ipareplica_prepare.config_ca_host_name }}\"\n      config_ips: \"{{ result_ipareplica_prepare.config_ips }}\"\n    register: result_ipareplica_setup_ds\n\n  - name: Install - Create IPA conf\n    ipareplica_create_ipa_conf:\n      ### basic ###\n      dm_password: \"{{ ipadm_password | default(omit) }}\"\n      password: \"{{ ipaadmin_password | default(omit) }}\"\n      ip_addresses: \"{{ ipareplica_ip_addresses | default([]) }}\"\n      domain: \"{{ result_ipareplica_test.domain }}\"\n      realm: \"{{ result_ipareplica_test.realm }}\"\n      hostname: \"{{ result_ipareplica_test.hostname }}\"\n      ca_cert_files: \"{{ ipareplica_ca_cert_files | default([]) }}\"\n      no_host_dns: \"{{ ipareplica_no_host_dns }}\"\n      ### replica ###\n      setup_adtrust: \"{{ result_ipareplica_test.setup_adtrust }}\"\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      setup_dns: \"{{ ipareplica_setup_dns }}\"\n      ### ssl certificate ###\n      dirsrv_cert_files: \"{{ ipareplica_dirsrv_cert_files | default([]) }}\"\n      ### client ###\n      force_join: \"{{ ipaclient_force_join }}\"\n      ### ad trust ###\n      netbios_name: \"{{ ipareplica_netbios_name | default(omit) }}\"\n      rid_base: \"{{ ipareplica_rid_base | default(omit) }}\"\n      secondary_rid_base: \"{{ ipareplica_secondary_rid_base | default(omit) }}\"\n      ### additional ###\n      server: \"{{ result_ipareplica_test.server }}\"\n      config_master_host_name:\n        \"{{ result_ipareplica_install_ca_certs.config_master_host_name }}\"\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      installer_ccache: \"{{ result_ipareplica_prepare.installer_ccache }}\"\n      _ca_enabled: \"{{ result_ipareplica_prepare._ca_enabled }}\"\n      _kra_enabled: \"{{ result_ipareplica_prepare._kra_enabled }}\"\n      _dirsrv_pkcs12_info: \"{{ result_ipareplica_prepare._dirsrv_pkcs12_info }}\"\n      _http_pkcs12_info: \"{{ result_ipareplica_prepare._http_pkcs12_info }}\"\n      _pkinit_pkcs12_info: \"{{ result_ipareplica_prepare._pkinit_pkcs12_info }}\"\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      _add_to_ipaservers: \"{{ result_ipareplica_prepare._add_to_ipaservers }}\"\n      _ca_subject: \"{{ result_ipareplica_prepare._ca_subject }}\"\n      _subject_base: \"{{ result_ipareplica_prepare._subject_base }}\"\n      dirman_password: \"{{ ipareplica_dirman_password }}\"\n\n  - name: Install - Setup KRB\n    ipareplica_setup_krb:\n      ### server ###\n      setup_ca: \"{{ ipareplica_setup_ca }}\"\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      no_pkinit: \"{{ ipareplica_no_pkinit }}\"\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### additional ###\n      config_master_host_name:\n        \"{{ result_ipareplica_install_ca_certs.config_master_host_name }}\"\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      _pkinit_pkcs12_info: \"{{ result_ipareplica_prepare._pkinit_pkcs12_info }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n\n  - name: Install - DS enable SSL\n    ipareplica_ds_enable_ssl:\n      ### server ###\n      setup_ca: \"{{ ipareplica_setup_ca }}\"\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      no_pkinit: \"{{ ipareplica_no_pkinit }}\"\n      dirsrv_config_file: \"{{ ipareplica_dirsrv_config_file | default(omit) }}\"\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### additional ###\n      config_master_host_name:\n        \"{{ result_ipareplica_install_ca_certs.config_master_host_name }}\"\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      _ca_enabled: \"{{ result_ipareplica_prepare._ca_enabled }}\"\n      _ca_file: \"{{ result_ipareplica_prepare._ca_file }}\"\n      _pkinit_pkcs12_info: \"{{ result_ipareplica_prepare._pkinit_pkcs12_info }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      dirman_password: \"{{ ipareplica_dirman_password }}\"\n      ds_ca_subject: \"{{ result_ipareplica_setup_ds.ds_ca_subject }}\"\n\n  - name: Install - Setup http\n    ipareplica_setup_http:\n      ### server ###\n      setup_ca: \"{{ ipareplica_setup_ca }}\"\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      no_pkinit: \"{{ ipareplica_no_pkinit }}\"\n      no_ui_redirect: \"{{ ipareplica_no_ui_redirect }}\"\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### additional ###\n      config_master_host_name:\n        \"{{ result_ipareplica_install_ca_certs.config_master_host_name }}\"\n      config_ca_host_name: \"{{ result_ipareplica_prepare.config_ca_host_name }}\"\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      _ca_enabled: \"{{ result_ipareplica_prepare._ca_enabled }}\"\n      _ca_file: \"{{ result_ipareplica_prepare._ca_file }}\"\n      _http_pkcs12_info: \"{{ result_ipareplica_prepare._http_pkcs12_info }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      dirman_password: \"{{ ipareplica_dirman_password }}\"\n\n  - name: Install - Setup otpd\n    ipareplica_setup_otpd:\n      ### server ###\n      setup_ca: \"{{ ipareplica_setup_ca }}\"\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      no_pkinit: \"{{ ipareplica_no_pkinit }}\"\n      no_ui_redirect: \"{{ ipareplica_no_ui_redirect }}\"\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### additional ###\n      config_master_host_name:\n        \"{{ result_ipareplica_install_ca_certs.config_master_host_name }}\"\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      _ca_enabled: \"{{ result_ipareplica_prepare._ca_enabled }}\"\n      _ca_file: \"{{ result_ipareplica_prepare._ca_file }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      dirman_password: \"{{ ipareplica_dirman_password }}\"\n\n  - name: Install - Setup custodia\n    ipareplica_setup_custodia:\n      ### server ###\n      setup_ca: \"{{ ipareplica_setup_ca }}\"\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      no_pkinit: \"{{ ipareplica_no_pkinit }}\"\n      no_ui_redirect: \"{{ ipareplica_no_ui_redirect }}\"\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### additional ###\n      config_master_host_name:\n        \"{{ result_ipareplica_prepare.config_master_host_name }}\"\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      _ca_enabled: \"{{ result_ipareplica_prepare._ca_enabled }}\"\n      _ca_file: \"{{ result_ipareplica_prepare._ca_file }}\"\n      _pkinit_pkcs12_info: \"{{ result_ipareplica_prepare._pkinit_pkcs12_info }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      dirman_password: \"{{ ipareplica_dirman_password }}\"\n\n  - name: Install - Setup CA\n    ipareplica_setup_ca:\n      ### server ###\n      setup_ca: \"{{ ipareplica_setup_ca }}\"\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      no_pkinit: \"{{ ipareplica_no_pkinit }}\"\n      no_ui_redirect: \"{{ ipareplica_no_ui_redirect }}\"\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### additional ###\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      _ca_enabled: \"{{ result_ipareplica_prepare._ca_enabled }}\"\n      _ca_file: \"{{ result_ipareplica_prepare._ca_file }}\"\n      _ca_subject: \"{{ result_ipareplica_prepare._ca_subject }}\"\n      _subject_base: \"{{ result_ipareplica_prepare._subject_base }}\"\n      _pkinit_pkcs12_info: \"{{ result_ipareplica_prepare._pkinit_pkcs12_info }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      dirman_password: \"{{ ipareplica_dirman_password }}\"\n      config_setup_ca: \"{{ result_ipareplica_prepare.config_setup_ca }}\"\n      config_master_host_name:\n        \"{{ result_ipareplica_install_ca_certs.config_master_host_name }}\"\n      config_ca_host_name:\n        \"{{ result_ipareplica_install_ca_certs.config_ca_host_name }}\"\n      config_ips: \"{{ result_ipareplica_prepare.config_ips }}\"\n    when: result_ipareplica_prepare._ca_enabled\n\n  - name: Install - KRB enable SSL\n    ipareplica_krb_enable_ssl:\n      ### server ###\n      setup_ca: \"{{ ipareplica_setup_ca }}\"\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      no_pkinit: \"{{ ipareplica_no_pkinit }}\"\n      # no_ui_redirect: \"{{ ipareplica_no_ui_redirect }}\"\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### additional ###\n      config_master_host_name:\n        \"{{ result_ipareplica_install_ca_certs.config_master_host_name }}\"\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      _ca_enabled: \"{{ result_ipareplica_prepare._ca_enabled }}\"\n      _ca_file: \"{{ result_ipareplica_prepare._ca_file }}\"\n      _pkinit_pkcs12_info: \"{{ result_ipareplica_prepare._pkinit_pkcs12_info }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      dirman_password: \"{{ ipareplica_dirman_password }}\"\n\n  - name: Install - DS apply updates\n    ipareplica_ds_apply_updates:\n      ### server ###\n      setup_ca: \"{{ ipareplica_setup_ca }}\"\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      no_pkinit: \"{{ ipareplica_no_pkinit }}\"\n      no_ui_redirect: \"{{ ipareplica_no_ui_redirect }}\"\n      dirsrv_config_file: \"{{ ipareplica_dirsrv_config_file | default(omit) }}\"\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### additional ###\n      config_master_host_name:\n        \"{{ result_ipareplica_install_ca_certs.config_master_host_name }}\"\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      _ca_enabled: \"{{ result_ipareplica_prepare._ca_enabled }}\"\n      _ca_file: \"{{ result_ipareplica_prepare._ca_file }}\"\n      _pkinit_pkcs12_info: \"{{ result_ipareplica_prepare._pkinit_pkcs12_info }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      dirman_password: \"{{ ipareplica_dirman_password }}\"\n      ds_ca_subject: \"{{ result_ipareplica_setup_ds.ds_ca_subject }}\"\n\n  - name: Install - Setup kra\n    ipareplica_setup_kra:\n      ### basic ###\n      dm_password: \"{{ ipadm_password | default(omit) }}\"\n      password: \"{{ ipaadmin_password | default(omit) }}\"\n      ip_addresses: \"{{ ipareplica_ip_addresses | default([]) }}\"\n      domain: \"{{ result_ipareplica_test.domain }}\"\n      realm: \"{{ result_ipareplica_test.realm }}\"\n      hostname: \"{{ result_ipareplica_test.hostname }}\"\n      ca_cert_files: \"{{ ipareplica_ca_cert_files | default([]) }}\"\n      no_host_dns: \"{{ ipareplica_no_host_dns }}\"\n      ### replica ###\n      setup_adtrust: \"{{ result_ipareplica_test.setup_adtrust }}\"\n      setup_ca: \"{{ ipareplica_setup_ca }}\"\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      setup_dns: \"{{ ipareplica_setup_dns }}\"\n      ### ssl certificate ###\n      dirsrv_cert_files: \"{{ ipareplica_dirsrv_cert_files | default([]) }}\"\n      ### client ###\n      force_join: \"{{ ipaclient_force_join }}\"\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### additional ###\n      server: \"{{ result_ipareplica_test.server }}\"\n      config_master_host_name:\n        \"{{ result_ipareplica_prepare.config_master_host_name }}\"\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      installer_ccache: \"{{ result_ipareplica_prepare.installer_ccache }}\"\n      _ca_enabled: \"{{ result_ipareplica_prepare._ca_enabled }}\"\n      _kra_enabled: \"{{ result_ipareplica_prepare._kra_enabled }}\"\n      _dirsrv_pkcs12_info: \"{{ result_ipareplica_prepare._dirsrv_pkcs12_info }}\"\n      _http_pkcs12_info: \"{{ result_ipareplica_prepare._http_pkcs12_info }}\"\n      _pkinit_pkcs12_info: \"{{ result_ipareplica_prepare._pkinit_pkcs12_info }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      _add_to_ipaservers: \"{{ result_ipareplica_prepare._add_to_ipaservers }}\"\n      _ca_subject: \"{{ result_ipareplica_prepare._ca_subject }}\"\n      _subject_base: \"{{ result_ipareplica_prepare._subject_base }}\"\n    when: result_ipareplica_test.setup_kra\n\n  - name: Install - Restart KDC\n    ipareplica_restart_kdc:\n      ### server ###\n      setup_ca: \"{{ ipareplica_setup_ca }}\"\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      no_pkinit: \"{{ ipareplica_no_pkinit }}\"\n      no_ui_redirect: \"{{ ipareplica_no_ui_redirect }}\"\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### additional ###\n      config_master_host_name:\n        \"{{ result_ipareplica_install_ca_certs.config_master_host_name }}\"\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      _ca_enabled: \"{{ result_ipareplica_prepare._ca_enabled }}\"\n      _ca_file: \"{{ result_ipareplica_prepare._ca_file }}\"\n      # _pkinit_pkcs12_info: \"{{ result_ipareplica_prepare._pkinit_pkcs12_info }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      dirman_password: \"{{ ipareplica_dirman_password }}\"\n\n  - name: Install - Custodia import dm password\n    ipareplica_custodia_import_dm_password:\n      ### server ###\n      setup_ca: \"{{ ipareplica_setup_ca }}\"\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      no_pkinit: \"{{ ipareplica_no_pkinit }}\"\n      no_ui_redirect: \"{{ ipareplica_no_ui_redirect }}\"\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### additional ###\n      config_master_host_name:\n        \"{{ result_ipareplica_prepare.config_master_host_name }}\"\n      config_ca_host_name: \"{{ result_ipareplica_prepare.config_ca_host_name }}\"\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      _ca_enabled: \"{{ result_ipareplica_prepare._ca_enabled }}\"\n      _ca_file: \"{{ result_ipareplica_prepare._ca_file }}\"\n      _pkinit_pkcs12_info: \"{{ result_ipareplica_prepare._pkinit_pkcs12_info }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      dirman_password: \"{{ ipareplica_dirman_password }}\"\n      config_setup_ca: \"{{ result_ipareplica_prepare.config_setup_ca }}\"\n\n  - name: Install - Promote SSSD\n    ipareplica_promote_sssd:\n      ### replica ###\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### additional ###\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      config_setup_ca: \"{{ result_ipareplica_prepare.config_setup_ca }}\"\n      config_master_host_name:\n        \"{{ result_ipareplica_prepare.config_master_host_name }}\"\n\n  - name: Install - Promote openldap.conf\n    ipareplica_promote_openldap_conf:\n      ### replica ###\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### additional ###\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      config_setup_ca: \"{{ result_ipareplica_prepare.config_setup_ca }}\"\n      config_master_host_name:\n        \"{{ result_ipareplica_prepare.config_master_host_name }}\"\n\n  - name: Install - Setup DNS\n    ipareplica_setup_dns:\n      ### server ###\n      setup_dns: \"{{ ipareplica_setup_dns }}\"\n      ### replica ###\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### dns ###\n      zonemgr: \"{{ ipareplica_zonemgr | default(omit) }}\"\n      forwarders: \"{{ ipareplica_forwarders | default([]) }}\"\n      forward_policy: \"{{ result_ipareplica_prepare.forward_policy if\n                          result_ipareplica_prepare.forward_policy is\n                          not none else omit }}\"\n      no_dnssec_validation: \"{{ ipareplica_no_dnssec_validation }}\"\n      ### additional ###\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      setup_ca: \"{{ result_ipareplica_prepare.config_setup_ca }}\"\n      config_master_host_name:\n        \"{{ result_ipareplica_prepare.config_master_host_name }}\"\n\n  - name: Install - Setup adtrust\n    ipareplica_setup_adtrust:\n      ### replica ###\n      setup_kra: \"{{ result_ipareplica_test.setup_kra }}\"\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### ad trust ###\n      enable_compat: \"{{ ipareplica_enable_compat }}\"\n      rid_base: \"{{ result_ipareplica_prepare.rid_base }}\"\n      secondary_rid_base: \"{{ result_ipareplica_prepare.secondary_rid_base }}\"\n      ### additional ###\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      setup_ca: \"{{ result_ipareplica_prepare.config_setup_ca }}\"\n      config_master_host_name:\n        \"{{ result_ipareplica_prepare.config_master_host_name }}\"\n      adtrust_netbios_name:\n        \"{{ result_ipareplica_prepare.adtrust_netbios_name }}\"\n      adtrust_reset_netbios_name:\n        \"{{ result_ipareplica_prepare.adtrust_reset_netbios_name }}\"\n    when: result_ipareplica_test.setup_adtrust\n\n  - name: Install - Enable IPA\n    ipareplica_enable_ipa:\n      hostname: \"{{ result_ipareplica_test.hostname }}\"\n      hidden_replica: \"{{ ipareplica_hidden_replica }}\"\n      ### server ###\n      ### certificate system ###\n      subject_base: \"{{ result_ipareplica_prepare.subject_base }}\"\n      ### additional ###\n      ccache: \"{{ result_ipareplica_prepare.ccache }}\"\n      _top_dir: \"{{ result_ipareplica_prepare._top_dir }}\"\n      setup_ca: \"{{ result_ipareplica_prepare.config_setup_ca }}\"\n      config_master_host_name:\n        \"{{ result_ipareplica_prepare.config_master_host_name }}\"\n    register: result_ipareplica_enable_ipa\n\n  - name: Install - Cleanup root IPA cache\n    file:\n      path: \"/root/.ipa_cache\"\n      state: absent\n    when: result_ipareplica_enable_ipa.changed\n\n  when: not ansible_check_mode and\n        not (result_ipareplica_test.client_already_configured is defined or\n             result_ipareplica_test.server_already_configured is defined)\n"}, {"commit_sha": "a10c5f4577e6e74feb1fadec4bcbab039b8b180a", "sha": "3cacc8c07cabbe5e38e8e598e41bd4b559f90554", "filename": "tasks/setup-repository.yml", "repository": "haxorof/ansible-role-docker-ce", "decoded_content": "---\n- name: Ensure python and deps for Ansible modules\n  become: true\n  raw: dnf install -y python2 python2-dnf libselinux-python\n  changed_when: false\n  when: _docker_os_dist == \"Fedora\"\n\n- name: Update APT cache\n  become: true\n  apt:\n    update_cache: yes\n  changed_when: false\n  register: _pkg_result\n  until: _pkg_result|succeeded\n  when: _docker_os_dist == \"Ubuntu\" or\n        _docker_os_dist == \"Debian\"\n\n- name: Ensure packages are installed for repository setup\n  become: true\n  package:\n    name: \"{{ item }}\"\n    state: present\n  with_items:\n    - \"{{ docker_repository_related_packages[_docker_os_dist] }}\"\n  register: _pkg_result\n  until: _pkg_result|succeeded\n  when: _docker_os_dist == \"Ubuntu\" or\n        _docker_os_dist == \"Debian\" or\n        _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"RedHat\"\n\n- name: Add Docker official GPG key\n  become: true\n  apt_key:\n    url: https://download.docker.com/linux/{{ _docker_os_dist|lower }}/gpg\n    state: present\n  register: _pkg_result\n  until: _pkg_result|succeeded\n  when: (_docker_os_dist == \"Ubuntu\" and _docker_os_dist_major_version > '14') or\n        (_docker_os_dist == \"Debian\" and _docker_os_dist_major_version > '7')\n\n- name: Add Docker APT key (alternative for older Ubuntu systems without SNI).\n  become: true\n  shell: \"curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\"\n  args:\n    warn: false\n  changed_when: false\n  when: (_docker_os_dist == \"Ubuntu\" and _docker_os_dist_major_version == '14') or (_docker_os_dist == \"Debian\" and\n    _docker_os_dist_major_version == '7')\n  tags:\n    - skip_ansible_lint\n\n- name: Determine channels to be enabled and/or disabled\n  set_fact:\n    _docker_disable_channels: \"{{ docker_channels | difference(_docker_merged_channels) }}\"\n    _docker_enable_channels: \"{{ docker_channels | intersect(_docker_merged_channels) }}\"\n  vars:\n    _docker_mandatory_channel: []\n    _docker_merged_channels: \"{{ _docker_mandatory_channel }} + [ '{{ docker_channel }}' ]\"\n\n- name: Add Docker CE repository with correct channels (Ubuntu/Debian)\n  become: true\n  apt_repository:\n    repo: \"deb [arch=amd64] https://download.docker.com/linux/{{ _docker_os_dist|lower }} \\\n      {{ _docker_os_dist_release }} {{ _docker_enable_channels | join(' ') }}\"\n    state: present\n    filename: 'docker-ce'\n  when: _docker_os_dist == \"Ubuntu\" or\n        _docker_os_dist == \"Debian\"\n\n# Backport is required but not documented by Docker: https://github.com/moby/moby/issues/16878\n- name: Add backport repository for Debian Wheezy\n  become: true\n  apt_repository:\n    repo: deb [arch=amd64] http://ftp.debian.org/debian wheezy-backports main\n    state: present\n    filename: 'debian-backport'\n  when: _docker_os_dist == \"Debian\" and _docker_os_dist_major_version == '7'\n\n- name: Add Docker CE repository (Fedora/CentOS/RedHat)\n  become: true\n  get_url:\n    url: \"{{ docker_repository_url[_docker_os_dist] }}\"\n    dest: /etc/yum.repos.d/docker-ce.repo\n    mode: 0644\n  register: _docker_repo\n  when: _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"Fedora\" or\n        _docker_os_dist == \"RedHat\"\n\n- name: Disable Docker CE repository channels (Fedora/CentOS/RedHat)\n  become: true\n  shell: \"{{ docker_cmd_enable_disable_edge_repo[_docker_os_dist] }}\"\n  with_items: \"{{ _docker_disable_channels }}\"\n  changed_when: false\n  vars:\n    _item_enabled: false\n  when: _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"Fedora\" or\n        _docker_os_dist == \"RedHat\"\n  tags:\n    - skip_ansible_lint\n\n- name: Enable Docker CE repository channels (Fedora/CentOS/RedHat)\n  become: true\n  shell: \"{{ docker_cmd_enable_disable_edge_repo[_docker_os_dist] }}\"\n  with_items: \"{{ _docker_enable_channels }}\"\n  changed_when: false\n  vars:\n    _item_enabled: true\n  when: _docker_os_dist == \"CentOS\" or\n        _docker_os_dist == \"Fedora\" or\n        _docker_os_dist == \"RedHat\"\n  tags:\n    - skip_ansible_lint\n\n# disable rt-beta so we don't get a 403 error retrieving repomd.xml\n- name: Check if rhel-7-server-rt-beta-rpms Repository is enabled (RedHat)\n  become: true\n  shell: \"subscription-manager repos --list-enabled | grep rhel-7-server-rt-beta-rpms\"\n  register: cmd_rhel_rt_beta_repo_enabled\n  when: _docker_os_dist == \"RedHat\"\n  changed_when: false\n  failed_when: cmd_rhel_rt_beta_repo_enabled.rc not in [ 0, 1 ]\n  tags:\n    - skip_ansible_lint\n\n- name: Disable rhel-7-server-rt-beta-rpms Repository (RedHat)\n  become: true\n  shell: \"subscription-manager repos --disable=rhel-7-server-rt-beta-rpms\"\n  when: _docker_os_dist == \"RedHat\" and cmd_rhel_rt_beta_repo_enabled.rc == 0\n  tags:\n    - skip_ansible_lint\n\n# container-selinux package wants this\n- name: Check if rhel-7-server-extras-rpms Repository is enabled (RedHat)\n  become: true\n  shell: \"subscription-manager repos --list-enabled | grep rhel-7-server-extras-rpms\"\n  register: cmd_rhel_extras_repo_enabled\n  when: _docker_os_dist == \"RedHat\"\n  changed_when: false\n  failed_when: cmd_rhel_extras_repo_enabled.rc not in [ 0, 1 ]\n  tags:\n    - skip_ansible_lint\n\n- name: Enable rhel-7-server-extras-rpms Repository (RedHat)\n  become: true\n  shell: \"subscription-manager repos --enable=rhel-7-server-extras-rpms\"\n  when: _docker_os_dist == \"RedHat\" and cmd_rhel_extras_repo_enabled.rc == 1\n  tags:\n    - skip_ansible_lint\n\n- name: Update repository cache\n  become: true\n  shell: \"{{ docker_cmd_update_repo_cache[_docker_os_dist] }}\"\n  args:\n    warn: false\n  changed_when: false\n  tags:\n    - skip_ansible_lint"}]